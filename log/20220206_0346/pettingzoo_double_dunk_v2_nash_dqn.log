pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
double_dunk_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'double_dunk_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 5, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220206_0346/pettingzoo_double_dunk_v2_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220206_0346/pettingzoo_double_dunk_v2_nash_dqn.
Episode: 1/10000 (0.0100%),                 avg. length: 2857.0,                last time consumption/overall running time: 23.0762s / 23.0762 s
env0_first_0:                 episode reward: -24.0000,                 loss: 0.0945
env0_second_0:                 episode reward: 24.0000,                 loss: 0.0921
env1_first_0:                 episode reward: -20.0000,                 loss: nan
env1_second_0:                 episode reward: 20.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 3966.7,                last time consumption/overall running time: 892.8870s / 915.9632 s
env0_first_0:                 episode reward: -29.4500,                 loss: 0.0851
env0_second_0:                 episode reward: 29.4500,                 loss: 0.0803
env1_first_0:                 episode reward: -26.4500,                 loss: nan
env1_second_0:                 episode reward: 26.4500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 3991.0,                last time consumption/overall running time: 1091.1500s / 2007.1132 s
env0_first_0:                 episode reward: -26.9000,                 loss: 0.0984
env0_second_0:                 episode reward: 26.9000,                 loss: 0.0810
env1_first_0:                 episode reward: -25.9000,                 loss: nan
env1_second_0:                 episode reward: 25.9000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 3881.7,                last time consumption/overall running time: 1061.3535s / 3068.4667 s
env0_first_0:                 episode reward: -23.8500,                 loss: 0.0891
env0_second_0:                 episode reward: 23.8500,                 loss: 0.0982
env1_first_0:                 episode reward: -24.5500,                 loss: nan
env1_second_0:                 episode reward: 24.5500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 3562.95,                last time consumption/overall running time: 965.6277s / 4034.0945 s
env0_first_0:                 episode reward: -21.4500,                 loss: 0.0938
env0_second_0:                 episode reward: 21.4500,                 loss: 0.0958
env1_first_0:                 episode reward: -22.4500,                 loss: nan
env1_second_0:                 episode reward: 22.4500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 3921.95,                last time consumption/overall running time: 1058.3972s / 5092.4917 s
env0_first_0:                 episode reward: -22.7500,                 loss: 0.0886
env0_second_0:                 episode reward: 22.7500,                 loss: 0.0901
env1_first_0:                 episode reward: -23.3500,                 loss: nan
env1_second_0:                 episode reward: 23.3500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 3850.15,                last time consumption/overall running time: 1041.1825s / 6133.6742 s
env0_first_0:                 episode reward: -23.1000,                 loss: 0.0926
env0_second_0:                 episode reward: 23.1000,                 loss: 0.0927
env1_first_0:                 episode reward: -26.9500,                 loss: nan
env1_second_0:                 episode reward: 26.9500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 3748.9,                last time consumption/overall running time: 1013.5122s / 7147.1863 s
env0_first_0:                 episode reward: -26.4000,                 loss: 0.0880
env0_second_0:                 episode reward: 26.4000,                 loss: 0.0877
env1_first_0:                 episode reward: -23.1000,                 loss: nan
env1_second_0:                 episode reward: 23.1000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 4031.6,                last time consumption/overall running time: 1107.7002s / 8254.8866 s
env0_first_0:                 episode reward: -25.9000,                 loss: 0.0861
env0_second_0:                 episode reward: 25.9000,                 loss: 0.0888
env1_first_0:                 episode reward: -25.2500,                 loss: nan
env1_second_0:                 episode reward: 25.2500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 3934.7,                last time consumption/overall running time: 1081.0052s / 9335.8918 s
env0_first_0:                 episode reward: -22.9000,                 loss: 0.0903
env0_second_0:                 episode reward: 22.9000,                 loss: 0.0869
env1_first_0:                 episode reward: -25.9000,                 loss: nan
env1_second_0:                 episode reward: 25.9000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 3746.85,                last time consumption/overall running time: 1020.9827s / 10356.8745 s
env0_first_0:                 episode reward: -22.9000,                 loss: 0.0840
env0_second_0:                 episode reward: 22.9000,                 loss: 0.0885
env1_first_0:                 episode reward: -26.4000,                 loss: nan
env1_second_0:                 episode reward: 26.4000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 3727.05,                last time consumption/overall running time: 1013.7144s / 11370.5889 s
env0_first_0:                 episode reward: -25.2500,                 loss: 0.0844
env0_second_0:                 episode reward: 25.2500,                 loss: 0.0846
env1_first_0:                 episode reward: -25.4000,                 loss: nan
env1_second_0:                 episode reward: 25.4000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 3785.4,                last time consumption/overall running time: 1023.2941s / 12393.8829 s
env0_first_0:                 episode reward: -24.0000,                 loss: 0.0814
env0_second_0:                 episode reward: 24.0000,                 loss: 0.0845
env1_first_0:                 episode reward: -24.5500,                 loss: nan
env1_second_0:                 episode reward: 24.5500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 3795.3,                last time consumption/overall running time: 1048.7874s / 13442.6704 s
env0_first_0:                 episode reward: -26.0000,                 loss: 0.0866
env0_second_0:                 episode reward: 26.0000,                 loss: 0.0820
env1_first_0:                 episode reward: -24.1500,                 loss: nan
env1_second_0:                 episode reward: 24.1500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 4033.5,                last time consumption/overall running time: 1100.4984s / 14543.1688 s
env0_first_0:                 episode reward: -28.0000,                 loss: 0.0812
env0_second_0:                 episode reward: 28.0000,                 loss: 0.0851
env1_first_0:                 episode reward: -30.2000,                 loss: nan
env1_second_0:                 episode reward: 30.2000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 3883.0,                last time consumption/overall running time: 1066.6415s / 15609.8102 s
env0_first_0:                 episode reward: -25.0500,                 loss: 0.0862
env0_second_0:                 episode reward: 25.0500,                 loss: 0.0848
env1_first_0:                 episode reward: -27.2000,                 loss: nan
env1_second_0:                 episode reward: 27.2000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 3886.9,                last time consumption/overall running time: 1063.8125s / 16673.6227 s
env0_first_0:                 episode reward: -27.2000,                 loss: 0.0818
env0_second_0:                 episode reward: 27.2000,                 loss: 0.0878
env1_first_0:                 episode reward: -25.1500,                 loss: nan
env1_second_0:                 episode reward: 25.1500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 4112.75,                last time consumption/overall running time: 1127.7462s / 17801.3689 s
env0_first_0:                 episode reward: -25.4000,                 loss: 0.0826
env0_second_0:                 episode reward: 25.4000,                 loss: 0.0805
env1_first_0:                 episode reward: -27.1500,                 loss: nan
env1_second_0:                 episode reward: 27.1500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 3988.8,                last time consumption/overall running time: 1095.8284s / 18897.1973 s
env0_first_0:                 episode reward: -26.6500,                 loss: 0.0793
env0_second_0:                 episode reward: 26.6500,                 loss: 0.0770
env1_first_0:                 episode reward: -28.5500,                 loss: nan
env1_second_0:                 episode reward: 28.5500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 4181.7,                last time consumption/overall running time: 1157.1868s / 20054.3841 s
env0_first_0:                 episode reward: -28.5000,                 loss: 0.0856
env0_second_0:                 episode reward: 28.5000,                 loss: 0.0800
env1_first_0:                 episode reward: -29.4000,                 loss: nan
env1_second_0:                 episode reward: 29.4000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 3900.25,                last time consumption/overall running time: 1062.8101s / 21117.1941 s
env0_first_0:                 episode reward: -25.0000,                 loss: 0.0800
env0_second_0:                 episode reward: 25.0000,                 loss: 0.0780
env1_first_0:                 episode reward: -27.8500,                 loss: nan
env1_second_0:                 episode reward: 27.8500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 3692.75,                last time consumption/overall running time: 1017.9895s / 22135.1836 s
env0_first_0:                 episode reward: -24.0000,                 loss: 0.0834
env0_second_0:                 episode reward: 24.0000,                 loss: 0.0826
env1_first_0:                 episode reward: -23.6000,                 loss: nan
env1_second_0:                 episode reward: 23.6000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 3982.7,                last time consumption/overall running time: 1092.8282s / 23228.0119 s
env0_first_0:                 episode reward: -29.9500,                 loss: 0.0826
env0_second_0:                 episode reward: 29.9500,                 loss: 0.0797
env1_first_0:                 episode reward: -26.7500,                 loss: nan
env1_second_0:                 episode reward: 26.7500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 4056.7,                last time consumption/overall running time: 1115.5974s / 24343.6093 s
env0_first_0:                 episode reward: -29.0500,                 loss: 0.0808
env0_second_0:                 episode reward: 29.0500,                 loss: 0.0811
env1_first_0:                 episode reward: -29.2500,                 loss: nan
env1_second_0:                 episode reward: 29.2500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 4077.45,                last time consumption/overall running time: 1120.9396s / 25464.5490 s
env0_first_0:                 episode reward: -25.9000,                 loss: 0.0795
env0_second_0:                 episode reward: 25.9000,                 loss: 0.0793
env1_first_0:                 episode reward: -26.0000,                 loss: nan
env1_second_0:                 episode reward: 26.0000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 3873.3,                last time consumption/overall running time: 1051.4065s / 26515.9555 s
env0_first_0:                 episode reward: -26.7500,                 loss: 0.0810
env0_second_0:                 episode reward: 26.7500,                 loss: 0.0798
env1_first_0:                 episode reward: -26.3500,                 loss: nan
env1_second_0:                 episode reward: 26.3500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 3948.15,                last time consumption/overall running time: 1088.2440s / 27604.1995 s
env0_first_0:                 episode reward: -24.8000,                 loss: 0.0811
env0_second_0:                 episode reward: 24.8000,                 loss: 0.0812
env1_first_0:                 episode reward: -25.7000,                 loss: nan
env1_second_0:                 episode reward: 25.7000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 3889.8,                last time consumption/overall running time: 1067.2508s / 28671.4503 s
env0_first_0:                 episode reward: -24.8500,                 loss: 0.0778
env0_second_0:                 episode reward: 24.8500,                 loss: 0.0790
env1_first_0:                 episode reward: -26.0500,                 loss: nan
env1_second_0:                 episode reward: 26.0500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 4093.8,                last time consumption/overall running time: 1125.4699s / 29796.9202 s
env0_first_0:                 episode reward: -30.7500,                 loss: 0.0832
env0_second_0:                 episode reward: 30.7500,                 loss: 0.0782
env1_first_0:                 episode reward: -27.1500,                 loss: nan
env1_second_0:                 episode reward: 27.1500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 4256.75,                last time consumption/overall running time: 1186.8321s / 30983.7523 s
env0_first_0:                 episode reward: -29.5500,                 loss: 0.0828
env0_second_0:                 episode reward: 29.5500,                 loss: 0.0765
env1_first_0:                 episode reward: -29.7500,                 loss: nan
env1_second_0:                 episode reward: 29.7500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 3894.65,                last time consumption/overall running time: 1067.3633s / 32051.1156 s
env0_first_0:                 episode reward: -27.4500,                 loss: 0.0760
env0_second_0:                 episode reward: 27.4500,                 loss: 0.0745
env1_first_0:                 episode reward: -26.0500,                 loss: nan
env1_second_0:                 episode reward: 26.0500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 3702.4,                last time consumption/overall running time: 1020.1478s / 33071.2634 s
env0_first_0:                 episode reward: -26.9000,                 loss: 0.0753
env0_second_0:                 episode reward: 26.9000,                 loss: 0.0748
env1_first_0:                 episode reward: -22.2500,                 loss: nan
env1_second_0:                 episode reward: 22.2500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 4049.95,                last time consumption/overall running time: 1110.6170s / 34181.8804 s
env0_first_0:                 episode reward: -26.8500,                 loss: 0.0770
env0_second_0:                 episode reward: 26.8500,                 loss: 0.0737
env1_first_0:                 episode reward: -27.1000,                 loss: nan
env1_second_0:                 episode reward: 27.1000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 3882.75,                last time consumption/overall running time: 1075.7431s / 35257.6235 s
env0_first_0:                 episode reward: -27.8000,                 loss: 0.0760
env0_second_0:                 episode reward: 27.8000,                 loss: 0.0689
env1_first_0:                 episode reward: -28.7000,                 loss: nan
env1_second_0:                 episode reward: 28.7000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 3824.05,                last time consumption/overall running time: 1055.1942s / 36312.8177 s
env0_first_0:                 episode reward: -29.7000,                 loss: 0.0772
env0_second_0:                 episode reward: 29.7000,                 loss: 0.0645
env1_first_0:                 episode reward: -28.1000,                 loss: nan
env1_second_0:                 episode reward: 28.1000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 3908.15,                last time consumption/overall running time: 1074.4069s / 37387.2246 s
env0_first_0:                 episode reward: -26.4000,                 loss: 0.0761
env0_second_0:                 episode reward: 26.4000,                 loss: 0.0627
env1_first_0:                 episode reward: -25.7000,                 loss: nan
env1_second_0:                 episode reward: 25.7000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 3896.7,                last time consumption/overall running time: 1076.6326s / 38463.8572 s
env0_first_0:                 episode reward: -24.9500,                 loss: 0.0711
env0_second_0:                 episode reward: 24.9500,                 loss: 0.0610
env1_first_0:                 episode reward: -27.8000,                 loss: nan
env1_second_0:                 episode reward: 27.8000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 4147.95,                last time consumption/overall running time: 1139.7826s / 39603.6398 s
env0_first_0:                 episode reward: -31.7500,                 loss: 0.0730
env0_second_0:                 episode reward: 31.7500,                 loss: 0.0585
env1_first_0:                 episode reward: -30.3000,                 loss: nan
env1_second_0:                 episode reward: 30.3000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 4010.9,                last time consumption/overall running time: 1118.1745s / 40721.8143 s
env0_first_0:                 episode reward: -26.2000,                 loss: 0.0684
env0_second_0:                 episode reward: 26.2000,                 loss: 0.0546
env1_first_0:                 episode reward: -27.5500,                 loss: nan
env1_second_0:                 episode reward: 27.5500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 3554.8,                last time consumption/overall running time: 991.0713s / 41712.8856 s
env0_first_0:                 episode reward: -23.5000,                 loss: 0.0638
env0_second_0:                 episode reward: 23.5000,                 loss: 0.0526
env1_first_0:                 episode reward: -26.1000,                 loss: nan
env1_second_0:                 episode reward: 26.1000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 3728.35,                last time consumption/overall running time: 1037.7463s / 42750.6320 s
env0_first_0:                 episode reward: -27.6000,                 loss: 0.0648
env0_second_0:                 episode reward: 27.6000,                 loss: 0.0511
env1_first_0:                 episode reward: -27.1500,                 loss: nan
env1_second_0:                 episode reward: 27.1500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 3715.15,                last time consumption/overall running time: 1025.5940s / 43776.2260 s
env0_first_0:                 episode reward: -24.3500,                 loss: 0.0607
env0_second_0:                 episode reward: 24.3500,                 loss: 0.0516
env1_first_0:                 episode reward: -26.8500,                 loss: nan
env1_second_0:                 episode reward: 26.8500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 3701.8,                last time consumption/overall running time: 1022.9986s / 44799.2246 s
env0_first_0:                 episode reward: -25.4500,                 loss: 0.0568
env0_second_0:                 episode reward: 25.4500,                 loss: 0.0518
env1_first_0:                 episode reward: -24.3500,                 loss: nan
env1_second_0:                 episode reward: 24.3500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 3678.15,                last time consumption/overall running time: 1007.1590s / 45806.3836 s
env0_first_0:                 episode reward: -24.0500,                 loss: 0.0568
env0_second_0:                 episode reward: 24.0500,                 loss: 0.0501
env1_first_0:                 episode reward: -22.6000,                 loss: nan
env1_second_0:                 episode reward: 22.6000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 4058.7,                last time consumption/overall running time: 1122.6413s / 46929.0248 s
env0_first_0:                 episode reward: -28.7000,                 loss: 0.0548
env0_second_0:                 episode reward: 28.7000,                 loss: 0.0497
env1_first_0:                 episode reward: -27.3500,                 loss: nan
env1_second_0:                 episode reward: 27.3500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 3931.35,                last time consumption/overall running time: 1088.3772s / 48017.4020 s
env0_first_0:                 episode reward: -26.6500,                 loss: 0.0490
env0_second_0:                 episode reward: 26.6500,                 loss: 0.0448
env1_first_0:                 episode reward: -26.3000,                 loss: nan
env1_second_0:                 episode reward: 26.3000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 3743.9,                last time consumption/overall running time: 1034.4793s / 49051.8814 s
env0_first_0:                 episode reward: -27.8000,                 loss: 0.0478
env0_second_0:                 episode reward: 27.8000,                 loss: 0.0437
env1_first_0:                 episode reward: -28.2500,                 loss: nan
env1_second_0:                 episode reward: 28.2500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 3657.5,                last time consumption/overall running time: 1005.7151s / 50057.5965 s
env0_first_0:                 episode reward: -28.6500,                 loss: 0.0465
env0_second_0:                 episode reward: 28.6500,                 loss: 0.0451
env1_first_0:                 episode reward: -25.9000,                 loss: nan
env1_second_0:                 episode reward: 25.9000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 3964.45,                last time consumption/overall running time: 1087.3001s / 51144.8966 s
env0_first_0:                 episode reward: -28.4500,                 loss: 0.0436
env0_second_0:                 episode reward: 28.4500,                 loss: 0.0425
env1_first_0:                 episode reward: -26.8500,                 loss: nan
env1_second_0:                 episode reward: 26.8500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 3824.15,                last time consumption/overall running time: 1058.6173s / 52203.5139 s
env0_first_0:                 episode reward: -27.6000,                 loss: 0.0447
env0_second_0:                 episode reward: 27.6000,                 loss: 0.0429
env1_first_0:                 episode reward: -25.1500,                 loss: nan
env1_second_0:                 episode reward: 25.1500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 3880.45,                last time consumption/overall running time: 1070.5723s / 53274.0862 s
env0_first_0:                 episode reward: -27.4000,                 loss: 0.0417
env0_second_0:                 episode reward: 27.4000,                 loss: 0.0410
env1_first_0:                 episode reward: -24.2500,                 loss: nan
env1_second_0:                 episode reward: 24.2500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 3907.2,                last time consumption/overall running time: 1087.2710s / 54361.3573 s
env0_first_0:                 episode reward: -26.0000,                 loss: 0.0432
env0_second_0:                 episode reward: 26.0000,                 loss: 0.0445
env1_first_0:                 episode reward: -27.9000,                 loss: nan
env1_second_0:                 episode reward: 27.9000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 3890.5,                last time consumption/overall running time: 1094.0380s / 55455.3953 s
env0_first_0:                 episode reward: -26.1000,                 loss: 0.0453
env0_second_0:                 episode reward: 26.1000,                 loss: 0.0463
env1_first_0:                 episode reward: -27.1500,                 loss: nan
env1_second_0:                 episode reward: 27.1500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 3747.4,                last time consumption/overall running time: 1029.1370s / 56484.5324 s
env0_first_0:                 episode reward: -25.9500,                 loss: 0.0462
env0_second_0:                 episode reward: 25.9500,                 loss: 0.0446
env1_first_0:                 episode reward: -25.9000,                 loss: nan
env1_second_0:                 episode reward: 25.9000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 3749.3,                last time consumption/overall running time: 1042.9886s / 57527.5210 s
env0_first_0:                 episode reward: -28.6500,                 loss: 0.0446
env0_second_0:                 episode reward: 28.6500,                 loss: 0.0445
env1_first_0:                 episode reward: -24.9500,                 loss: nan
env1_second_0:                 episode reward: 24.9500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 3619.65,                last time consumption/overall running time: 1012.5552s / 58540.0761 s
env0_first_0:                 episode reward: -25.0000,                 loss: 0.0458
env0_second_0:                 episode reward: 25.0000,                 loss: 0.0448
env1_first_0:                 episode reward: -21.7000,                 loss: nan
env1_second_0:                 episode reward: 21.7000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 3783.3,                last time consumption/overall running time: 1046.9393s / 59587.0155 s
env0_first_0:                 episode reward: -23.1500,                 loss: 0.0457
env0_second_0:                 episode reward: 23.1500,                 loss: 0.0433
env1_first_0:                 episode reward: -24.1000,                 loss: nan
env1_second_0:                 episode reward: 24.1000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 4057.05,                last time consumption/overall running time: 1120.3590s / 60707.3745 s
env0_first_0:                 episode reward: -27.3500,                 loss: 0.0439
env0_second_0:                 episode reward: 27.3500,                 loss: 0.0431
env1_first_0:                 episode reward: -27.5500,                 loss: nan
env1_second_0:                 episode reward: 27.5500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 3693.05,                last time consumption/overall running time: 1025.8628s / 61733.2373 s
env0_first_0:                 episode reward: -24.0000,                 loss: 0.0416
env0_second_0:                 episode reward: 24.0000,                 loss: 0.0410
env1_first_0:                 episode reward: -27.0500,                 loss: nan
env1_second_0:                 episode reward: 27.0500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 3874.6,                last time consumption/overall running time: 1084.9317s / 62818.1691 s
env0_first_0:                 episode reward: -26.3500,                 loss: 0.0410
env0_second_0:                 episode reward: 26.3500,                 loss: 0.0401
env1_first_0:                 episode reward: -26.2000,                 loss: nan
env1_second_0:                 episode reward: 26.2000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 3627.4,                last time consumption/overall running time: 998.8263s / 63816.9953 s
env0_first_0:                 episode reward: -26.3500,                 loss: 0.0428
env0_second_0:                 episode reward: 26.3500,                 loss: 0.0428
env1_first_0:                 episode reward: -22.3500,                 loss: nan
env1_second_0:                 episode reward: 22.3500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 3900.05,                last time consumption/overall running time: 1069.3775s / 64886.3728 s
env0_first_0:                 episode reward: -26.8500,                 loss: 0.0452
env0_second_0:                 episode reward: 26.8500,                 loss: 0.0416
env1_first_0:                 episode reward: -29.3000,                 loss: nan
env1_second_0:                 episode reward: 29.3000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 3876.25,                last time consumption/overall running time: 1061.9065s / 65948.2793 s
env0_first_0:                 episode reward: -25.6000,                 loss: 0.0410
env0_second_0:                 episode reward: 25.6000,                 loss: 0.0413
env1_first_0:                 episode reward: -24.2000,                 loss: nan
env1_second_0:                 episode reward: 24.2000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 3545.8,                last time consumption/overall running time: 977.0362s / 66925.3155 s
env0_first_0:                 episode reward: -23.3500,                 loss: 0.0424
env0_second_0:                 episode reward: 23.3500,                 loss: 0.0420
env1_first_0:                 episode reward: -20.7500,                 loss: nan
env1_second_0:                 episode reward: 20.7500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 3872.0,                last time consumption/overall running time: 1075.0908s / 68000.4063 s
env0_first_0:                 episode reward: -27.4000,                 loss: 0.0429
env0_second_0:                 episode reward: 27.4000,                 loss: 0.0415
env1_first_0:                 episode reward: -26.1500,                 loss: nan
env1_second_0:                 episode reward: 26.1500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 4085.05,                last time consumption/overall running time: 1133.5239s / 69133.9302 s
env0_first_0:                 episode reward: -26.7000,                 loss: 0.0407
env0_second_0:                 episode reward: 26.7000,                 loss: 0.0403
env1_first_0:                 episode reward: -27.1500,                 loss: nan
env1_second_0:                 episode reward: 27.1500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 3944.5,                last time consumption/overall running time: 1089.9758s / 70223.9060 s
env0_first_0:                 episode reward: -26.3500,                 loss: 0.0416
env0_second_0:                 episode reward: 26.3500,                 loss: 0.0420
env1_first_0:                 episode reward: -26.6000,                 loss: nan
env1_second_0:                 episode reward: 26.6000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 3585.95,                last time consumption/overall running time: 979.1784s / 71203.0844 s
env0_first_0:                 episode reward: -23.9000,                 loss: 0.0437
env0_second_0:                 episode reward: 23.9000,                 loss: 0.0430
env1_first_0:                 episode reward: -24.1500,                 loss: nan
env1_second_0:                 episode reward: 24.1500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 3718.95,                last time consumption/overall running time: 1031.0440s / 72234.1284 s
env0_first_0:                 episode reward: -25.6000,                 loss: 0.0436
env0_second_0:                 episode reward: 25.6000,                 loss: 0.0421
env1_first_0:                 episode reward: -25.6500,                 loss: nan
env1_second_0:                 episode reward: 25.6500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 3962.15,                last time consumption/overall running time: 1097.3173s / 73331.4457 s
env0_first_0:                 episode reward: -24.9500,                 loss: 0.0424
env0_second_0:                 episode reward: 24.9500,                 loss: 0.0432
env1_first_0:                 episode reward: -25.7000,                 loss: nan
env1_second_0:                 episode reward: 25.7000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 3685.5,                last time consumption/overall running time: 1024.6400s / 74356.0856 s
env0_first_0:                 episode reward: -26.2000,                 loss: 0.0426
env0_second_0:                 episode reward: 26.2000,                 loss: 0.0418
env1_first_0:                 episode reward: -25.6500,                 loss: nan
env1_second_0:                 episode reward: 25.6500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 3630.85,                last time consumption/overall running time: 992.6670s / 75348.7526 s
env0_first_0:                 episode reward: -21.0000,                 loss: 0.0402
env0_second_0:                 episode reward: 21.0000,                 loss: 0.0385
env1_first_0:                 episode reward: -22.5000,                 loss: nan
env1_second_0:                 episode reward: 22.5000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 3605.35,                last time consumption/overall running time: 996.5815s / 76345.3341 s
env0_first_0:                 episode reward: -24.1000,                 loss: 0.0396
env0_second_0:                 episode reward: 24.1000,                 loss: 0.0409
env1_first_0:                 episode reward: -22.7500,                 loss: nan
env1_second_0:                 episode reward: 22.7500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 3956.35,                last time consumption/overall running time: 1086.6814s / 77432.0155 s
env0_first_0:                 episode reward: -27.0500,                 loss: 0.0410
env0_second_0:                 episode reward: 27.0500,                 loss: 0.0407
env1_first_0:                 episode reward: -25.5500,                 loss: nan
env1_second_0:                 episode reward: 25.5500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 3827.4,                last time consumption/overall running time: 1050.0812s / 78482.0967 s
env0_first_0:                 episode reward: -23.8500,                 loss: 0.0437
env0_second_0:                 episode reward: 23.8500,                 loss: 0.0400
env1_first_0:                 episode reward: -24.0000,                 loss: nan
env1_second_0:                 episode reward: 24.0000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 3708.55,                last time consumption/overall running time: 1010.3768s / 79492.4735 s
env0_first_0:                 episode reward: -25.8500,                 loss: 0.0415
env0_second_0:                 episode reward: 25.8500,                 loss: 0.0408
env1_first_0:                 episode reward: -24.6000,                 loss: nan
env1_second_0:                 episode reward: 24.6000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 3631.35,                last time consumption/overall running time: 1014.2510s / 80506.7244 s
env0_first_0:                 episode reward: -23.2000,                 loss: 0.0407
env0_second_0:                 episode reward: 23.2000,                 loss: 0.0426
env1_first_0:                 episode reward: -24.7000,                 loss: nan
env1_second_0:                 episode reward: 24.7000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 3906.35,                last time consumption/overall running time: 1098.5640s / 81605.2885 s
env0_first_0:                 episode reward: -25.3000,                 loss: 0.0388
env0_second_0:                 episode reward: 25.3000,                 loss: 0.0397
env1_first_0:                 episode reward: -27.4000,                 loss: nan
env1_second_0:                 episode reward: 27.4000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 3496.65,                last time consumption/overall running time: 984.4591s / 82589.7476 s
env0_first_0:                 episode reward: -22.4000,                 loss: 0.0398
env0_second_0:                 episode reward: 22.4000,                 loss: 0.0367
env1_first_0:                 episode reward: -22.1500,                 loss: nan
env1_second_0:                 episode reward: 22.1500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 3603.85,                last time consumption/overall running time: 978.9098s / 83568.6574 s
env0_first_0:                 episode reward: -25.2000,                 loss: 0.0395
env0_second_0:                 episode reward: 25.2000,                 loss: 0.0386
env1_first_0:                 episode reward: -26.7000,                 loss: nan
env1_second_0:                 episode reward: 26.7000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 3815.1,                last time consumption/overall running time: 1037.0648s / 84605.7222 s
env0_first_0:                 episode reward: -25.7000,                 loss: 0.0410
env0_second_0:                 episode reward: 25.7000,                 loss: 0.0400
env1_first_0:                 episode reward: -25.6500,                 loss: nan
env1_second_0:                 episode reward: 25.6500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 3595.35,                last time consumption/overall running time: 994.2339s / 85599.9561 s
env0_first_0:                 episode reward: -22.3000,                 loss: 0.0369
env0_second_0:                 episode reward: 22.3000,                 loss: 0.0377
env1_first_0:                 episode reward: -23.3000,                 loss: nan
env1_second_0:                 episode reward: 23.3000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 3820.1,                last time consumption/overall running time: 1063.0939s / 86663.0499 s
env0_first_0:                 episode reward: -25.7500,                 loss: 0.0389
env0_second_0:                 episode reward: 25.7500,                 loss: 0.0367
env1_first_0:                 episode reward: -25.5000,                 loss: nan
env1_second_0:                 episode reward: 25.5000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 3708.6,                last time consumption/overall running time: 1039.3484s / 87702.3983 s
env0_first_0:                 episode reward: -22.6000,                 loss: 0.0388
env0_second_0:                 episode reward: 22.6000,                 loss: 0.0409
env1_first_0:                 episode reward: -22.9000,                 loss: nan
env1_second_0:                 episode reward: 22.9000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 3764.15,                last time consumption/overall running time: 1034.6242s / 88737.0225 s
env0_first_0:                 episode reward: -24.3500,                 loss: 0.0401
env0_second_0:                 episode reward: 24.3500,                 loss: 0.0391
env1_first_0:                 episode reward: -24.7500,                 loss: nan
env1_second_0:                 episode reward: 24.7500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 3485.9,                last time consumption/overall running time: 947.4839s / 89684.5064 s
env0_first_0:                 episode reward: -24.2500,                 loss: 0.0373
env0_second_0:                 episode reward: 24.2500,                 loss: 0.0402
env1_first_0:                 episode reward: -22.5500,                 loss: nan
env1_second_0:                 episode reward: 22.5500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 3512.6,                last time consumption/overall running time: 975.2635s / 90659.7699 s
env0_first_0:                 episode reward: -19.8500,                 loss: 0.0398
env0_second_0:                 episode reward: 19.8500,                 loss: 0.0403
env1_first_0:                 episode reward: -22.3500,                 loss: nan
env1_second_0:                 episode reward: 22.3500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 3187.95,                last time consumption/overall running time: 887.7068s / 91547.4767 s
env0_first_0:                 episode reward: -17.8500,                 loss: 0.0408
env0_second_0:                 episode reward: 17.8500,                 loss: 0.0402
env1_first_0:                 episode reward: -20.7500,                 loss: nan
env1_second_0:                 episode reward: 20.7500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 3654.95,                last time consumption/overall running time: 1004.3921s / 92551.8689 s
env0_first_0:                 episode reward: -22.4000,                 loss: 0.0406
env0_second_0:                 episode reward: 22.4000,                 loss: 0.0390
env1_first_0:                 episode reward: -21.4500,                 loss: nan
env1_second_0:                 episode reward: 21.4500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 3638.5,                last time consumption/overall running time: 1002.4105s / 93554.2794 s
env0_first_0:                 episode reward: -20.8000,                 loss: 0.0394
env0_second_0:                 episode reward: 20.8000,                 loss: 0.0387
env1_first_0:                 episode reward: -21.6500,                 loss: nan
env1_second_0:                 episode reward: 21.6500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 3479.2,                last time consumption/overall running time: 944.9946s / 94499.2740 s
env0_first_0:                 episode reward: -21.4500,                 loss: 0.0388
env0_second_0:                 episode reward: 21.4500,                 loss: 0.0384
env1_first_0:                 episode reward: -21.6000,                 loss: nan
env1_second_0:                 episode reward: 21.6000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 3715.8,                last time consumption/overall running time: 1038.8166s / 95538.0905 s
env0_first_0:                 episode reward: -20.3000,                 loss: 0.0381
env0_second_0:                 episode reward: 20.3000,                 loss: 0.0384
env1_first_0:                 episode reward: -23.2000,                 loss: nan
env1_second_0:                 episode reward: 23.2000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 3409.4,                last time consumption/overall running time: 939.4372s / 96477.5278 s
env0_first_0:                 episode reward: -19.3000,                 loss: 0.0389
env0_second_0:                 episode reward: 19.3000,                 loss: 0.0370
env1_first_0:                 episode reward: -19.0000,                 loss: nan
env1_second_0:                 episode reward: 19.0000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 3332.0,                last time consumption/overall running time: 936.5823s / 97414.1101 s
env0_first_0:                 episode reward: -21.1500,                 loss: 0.0383
env0_second_0:                 episode reward: 21.1500,                 loss: 0.0394
env1_first_0:                 episode reward: -23.5500,                 loss: nan
env1_second_0:                 episode reward: 23.5500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 3666.6,                last time consumption/overall running time: 1034.4272s / 98448.5373 s
env0_first_0:                 episode reward: -21.5000,                 loss: 0.0380
env0_second_0:                 episode reward: 21.5000,                 loss: 0.0370
env1_first_0:                 episode reward: -24.9500,                 loss: nan
env1_second_0:                 episode reward: 24.9500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 3469.1,                last time consumption/overall running time: 950.7148s / 99399.2521 s
env0_first_0:                 episode reward: -19.2500,                 loss: 0.0373
env0_second_0:                 episode reward: 19.2500,                 loss: 0.0375
env1_first_0:                 episode reward: -19.9000,                 loss: nan
env1_second_0:                 episode reward: 19.9000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 3595.9,                last time consumption/overall running time: 992.3284s / 100391.5804 s
env0_first_0:                 episode reward: -23.5500,                 loss: 0.0378
env0_second_0:                 episode reward: 23.5500,                 loss: 0.0406
env1_first_0:                 episode reward: -20.5500,                 loss: nan
env1_second_0:                 episode reward: 20.5500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 3150.35,                last time consumption/overall running time: 875.7107s / 101267.2912 s
env0_first_0:                 episode reward: -15.2000,                 loss: 0.0389
env0_second_0:                 episode reward: 15.2000,                 loss: 0.0390
env1_first_0:                 episode reward: -18.3500,                 loss: nan
env1_second_0:                 episode reward: 18.3500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 3207.8,                last time consumption/overall running time: 883.2397s / 102150.5309 s
env0_first_0:                 episode reward: -18.4000,                 loss: 0.0396
env0_second_0:                 episode reward: 18.4000,                 loss: 0.0401
env1_first_0:                 episode reward: -18.1500,                 loss: nan
env1_second_0:                 episode reward: 18.1500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 3474.7,                last time consumption/overall running time: 954.3448s / 103104.8757 s
env0_first_0:                 episode reward: -20.5500,                 loss: 0.0396
env0_second_0:                 episode reward: 20.5500,                 loss: 0.0379
env1_first_0:                 episode reward: -24.6000,                 loss: nan
env1_second_0:                 episode reward: 24.6000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 3711.75,                last time consumption/overall running time: 1016.0731s / 104120.9488 s
env0_first_0:                 episode reward: -22.6500,                 loss: 0.0388
env0_second_0:                 episode reward: 22.6500,                 loss: 0.0372
env1_first_0:                 episode reward: -20.8000,                 loss: nan
env1_second_0:                 episode reward: 20.8000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 3378.95,                last time consumption/overall running time: 906.5090s / 105027.4578 s
env0_first_0:                 episode reward: -21.7000,                 loss: 0.0382
env0_second_0:                 episode reward: 21.7000,                 loss: 0.0370
env1_first_0:                 episode reward: -20.3500,                 loss: nan
env1_second_0:                 episode reward: 20.3500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 3747.05,                last time consumption/overall running time: 1038.1345s / 106065.5923 s
env0_first_0:                 episode reward: -23.6500,                 loss: 0.0368
env0_second_0:                 episode reward: 23.6500,                 loss: 0.0371
env1_first_0:                 episode reward: -22.0500,                 loss: nan
env1_second_0:                 episode reward: 22.0500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 3457.95,                last time consumption/overall running time: 958.3165s / 107023.9087 s
env0_first_0:                 episode reward: -20.2000,                 loss: 0.0386
env0_second_0:                 episode reward: 20.2000,                 loss: 0.0369
env1_first_0:                 episode reward: -20.8000,                 loss: nan
env1_second_0:                 episode reward: 20.8000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 3614.2,                last time consumption/overall running time: 1000.2858s / 108024.1945 s
env0_first_0:                 episode reward: -22.9500,                 loss: 0.0393
env0_second_0:                 episode reward: 22.9500,                 loss: 0.0400
env1_first_0:                 episode reward: -22.5000,                 loss: nan
env1_second_0:                 episode reward: 22.5000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 3323.45,                last time consumption/overall running time: 905.8843s / 108930.0788 s
env0_first_0:                 episode reward: -18.8000,                 loss: 0.0374
env0_second_0:                 episode reward: 18.8000,                 loss: 0.0375
env1_first_0:                 episode reward: -20.3500,                 loss: nan
env1_second_0:                 episode reward: 20.3500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 3455.75,                last time consumption/overall running time: 945.2760s / 109875.3549 s
env0_first_0:                 episode reward: -21.8500,                 loss: 0.0384
env0_second_0:                 episode reward: 21.8500,                 loss: 0.0368
env1_first_0:                 episode reward: -21.1000,                 loss: nan
env1_second_0:                 episode reward: 21.1000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 3290.3,                last time consumption/overall running time: 887.4318s / 110762.7867 s
env0_first_0:                 episode reward: -20.7000,                 loss: 0.0386
env0_second_0:                 episode reward: 20.7000,                 loss: 0.0397
env1_first_0:                 episode reward: -19.4500,                 loss: nan
env1_second_0:                 episode reward: 19.4500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 3509.9,                last time consumption/overall running time: 940.6430s / 111703.4296 s
env0_first_0:                 episode reward: -20.4500,                 loss: 0.0376
env0_second_0:                 episode reward: 20.4500,                 loss: 0.0377
env1_first_0:                 episode reward: -22.4000,                 loss: nan
env1_second_0:                 episode reward: 22.4000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 3446.9,                last time consumption/overall running time: 950.4542s / 112653.8839 s
env0_first_0:                 episode reward: -19.2000,                 loss: 0.0362
env0_second_0:                 episode reward: 19.2000,                 loss: 0.0368
env1_first_0:                 episode reward: -19.3500,                 loss: nan
env1_second_0:                 episode reward: 19.3500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 3560.6,                last time consumption/overall running time: 983.8358s / 113637.7197 s
env0_first_0:                 episode reward: -22.7000,                 loss: 0.0377
env0_second_0:                 episode reward: 22.7000,                 loss: 0.0379
env1_first_0:                 episode reward: -21.4500,                 loss: nan
env1_second_0:                 episode reward: 21.4500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 3280.1,                last time consumption/overall running time: 885.3042s / 114523.0239 s
env0_first_0:                 episode reward: -20.7000,                 loss: 0.0375
env0_second_0:                 episode reward: 20.7000,                 loss: 0.0384
env1_first_0:                 episode reward: -19.4500,                 loss: nan
env1_second_0:                 episode reward: 19.4500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 3376.0,                last time consumption/overall running time: 912.4450s / 115435.4689 s
env0_first_0:                 episode reward: -19.5000,                 loss: 0.0377
env0_second_0:                 episode reward: 19.5000,                 loss: 0.0370
env1_first_0:                 episode reward: -19.8500,                 loss: nan
env1_second_0:                 episode reward: 19.8500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 3660.35,                last time consumption/overall running time: 982.9067s / 116418.3756 s
env0_first_0:                 episode reward: -23.1500,                 loss: 0.0381
env0_second_0:                 episode reward: 23.1500,                 loss: 0.0368
env1_first_0:                 episode reward: -22.0000,                 loss: nan
env1_second_0:                 episode reward: 22.0000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 3118.55,                last time consumption/overall running time: 848.0621s / 117266.4377 s
env0_first_0:                 episode reward: -16.2000,                 loss: 0.0361
env0_second_0:                 episode reward: 16.2000,                 loss: 0.0361
env1_first_0:                 episode reward: -17.1500,                 loss: nan
env1_second_0:                 episode reward: 17.1500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 3578.8,                last time consumption/overall running time: 981.0830s / 118247.5207 s
env0_first_0:                 episode reward: -19.9000,                 loss: 0.0355
env0_second_0:                 episode reward: 19.9000,                 loss: 0.0353
env1_first_0:                 episode reward: -19.5500,                 loss: nan
env1_second_0:                 episode reward: 19.5500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 3548.4,                last time consumption/overall running time: 969.8167s / 119217.3374 s
env0_first_0:                 episode reward: -19.8500,                 loss: 0.0343
env0_second_0:                 episode reward: 19.8500,                 loss: 0.0341
env1_first_0:                 episode reward: -21.0000,                 loss: nan
env1_second_0:                 episode reward: 21.0000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 3308.8,                last time consumption/overall running time: 895.8921s / 120113.2295 s
env0_first_0:                 episode reward: -18.5000,                 loss: 0.0349
env0_second_0:                 episode reward: 18.5000,                 loss: 0.0346
env1_first_0:                 episode reward: -20.9000,                 loss: nan
env1_second_0:                 episode reward: 20.9000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 3564.95,                last time consumption/overall running time: 955.0890s / 121068.3185 s
env0_first_0:                 episode reward: -19.0500,                 loss: 0.0345
env0_second_0:                 episode reward: 19.0500,                 loss: 0.0352
env1_first_0:                 episode reward: -23.7000,                 loss: nan
env1_second_0:                 episode reward: 23.7000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 3509.05,                last time consumption/overall running time: 946.1331s / 122014.4516 s
env0_first_0:                 episode reward: -23.1500,                 loss: 0.0359
env0_second_0:                 episode reward: 23.1500,                 loss: 0.0356
env1_first_0:                 episode reward: -23.1000,                 loss: nan
env1_second_0:                 episode reward: 23.1000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 3591.05,                last time consumption/overall running time: 967.1785s / 122981.6300 s
env0_first_0:                 episode reward: -22.6000,                 loss: 0.0376
env0_second_0:                 episode reward: 22.6000,                 loss: 0.0358
env1_first_0:                 episode reward: -22.7000,                 loss: nan
env1_second_0:                 episode reward: 22.7000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 3260.1,                last time consumption/overall running time: 880.7383s / 123862.3683 s
env0_first_0:                 episode reward: -17.9000,                 loss: 0.0362
env0_second_0:                 episode reward: 17.9000,                 loss: 0.0342
env1_first_0:                 episode reward: -17.8500,                 loss: nan
env1_second_0:                 episode reward: 17.8500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 3312.75,                last time consumption/overall running time: 910.3467s / 124772.7150 s
env0_first_0:                 episode reward: -19.6000,                 loss: 0.0361
env0_second_0:                 episode reward: 19.6000,                 loss: 0.0367
env1_first_0:                 episode reward: -16.7000,                 loss: nan
env1_second_0:                 episode reward: 16.7000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 3398.8,                last time consumption/overall running time: 921.9645s / 125694.6795 s
env0_first_0:                 episode reward: -19.5500,                 loss: 0.0378
env0_second_0:                 episode reward: 19.5500,                 loss: 0.0384
env1_first_0:                 episode reward: -20.6500,                 loss: nan
env1_second_0:                 episode reward: 20.6500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 3441.65,                last time consumption/overall running time: 924.7388s / 126619.4183 s
env0_first_0:                 episode reward: -21.3000,                 loss: 0.0397
env0_second_0:                 episode reward: 21.3000,                 loss: 0.0372
env1_first_0:                 episode reward: -18.8000,                 loss: nan
env1_second_0:                 episode reward: 18.8000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 3324.6,                last time consumption/overall running time: 901.5322s / 127520.9505 s
env0_first_0:                 episode reward: -21.1500,                 loss: 0.0369
env0_second_0:                 episode reward: 21.1500,                 loss: 0.0379
env1_first_0:                 episode reward: -20.1000,                 loss: nan
env1_second_0:                 episode reward: 20.1000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 3539.7,                last time consumption/overall running time: 968.3260s / 128489.2765 s
env0_first_0:                 episode reward: -18.2000,                 loss: 0.0379
env0_second_0:                 episode reward: 18.2000,                 loss: 0.0384
env1_first_0:                 episode reward: -19.2500,                 loss: nan
env1_second_0:                 episode reward: 19.2500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 3220.35,                last time consumption/overall running time: 869.4892s / 129358.7657 s
env0_first_0:                 episode reward: -18.4000,                 loss: 0.0385
env0_second_0:                 episode reward: 18.4000,                 loss: 0.0377
env1_first_0:                 episode reward: -17.1500,                 loss: nan
env1_second_0:                 episode reward: 17.1500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 3530.65,                last time consumption/overall running time: 939.7738s / 130298.5395 s
env0_first_0:                 episode reward: -20.6000,                 loss: 0.0375
env0_second_0:                 episode reward: 20.6000,                 loss: 0.0377
env1_first_0:                 episode reward: -24.5500,                 loss: nan
env1_second_0:                 episode reward: 24.5500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 3546.05,                last time consumption/overall running time: 952.6246s / 131251.1641 s
env0_first_0:                 episode reward: -21.6000,                 loss: 0.0368
env0_second_0:                 episode reward: 21.6000,                 loss: 0.0371
env1_first_0:                 episode reward: -20.7500,                 loss: nan
env1_second_0:                 episode reward: 20.7500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 3466.15,                last time consumption/overall running time: 944.5087s / 132195.6729 s
env0_first_0:                 episode reward: -19.0500,                 loss: 0.0369
env0_second_0:                 episode reward: 19.0500,                 loss: 0.0362
env1_first_0:                 episode reward: -18.4000,                 loss: nan
env1_second_0:                 episode reward: 18.4000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 3253.75,                last time consumption/overall running time: 880.2384s / 133075.9113 s
env0_first_0:                 episode reward: -18.5500,                 loss: 0.0381
env0_second_0:                 episode reward: 18.5500,                 loss: 0.0385
env1_first_0:                 episode reward: -17.3000,                 loss: nan
env1_second_0:                 episode reward: 17.3000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 3333.95,                last time consumption/overall running time: 905.8509s / 133981.7622 s
env0_first_0:                 episode reward: -16.0000,                 loss: 0.0377
env0_second_0:                 episode reward: 16.0000,                 loss: 0.0379
env1_first_0:                 episode reward: -19.4500,                 loss: nan
env1_second_0:                 episode reward: 19.4500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 3392.9,                last time consumption/overall running time: 913.4230s / 134895.1852 s
env0_first_0:                 episode reward: -14.2000,                 loss: 0.0368
env0_second_0:                 episode reward: 14.2000,                 loss: 0.0368
env1_first_0:                 episode reward: -17.5500,                 loss: nan
env1_second_0:                 episode reward: 17.5500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 3375.0,                last time consumption/overall running time: 914.9506s / 135810.1358 s
env0_first_0:                 episode reward: -20.7500,                 loss: 0.0364
env0_second_0:                 episode reward: 20.7500,                 loss: 0.0358
env1_first_0:                 episode reward: -20.3000,                 loss: nan
env1_second_0:                 episode reward: 20.3000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 3376.35,                last time consumption/overall running time: 906.8420s / 136716.9778 s
env0_first_0:                 episode reward: -16.7000,                 loss: 0.0358
env0_second_0:                 episode reward: 16.7000,                 loss: 0.0359
env1_first_0:                 episode reward: -18.8000,                 loss: nan
env1_second_0:                 episode reward: 18.8000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 3346.6,                last time consumption/overall running time: 914.4089s / 137631.3867 s
env0_first_0:                 episode reward: -16.1000,                 loss: 0.0374
env0_second_0:                 episode reward: 16.1000,                 loss: 0.0353
env1_first_0:                 episode reward: -16.8500,                 loss: nan
env1_second_0:                 episode reward: 16.8500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 3238.65,                last time consumption/overall running time: 895.2717s / 138526.6584 s
env0_first_0:                 episode reward: -15.4500,                 loss: 0.0358
env0_second_0:                 episode reward: 15.4500,                 loss: 0.0354
env1_first_0:                 episode reward: -17.1000,                 loss: nan
env1_second_0:                 episode reward: 17.1000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 3297.95,                last time consumption/overall running time: 883.5833s / 139410.2416 s
env0_first_0:                 episode reward: -17.4000,                 loss: 0.0377
env0_second_0:                 episode reward: 17.4000,                 loss: 0.0372
env1_first_0:                 episode reward: -15.5500,                 loss: nan
env1_second_0:                 episode reward: 15.5500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 3023.9,                last time consumption/overall running time: 814.6138s / 140224.8555 s
env0_first_0:                 episode reward: -16.2500,                 loss: 0.0373
env0_second_0:                 episode reward: 16.2500,                 loss: 0.0384
env1_first_0:                 episode reward: -14.6500,                 loss: nan
env1_second_0:                 episode reward: 14.6500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 3248.7,                last time consumption/overall running time: 883.9035s / 141108.7590 s
env0_first_0:                 episode reward: -16.5500,                 loss: 0.0393
env0_second_0:                 episode reward: 16.5500,                 loss: 0.0381
env1_first_0:                 episode reward: -16.7000,                 loss: nan
env1_second_0:                 episode reward: 16.7000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 3183.25,                last time consumption/overall running time: 864.4814s / 141973.2404 s
env0_first_0:                 episode reward: -15.1000,                 loss: 0.0382
env0_second_0:                 episode reward: 15.1000,                 loss: 0.0383
env1_first_0:                 episode reward: -15.1000,                 loss: nan
env1_second_0:                 episode reward: 15.1000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 3230.5,                last time consumption/overall running time: 866.2586s / 142839.4991 s
env0_first_0:                 episode reward: -17.8500,                 loss: 0.0376
env0_second_0:                 episode reward: 17.8500,                 loss: 0.0376
env1_first_0:                 episode reward: -14.9000,                 loss: nan
env1_second_0:                 episode reward: 14.9000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 3247.6,                last time consumption/overall running time: 869.3722s / 143708.8713 s
env0_first_0:                 episode reward: -16.5500,                 loss: 0.0385
env0_second_0:                 episode reward: 16.5500,                 loss: 0.0379
env1_first_0:                 episode reward: -18.5500,                 loss: nan
env1_second_0:                 episode reward: 18.5500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 3456.3,                last time consumption/overall running time: 923.2106s / 144632.0818 s
env0_first_0:                 episode reward: -17.5500,                 loss: 0.0382
env0_second_0:                 episode reward: 17.5500,                 loss: 0.0375
env1_first_0:                 episode reward: -17.3000,                 loss: nan
env1_second_0:                 episode reward: 17.3000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 3188.15,                last time consumption/overall running time: 849.1740s / 145481.2558 s
env0_first_0:                 episode reward: -16.8000,                 loss: 0.0384
env0_second_0:                 episode reward: 16.8000,                 loss: 0.0370
env1_first_0:                 episode reward: -15.5500,                 loss: nan
env1_second_0:                 episode reward: 15.5500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 3216.4,                last time consumption/overall running time: 862.2479s / 146343.5037 s
env0_first_0:                 episode reward: -16.6500,                 loss: 0.0363
env0_second_0:                 episode reward: 16.6500,                 loss: 0.0365
env1_first_0:                 episode reward: -14.5000,                 loss: nan
env1_second_0:                 episode reward: 14.5000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 3294.35,                last time consumption/overall running time: 910.5945s / 147254.0982 s
env0_first_0:                 episode reward: -12.0500,                 loss: 0.0366
env0_second_0:                 episode reward: 12.0500,                 loss: 0.0364
env1_first_0:                 episode reward: -17.6500,                 loss: nan
env1_second_0:                 episode reward: 17.6500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 3301.6,                last time consumption/overall running time: 891.5900s / 148145.6882 s
env0_first_0:                 episode reward: -18.2000,                 loss: 0.0391
env0_second_0:                 episode reward: 18.2000,                 loss: 0.0380
env1_first_0:                 episode reward: -17.4500,                 loss: nan
env1_second_0:                 episode reward: 17.4500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 3177.0,                last time consumption/overall running time: 883.2975s / 149028.9857 s
env0_first_0:                 episode reward: -14.4500,                 loss: 0.0384
env0_second_0:                 episode reward: 14.4500,                 loss: 0.0365
env1_first_0:                 episode reward: -14.3000,                 loss: nan
env1_second_0:                 episode reward: 14.3000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 3285.4,                last time consumption/overall running time: 906.1362s / 149935.1219 s
env0_first_0:                 episode reward: -15.0000,                 loss: 0.0371
env0_second_0:                 episode reward: 15.0000,                 loss: 0.0361
env1_first_0:                 episode reward: -16.6500,                 loss: nan
env1_second_0:                 episode reward: 16.6500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 3273.0,                last time consumption/overall running time: 875.9418s / 150811.0637 s
env0_first_0:                 episode reward: -15.8500,                 loss: 0.0358
env0_second_0:                 episode reward: 15.8500,                 loss: 0.0374
env1_first_0:                 episode reward: -15.8000,                 loss: nan
env1_second_0:                 episode reward: 15.8000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 3295.05,                last time consumption/overall running time: 904.3014s / 151715.3651 s
env0_first_0:                 episode reward: -16.2000,                 loss: 0.0377
env0_second_0:                 episode reward: 16.2000,                 loss: 0.0380
env1_first_0:                 episode reward: -15.6500,                 loss: nan
env1_second_0:                 episode reward: 15.6500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 3295.5,                last time consumption/overall running time: 896.1229s / 152611.4880 s
env0_first_0:                 episode reward: -16.9000,                 loss: 0.0364
env0_second_0:                 episode reward: 16.9000,                 loss: 0.0380
env1_first_0:                 episode reward: -19.5000,                 loss: nan
env1_second_0:                 episode reward: 19.5000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 3400.05,                last time consumption/overall running time: 911.6020s / 153523.0901 s
env0_first_0:                 episode reward: -18.8000,                 loss: 0.0385
env0_second_0:                 episode reward: 18.8000,                 loss: 0.0376
env1_first_0:                 episode reward: -15.9500,                 loss: nan
env1_second_0:                 episode reward: 15.9500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 3383.2,                last time consumption/overall running time: 918.1816s / 154441.2716 s
env0_first_0:                 episode reward: -17.0500,                 loss: 0.0398
env0_second_0:                 episode reward: 17.0500,                 loss: 0.0384
env1_first_0:                 episode reward: -14.6500,                 loss: nan
env1_second_0:                 episode reward: 14.6500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 3459.25,                last time consumption/overall running time: 964.6739s / 155405.9456 s
env0_first_0:                 episode reward: -16.5000,                 loss: 0.0388
env0_second_0:                 episode reward: 16.5000,                 loss: 0.0385
env1_first_0:                 episode reward: -17.9500,                 loss: nan
env1_second_0:                 episode reward: 17.9500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 3214.5,                last time consumption/overall running time: 891.0354s / 156296.9809 s
env0_first_0:                 episode reward: -15.8500,                 loss: 0.0401
env0_second_0:                 episode reward: 15.8500,                 loss: 0.0395
env1_first_0:                 episode reward: -14.8000,                 loss: nan
env1_second_0:                 episode reward: 14.8000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 3108.65,                last time consumption/overall running time: 843.1834s / 157140.1643 s
env0_first_0:                 episode reward: -14.4500,                 loss: 0.0413
env0_second_0:                 episode reward: 14.4500,                 loss: 0.0389
env1_first_0:                 episode reward: -13.8000,                 loss: nan
env1_second_0:                 episode reward: 13.8000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 3110.2,                last time consumption/overall running time: 859.8084s / 157999.9726 s
env0_first_0:                 episode reward: -13.0500,                 loss: 0.0378
env0_second_0:                 episode reward: 13.0500,                 loss: 0.0372
env1_first_0:                 episode reward: -14.6500,                 loss: nan
env1_second_0:                 episode reward: 14.6500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 3414.25,                last time consumption/overall running time: 939.7891s / 158939.7618 s
env0_first_0:                 episode reward: -17.3500,                 loss: 0.0380
env0_second_0:                 episode reward: 17.3500,                 loss: 0.0371
env1_first_0:                 episode reward: -16.6500,                 loss: nan
env1_second_0:                 episode reward: 16.6500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 3130.75,                last time consumption/overall running time: 857.3872s / 159797.1489 s
env0_first_0:                 episode reward: -15.6500,                 loss: 0.0370
env0_second_0:                 episode reward: 15.6500,                 loss: 0.0368
env1_first_0:                 episode reward: -15.2500,                 loss: nan
env1_second_0:                 episode reward: 15.2500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 3057.65,                last time consumption/overall running time: 841.4377s / 160638.5867 s
env0_first_0:                 episode reward: -13.3000,                 loss: 0.0380
env0_second_0:                 episode reward: 13.3000,                 loss: 0.0369
env1_first_0:                 episode reward: -16.1000,                 loss: nan
env1_second_0:                 episode reward: 16.1000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 3174.75,                last time consumption/overall running time: 861.9147s / 161500.5014 s
env0_first_0:                 episode reward: -14.9500,                 loss: 0.0373
env0_second_0:                 episode reward: 14.9500,                 loss: 0.0372
env1_first_0:                 episode reward: -14.2500,                 loss: nan
env1_second_0:                 episode reward: 14.2500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 3291.75,                last time consumption/overall running time: 888.6055s / 162389.1069 s
env0_first_0:                 episode reward: -15.1500,                 loss: 0.0367
env0_second_0:                 episode reward: 15.1500,                 loss: 0.0369
env1_first_0:                 episode reward: -16.2500,                 loss: nan
env1_second_0:                 episode reward: 16.2500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 3200.25,                last time consumption/overall running time: 873.2551s / 163262.3619 s