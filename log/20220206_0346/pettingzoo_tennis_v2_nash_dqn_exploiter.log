pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 5, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 3}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220206_0346/pettingzoo_tennis_v2_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/20220206_0346/pettingzoo_tennis_v2_nash_dqn_exploiter.
Episode: 1/10000 (0.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 98.1782s / 98.1782 s
env0_first_0:                 episode reward: 30.0000,                 loss: 0.0901
env0_second_0:                 episode reward: -30.0000,                 loss: 0.0902
env1_first_0:                 episode reward: 26.0000,                 loss: nan
env1_second_0:                 episode reward: -26.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 4829.5,                last time consumption/overall running time: 1301.5507s / 1399.7289 s
env0_first_0:                 episode reward: 7.5000,                 loss: 0.0718
env0_second_0:                 episode reward: -7.5000,                 loss: 0.0721
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 2129.4,                last time consumption/overall running time: 643.7821s / 2043.5110 s
env0_first_0:                 episode reward: -13.9500,                 loss: 0.0493
env0_second_0:                 episode reward: 13.9500,                 loss: 0.0577
env1_first_0:                 episode reward: -12.1500,                 loss: nan
env1_second_0:                 episode reward: 12.1500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1844.5,                last time consumption/overall running time: 559.7854s / 2603.2964 s
env0_first_0:                 episode reward: -15.8500,                 loss: 0.0609
env0_second_0:                 episode reward: 15.8500,                 loss: 0.0655
env1_first_0:                 episode reward: -16.1500,                 loss: nan
env1_second_0:                 episode reward: 16.1500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1796.3,                last time consumption/overall running time: 547.1117s / 3150.4081 s
env0_first_0:                 episode reward: -20.7500,                 loss: 0.0605
env0_second_0:                 episode reward: 20.7500,                 loss: 0.0546
env1_first_0:                 episode reward: -19.9000,                 loss: nan
env1_second_0:                 episode reward: 19.9000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1619.05,                last time consumption/overall running time: 496.5141s / 3646.9222 s
env0_first_0:                 episode reward: -23.4500,                 loss: 0.0626
env0_second_0:                 episode reward: 23.4500,                 loss: 0.0529
env1_first_0:                 episode reward: -22.6500,                 loss: nan
env1_second_0:                 episode reward: 22.6500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1633.45,                last time consumption/overall running time: 497.9251s / 4144.8473 s
env0_first_0:                 episode reward: -22.8000,                 loss: 0.0556
env0_second_0:                 episode reward: 22.8000,                 loss: 0.0522
env1_first_0:                 episode reward: -23.3500,                 loss: nan
env1_second_0:                 episode reward: 23.3500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1619.2,                last time consumption/overall running time: 499.2109s / 4644.0582 s
env0_first_0:                 episode reward: -22.6500,                 loss: 0.0474
env0_second_0:                 episode reward: 22.6500,                 loss: 0.0470
env1_first_0:                 episode reward: -23.3000,                 loss: nan
env1_second_0:                 episode reward: 23.3000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1558.1,                last time consumption/overall running time: 473.5689s / 5117.6271 s
env0_first_0:                 episode reward: -23.0000,                 loss: 0.0414
env0_second_0:                 episode reward: 23.0000,                 loss: 0.0405
env1_first_0:                 episode reward: -24.3000,                 loss: nan
env1_second_0:                 episode reward: 24.3000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1531.0,                last time consumption/overall running time: 473.5944s / 5591.2215 s
env0_first_0:                 episode reward: -22.7000,                 loss: 0.0356
env0_second_0:                 episode reward: 22.7000,                 loss: 0.0369
env1_first_0:                 episode reward: -24.0500,                 loss: nan
env1_second_0:                 episode reward: 24.0500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1553.8,                last time consumption/overall running time: 477.4330s / 6068.6545 s
env0_first_0:                 episode reward: -22.1500,                 loss: 0.0318
env0_second_0:                 episode reward: 22.1500,                 loss: 0.0350
env1_first_0:                 episode reward: -22.3500,                 loss: nan
env1_second_0:                 episode reward: 22.3500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1599.75,                last time consumption/overall running time: 485.3059s / 6553.9604 s
env0_first_0:                 episode reward: -23.2000,                 loss: 0.0306
env0_second_0:                 episode reward: 23.2000,                 loss: 0.0294
env1_first_0:                 episode reward: -24.2000,                 loss: nan
env1_second_0:                 episode reward: 24.2000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1618.0,                last time consumption/overall running time: 496.0702s / 7050.0306 s
env0_first_0:                 episode reward: -23.2500,                 loss: 0.0322
env0_second_0:                 episode reward: 23.2500,                 loss: 0.0304
env1_first_0:                 episode reward: -23.5500,                 loss: nan
env1_second_0:                 episode reward: 23.5500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1585.0,                last time consumption/overall running time: 480.7409s / 7530.7715 s
env0_first_0:                 episode reward: -24.6500,                 loss: 0.0344
env0_second_0:                 episode reward: 24.6500,                 loss: 0.0369
env1_first_0:                 episode reward: -21.5000,                 loss: nan
env1_second_0:                 episode reward: 21.5000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1612.95,                last time consumption/overall running time: 495.2500s / 8026.0215 s
env0_first_0:                 episode reward: -25.7500,                 loss: 0.0373
env0_second_0:                 episode reward: 25.7500,                 loss: 0.0385
env1_first_0:                 episode reward: -22.6500,                 loss: nan
env1_second_0:                 episode reward: 22.6500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1600.8,                last time consumption/overall running time: 491.0544s / 8517.0759 s
env0_first_0:                 episode reward: -25.8500,                 loss: 0.0354
env0_second_0:                 episode reward: 25.8500,                 loss: 0.0345
env1_first_0:                 episode reward: -25.5000,                 loss: nan
env1_second_0:                 episode reward: 25.5000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1584.7,                last time consumption/overall running time: 489.8389s / 9006.9149 s
env0_first_0:                 episode reward: -26.1000,                 loss: 0.0331
env0_second_0:                 episode reward: 26.1000,                 loss: 0.0347
env1_first_0:                 episode reward: -25.5000,                 loss: nan
env1_second_0:                 episode reward: 25.5000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1521.0,                last time consumption/overall running time: 469.0262s / 9475.9411 s
env0_first_0:                 episode reward: -25.9000,                 loss: 0.0290
env0_second_0:                 episode reward: 25.9000,                 loss: 0.0305
env1_first_0:                 episode reward: -25.1500,                 loss: nan
env1_second_0:                 episode reward: 25.1500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1554.85,                last time consumption/overall running time: 478.5324s / 9954.4735 s
env0_first_0:                 episode reward: -25.6500,                 loss: 0.0280
env0_second_0:                 episode reward: 25.6500,                 loss: 0.0284
env1_first_0:                 episode reward: -27.4000,                 loss: nan
env1_second_0:                 episode reward: 27.4000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1516.6,                last time consumption/overall running time: 467.5285s / 10422.0019 s
env0_first_0:                 episode reward: -25.8000,                 loss: 0.0276
env0_second_0:                 episode reward: 25.8000,                 loss: 0.0266
env1_first_0:                 episode reward: -26.0000,                 loss: nan
env1_second_0:                 episode reward: 26.0000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1547.8,                last time consumption/overall running time: 475.7519s / 10897.7539 s
env0_first_0:                 episode reward: -25.7500,                 loss: 0.0280
env0_second_0:                 episode reward: 25.7500,                 loss: 0.0267
env1_first_0:                 episode reward: -26.6000,                 loss: nan
env1_second_0:                 episode reward: 26.6000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1527.0,                last time consumption/overall running time: 469.0388s / 11366.7927 s
env0_first_0:                 episode reward: -26.8000,                 loss: 0.0265
env0_second_0:                 episode reward: 26.8000,                 loss: 0.0283
env1_first_0:                 episode reward: -26.6500,                 loss: nan
env1_second_0:                 episode reward: 26.6500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1787.8,                last time consumption/overall running time: 547.0617s / 11913.8544 s
env0_first_0:                 episode reward: -19.6000,                 loss: 0.0289
env0_second_0:                 episode reward: 19.6000,                 loss: 0.0302
env1_first_0:                 episode reward: -19.9000,                 loss: nan
env1_second_0:                 episode reward: 19.9000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1508.8,                last time consumption/overall running time: 467.3357s / 12381.1901 s
env0_first_0:                 episode reward: -27.7000,                 loss: 0.0297
env0_second_0:                 episode reward: 27.7000,                 loss: 0.0334
env1_first_0:                 episode reward: -26.1500,                 loss: nan
env1_second_0:                 episode reward: 26.1500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1520.9,                last time consumption/overall running time: 468.3241s / 12849.5141 s
env0_first_0:                 episode reward: -27.4500,                 loss: 0.0287
env0_second_0:                 episode reward: 27.4500,                 loss: 0.0310
env1_first_0:                 episode reward: -28.4000,                 loss: nan
env1_second_0:                 episode reward: 28.4000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1518.15,                last time consumption/overall running time: 463.0390s / 13312.5531 s
env0_first_0:                 episode reward: -27.9000,                 loss: 0.0266
env0_second_0:                 episode reward: 27.9000,                 loss: 0.0300
env1_first_0:                 episode reward: -27.9500,                 loss: nan
env1_second_0:                 episode reward: 27.9500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1511.5,                last time consumption/overall running time: 454.1014s / 13766.6545 s
env0_first_0:                 episode reward: -28.6500,                 loss: 0.0260
env0_second_0:                 episode reward: 28.6500,                 loss: 0.0277
env1_first_0:                 episode reward: -28.5000,                 loss: nan
env1_second_0:                 episode reward: 28.5000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1544.9,                last time consumption/overall running time: 475.8250s / 14242.4795 s
env0_first_0:                 episode reward: -29.3000,                 loss: 0.0274
env0_second_0:                 episode reward: 29.3000,                 loss: 0.0286
env1_first_0:                 episode reward: -28.2000,                 loss: nan
env1_second_0:                 episode reward: 28.2000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1528.5,                last time consumption/overall running time: 467.9656s / 14710.4451 s
env0_first_0:                 episode reward: -28.8500,                 loss: 0.0252
env0_second_0:                 episode reward: 28.8500,                 loss: 0.0278
env1_first_0:                 episode reward: -29.8000,                 loss: nan
env1_second_0:                 episode reward: 29.8000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1547.7,                last time consumption/overall running time: 478.5938s / 15189.0389 s
env0_first_0:                 episode reward: -28.9000,                 loss: 0.0265
env0_second_0:                 episode reward: 28.9000,                 loss: 0.0273
env1_first_0:                 episode reward: -27.4000,                 loss: nan
env1_second_0:                 episode reward: 27.4000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1524.75,                last time consumption/overall running time: 471.9986s / 15661.0375 s
env0_first_0:                 episode reward: -27.2000,                 loss: 0.0280
env0_second_0:                 episode reward: 27.2000,                 loss: 0.0291
env1_first_0:                 episode reward: -27.4000,                 loss: nan
env1_second_0:                 episode reward: 27.4000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1528.1,                last time consumption/overall running time: 465.7388s / 16126.7763 s
env0_first_0:                 episode reward: -28.0000,                 loss: 0.0278
env0_second_0:                 episode reward: 28.0000,                 loss: 0.0306
env1_first_0:                 episode reward: -29.2000,                 loss: nan
env1_second_0:                 episode reward: 29.2000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1576.3,                last time consumption/overall running time: 484.0332s / 16610.8095 s
env0_first_0:                 episode reward: -28.2000,                 loss: 0.0282
env0_second_0:                 episode reward: 28.2000,                 loss: 0.0317
env1_first_0:                 episode reward: -27.7500,                 loss: nan
env1_second_0:                 episode reward: 27.7500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1499.3,                last time consumption/overall running time: 465.2180s / 17076.0275 s
env0_first_0:                 episode reward: -30.1000,                 loss: 0.0276
env0_second_0:                 episode reward: 30.1000,                 loss: 0.0297
env1_first_0:                 episode reward: -28.5000,                 loss: nan
env1_second_0:                 episode reward: 28.5000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1565.35,                last time consumption/overall running time: 482.5830s / 17558.6105 s
env0_first_0:                 episode reward: -27.6500,                 loss: 0.0258
env0_second_0:                 episode reward: 27.6500,                 loss: 0.0278
env1_first_0:                 episode reward: -28.7000,                 loss: nan
env1_second_0:                 episode reward: 28.7000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1528.45,                last time consumption/overall running time: 470.2071s / 18028.8176 s
env0_first_0:                 episode reward: -28.8500,                 loss: 0.0254
env0_second_0:                 episode reward: 28.8500,                 loss: 0.0272
env1_first_0:                 episode reward: -27.8500,                 loss: nan
env1_second_0:                 episode reward: 27.8500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1554.75,                last time consumption/overall running time: 473.4749s / 18502.2925 s
env0_first_0:                 episode reward: -27.9500,                 loss: 0.0277
env0_second_0:                 episode reward: 27.9500,                 loss: 0.0267
env1_first_0:                 episode reward: -28.5000,                 loss: nan
env1_second_0:                 episode reward: 28.5000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1530.2,                last time consumption/overall running time: 471.3000s / 18973.5925 s
env0_first_0:                 episode reward: -27.2500,                 loss: 0.0264
env0_second_0:                 episode reward: 27.2500,                 loss: 0.0276
env1_first_0:                 episode reward: -28.2000,                 loss: nan
env1_second_0:                 episode reward: 28.2000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 1623.15,                last time consumption/overall running time: 492.7554s / 19466.3479 s
env0_first_0:                 episode reward: -27.0000,                 loss: 0.0277
env0_second_0:                 episode reward: 27.0000,                 loss: 0.0283
env1_first_0:                 episode reward: -28.2000,                 loss: nan
env1_second_0:                 episode reward: 28.2000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1522.85,                last time consumption/overall running time: 467.6532s / 19934.0010 s
env0_first_0:                 episode reward: -27.3500,                 loss: 0.0309
env0_second_0:                 episode reward: 27.3500,                 loss: 0.0286
env1_first_0:                 episode reward: -27.0000,                 loss: nan
env1_second_0:                 episode reward: 27.0000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1540.4,                last time consumption/overall running time: 470.5505s / 20404.5515 s
env0_first_0:                 episode reward: -28.7000,                 loss: 0.0290
env0_second_0:                 episode reward: 28.7000,                 loss: 0.0293
env1_first_0:                 episode reward: -29.2500,                 loss: nan
env1_second_0:                 episode reward: 29.2500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1554.15,                last time consumption/overall running time: 474.7024s / 20879.2539 s
env0_first_0:                 episode reward: -27.2500,                 loss: 0.0280
env0_second_0:                 episode reward: 27.2500,                 loss: 0.0278
env1_first_0:                 episode reward: -28.7500,                 loss: nan
env1_second_0:                 episode reward: 28.7500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1530.8,                last time consumption/overall running time: 466.1302s / 21345.3841 s
env0_first_0:                 episode reward: -29.1000,                 loss: 0.0282
env0_second_0:                 episode reward: 29.1000,                 loss: 0.0266
env1_first_0:                 episode reward: -27.7500,                 loss: nan
env1_second_0:                 episode reward: 27.7500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1581.55,                last time consumption/overall running time: 481.6374s / 21827.0215 s
env0_first_0:                 episode reward: -28.3500,                 loss: 0.0279
env0_second_0:                 episode reward: 28.3500,                 loss: 0.0278
env1_first_0:                 episode reward: -29.4500,                 loss: nan
env1_second_0:                 episode reward: 29.4500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1532.4,                last time consumption/overall running time: 466.1575s / 22293.1790 s
env0_first_0:                 episode reward: -27.7500,                 loss: 0.0277
env0_second_0:                 episode reward: 27.7500,                 loss: 0.0272
env1_first_0:                 episode reward: -27.1000,                 loss: nan
env1_second_0:                 episode reward: 27.1000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1543.55,                last time consumption/overall running time: 478.2645s / 22771.4435 s
env0_first_0:                 episode reward: -26.3000,                 loss: 0.0265
env0_second_0:                 episode reward: 26.3000,                 loss: 0.0282
env1_first_0:                 episode reward: -27.4500,                 loss: nan
env1_second_0:                 episode reward: 27.4500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1502.55,                last time consumption/overall running time: 464.6270s / 23236.0705 s
env0_first_0:                 episode reward: -26.4000,                 loss: 0.0247
env0_second_0:                 episode reward: 26.4000,                 loss: 0.0251
env1_first_0:                 episode reward: -28.4500,                 loss: nan
env1_second_0:                 episode reward: 28.4500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1506.4,                last time consumption/overall running time: 465.8518s / 23701.9223 s
env0_first_0:                 episode reward: -28.2000,                 loss: 0.0249
env0_second_0:                 episode reward: 28.2000,                 loss: 0.0245
env1_first_0:                 episode reward: -28.7500,                 loss: nan
env1_second_0:                 episode reward: 28.7500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1500.5,                last time consumption/overall running time: 459.3488s / 24161.2711 s
env0_first_0:                 episode reward: -28.2500,                 loss: 0.0250
env0_second_0:                 episode reward: 28.2500,                 loss: 0.0248
env1_first_0:                 episode reward: -28.8000,                 loss: nan
env1_second_0:                 episode reward: 28.8000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1517.2,                last time consumption/overall running time: 465.4866s / 24626.7577 s
env0_first_0:                 episode reward: -27.9000,                 loss: 0.0247
env0_second_0:                 episode reward: 27.9000,                 loss: 0.0236
env1_first_0:                 episode reward: -28.3000,                 loss: nan
env1_second_0:                 episode reward: 28.3000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1550.05,                last time consumption/overall running time: 477.6693s / 25104.4270 s
env0_first_0:                 episode reward: -28.1500,                 loss: 0.0256
env0_second_0:                 episode reward: 28.1500,                 loss: 0.0247
env1_first_0:                 episode reward: -28.9000,                 loss: nan
env1_second_0:                 episode reward: 28.9000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1525.55,                last time consumption/overall running time: 466.7969s / 25571.2239 s
env0_first_0:                 episode reward: -28.4000,                 loss: 0.0245
env0_second_0:                 episode reward: 28.4000,                 loss: 0.0247
env1_first_0:                 episode reward: -28.2500,                 loss: nan
env1_second_0:                 episode reward: 28.2500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1539.35,                last time consumption/overall running time: 468.7280s / 26039.9520 s
env0_first_0:                 episode reward: -28.4000,                 loss: 0.0248
env0_second_0:                 episode reward: 28.4000,                 loss: 0.0232
env1_first_0:                 episode reward: -29.2000,                 loss: nan
env1_second_0:                 episode reward: 29.2000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1511.25,                last time consumption/overall running time: 461.4915s / 26501.4435 s
env0_first_0:                 episode reward: -28.0000,                 loss: 0.0239
env0_second_0:                 episode reward: 28.0000,                 loss: 0.0239
env1_first_0:                 episode reward: -27.3500,                 loss: nan
env1_second_0:                 episode reward: 27.3500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1528.85,                last time consumption/overall running time: 473.8872s / 26975.3307 s
env0_first_0:                 episode reward: -28.1000,                 loss: 0.0245
env0_second_0:                 episode reward: 28.1000,                 loss: 0.0257
env1_first_0:                 episode reward: -26.3500,                 loss: nan
env1_second_0:                 episode reward: 26.3500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1540.7,                last time consumption/overall running time: 473.6996s / 27449.0303 s
env0_first_0:                 episode reward: -27.0000,                 loss: 0.0243
env0_second_0:                 episode reward: 27.0000,                 loss: 0.0254
env1_first_0:                 episode reward: -27.2000,                 loss: nan
env1_second_0:                 episode reward: 27.2000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1528.05,                last time consumption/overall running time: 463.8289s / 27912.8593 s
env0_first_0:                 episode reward: -27.3000,                 loss: 0.0267
env0_second_0:                 episode reward: 27.3000,                 loss: 0.0269
env1_first_0:                 episode reward: -27.9000,                 loss: nan
env1_second_0:                 episode reward: 27.9000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1525.3,                last time consumption/overall running time: 472.6299s / 28385.4892 s
env0_first_0:                 episode reward: -26.2500,                 loss: 0.0252
env0_second_0:                 episode reward: 26.2500,                 loss: 0.0250
env1_first_0:                 episode reward: -27.4000,                 loss: nan
env1_second_0:                 episode reward: 27.4000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1586.4,                last time consumption/overall running time: 486.7960s / 28872.2851 s
env0_first_0:                 episode reward: -27.8000,                 loss: 0.0255
env0_second_0:                 episode reward: 27.8000,                 loss: 0.0251
env1_first_0:                 episode reward: -27.7000,                 loss: nan
env1_second_0:                 episode reward: 27.7000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1547.1,                last time consumption/overall running time: 480.6156s / 29352.9007 s
env0_first_0:                 episode reward: -25.7500,                 loss: 0.0272
env0_second_0:                 episode reward: 25.7500,                 loss: 0.0260
env1_first_0:                 episode reward: -26.5000,                 loss: nan
env1_second_0:                 episode reward: 26.5000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1513.75,                last time consumption/overall running time: 473.1046s / 29826.0053 s
env0_first_0:                 episode reward: -26.7000,                 loss: 0.0253
env0_second_0:                 episode reward: 26.7000,                 loss: 0.0246
env1_first_0:                 episode reward: -26.6000,                 loss: nan
env1_second_0:                 episode reward: 26.6000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1534.0,                last time consumption/overall running time: 479.8563s / 30305.8616 s
env0_first_0:                 episode reward: -26.0500,                 loss: 0.0237
env0_second_0:                 episode reward: 26.0500,                 loss: 0.0235
env1_first_0:                 episode reward: -25.6500,                 loss: nan
env1_second_0:                 episode reward: 25.6500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1520.7,                last time consumption/overall running time: 460.3833s / 30766.2449 s
env0_first_0:                 episode reward: -26.6500,                 loss: 0.0232
env0_second_0:                 episode reward: 26.6500,                 loss: 0.0236
env1_first_0:                 episode reward: -25.2500,                 loss: nan
env1_second_0:                 episode reward: 25.2500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1509.45,                last time consumption/overall running time: 465.4585s / 31231.7034 s
env0_first_0:                 episode reward: -25.7500,                 loss: 0.0247
env0_second_0:                 episode reward: 25.7500,                 loss: 0.0234
env1_first_0:                 episode reward: -26.5500,                 loss: nan
env1_second_0:                 episode reward: 26.5500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1549.75,                last time consumption/overall running time: 475.2334s / 31706.9368 s
env0_first_0:                 episode reward: -27.5000,                 loss: 0.0244
env0_second_0:                 episode reward: 27.5000,                 loss: 0.0262
env1_first_0:                 episode reward: -26.9500,                 loss: nan
env1_second_0:                 episode reward: 26.9500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1541.85,                last time consumption/overall running time: 476.3605s / 32183.2973 s
env0_first_0:                 episode reward: -26.8500,                 loss: 0.0234
env0_second_0:                 episode reward: 26.8500,                 loss: 0.0250
env1_first_0:                 episode reward: -26.9000,                 loss: nan
env1_second_0:                 episode reward: 26.9000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1511.3,                last time consumption/overall running time: 456.3395s / 32639.6367 s
env0_first_0:                 episode reward: -27.7000,                 loss: 0.0242
env0_second_0:                 episode reward: 27.7000,                 loss: 0.0237
env1_first_0:                 episode reward: -26.6500,                 loss: nan
env1_second_0:                 episode reward: 26.6500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1523.15,                last time consumption/overall running time: 468.4391s / 33108.0758 s
env0_first_0:                 episode reward: -26.0000,                 loss: 0.0236
env0_second_0:                 episode reward: 26.0000,                 loss: 0.0235
env1_first_0:                 episode reward: -27.0500,                 loss: nan
env1_second_0:                 episode reward: 27.0500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1543.15,                last time consumption/overall running time: 476.1410s / 33584.2169 s
env0_first_0:                 episode reward: -25.8500,                 loss: 0.0243
env0_second_0:                 episode reward: 25.8500,                 loss: 0.0250
env1_first_0:                 episode reward: -27.4000,                 loss: nan
env1_second_0:                 episode reward: 27.4000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1504.8,                last time consumption/overall running time: 455.8950s / 34040.1119 s
env0_first_0:                 episode reward: -26.4000,                 loss: 0.0236
env0_second_0:                 episode reward: 26.4000,                 loss: 0.0244
env1_first_0:                 episode reward: -27.1000,                 loss: nan
env1_second_0:                 episode reward: 27.1000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1523.7,                last time consumption/overall running time: 462.9453s / 34503.0572 s
env0_first_0:                 episode reward: -26.7000,                 loss: 0.0226
env0_second_0:                 episode reward: 26.7000,                 loss: 0.0250
env1_first_0:                 episode reward: -26.9500,                 loss: nan
env1_second_0:                 episode reward: 26.9500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1555.9,                last time consumption/overall running time: 479.1590s / 34982.2162 s
env0_first_0:                 episode reward: -26.0000,                 loss: 0.0238
env0_second_0:                 episode reward: 26.0000,                 loss: 0.0225
env1_first_0:                 episode reward: -25.0000,                 loss: nan
env1_second_0:                 episode reward: 25.0000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 1500.0,                last time consumption/overall running time: 464.2028s / 35446.4190 s
env0_first_0:                 episode reward: -25.9500,                 loss: 0.0226
env0_second_0:                 episode reward: 25.9500,                 loss: 0.0231
env1_first_0:                 episode reward: -25.5500,                 loss: nan
env1_second_0:                 episode reward: 25.5500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1537.35,                last time consumption/overall running time: 468.5161s / 35914.9352 s
env0_first_0:                 episode reward: -25.4000,                 loss: 0.0238
env0_second_0:                 episode reward: 25.4000,                 loss: 0.0238
env1_first_0:                 episode reward: -25.9500,                 loss: nan
env1_second_0:                 episode reward: 25.9500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1560.2,                last time consumption/overall running time: 478.7500s / 36393.6852 s
env0_first_0:                 episode reward: -25.9000,                 loss: 0.0254
env0_second_0:                 episode reward: 25.9000,                 loss: 0.0256
env1_first_0:                 episode reward: -25.7500,                 loss: nan
env1_second_0:                 episode reward: 25.7500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1510.65,                last time consumption/overall running time: 458.8725s / 36852.5576 s
env0_first_0:                 episode reward: -25.5500,                 loss: 0.0253
env0_second_0:                 episode reward: 25.5500,                 loss: 0.0235
env1_first_0:                 episode reward: -25.9500,                 loss: nan
env1_second_0:                 episode reward: 25.9500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1520.45,                last time consumption/overall running time: 469.0494s / 37321.6070 s
env0_first_0:                 episode reward: -26.4500,                 loss: 0.0234
env0_second_0:                 episode reward: 26.4500,                 loss: 0.0240
env1_first_0:                 episode reward: -26.4000,                 loss: nan
env1_second_0:                 episode reward: 26.4000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1542.65,                last time consumption/overall running time: 473.6961s / 37795.3031 s
env0_first_0:                 episode reward: -27.6500,                 loss: 0.0235
env0_second_0:                 episode reward: 27.6500,                 loss: 0.0246
env1_first_0:                 episode reward: -27.1000,                 loss: nan
env1_second_0:                 episode reward: 27.1000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1517.5,                last time consumption/overall running time: 467.7364s / 38263.0396 s
env0_first_0:                 episode reward: -26.2500,                 loss: 0.0236
env0_second_0:                 episode reward: 26.2500,                 loss: 0.0241
env1_first_0:                 episode reward: -26.7500,                 loss: nan
env1_second_0:                 episode reward: 26.7500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1533.45,                last time consumption/overall running time: 466.0083s / 38729.0479 s
env0_first_0:                 episode reward: -26.0500,                 loss: 0.0238
env0_second_0:                 episode reward: 26.0500,                 loss: 0.0261
env1_first_0:                 episode reward: -26.4500,                 loss: nan
env1_second_0:                 episode reward: 26.4500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1638.8,                last time consumption/overall running time: 502.1685s / 39231.2164 s