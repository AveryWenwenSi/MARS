pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=5, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=5, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 5, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 3}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220206_0346/pettingzoo_surround_v1_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/20220206_0346/pettingzoo_surround_v1_nash_dqn_exploiter.
Episode: 1/10000 (0.0100%),                 avg. length: 1345.0,                last time consumption/overall running time: 9.1696s / 9.1696 s
env0_first_0:                 episode reward: 6.0000,                 loss: 0.0474
env0_second_0:                 episode reward: -6.0000,                 loss: 0.0476
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1404.95,                last time consumption/overall running time: 237.5998s / 246.7694 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0466
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0470
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1297.95,                last time consumption/overall running time: 276.7403s / 523.5097 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0430
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0430
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1175.35,                last time consumption/overall running time: 271.7446s / 795.2543 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0391
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0396
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1160.55,                last time consumption/overall running time: 275.9362s / 1071.1904 s
env0_first_0:                 episode reward: -7.1000,                 loss: 0.0355
env0_second_0:                 episode reward: 7.1000,                 loss: 0.0342
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1133.45,                last time consumption/overall running time: 285.0830s / 1356.2734 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.0311
env0_second_0:                 episode reward: 7.1500,                 loss: 0.0297
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1118.55,                last time consumption/overall running time: 282.6612s / 1638.9346 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0272
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0263
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1110.8,                last time consumption/overall running time: 276.4628s / 1915.3975 s
env0_first_0:                 episode reward: -7.5000,                 loss: 0.0246
env0_second_0:                 episode reward: 7.5000,                 loss: 0.0260
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1154.15,                last time consumption/overall running time: 287.2909s / 2202.6883 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.0242
env0_second_0:                 episode reward: 7.1500,                 loss: 0.0290
env1_first_0:                 episode reward: -8.0500,                 loss: nan
env1_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1094.6,                last time consumption/overall running time: 273.6817s / 2476.3700 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0240
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0302
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1098.9,                last time consumption/overall running time: 274.2249s / 2750.5949 s
env0_first_0:                 episode reward: -8.7000,                 loss: 0.0243
env0_second_0:                 episode reward: 8.7000,                 loss: 0.0276
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1074.9,                last time consumption/overall running time: 269.5699s / 3020.1648 s
env0_first_0:                 episode reward: -7.4500,                 loss: 0.0239
env0_second_0:                 episode reward: 7.4500,                 loss: 0.0252
env1_first_0:                 episode reward: -7.9500,                 loss: nan
env1_second_0:                 episode reward: 7.9500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1108.2,                last time consumption/overall running time: 274.2429s / 3294.4077 s
env0_first_0:                 episode reward: -8.3500,                 loss: 0.0236
env0_second_0:                 episode reward: 8.3500,                 loss: 0.0236
env1_first_0:                 episode reward: -8.0500,                 loss: nan
env1_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1176.55,                last time consumption/overall running time: 292.4107s / 3586.8184 s
env0_first_0:                 episode reward: -7.5000,                 loss: 0.0209
env0_second_0:                 episode reward: 7.5000,                 loss: 0.0209
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1115.7,                last time consumption/overall running time: 282.5821s / 3869.4004 s
env0_first_0:                 episode reward: -7.4000,                 loss: 0.0213
env0_second_0:                 episode reward: 7.4000,                 loss: 0.0184
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1148.0,                last time consumption/overall running time: 287.1698s / 4156.5702 s
env0_first_0:                 episode reward: -7.3000,                 loss: 0.0207
env0_second_0:                 episode reward: 7.3000,                 loss: 0.0196
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1104.4,                last time consumption/overall running time: 281.5503s / 4438.1205 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0202
env0_second_0:                 episode reward: 7.7000,                 loss: 0.0214
env1_first_0:                 episode reward: -8.5000,                 loss: nan
env1_second_0:                 episode reward: 8.5000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1094.95,                last time consumption/overall running time: 273.8817s / 4712.0022 s
env0_first_0:                 episode reward: -8.7000,                 loss: 0.0210
env0_second_0:                 episode reward: 8.7000,                 loss: 0.0225
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1137.4,                last time consumption/overall running time: 279.1984s / 4991.2006 s
env0_first_0:                 episode reward: -7.4000,                 loss: 0.0217
env0_second_0:                 episode reward: 7.4000,                 loss: 0.0215
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1223.4,                last time consumption/overall running time: 308.2585s / 5299.4591 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0214
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0224
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1141.0,                last time consumption/overall running time: 285.2870s / 5584.7461 s
env0_first_0:                 episode reward: -7.3000,                 loss: 0.0218
env0_second_0:                 episode reward: 7.3000,                 loss: 0.0222
env1_first_0:                 episode reward: -8.0500,                 loss: nan
env1_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1162.0,                last time consumption/overall running time: 282.0939s / 5866.8400 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0202
env0_second_0:                 episode reward: 7.7000,                 loss: 0.0218
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1126.35,                last time consumption/overall running time: 287.6878s / 6154.5278 s
env0_first_0:                 episode reward: -8.0500,                 loss: 0.0199
env0_second_0:                 episode reward: 8.0500,                 loss: 0.0207
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1199.5,                last time consumption/overall running time: 298.0641s / 6452.5919 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0200
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0204
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1097.3,                last time consumption/overall running time: 270.4198s / 6723.0118 s
env0_first_0:                 episode reward: -8.4000,                 loss: 0.0191
env0_second_0:                 episode reward: 8.4000,                 loss: 0.0184
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1139.6,                last time consumption/overall running time: 284.8872s / 7007.8989 s
env0_first_0:                 episode reward: -8.4000,                 loss: 0.0176
env0_second_0:                 episode reward: 8.4000,                 loss: 0.0160
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1116.55,                last time consumption/overall running time: 280.3054s / 7288.2044 s
env0_first_0:                 episode reward: -8.0500,                 loss: 0.0170
env0_second_0:                 episode reward: 8.0500,                 loss: 0.0150
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1059.5,                last time consumption/overall running time: 261.4335s / 7549.6379 s
env0_first_0:                 episode reward: -8.1000,                 loss: 0.0153
env0_second_0:                 episode reward: 8.1000,                 loss: 0.0142
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1134.95,                last time consumption/overall running time: 285.0696s / 7834.7075 s
env0_first_0:                 episode reward: -6.7000,                 loss: 0.0154
env0_second_0:                 episode reward: 6.7000,                 loss: 0.0153
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1173.75,                last time consumption/overall running time: 291.3082s / 8126.0157 s
env0_first_0:                 episode reward: -7.3000,                 loss: 0.0173
env0_second_0:                 episode reward: 7.3000,                 loss: 0.0171
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1108.7,                last time consumption/overall running time: 275.5506s / 8401.5663 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0189
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0182
env1_first_0:                 episode reward: -8.7000,                 loss: nan
env1_second_0:                 episode reward: 8.7000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1144.8,                last time consumption/overall running time: 285.5042s / 8687.0705 s
env0_first_0:                 episode reward: -7.6000,                 loss: 0.0179
env0_second_0:                 episode reward: 7.6000,                 loss: 0.0207
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1105.05,                last time consumption/overall running time: 272.8371s / 8959.9076 s
env0_first_0:                 episode reward: -7.4000,                 loss: 0.0174
env0_second_0:                 episode reward: 7.4000,                 loss: 0.0201
env1_first_0:                 episode reward: -8.3500,                 loss: nan
env1_second_0:                 episode reward: 8.3500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1148.8,                last time consumption/overall running time: 286.5137s / 9246.4213 s
env0_first_0:                 episode reward: -7.7500,                 loss: 0.0172
env0_second_0:                 episode reward: 7.7500,                 loss: 0.0185
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1163.2,                last time consumption/overall running time: 291.4915s / 9537.9129 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0183
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0184
env1_first_0:                 episode reward: -7.9000,                 loss: nan
env1_second_0:                 episode reward: 7.9000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1099.8,                last time consumption/overall running time: 274.1634s / 9812.0763 s
env0_first_0:                 episode reward: -7.5500,                 loss: 0.0188
env0_second_0:                 episode reward: 7.5500,                 loss: 0.0179
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1124.35,                last time consumption/overall running time: 272.5835s / 10084.6598 s
env0_first_0:                 episode reward: -8.5000,                 loss: 0.0184
env0_second_0:                 episode reward: 8.5000,                 loss: 0.0184
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1096.15,                last time consumption/overall running time: 274.0176s / 10358.6774 s
env0_first_0:                 episode reward: -8.3500,                 loss: 0.0171
env0_second_0:                 episode reward: 8.3500,                 loss: 0.0171
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 1063.35,                last time consumption/overall running time: 266.1961s / 10624.8734 s
env0_first_0:                 episode reward: -8.5500,                 loss: 0.0143
env0_second_0:                 episode reward: 8.5500,                 loss: 0.0161
env1_first_0:                 episode reward: -8.3000,                 loss: nan
env1_second_0:                 episode reward: 8.3000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1125.9,                last time consumption/overall running time: 277.7689s / 10902.6424 s
env0_first_0:                 episode reward: -8.4000,                 loss: 0.0135
env0_second_0:                 episode reward: 8.4000,                 loss: 0.0152
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1116.6,                last time consumption/overall running time: 273.9422s / 11176.5845 s
env0_first_0:                 episode reward: -8.3500,                 loss: 0.0131
env0_second_0:                 episode reward: 8.3500,                 loss: 0.0135
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1146.15,                last time consumption/overall running time: 283.6363s / 11460.2209 s
env0_first_0:                 episode reward: -8.3000,                 loss: 0.0140
env0_second_0:                 episode reward: 8.3000,                 loss: 0.0155
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1126.9,                last time consumption/overall running time: 276.6018s / 11736.8226 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0158
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0172
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1152.6,                last time consumption/overall running time: 279.6951s / 12016.5178 s
env0_first_0:                 episode reward: -7.6000,                 loss: 0.0178
env0_second_0:                 episode reward: 7.6000,                 loss: 0.0190
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1115.85,                last time consumption/overall running time: 274.6807s / 12291.1984 s
env0_first_0:                 episode reward: -8.4500,                 loss: 0.0192
env0_second_0:                 episode reward: 8.4500,                 loss: 0.0204
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1061.6,                last time consumption/overall running time: 265.0688s / 12556.2672 s
env0_first_0:                 episode reward: -8.0000,                 loss: 0.0189
env0_second_0:                 episode reward: 8.0000,                 loss: 0.0201
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1082.75,                last time consumption/overall running time: 271.0375s / 12827.3047 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0180
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0176
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1162.0,                last time consumption/overall running time: 291.0704s / 13118.3751 s
env0_first_0:                 episode reward: -8.0500,                 loss: 0.0163
env0_second_0:                 episode reward: 8.0500,                 loss: 0.0166
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1188.3,                last time consumption/overall running time: 299.0308s / 13417.4059 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0148
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0148
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1167.55,                last time consumption/overall running time: 289.9579s / 13707.3639 s
env0_first_0:                 episode reward: -7.2000,                 loss: 0.0159
env0_second_0:                 episode reward: 7.2000,                 loss: 0.0157
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1128.5,                last time consumption/overall running time: 278.6318s / 13985.9957 s
env0_first_0:                 episode reward: -8.1000,                 loss: 0.0165
env0_second_0:                 episode reward: 8.1000,                 loss: 0.0167
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1323.8,                last time consumption/overall running time: 330.4958s / 14316.4914 s
env0_first_0:                 episode reward: -4.5500,                 loss: 0.0191
env0_second_0:                 episode reward: 4.5500,                 loss: 0.0202
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1249.15,                last time consumption/overall running time: 303.1368s / 14619.6282 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0241
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0251
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1181.8,                last time consumption/overall running time: 286.3549s / 14905.9831 s
env0_first_0:                 episode reward: -6.8000,                 loss: 0.0237
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0258
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1147.95,                last time consumption/overall running time: 285.7694s / 15191.7525 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0233
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0238
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1130.55,                last time consumption/overall running time: 273.4380s / 15465.1904 s
env0_first_0:                 episode reward: -8.7000,                 loss: 0.0217
env0_second_0:                 episode reward: 8.7000,                 loss: 0.0210
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1143.7,                last time consumption/overall running time: 275.9573s / 15741.1478 s
env0_first_0:                 episode reward: -8.1500,                 loss: 0.0193
env0_second_0:                 episode reward: 8.1500,                 loss: 0.0173
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1110.05,                last time consumption/overall running time: 273.7199s / 16014.8676 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0168
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0159
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1115.75,                last time consumption/overall running time: 270.5957s / 16285.4634 s
env0_first_0:                 episode reward: -7.8500,                 loss: 0.0154
env0_second_0:                 episode reward: 7.8500,                 loss: 0.0161
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1101.15,                last time consumption/overall running time: 275.0883s / 16560.5516 s
env0_first_0:                 episode reward: -8.0000,                 loss: 0.0156
env0_second_0:                 episode reward: 8.0000,                 loss: 0.0161
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1154.8,                last time consumption/overall running time: 290.2726s / 16850.8242 s
env0_first_0:                 episode reward: -7.1000,                 loss: 0.0168
env0_second_0:                 episode reward: 7.1000,                 loss: 0.0160
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1098.0,                last time consumption/overall running time: 269.1376s / 17119.9619 s
env0_first_0:                 episode reward: -9.0500,                 loss: 0.0167
env0_second_0:                 episode reward: 9.0500,                 loss: 0.0151
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1156.85,                last time consumption/overall running time: 286.9618s / 17406.9236 s
env0_first_0:                 episode reward: -7.8500,                 loss: 0.0176
env0_second_0:                 episode reward: 7.8500,                 loss: 0.0156
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1103.25,                last time consumption/overall running time: 269.7297s / 17676.6533 s
env0_first_0:                 episode reward: -8.6500,                 loss: 0.0164
env0_second_0:                 episode reward: 8.6500,                 loss: 0.0162
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1189.25,                last time consumption/overall running time: 290.9899s / 17967.6432 s
env0_first_0:                 episode reward: -7.7500,                 loss: 0.0161
env0_second_0:                 episode reward: 7.7500,                 loss: 0.0157
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1108.0,                last time consumption/overall running time: 268.2821s / 18235.9253 s
env0_first_0:                 episode reward: -8.4500,                 loss: 0.0163
env0_second_0:                 episode reward: 8.4500,                 loss: 0.0157
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1118.5,                last time consumption/overall running time: 274.1551s / 18510.0804 s
env0_first_0:                 episode reward: -7.3000,                 loss: 0.0167
env0_second_0:                 episode reward: 7.3000,                 loss: 0.0156
env1_first_0:                 episode reward: -8.2500,                 loss: nan
env1_second_0:                 episode reward: 8.2500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1142.75,                last time consumption/overall running time: 279.4857s / 18789.5661 s
env0_first_0:                 episode reward: -7.6500,                 loss: 0.0163
env0_second_0:                 episode reward: 7.6500,                 loss: 0.0166
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1167.8,                last time consumption/overall running time: 288.9508s / 19078.5169 s
env0_first_0:                 episode reward: -7.6500,                 loss: 0.0167
env0_second_0:                 episode reward: 7.6500,                 loss: 0.0172
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1101.25,                last time consumption/overall running time: 267.8668s / 19346.3837 s
env0_first_0:                 episode reward: -8.3500,                 loss: 0.0182
env0_second_0:                 episode reward: 8.3500,                 loss: 0.0171
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1140.0,                last time consumption/overall running time: 285.2610s / 19631.6447 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0178
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0179
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1163.55,                last time consumption/overall running time: 290.7005s / 19922.3452 s
env0_first_0:                 episode reward: -8.1000,                 loss: 0.0171
env0_second_0:                 episode reward: 8.1000,                 loss: 0.0180
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 1191.55,                last time consumption/overall running time: 291.7038s / 20214.0489 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.0171
env0_second_0:                 episode reward: 7.1500,                 loss: 0.0177
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1148.8,                last time consumption/overall running time: 285.9039s / 20499.9528 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0171
env0_second_0:                 episode reward: 7.7000,                 loss: 0.0176
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1160.25,                last time consumption/overall running time: 284.5684s / 20784.5212 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.0173
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0185
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1095.0,                last time consumption/overall running time: 275.9700s / 21060.4912 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0177
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0175
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1087.15,                last time consumption/overall running time: 267.4861s / 21327.9773 s
env0_first_0:                 episode reward: -8.2000,                 loss: 0.0174
env0_second_0:                 episode reward: 8.2000,                 loss: 0.0169
env1_first_0:                 episode reward: -7.9000,                 loss: nan
env1_second_0:                 episode reward: 7.9000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1088.6,                last time consumption/overall running time: 272.9699s / 21600.9472 s
env0_first_0:                 episode reward: -8.2000,                 loss: 0.0158
env0_second_0:                 episode reward: 8.2000,                 loss: 0.0173
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1125.95,                last time consumption/overall running time: 282.1805s / 21883.1277 s
env0_first_0:                 episode reward: -8.5500,                 loss: 0.0139
env0_second_0:                 episode reward: 8.5500,                 loss: 0.0149
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1113.0,                last time consumption/overall running time: 275.3506s / 22158.4784 s
env0_first_0:                 episode reward: -7.1000,                 loss: 0.0135
env0_second_0:                 episode reward: 7.1000,                 loss: 0.0140
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1175.95,                last time consumption/overall running time: 295.4867s / 22453.9651 s
env0_first_0:                 episode reward: -7.8500,                 loss: 0.0149
env0_second_0:                 episode reward: 7.8500,                 loss: 0.0147
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1124.8,                last time consumption/overall running time: 276.2172s / 22730.1823 s
env0_first_0:                 episode reward: -8.4000,                 loss: 0.0153
env0_second_0:                 episode reward: 8.4000,                 loss: 0.0158
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1148.5,                last time consumption/overall running time: 288.3049s / 23018.4872 s
env0_first_0:                 episode reward: -7.6000,                 loss: 0.0154
env0_second_0:                 episode reward: 7.6000,                 loss: 0.0164
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1104.35,                last time consumption/overall running time: 277.5259s / 23296.0132 s
env0_first_0:                 episode reward: -7.6000,                 loss: 0.0160
env0_second_0:                 episode reward: 7.6000,                 loss: 0.0156
env1_first_0:                 episode reward: -7.9000,                 loss: nan
env1_second_0:                 episode reward: 7.9000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1077.15,                last time consumption/overall running time: 263.5832s / 23559.5964 s
env0_first_0:                 episode reward: -8.3500,                 loss: 0.0155
env0_second_0:                 episode reward: 8.3500,                 loss: 0.0160
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1172.3,                last time consumption/overall running time: 289.6971s / 23849.2935 s
env0_first_0:                 episode reward: -8.5000,                 loss: 0.0157
env0_second_0:                 episode reward: 8.5000,                 loss: 0.0150
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1182.7,                last time consumption/overall running time: 286.2471s / 24135.5406 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0162
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0168
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1183.5,                last time consumption/overall running time: 289.5089s / 24425.0496 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0153
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0163
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1106.55,                last time consumption/overall running time: 273.0225s / 24698.0720 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0150
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0154
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1141.55,                last time consumption/overall running time: 278.8363s / 24976.9083 s
env0_first_0:                 episode reward: -7.6000,                 loss: 0.0157
env0_second_0:                 episode reward: 7.6000,                 loss: 0.0152
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1153.05,                last time consumption/overall running time: 287.1301s / 25264.0385 s
env0_first_0:                 episode reward: -8.0000,                 loss: 0.0161
env0_second_0:                 episode reward: 8.0000,                 loss: 0.0156
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1153.1,                last time consumption/overall running time: 280.1298s / 25544.1683 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0157
env0_second_0:                 episode reward: 7.7000,                 loss: 0.0153
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1078.3,                last time consumption/overall running time: 261.7345s / 25805.9029 s
env0_first_0:                 episode reward: -7.9000,                 loss: 0.0160
env0_second_0:                 episode reward: 7.9000,                 loss: 0.0147
env1_first_0:                 episode reward: -8.0500,                 loss: nan
env1_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1106.3,                last time consumption/overall running time: 270.7231s / 26076.6259 s
env0_first_0:                 episode reward: -6.9000,                 loss: 0.0167
env0_second_0:                 episode reward: 6.9000,                 loss: 0.0151
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1175.45,                last time consumption/overall running time: 288.4097s / 26365.0356 s
env0_first_0:                 episode reward: -7.5000,                 loss: 0.0163
env0_second_0:                 episode reward: 7.5000,                 loss: 0.0165
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1135.5,                last time consumption/overall running time: 276.1377s / 26641.1733 s
env0_first_0:                 episode reward: -7.5500,                 loss: 0.0171
env0_second_0:                 episode reward: 7.5500,                 loss: 0.0169
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1181.75,                last time consumption/overall running time: 286.8351s / 26928.0085 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0174
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0169
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 1181.2,                last time consumption/overall running time: 288.3662s / 27216.3746 s
env0_first_0:                 episode reward: -7.7500,                 loss: 0.0172
env0_second_0:                 episode reward: 7.7500,                 loss: 0.0177
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 1129.25,                last time consumption/overall running time: 275.7097s / 27492.0843 s
env0_first_0:                 episode reward: -7.6000,                 loss: 0.0172
env0_second_0:                 episode reward: 7.6000,                 loss: 0.0173
env1_first_0:                 episode reward: -8.3500,                 loss: nan
env1_second_0:                 episode reward: 8.3500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 1177.15,                last time consumption/overall running time: 287.4724s / 27779.5568 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0167
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0165
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1135.7,                last time consumption/overall running time: 279.4523s / 28059.0090 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0176
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0166
env1_first_0:                 episode reward: -8.0500,                 loss: nan
env1_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1150.7,                last time consumption/overall running time: 282.4766s / 28341.4857 s
env0_first_0:                 episode reward: -7.5000,                 loss: 0.0180
env0_second_0:                 episode reward: 7.5000,                 loss: 0.0169
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1155.05,                last time consumption/overall running time: 282.8379s / 28624.3236 s
env0_first_0:                 episode reward: -6.9500,                 loss: 0.0182
env0_second_0:                 episode reward: 6.9500,                 loss: 0.0169
env1_first_0:                 episode reward: -8.3000,                 loss: nan
env1_second_0:                 episode reward: 8.3000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1170.95,                last time consumption/overall running time: 285.3517s / 28909.6753 s
env0_first_0:                 episode reward: -6.9000,                 loss: 0.0183
env0_second_0:                 episode reward: 6.9000,                 loss: 0.0171
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 1191.15,                last time consumption/overall running time: 291.7763s / 29201.4515 s
env0_first_0:                 episode reward: -7.2000,                 loss: 0.0177
env0_second_0:                 episode reward: 7.2000,                 loss: 0.0172
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 1128.65,                last time consumption/overall running time: 277.8212s / 29479.2727 s
env0_first_0:                 episode reward: -7.9000,                 loss: 0.0178
env0_second_0:                 episode reward: 7.9000,                 loss: 0.0174
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 1138.75,                last time consumption/overall running time: 280.7364s / 29760.0092 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0181
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0187
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 1227.15,                last time consumption/overall running time: 300.6710s / 30060.6802 s
env0_first_0:                 episode reward: -6.9000,                 loss: 0.0180
env0_second_0:                 episode reward: 6.9000,                 loss: 0.0186
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1145.9,                last time consumption/overall running time: 280.8663s / 30341.5465 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0178
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0192
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1184.0,                last time consumption/overall running time: 287.1531s / 30628.6996 s
env0_first_0:                 episode reward: -8.1000,                 loss: 0.0180
env0_second_0:                 episode reward: 8.1000,                 loss: 0.0193
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 1204.05,                last time consumption/overall running time: 298.5155s / 30927.2151 s
env0_first_0:                 episode reward: -7.7500,                 loss: 0.0178
env0_second_0:                 episode reward: 7.7500,                 loss: 0.0193
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1147.1,                last time consumption/overall running time: 288.2881s / 31215.5032 s
env0_first_0:                 episode reward: -8.4000,                 loss: 0.0176
env0_second_0:                 episode reward: 8.4000,                 loss: 0.0182
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 1158.65,                last time consumption/overall running time: 288.0932s / 31503.5964 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0177
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0176
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 1196.1,                last time consumption/overall running time: 291.1964s / 31794.7928 s
env0_first_0:                 episode reward: -7.3000,                 loss: 0.0177
env0_second_0:                 episode reward: 7.3000,                 loss: 0.0178
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1158.75,                last time consumption/overall running time: 284.4578s / 32079.2506 s
env0_first_0:                 episode reward: -8.2500,                 loss: 0.0181
env0_second_0:                 episode reward: 8.2500,                 loss: 0.0173
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 1174.55,                last time consumption/overall running time: 281.5628s / 32360.8134 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0183
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0157
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 1145.65,                last time consumption/overall running time: 278.0517s / 32638.8651 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0187
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0155
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 1140.6,                last time consumption/overall running time: 276.7823s / 32915.6474 s
env0_first_0:                 episode reward: -7.3500,                 loss: 0.0175
env0_second_0:                 episode reward: 7.3500,                 loss: 0.0163
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1140.85,                last time consumption/overall running time: 281.6102s / 33197.2576 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0163
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0161
env1_first_0:                 episode reward: -8.0500,                 loss: nan
env1_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 1125.15,                last time consumption/overall running time: 278.7860s / 33476.0436 s
env0_first_0:                 episode reward: -7.6000,                 loss: 0.0163
env0_second_0:                 episode reward: 7.6000,                 loss: 0.0158
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 1183.6,                last time consumption/overall running time: 293.4003s / 33769.4439 s
env0_first_0:                 episode reward: -7.5500,                 loss: 0.0164
env0_second_0:                 episode reward: 7.5500,                 loss: 0.0167
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 1188.05,                last time consumption/overall running time: 296.3426s / 34065.7864 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0163
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0177
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 1116.7,                last time consumption/overall running time: 275.2594s / 34341.0458 s
env0_first_0:                 episode reward: -6.9000,                 loss: 0.0168
env0_second_0:                 episode reward: 6.9000,                 loss: 0.0173
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1137.7,                last time consumption/overall running time: 281.5198s / 34622.5656 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0172
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0176
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 1173.4,                last time consumption/overall running time: 286.0916s / 34908.6572 s
env0_first_0:                 episode reward: -7.9000,                 loss: 0.0187
env0_second_0:                 episode reward: 7.9000,                 loss: 0.0186
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 1172.2,                last time consumption/overall running time: 288.0831s / 35196.7403 s
env0_first_0:                 episode reward: -7.7500,                 loss: 0.0183
env0_second_0:                 episode reward: 7.7500,                 loss: 0.0184
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 1130.15,                last time consumption/overall running time: 275.9627s / 35472.7030 s
env0_first_0:                 episode reward: -7.9000,                 loss: 0.0174
env0_second_0:                 episode reward: 7.9000,                 loss: 0.0179
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 1155.35,                last time consumption/overall running time: 284.0034s / 35756.7065 s
env0_first_0:                 episode reward: -7.3500,                 loss: 0.0162
env0_second_0:                 episode reward: 7.3500,                 loss: 0.0174
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 1116.35,                last time consumption/overall running time: 272.0309s / 36028.7374 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0152
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0169
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1177.6,                last time consumption/overall running time: 287.8338s / 36316.5712 s
env0_first_0:                 episode reward: -7.4000,                 loss: 0.0151
env0_second_0:                 episode reward: 7.4000,                 loss: 0.0166
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 1123.25,                last time consumption/overall running time: 278.5286s / 36595.0998 s
env0_first_0:                 episode reward: -8.1500,                 loss: 0.0154
env0_second_0:                 episode reward: 8.1500,                 loss: 0.0161
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1107.0,                last time consumption/overall running time: 273.0382s / 36868.1380 s
env0_first_0:                 episode reward: -7.8500,                 loss: 0.0144
env0_second_0:                 episode reward: 7.8500,                 loss: 0.0157
env1_first_0:                 episode reward: -8.4500,                 loss: nan