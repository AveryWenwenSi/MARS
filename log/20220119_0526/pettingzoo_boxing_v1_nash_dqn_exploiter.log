pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 3}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0526/pettingzoo_boxing_v1_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0526/pettingzoo_boxing_v1_nash_dqn_exploiter.
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 69.4915s / 69.4915 s
env0_first_0:                 episode reward: 7.0000,                 loss: 0.0093
env0_second_0:                 episode reward: -7.0000,                 loss: 0.0103
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 1732.3071s / 1801.7987 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0216
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0212
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2076.9714s / 3878.7701 s
env0_first_0:                 episode reward: 2.1000,                 loss: 0.0248
env0_second_0:                 episode reward: -2.1000,                 loss: 0.0252
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2167.4035s / 6046.1735 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0232
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0225
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2210.5538s / 8256.7274 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0245
env0_second_0:                 episode reward: 2.2000,                 loss: 0.0246
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2209.1611s / 10465.8885 s
env0_first_0:                 episode reward: -8.8500,                 loss: 0.0287
env0_second_0:                 episode reward: 8.8500,                 loss: 0.0280
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2211.9448s / 12677.8333 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0419
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0391
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2215.6960s / 14893.5294 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0376
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0349
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2207.9891s / 17101.5185 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0311
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0310
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2202.8025s / 19304.3210 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0243
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0212
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2201.5421s / 21505.8631 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0200
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0179
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2213.5867s / 23719.4497 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0214
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0196
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2203.9018s / 25923.3515 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0203
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0204
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2211.6967s / 28135.0483 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0329
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0329
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1779.8,                last time consumption/overall running time: 2203.0133s / 30338.0616 s
env0_first_0:                 episode reward: -19.7500,                 loss: 0.0519
env0_second_0:                 episode reward: 19.7500,                 loss: 0.0489
env1_first_0:                 episode reward: -24.0500,                 loss: nan
env1_second_0:                 episode reward: 24.0500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1555.55,                last time consumption/overall running time: 1937.3282s / 32275.3898 s
env0_first_0:                 episode reward: -41.5000,                 loss: 0.0715
env0_second_0:                 episode reward: 41.5000,                 loss: 0.0644
env1_first_0:                 episode reward: -41.3500,                 loss: nan
env1_second_0:                 episode reward: 41.3500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1069.35,                last time consumption/overall running time: 1334.4998s / 33609.8896 s
env0_first_0:                 episode reward: -65.6000,                 loss: 0.0939
env0_second_0:                 episode reward: 65.6000,                 loss: 0.0827
env1_first_0:                 episode reward: -63.3500,                 loss: nan
env1_second_0:                 episode reward: 63.3500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1070.7,                last time consumption/overall running time: 1358.0252s / 34967.9147 s
env0_first_0:                 episode reward: -56.3000,                 loss: 0.1252
env0_second_0:                 episode reward: 56.3000,                 loss: 0.1038
env1_first_0:                 episode reward: -72.5500,                 loss: nan
env1_second_0:                 episode reward: 72.5500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 650.7,                last time consumption/overall running time: 810.3824s / 35778.2972 s
env0_first_0:                 episode reward: -77.2500,                 loss: 0.1483
env0_second_0:                 episode reward: 77.2500,                 loss: 0.1272
env1_first_0:                 episode reward: -70.2500,                 loss: nan
env1_second_0:                 episode reward: 70.2500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 663.05,                last time consumption/overall running time: 826.5415s / 36604.8387 s
env0_first_0:                 episode reward: -77.8500,                 loss: 0.1787
env0_second_0:                 episode reward: 77.8500,                 loss: 0.1612
env1_first_0:                 episode reward: -66.8500,                 loss: nan
env1_second_0:                 episode reward: 66.8500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 655.75,                last time consumption/overall running time: 869.4178s / 37474.2565 s
env0_first_0:                 episode reward: -72.7000,                 loss: 0.2299
env0_second_0:                 episode reward: 72.7000,                 loss: 0.2025
env1_first_0:                 episode reward: -69.9000,                 loss: nan
env1_second_0:                 episode reward: 69.9000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 358.65,                last time consumption/overall running time: 527.7998s / 38002.0562 s
env0_first_0:                 episode reward: -77.1000,                 loss: 0.2669
env0_second_0:                 episode reward: 77.1000,                 loss: 0.2362
env1_first_0:                 episode reward: -82.3500,                 loss: nan
env1_second_0:                 episode reward: 82.3500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 540.2,                last time consumption/overall running time: 796.5577s / 38798.6139 s
env0_first_0:                 episode reward: -77.8500,                 loss: 0.3088
env0_second_0:                 episode reward: 77.8500,                 loss: 0.2701
env1_first_0:                 episode reward: -78.9500,                 loss: nan
env1_second_0:                 episode reward: 78.9500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 479.45,                last time consumption/overall running time: 712.1986s / 39510.8126 s
env0_first_0:                 episode reward: -82.1500,                 loss: 0.3409
env0_second_0:                 episode reward: 82.1500,                 loss: 0.2856
env1_first_0:                 episode reward: -73.3500,                 loss: nan
env1_second_0:                 episode reward: 73.3500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 299.3,                last time consumption/overall running time: 443.6070s / 39954.4196 s
env0_first_0:                 episode reward: -85.7500,                 loss: 0.3582
env0_second_0:                 episode reward: 85.7500,                 loss: 0.2797
env1_first_0:                 episode reward: -78.9500,                 loss: nan
env1_second_0:                 episode reward: 78.9500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 277.7,                last time consumption/overall running time: 411.8934s / 40366.3130 s
env0_first_0:                 episode reward: -81.9000,                 loss: 0.3922
env0_second_0:                 episode reward: 81.9000,                 loss: 0.3039
env1_first_0:                 episode reward: -83.4000,                 loss: nan
env1_second_0:                 episode reward: 83.4000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 300.95,                last time consumption/overall running time: 447.1151s / 40813.4280 s
env0_first_0:                 episode reward: -82.7000,                 loss: 0.3608
env0_second_0:                 episode reward: 82.7000,                 loss: 0.2904
env1_first_0:                 episode reward: -89.9500,                 loss: nan
env1_second_0:                 episode reward: 89.9500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 348.4,                last time consumption/overall running time: 520.6528s / 41334.0808 s
env0_first_0:                 episode reward: -73.0500,                 loss: 0.4385
env0_second_0:                 episode reward: 73.0500,                 loss: 0.2995
env1_first_0:                 episode reward: -91.7000,                 loss: nan
env1_second_0:                 episode reward: 91.7000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 360.2,                last time consumption/overall running time: 538.9602s / 41873.0410 s
env0_first_0:                 episode reward: -81.1500,                 loss: 0.4643
env0_second_0:                 episode reward: 81.1500,                 loss: 0.3141
env1_first_0:                 episode reward: -83.9000,                 loss: nan
env1_second_0:                 episode reward: 83.9000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 320.0,                last time consumption/overall running time: 482.6801s / 42355.7211 s
env0_first_0:                 episode reward: -79.0500,                 loss: 0.4600
env0_second_0:                 episode reward: 79.0500,                 loss: 0.3028
env1_first_0:                 episode reward: -84.4000,                 loss: nan
env1_second_0:                 episode reward: 84.4000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 292.85,                last time consumption/overall running time: 433.0617s / 42788.7828 s
env0_first_0:                 episode reward: -88.1500,                 loss: 0.5000
env0_second_0:                 episode reward: 88.1500,                 loss: 0.3278
env1_first_0:                 episode reward: -69.5000,                 loss: nan
env1_second_0:                 episode reward: 69.5000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 263.15,                last time consumption/overall running time: 392.9687s / 43181.7516 s
env0_first_0:                 episode reward: -77.4500,                 loss: 0.5266
env0_second_0:                 episode reward: 77.4500,                 loss: 0.3434
env1_first_0:                 episode reward: -86.7000,                 loss: nan
env1_second_0:                 episode reward: 86.7000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 290.05,                last time consumption/overall running time: 434.7310s / 43616.4826 s
env0_first_0:                 episode reward: -80.5500,                 loss: 0.5559
env0_second_0:                 episode reward: 80.5500,                 loss: 0.3548
env1_first_0:                 episode reward: -84.1500,                 loss: nan
env1_second_0:                 episode reward: 84.1500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 269.45,                last time consumption/overall running time: 397.1398s / 44013.6223 s
env0_first_0:                 episode reward: -75.5500,                 loss: 0.5820
env0_second_0:                 episode reward: 75.5500,                 loss: 0.3806
env1_first_0:                 episode reward: -86.9000,                 loss: nan
env1_second_0:                 episode reward: 86.9000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 309.35,                last time consumption/overall running time: 456.2650s / 44469.8874 s
env0_first_0:                 episode reward: -80.4000,                 loss: 0.6211
env0_second_0:                 episode reward: 80.4000,                 loss: 0.3831
env1_first_0:                 episode reward: -68.8500,                 loss: nan
env1_second_0:                 episode reward: 68.8500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 259.6,                last time consumption/overall running time: 385.4256s / 44855.3129 s
env0_first_0:                 episode reward: -80.3500,                 loss: 0.6746
env0_second_0:                 episode reward: 80.3500,                 loss: 0.4362
env1_first_0:                 episode reward: -89.0000,                 loss: nan
env1_second_0:                 episode reward: 89.0000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 249.55,                last time consumption/overall running time: 372.2430s / 45227.5559 s
env0_first_0:                 episode reward: -84.7000,                 loss: 0.6438
env0_second_0:                 episode reward: 84.7000,                 loss: 0.4108
env1_first_0:                 episode reward: -82.6000,                 loss: nan
env1_second_0:                 episode reward: 82.6000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 243.95,                last time consumption/overall running time: 363.3620s / 45590.9180 s
env0_first_0:                 episode reward: -97.6000,                 loss: 0.6405
env0_second_0:                 episode reward: 97.6000,                 loss: 0.4540
env1_first_0:                 episode reward: -82.7000,                 loss: nan
env1_second_0:                 episode reward: 82.7000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 240.6,                last time consumption/overall running time: 355.4797s / 45946.3977 s
env0_first_0:                 episode reward: -89.0500,                 loss: 0.6034
env0_second_0:                 episode reward: 89.0500,                 loss: 0.4280
env1_first_0:                 episode reward: -90.5000,                 loss: nan
env1_second_0:                 episode reward: 90.5000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 261.7,                last time consumption/overall running time: 391.4736s / 46337.8713 s
env0_first_0:                 episode reward: -83.8500,                 loss: 0.6535
env0_second_0:                 episode reward: 83.8500,                 loss: 0.4483
env1_first_0:                 episode reward: -87.8000,                 loss: nan
env1_second_0:                 episode reward: 87.8000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 267.75,                last time consumption/overall running time: 399.2240s / 46737.0953 s
env0_first_0:                 episode reward: -84.9000,                 loss: 0.6177
env0_second_0:                 episode reward: 84.9000,                 loss: 0.4359
env1_first_0:                 episode reward: -81.3500,                 loss: nan
env1_second_0:                 episode reward: 81.3500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 254.7,                last time consumption/overall running time: 378.2539s / 47115.3492 s
env0_first_0:                 episode reward: -84.7000,                 loss: 0.6321
env0_second_0:                 episode reward: 84.7000,                 loss: 0.4889
env1_first_0:                 episode reward: -89.8500,                 loss: nan
env1_second_0:                 episode reward: 89.8500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 239.75,                last time consumption/overall running time: 354.0585s / 47469.4078 s
env0_first_0:                 episode reward: -90.8000,                 loss: 0.6079
env0_second_0:                 episode reward: 90.8000,                 loss: 0.4788
env1_first_0:                 episode reward: -88.2500,                 loss: nan
env1_second_0:                 episode reward: 88.2500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 235.0,                last time consumption/overall running time: 349.7827s / 47819.1904 s
env0_first_0:                 episode reward: -87.8000,                 loss: 0.6029
env0_second_0:                 episode reward: 87.8000,                 loss: 0.5024
env1_first_0:                 episode reward: -89.5000,                 loss: nan
env1_second_0:                 episode reward: 89.5000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 234.5,                last time consumption/overall running time: 349.8981s / 48169.0885 s
env0_first_0:                 episode reward: -91.7500,                 loss: 0.5813
env0_second_0:                 episode reward: 91.7500,                 loss: 0.4717
env1_first_0:                 episode reward: -92.3500,                 loss: nan
env1_second_0:                 episode reward: 92.3500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 259.7,                last time consumption/overall running time: 383.3918s / 48552.4803 s
env0_first_0:                 episode reward: -88.3000,                 loss: 0.6335
env0_second_0:                 episode reward: 88.3000,                 loss: 0.4833
env1_first_0:                 episode reward: -84.0500,                 loss: nan
env1_second_0:                 episode reward: 84.0500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 282.35,                last time consumption/overall running time: 420.5029s / 48972.9832 s
env0_first_0:                 episode reward: -87.1000,                 loss: 0.6415
env0_second_0:                 episode reward: 87.1000,                 loss: 0.5129
env1_first_0:                 episode reward: -80.6000,                 loss: nan
env1_second_0:                 episode reward: 80.6000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 271.5,                last time consumption/overall running time: 402.1053s / 49375.0885 s
env0_first_0:                 episode reward: -80.6500,                 loss: 0.6664
env0_second_0:                 episode reward: 80.6500,                 loss: 0.5596
env1_first_0:                 episode reward: -89.1500,                 loss: nan
env1_second_0:                 episode reward: 89.1500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 274.55,                last time consumption/overall running time: 401.8546s / 49776.9432 s
env0_first_0:                 episode reward: -86.4000,                 loss: 0.6756
env0_second_0:                 episode reward: 86.4000,                 loss: 0.6131
env1_first_0:                 episode reward: -93.0500,                 loss: nan
env1_second_0:                 episode reward: 93.0500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 235.25,                last time consumption/overall running time: 349.9289s / 50126.8720 s
env0_first_0:                 episode reward: -93.0500,                 loss: 0.7206
env0_second_0:                 episode reward: 93.0500,                 loss: 0.6269
env1_first_0:                 episode reward: -91.8000,                 loss: nan
env1_second_0:                 episode reward: 91.8000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 234.85,                last time consumption/overall running time: 346.2486s / 50473.1206 s
env0_first_0:                 episode reward: -87.6500,                 loss: 0.6664
env0_second_0:                 episode reward: 87.6500,                 loss: 0.5731
env1_first_0:                 episode reward: -92.7500,                 loss: nan
env1_second_0:                 episode reward: 92.7500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 232.2,                last time consumption/overall running time: 344.8634s / 50817.9840 s
env0_first_0:                 episode reward: -92.6000,                 loss: 0.6672
env0_second_0:                 episode reward: 92.6000,                 loss: 0.6173
env1_first_0:                 episode reward: -87.5000,                 loss: nan
env1_second_0:                 episode reward: 87.5000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 240.75,                last time consumption/overall running time: 361.0815s / 51179.0655 s
env0_first_0:                 episode reward: -89.4500,                 loss: 0.6870
env0_second_0:                 episode reward: 89.4500,                 loss: 0.5651
env1_first_0:                 episode reward: -87.3500,                 loss: nan
env1_second_0:                 episode reward: 87.3500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 243.85,                last time consumption/overall running time: 362.3378s / 51541.4033 s
env0_first_0:                 episode reward: -86.3500,                 loss: 0.6607
env0_second_0:                 episode reward: 86.3500,                 loss: 0.6573
env1_first_0:                 episode reward: -95.4500,                 loss: nan
env1_second_0:                 episode reward: 95.4500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 238.7,                last time consumption/overall running time: 359.0137s / 51900.4170 s
env0_first_0:                 episode reward: -93.4500,                 loss: 0.6689
env0_second_0:                 episode reward: 93.4500,                 loss: 0.6267
env1_first_0:                 episode reward: -88.9000,                 loss: nan
env1_second_0:                 episode reward: 88.9000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 284.7,                last time consumption/overall running time: 423.5584s / 52323.9754 s
env0_first_0:                 episode reward: -82.8500,                 loss: 0.6530
env0_second_0:                 episode reward: 82.8500,                 loss: 0.7284
env1_first_0:                 episode reward: -81.3000,                 loss: nan
env1_second_0:                 episode reward: 81.3000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 245.85,                last time consumption/overall running time: 368.5174s / 52692.4928 s
env0_first_0:                 episode reward: -85.4000,                 loss: 0.6673
env0_second_0:                 episode reward: 85.4000,                 loss: 0.6988
env1_first_0:                 episode reward: -92.0000,                 loss: nan
env1_second_0:                 episode reward: 92.0000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 233.3,                last time consumption/overall running time: 347.1750s / 53039.6678 s
env0_first_0:                 episode reward: -94.1500,                 loss: 0.6209
env0_second_0:                 episode reward: 94.1500,                 loss: 0.7875
env1_first_0:                 episode reward: -90.9500,                 loss: nan
env1_second_0:                 episode reward: 90.9500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 225.55,                last time consumption/overall running time: 333.9433s / 53373.6110 s
env0_first_0:                 episode reward: -94.5000,                 loss: 0.5945
env0_second_0:                 episode reward: 94.5000,                 loss: 0.7202
env1_first_0:                 episode reward: -90.0500,                 loss: nan
env1_second_0:                 episode reward: 90.0500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 235.05,                last time consumption/overall running time: 349.5896s / 53723.2006 s
env0_first_0:                 episode reward: -93.2000,                 loss: 0.5819
env0_second_0:                 episode reward: 93.2000,                 loss: 0.7462
env1_first_0:                 episode reward: -92.1000,                 loss: nan
env1_second_0:                 episode reward: 92.1000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 229.85,                last time consumption/overall running time: 338.6566s / 54061.8572 s
env0_first_0:                 episode reward: -92.6500,                 loss: 0.5872
env0_second_0:                 episode reward: 92.6500,                 loss: 0.6980
env1_first_0:                 episode reward: -87.9500,                 loss: nan
env1_second_0:                 episode reward: 87.9500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 230.7,                last time consumption/overall running time: 344.7076s / 54406.5648 s
env0_first_0:                 episode reward: -89.6000,                 loss: 0.6261
env0_second_0:                 episode reward: 89.6000,                 loss: 0.7588
env1_first_0:                 episode reward: -94.2000,                 loss: nan
env1_second_0:                 episode reward: 94.2000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 229.9,                last time consumption/overall running time: 340.1897s / 54746.7545 s
env0_first_0:                 episode reward: -94.6000,                 loss: 0.5999
env0_second_0:                 episode reward: 94.6000,                 loss: 0.7394
env1_first_0:                 episode reward: -94.8000,                 loss: nan
env1_second_0:                 episode reward: 94.8000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 224.9,                last time consumption/overall running time: 333.0500s / 55079.8045 s
env0_first_0:                 episode reward: -90.9000,                 loss: 0.6166
env0_second_0:                 episode reward: 90.9000,                 loss: 0.7555
env1_first_0:                 episode reward: -90.5500,                 loss: nan
env1_second_0:                 episode reward: 90.5500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 221.6,                last time consumption/overall running time: 330.4949s / 55410.2994 s
env0_first_0:                 episode reward: -97.5500,                 loss: 0.6162
env0_second_0:                 episode reward: 97.5500,                 loss: 0.7373
env1_first_0:                 episode reward: -94.9500,                 loss: nan
env1_second_0:                 episode reward: 94.9500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 235.75,                last time consumption/overall running time: 343.9434s / 55754.2428 s
env0_first_0:                 episode reward: -87.8000,                 loss: 0.6355
env0_second_0:                 episode reward: 87.8000,                 loss: 0.7431
env1_first_0:                 episode reward: -90.1500,                 loss: nan
env1_second_0:                 episode reward: 90.1500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 228.45,                last time consumption/overall running time: 338.8163s / 56093.0591 s
env0_first_0:                 episode reward: -96.9500,                 loss: 0.5613
env0_second_0:                 episode reward: 96.9500,                 loss: 0.7462
env1_first_0:                 episode reward: -91.3000,                 loss: nan
env1_second_0:                 episode reward: 91.3000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 230.7,                last time consumption/overall running time: 339.5941s / 56432.6532 s
env0_first_0:                 episode reward: -87.3000,                 loss: 0.5931
env0_second_0:                 episode reward: 87.3000,                 loss: 0.6889
env1_first_0:                 episode reward: -91.4000,                 loss: nan
env1_second_0:                 episode reward: 91.4000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 220.55,                last time consumption/overall running time: 326.6422s / 56759.2954 s
env0_first_0:                 episode reward: -90.6500,                 loss: 0.5529
env0_second_0:                 episode reward: 90.6500,                 loss: 0.7544
env1_first_0:                 episode reward: -91.2000,                 loss: nan
env1_second_0:                 episode reward: 91.2000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 227.9,                last time consumption/overall running time: 340.0875s / 57099.3829 s
env0_first_0:                 episode reward: -92.6500,                 loss: 0.5027
env0_second_0:                 episode reward: 92.6500,                 loss: 0.7489
env1_first_0:                 episode reward: -93.5500,                 loss: nan
env1_second_0:                 episode reward: 93.5500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 227.95,                last time consumption/overall running time: 340.9705s / 57440.3534 s
env0_first_0:                 episode reward: -93.2000,                 loss: 0.5283
env0_second_0:                 episode reward: 93.2000,                 loss: 0.7801
env1_first_0:                 episode reward: -96.2500,                 loss: nan
env1_second_0:                 episode reward: 96.2500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 219.0,                last time consumption/overall running time: 322.0272s / 57762.3806 s
env0_first_0:                 episode reward: -95.3000,                 loss: 0.5224
env0_second_0:                 episode reward: 95.3000,                 loss: 0.6924
env1_first_0:                 episode reward: -93.9000,                 loss: nan
env1_second_0:                 episode reward: 93.9000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 224.8,                last time consumption/overall running time: 334.3863s / 58096.7669 s
env0_first_0:                 episode reward: -92.4000,                 loss: 0.5042
env0_second_0:                 episode reward: 92.4000,                 loss: 0.6965
env1_first_0:                 episode reward: -94.9500,                 loss: nan
env1_second_0:                 episode reward: 94.9500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 219.15,                last time consumption/overall running time: 322.7680s / 58419.5349 s
env0_first_0:                 episode reward: -96.7000,                 loss: 0.4698
env0_second_0:                 episode reward: 96.7000,                 loss: 0.7279
env1_first_0:                 episode reward: -96.1000,                 loss: nan
env1_second_0:                 episode reward: 96.1000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 227.5,                last time consumption/overall running time: 336.6874s / 58756.2224 s
env0_first_0:                 episode reward: -88.8500,                 loss: 0.5229
env0_second_0:                 episode reward: 88.8500,                 loss: 0.7012
env1_first_0:                 episode reward: -92.3500,                 loss: nan
env1_second_0:                 episode reward: 92.3500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 233.3,                last time consumption/overall running time: 345.2425s / 59101.4649 s
env0_first_0:                 episode reward: -92.6500,                 loss: 0.5097
env0_second_0:                 episode reward: 92.6500,                 loss: 0.7488
env1_first_0:                 episode reward: -93.0500,                 loss: nan
env1_second_0:                 episode reward: 93.0500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 228.55,                last time consumption/overall running time: 339.1824s / 59440.6472 s
env0_first_0:                 episode reward: -83.3000,                 loss: 0.5293
env0_second_0:                 episode reward: 83.3000,                 loss: 0.7312
env1_first_0:                 episode reward: -94.8000,                 loss: nan
env1_second_0:                 episode reward: 94.8000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 263.1,                last time consumption/overall running time: 389.2082s / 59829.8554 s
env0_first_0:                 episode reward: -91.1500,                 loss: 0.4610
env0_second_0:                 episode reward: 91.1500,                 loss: 0.6965
env1_first_0:                 episode reward: -92.0000,                 loss: nan
env1_second_0:                 episode reward: 92.0000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 234.25,                last time consumption/overall running time: 346.7511s / 60176.6065 s
env0_first_0:                 episode reward: -87.8000,                 loss: 0.5083
env0_second_0:                 episode reward: 87.8000,                 loss: 0.7692
env1_first_0:                 episode reward: -92.3000,                 loss: nan
env1_second_0:                 episode reward: 92.3000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 224.4,                last time consumption/overall running time: 331.1766s / 60507.7831 s
env0_first_0:                 episode reward: -87.7000,                 loss: 0.5665
env0_second_0:                 episode reward: 87.7000,                 loss: 0.7300
env1_first_0:                 episode reward: -90.0500,                 loss: nan
env1_second_0:                 episode reward: 90.0500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 224.15,                last time consumption/overall running time: 336.2680s / 60844.0511 s
env0_first_0:                 episode reward: -95.5000,                 loss: 0.5611
env0_second_0:                 episode reward: 95.5000,                 loss: 0.7818
env1_first_0:                 episode reward: -93.5500,                 loss: nan
env1_second_0:                 episode reward: 93.5500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 226.8,                last time consumption/overall running time: 337.4320s / 61181.4831 s
env0_first_0:                 episode reward: -93.5500,                 loss: 0.5296
env0_second_0:                 episode reward: 93.5500,                 loss: 0.8312
env1_first_0:                 episode reward: -90.1000,                 loss: nan
env1_second_0:                 episode reward: 90.1000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 224.1,                last time consumption/overall running time: 330.6955s / 61512.1786 s
env0_first_0:                 episode reward: -94.2500,                 loss: 0.5456
env0_second_0:                 episode reward: 94.2500,                 loss: 0.8237
env1_first_0:                 episode reward: -92.7000,                 loss: nan
env1_second_0:                 episode reward: 92.7000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 223.5,                last time consumption/overall running time: 328.9353s / 61841.1138 s
env0_first_0:                 episode reward: -92.3500,                 loss: 0.5570
env0_second_0:                 episode reward: 92.3500,                 loss: 0.8001
env1_first_0:                 episode reward: -91.3000,                 loss: nan
env1_second_0:                 episode reward: 91.3000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 223.05,                last time consumption/overall running time: 332.6009s / 62173.7147 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.5461
env0_second_0:                 episode reward: 88.5500,                 loss: 0.8149
env1_first_0:                 episode reward: -92.9500,                 loss: nan
env1_second_0:                 episode reward: 92.9500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 233.15,                last time consumption/overall running time: 343.2523s / 62516.9670 s
env0_first_0:                 episode reward: -91.7000,                 loss: 0.5470
env0_second_0:                 episode reward: 91.7000,                 loss: 0.8077
env1_first_0:                 episode reward: -87.9000,                 loss: nan
env1_second_0:                 episode reward: 87.9000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 230.3,                last time consumption/overall running time: 341.3654s / 62858.3324 s
env0_first_0:                 episode reward: -88.6000,                 loss: 0.6082
env0_second_0:                 episode reward: 88.6000,                 loss: 0.8757
env1_first_0:                 episode reward: -90.2500,                 loss: nan
env1_second_0:                 episode reward: 90.2500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 226.3,                last time consumption/overall running time: 335.6201s / 63193.9526 s
env0_first_0:                 episode reward: -93.1500,                 loss: 0.5329
env0_second_0:                 episode reward: 93.1500,                 loss: 0.7428
env1_first_0:                 episode reward: -94.1500,                 loss: nan
env1_second_0:                 episode reward: 94.1500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 231.65,                last time consumption/overall running time: 344.9844s / 63538.9369 s
env0_first_0:                 episode reward: -91.3500,                 loss: 0.6062
env0_second_0:                 episode reward: 91.3500,                 loss: 0.8311
env1_first_0:                 episode reward: -90.8500,                 loss: nan
env1_second_0:                 episode reward: 90.8500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 237.05,                last time consumption/overall running time: 351.9430s / 63890.8799 s
env0_first_0:                 episode reward: -88.9500,                 loss: 0.5803
env0_second_0:                 episode reward: 88.9500,                 loss: 0.8281
env1_first_0:                 episode reward: -92.3000,                 loss: nan
env1_second_0:                 episode reward: 92.3000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 232.65,                last time consumption/overall running time: 345.8214s / 64236.7013 s
env0_first_0:                 episode reward: -95.4000,                 loss: 0.5643
env0_second_0:                 episode reward: 95.4000,                 loss: 0.8576
env1_first_0:                 episode reward: -87.3500,                 loss: nan
env1_second_0:                 episode reward: 87.3500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 226.55,                last time consumption/overall running time: 337.3942s / 64574.0955 s
env0_first_0:                 episode reward: -96.3000,                 loss: 0.5080
env0_second_0:                 episode reward: 96.3000,                 loss: 0.8305
env1_first_0:                 episode reward: -93.1500,                 loss: nan
env1_second_0:                 episode reward: 93.1500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 225.2,                last time consumption/overall running time: 330.4783s / 64904.5738 s
env0_first_0:                 episode reward: -93.1000,                 loss: 0.5552
env0_second_0:                 episode reward: 93.1000,                 loss: 0.8137
env1_first_0:                 episode reward: -94.9000,                 loss: nan
env1_second_0:                 episode reward: 94.9000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 227.1,                last time consumption/overall running time: 333.2755s / 65237.8493 s
env0_first_0:                 episode reward: -93.5500,                 loss: 0.5139
env0_second_0:                 episode reward: 93.5500,                 loss: 0.8229
env1_first_0:                 episode reward: -95.8500,                 loss: nan
env1_second_0:                 episode reward: 95.8500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 224.45,                last time consumption/overall running time: 334.2650s / 65572.1143 s
env0_first_0:                 episode reward: -97.3000,                 loss: 0.5230
env0_second_0:                 episode reward: 97.3000,                 loss: 0.8033
env1_first_0:                 episode reward: -92.4500,                 loss: nan
env1_second_0:                 episode reward: 92.4500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 226.25,                last time consumption/overall running time: 334.3395s / 65906.4538 s
env0_first_0:                 episode reward: -95.1000,                 loss: 0.5223
env0_second_0:                 episode reward: 95.1000,                 loss: 0.8019
env1_first_0:                 episode reward: -87.2000,                 loss: nan
env1_second_0:                 episode reward: 87.2000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 239.1,                last time consumption/overall running time: 349.0789s / 66255.5327 s
env0_first_0:                 episode reward: -90.2500,                 loss: 0.5786
env0_second_0:                 episode reward: 90.2500,                 loss: 0.8477
env1_first_0:                 episode reward: -89.6500,                 loss: nan
env1_second_0:                 episode reward: 89.6500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 231.8,                last time consumption/overall running time: 342.8273s / 66598.3600 s
env0_first_0:                 episode reward: -93.3500,                 loss: 0.5937