pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 10000, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nfsp', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'eta': 0.1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0526/pettingzoo_tennis_v2_nfsp. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0526/pettingzoo_tennis_v2_nfsp.
Episode: 1/10000 (0.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 57.9219s / 57.9219 s
env0_first_0:                 episode reward: -208.0000,                 loss: nan
env0_second_0:                 episode reward: 208.0000,                 loss: nan
env1_first_0:                 episode reward: -87.0000,                 loss: nan
env1_second_0:                 episode reward: 87.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 3741.2,                last time consumption/overall running time: 2744.9647s / 2802.8866 s
env0_first_0:                 episode reward: -12.1000,                 loss: 0.0079
env0_second_0:                 episode reward: 12.1000,                 loss: 0.0065
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 6158.65,                last time consumption/overall running time: 4659.9545s / 7462.8411 s
env0_first_0:                 episode reward: -28.7500,                 loss: 0.0067
env0_second_0:                 episode reward: 28.7500,                 loss: 0.0066
env1_first_0:                 episode reward: -25.3000,                 loss: nan
env1_second_0:                 episode reward: 25.3000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 8465.8,                last time consumption/overall running time: 6412.5617s / 13875.4028 s
env0_first_0:                 episode reward: 22.1500,                 loss: 0.0045
env0_second_0:                 episode reward: -22.1500,                 loss: 0.0045
env1_first_0:                 episode reward: 15.0500,                 loss: nan
env1_second_0:                 episode reward: -15.0500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 8440.25,                last time consumption/overall running time: 6403.8870s / 20279.2898 s
env0_first_0:                 episode reward: 11.4500,                 loss: 0.0050
env0_second_0:                 episode reward: -11.4500,                 loss: 0.0054
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 8418.4,                last time consumption/overall running time: 6377.7863s / 26657.0762 s
env0_first_0:                 episode reward: 17.5000,                 loss: 0.0052
env0_second_0:                 episode reward: -17.5000,                 loss: 0.0051
env1_first_0:                 episode reward: 8.1000,                 loss: nan
env1_second_0:                 episode reward: -8.1000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 8077.9,                last time consumption/overall running time: 6124.4387s / 32781.5149 s
env0_first_0:                 episode reward: -8.3000,                 loss: 0.0054
env0_second_0:                 episode reward: 8.3000,                 loss: 0.0050
env1_first_0:                 episode reward: -9.4500,                 loss: nan
env1_second_0:                 episode reward: 9.4500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 7723.05,                last time consumption/overall running time: 6135.6129s / 38917.1278 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0054
env0_second_0:                 episode reward: -5.3500,                 loss: 0.0047
env1_first_0:                 episode reward: 18.1500,                 loss: nan
env1_second_0:                 episode reward: -18.1500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 8497.85,                last time consumption/overall running time: 7676.2946s / 46593.4224 s
env0_first_0:                 episode reward: -56.7500,                 loss: 0.0084
env0_second_0:                 episode reward: 56.7500,                 loss: 0.0074
env1_first_0:                 episode reward: -45.7500,                 loss: nan
env1_second_0:                 episode reward: 45.7500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 9003.5,                last time consumption/overall running time: 8119.8510s / 54713.2733 s
env0_first_0:                 episode reward: -19.4500,                 loss: 0.0106
env0_second_0:                 episode reward: 19.4500,                 loss: 0.0084
env1_first_0:                 episode reward: -37.9500,                 loss: nan
env1_second_0:                 episode reward: 37.9500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 8201.0,                last time consumption/overall running time: 7393.4715s / 62106.7448 s
env0_first_0:                 episode reward: -17.7000,                 loss: 0.0077
env0_second_0:                 episode reward: 17.7000,                 loss: 0.0079
env1_first_0:                 episode reward: -20.8500,                 loss: nan
env1_second_0:                 episode reward: 20.8500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 8952.7,                last time consumption/overall running time: 8067.0768s / 70173.8217 s
env0_first_0:                 episode reward: -12.2000,                 loss: 0.0077
env0_second_0:                 episode reward: 12.2000,                 loss: 0.0080
env1_first_0:                 episode reward: -50.8500,                 loss: nan
env1_second_0:                 episode reward: 50.8500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 8580.65,                last time consumption/overall running time: 7736.4601s / 77910.2817 s
env0_first_0:                 episode reward: -30.6000,                 loss: 0.0088
env0_second_0:                 episode reward: 30.6000,                 loss: 0.0084
env1_first_0:                 episode reward: -29.3000,                 loss: nan
env1_second_0:                 episode reward: 29.3000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 8187.3,                last time consumption/overall running time: 7376.1488s / 85286.4305 s
env0_first_0:                 episode reward: 18.4000,                 loss: 0.0092
env0_second_0:                 episode reward: -18.4000,                 loss: 0.0090
env1_first_0:                 episode reward: 22.3500,                 loss: nan
env1_second_0:                 episode reward: -22.3500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 9356.65,                last time consumption/overall running time: 8427.8518s / 93714.2823 s
env0_first_0:                 episode reward: -25.9500,                 loss: 0.0069
env0_second_0:                 episode reward: 25.9500,                 loss: 0.0081
env1_first_0:                 episode reward: -24.2000,                 loss: nan
env1_second_0:                 episode reward: 24.2000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 9125.45,                last time consumption/overall running time: 8184.3524s / 101898.6346 s
env0_first_0:                 episode reward: 45.3500,                 loss: 0.0094
env0_second_0:                 episode reward: -45.3500,                 loss: 0.0105
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 8594.45,                last time consumption/overall running time: 7708.6971s / 109607.3317 s
env0_first_0:                 episode reward: 8.3500,                 loss: 0.0085
env0_second_0:                 episode reward: -8.3500,                 loss: 0.0085
env1_first_0:                 episode reward: 10.2000,                 loss: nan
env1_second_0:                 episode reward: -10.2000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 8793.15,                last time consumption/overall running time: 7892.9869s / 117500.3186 s
env0_first_0:                 episode reward: 17.5000,                 loss: 0.0078
env0_second_0:                 episode reward: -17.5000,                 loss: 0.0086
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 9643.7,                last time consumption/overall running time: 8653.2482s / 126153.5668 s
env0_first_0:                 episode reward: 46.5000,                 loss: 0.0121
env0_second_0:                 episode reward: -46.5000,                 loss: 0.0093
env1_first_0:                 episode reward: 48.2500,                 loss: nan
env1_second_0:                 episode reward: -48.2500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 8722.85,                last time consumption/overall running time: 7829.4190s / 133982.9859 s
env0_first_0:                 episode reward: -38.5000,                 loss: 0.0129
env0_second_0:                 episode reward: 38.5000,                 loss: 0.0124
env1_first_0:                 episode reward: -34.7500,                 loss: nan
env1_second_0:                 episode reward: 34.7500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 9197.35,                last time consumption/overall running time: 8244.9907s / 142227.9766 s
env0_first_0:                 episode reward: -58.6000,                 loss: 0.0093
env0_second_0:                 episode reward: 58.6000,                 loss: 0.0105
env1_first_0:                 episode reward: -42.7000,                 loss: nan
env1_second_0:                 episode reward: 42.7000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 8695.15,                last time consumption/overall running time: 7146.0881s / 149374.0647 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0100
env0_second_0:                 episode reward: -4.1500,                 loss: 0.0123
env1_first_0:                 episode reward: -16.8000,                 loss: nan
env1_second_0:                 episode reward: 16.8000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 9465.75,                last time consumption/overall running time: 7300.8439s / 156674.9085 s
env0_first_0:                 episode reward: 7.4000,                 loss: 0.0074
env0_second_0:                 episode reward: -7.4000,                 loss: 0.0074
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 9162.6,                last time consumption/overall running time: 7059.3109s / 163734.2195 s
env0_first_0:                 episode reward: -14.8500,                 loss: 0.0071
env0_second_0:                 episode reward: 14.8500,                 loss: 0.0077
env1_first_0:                 episode reward: 18.8000,                 loss: nan
env1_second_0:                 episode reward: -18.8000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 9692.0,                last time consumption/overall running time: 7475.9314s / 171210.1509 s
env0_first_0:                 episode reward: 28.7500,                 loss: 0.0078
env0_second_0:                 episode reward: -28.7500,                 loss: 0.0081
env1_first_0:                 episode reward: 7.1500,                 loss: nan
env1_second_0:                 episode reward: -7.1500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 8909.9,                last time consumption/overall running time: 6870.2531s / 178080.4040 s
env0_first_0:                 episode reward: 26.1500,                 loss: 0.0076
env0_second_0:                 episode reward: -26.1500,                 loss: 0.0095
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 8983.15,                last time consumption/overall running time: 6924.1266s / 185004.5306 s
env0_first_0:                 episode reward: -14.7000,                 loss: 0.0075
env0_second_0:                 episode reward: 14.7000,                 loss: 0.0095
env1_first_0:                 episode reward: -8.0500,                 loss: nan
env1_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 9036.9,                last time consumption/overall running time: 6952.3914s / 191956.9220 s
env0_first_0:                 episode reward: -35.8500,                 loss: 0.0065
env0_second_0:                 episode reward: 35.8500,                 loss: 0.0082
env1_first_0:                 episode reward: -25.8500,                 loss: nan
env1_second_0:                 episode reward: 25.8500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 8532.35,                last time consumption/overall running time: 6549.5644s / 198506.4863 s
env0_first_0:                 episode reward: 27.1500,                 loss: 0.0070
env0_second_0:                 episode reward: -27.1500,                 loss: 0.0082
env1_first_0:                 episode reward: 31.1500,                 loss: nan
env1_second_0:                 episode reward: -31.1500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 9490.05,                last time consumption/overall running time: 7242.0086s / 205748.4950 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0091
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0092
env1_first_0:                 episode reward: 16.5500,                 loss: nan
env1_second_0:                 episode reward: -16.5500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 9661.45,                last time consumption/overall running time: 7378.9339s / 213127.4289 s
env0_first_0:                 episode reward: -40.1500,                 loss: 0.0065
env0_second_0:                 episode reward: 40.1500,                 loss: 0.0064
env1_first_0:                 episode reward: -22.5000,                 loss: nan
env1_second_0:                 episode reward: 22.5000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 9569.4,                last time consumption/overall running time: 7293.2471s / 220420.6760 s
env0_first_0:                 episode reward: 7.8500,                 loss: 0.0078
env0_second_0:                 episode reward: -7.8500,                 loss: 0.0071
env1_first_0:                 episode reward: 15.7000,                 loss: nan
env1_second_0:                 episode reward: -15.7000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 9407.25,                last time consumption/overall running time: 7175.1321s / 227595.8080 s
env0_first_0:                 episode reward: 28.5000,                 loss: 0.0075
env0_second_0:                 episode reward: -28.5000,                 loss: 0.0067
env1_first_0:                 episode reward: 34.8500,                 loss: nan
env1_second_0:                 episode reward: -34.8500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 9681.6,                last time consumption/overall running time: 7364.2972s / 234960.1052 s
env0_first_0:                 episode reward: -123.9500,                 loss: 0.0187
env0_second_0:                 episode reward: 123.9500,                 loss: 0.0108
env1_first_0:                 episode reward: -154.5000,                 loss: nan
env1_second_0:                 episode reward: 154.5000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 9565.4,                last time consumption/overall running time: 7279.2130s / 242239.3182 s
env0_first_0:                 episode reward: -121.9000,                 loss: 0.0117
env0_second_0:                 episode reward: 121.9000,                 loss: 0.0103
env1_first_0:                 episode reward: -133.5500,                 loss: nan
env1_second_0:                 episode reward: 133.5500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 8185.0,                last time consumption/overall running time: 6231.3289s / 248470.6471 s
env0_first_0:                 episode reward: -28.2500,                 loss: 0.0077
env0_second_0:                 episode reward: 28.2500,                 loss: 0.0085
env1_first_0:                 episode reward: -25.0500,                 loss: nan
env1_second_0:                 episode reward: 25.0500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 9272.05,                last time consumption/overall running time: 7048.4414s / 255519.0885 s
env0_first_0:                 episode reward: -48.4500,                 loss: 0.0077
env0_second_0:                 episode reward: 48.4500,                 loss: 0.0130
env1_first_0:                 episode reward: -19.7000,                 loss: nan
env1_second_0:                 episode reward: 19.7000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 7664.7,                last time consumption/overall running time: 5817.5010s / 261336.5895 s
env0_first_0:                 episode reward: 34.4500,                 loss: 0.0079
env0_second_0:                 episode reward: -34.4500,                 loss: 0.0121
env1_first_0:                 episode reward: 16.5000,                 loss: nan
env1_second_0:                 episode reward: -16.5000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 9654.55,                last time consumption/overall running time: 7332.5249s / 268669.1144 s
env0_first_0:                 episode reward: -38.2500,                 loss: 0.0086
env0_second_0:                 episode reward: 38.2500,                 loss: 0.0119
env1_first_0:                 episode reward: -36.8000,                 loss: nan
env1_second_0:                 episode reward: 36.8000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 9308.0,                last time consumption/overall running time: 7065.3504s / 275734.4648 s
env0_first_0:                 episode reward: -23.6000,                 loss: 0.0088
env0_second_0:                 episode reward: 23.6000,                 loss: 0.0102
env1_first_0:                 episode reward: 10.6500,                 loss: nan
env1_second_0:                 episode reward: -10.6500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 9426.65,                last time consumption/overall running time: 7147.9160s / 282882.3808 s
env0_first_0:                 episode reward: -34.5000,                 loss: 0.0163
env0_second_0:                 episode reward: 34.5000,                 loss: 0.0122
env1_first_0:                 episode reward: -15.8500,                 loss: nan
env1_second_0:                 episode reward: 15.8500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 8820.95,                last time consumption/overall running time: 6688.4827s / 289570.8635 s
env0_first_0:                 episode reward: 20.6500,                 loss: 0.0080
env0_second_0:                 episode reward: -20.6500,                 loss: 0.0089
env1_first_0:                 episode reward: 24.5000,                 loss: nan
env1_second_0:                 episode reward: -24.5000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 9476.8,                last time consumption/overall running time: 7170.6295s / 296741.4931 s
env0_first_0:                 episode reward: 28.6000,                 loss: 0.0099
env0_second_0:                 episode reward: -28.6000,                 loss: 0.0105
env1_first_0:                 episode reward: 28.4500,                 loss: nan
env1_second_0:                 episode reward: -28.4500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 9995.6,                last time consumption/overall running time: 7540.6722s / 304282.1653 s
env0_first_0:                 episode reward: 51.8000,                 loss: 0.0096
env0_second_0:                 episode reward: -51.8000,                 loss: 0.0108
env1_first_0:                 episode reward: 30.1000,                 loss: nan
env1_second_0:                 episode reward: -30.1000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 9769.5,                last time consumption/overall running time: 7361.7490s / 311643.9142 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0087
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0095
env1_first_0:                 episode reward: -24.9000,                 loss: nan
env1_second_0:                 episode reward: 24.9000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 9660.4,                last time consumption/overall running time: 7256.3973s / 318900.3116 s
env0_first_0:                 episode reward: 67.4500,                 loss: 0.0093
env0_second_0:                 episode reward: -67.4500,                 loss: 0.0093
env1_first_0:                 episode reward: 62.7000,                 loss: nan
env1_second_0:                 episode reward: -62.7000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 9843.45,                last time consumption/overall running time: 7369.6272s / 326269.9387 s
env0_first_0:                 episode reward: -52.7000,                 loss: 0.0088
env0_second_0:                 episode reward: 52.7000,                 loss: 0.0092
env1_first_0:                 episode reward: -55.4000,                 loss: nan
env1_second_0:                 episode reward: 55.4000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 9843.85,                last time consumption/overall running time: 7354.5697s / 333624.5084 s
env0_first_0:                 episode reward: 42.2500,                 loss: 0.0107
env0_second_0:                 episode reward: -42.2500,                 loss: 0.0186
env1_first_0:                 episode reward: 16.4000,                 loss: nan
env1_second_0:                 episode reward: -16.4000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 8195.8,                last time consumption/overall running time: 6113.8381s / 339738.3465 s
env0_first_0:                 episode reward: 82.7500,                 loss: 0.0104
env0_second_0:                 episode reward: -82.7500,                 loss: 0.0152
env1_first_0:                 episode reward: 95.5500,                 loss: nan
env1_second_0:                 episode reward: -95.5500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 7780.25,                last time consumption/overall running time: 5797.7779s / 345536.1244 s
env0_first_0:                 episode reward: 67.2500,                 loss: 0.0088
env0_second_0:                 episode reward: -67.2500,                 loss: 0.0099
env1_first_0:                 episode reward: 54.9500,                 loss: nan
env1_second_0:                 episode reward: -54.9500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 8301.4,                last time consumption/overall running time: 6181.6756s / 351717.8000 s
env0_first_0:                 episode reward: 27.8000,                 loss: 0.0088
env0_second_0:                 episode reward: -27.8000,                 loss: 0.0115
env1_first_0:                 episode reward: 20.2000,                 loss: nan
env1_second_0:                 episode reward: -20.2000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 7796.55,                last time consumption/overall running time: 5800.7516s / 357518.5516 s
env0_first_0:                 episode reward: -39.6500,                 loss: 0.0103
env0_second_0:                 episode reward: 39.6500,                 loss: 0.0138
env1_first_0:                 episode reward: -17.7000,                 loss: nan
env1_second_0:                 episode reward: 17.7000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 7305.7,                last time consumption/overall running time: 5434.7916s / 362953.3432 s
env0_first_0:                 episode reward: 60.7000,                 loss: 0.0093
env0_second_0:                 episode reward: -60.7000,                 loss: 0.0120
env1_first_0:                 episode reward: 61.9500,                 loss: nan
env1_second_0:                 episode reward: -61.9500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 9944.3,                last time consumption/overall running time: 7389.2400s / 370342.5832 s
env0_first_0:                 episode reward: 59.7500,                 loss: 0.0115
env0_second_0:                 episode reward: -59.7500,                 loss: 0.0143
env1_first_0:                 episode reward: 29.6500,                 loss: nan
env1_second_0:                 episode reward: -29.6500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 6418.15,                last time consumption/overall running time: 4768.4855s / 375111.0687 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0095
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0136
env1_first_0:                 episode reward: 12.2000,                 loss: nan
env1_second_0:                 episode reward: -12.2000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 7337.7,                last time consumption/overall running time: 5447.6908s / 380558.7595 s
env0_first_0:                 episode reward: 36.6500,                 loss: 0.0100
env0_second_0:                 episode reward: -36.6500,                 loss: 0.0110
env1_first_0:                 episode reward: 53.9000,                 loss: nan
env1_second_0:                 episode reward: -53.9000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 7952.95,                last time consumption/overall running time: 6192.3817s / 386751.1412 s
env0_first_0:                 episode reward: 15.2500,                 loss: 0.0098
env0_second_0:                 episode reward: -15.2500,                 loss: 0.0109
env1_first_0:                 episode reward: 28.5000,                 loss: nan
env1_second_0:                 episode reward: -28.5000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 5524.45,                last time consumption/overall running time: 5008.6092s / 391759.7505 s
env0_first_0:                 episode reward: 18.2500,                 loss: 0.0106
env0_second_0:                 episode reward: -18.2500,                 loss: 0.0086
env1_first_0:                 episode reward: 22.3000,                 loss: nan
env1_second_0:                 episode reward: -22.3000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 8668.55,                last time consumption/overall running time: 7851.6338s / 399611.3843 s
env0_first_0:                 episode reward: -12.1000,                 loss: 0.0184
env0_second_0:                 episode reward: 12.1000,                 loss: 0.0114
env1_first_0:                 episode reward: -10.8000,                 loss: nan
env1_second_0:                 episode reward: 10.8000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 9766.05,                last time consumption/overall running time: 8818.7578s / 408430.1421 s
env0_first_0:                 episode reward: -130.6000,                 loss: 0.0218
env0_second_0:                 episode reward: 130.6000,                 loss: 0.0156
env1_first_0:                 episode reward: -117.8500,                 loss: nan
env1_second_0:                 episode reward: 117.8500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 9039.15,                last time consumption/overall running time: 8151.3000s / 416581.4421 s
env0_first_0:                 episode reward: -57.6000,                 loss: 0.0223
env0_second_0:                 episode reward: 57.6000,                 loss: 0.0136
env1_first_0:                 episode reward: -68.6500,                 loss: nan
env1_second_0:                 episode reward: 68.6500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 8690.4,                last time consumption/overall running time: 7827.9418s / 424409.3839 s
env0_first_0:                 episode reward: -71.2500,                 loss: 0.0163
env0_second_0:                 episode reward: 71.2500,                 loss: 0.0115
env1_first_0:                 episode reward: -41.4000,                 loss: nan
env1_second_0:                 episode reward: 41.4000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 6634.15,                last time consumption/overall running time: 5973.5208s / 430382.9047 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.0125
env0_second_0:                 episode reward: -1.7500,                 loss: 0.0092
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 7048.05,                last time consumption/overall running time: 6342.0380s / 436724.9427 s
env0_first_0:                 episode reward: -20.1500,                 loss: 0.0131
env0_second_0:                 episode reward: 20.1500,                 loss: 0.0105
env1_first_0:                 episode reward: -16.4500,                 loss: nan
env1_second_0:                 episode reward: 16.4500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 5962.7,                last time consumption/overall running time: 5354.1677s / 442079.1105 s
env0_first_0:                 episode reward: -8.8000,                 loss: 0.0114
env0_second_0:                 episode reward: 8.8000,                 loss: 0.0124
env1_first_0:                 episode reward: -25.3500,                 loss: nan
env1_second_0:                 episode reward: 25.3500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 5263.0,                last time consumption/overall running time: 4719.8748s / 446798.9853 s
env0_first_0:                 episode reward: 24.7500,                 loss: 0.0082
env0_second_0:                 episode reward: -24.7500,                 loss: 0.0094
env1_first_0:                 episode reward: 8.7500,                 loss: nan
env1_second_0:                 episode reward: -8.7500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 5019.15,                last time consumption/overall running time: 4479.8081s / 451278.7934 s
env0_first_0:                 episode reward: 26.0000,                 loss: 0.0083
env0_second_0:                 episode reward: -26.0000,                 loss: 0.0100
env1_first_0:                 episode reward: 14.7500,                 loss: nan
env1_second_0:                 episode reward: -14.7500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 5505.35,                last time consumption/overall running time: 4908.1489s / 456186.9423 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0082
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0100
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 5571.75,                last time consumption/overall running time: 4971.2062s / 461158.1485 s
env0_first_0:                 episode reward: -31.2500,                 loss: 0.0089
env0_second_0:                 episode reward: 31.2500,                 loss: 0.0106
env1_first_0:                 episode reward: -27.5500,                 loss: nan
env1_second_0:                 episode reward: 27.5500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 5239.2,                last time consumption/overall running time: 4673.7630s / 465831.9115 s
env0_first_0:                 episode reward: 21.8000,                 loss: 0.0076
env0_second_0:                 episode reward: -21.8000,                 loss: 0.0090
env1_first_0:                 episode reward: 14.3000,                 loss: nan
env1_second_0:                 episode reward: -14.3000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 7692.7,                last time consumption/overall running time: 7806.8609s / 473638.7725 s
env0_first_0:                 episode reward: -57.4000,                 loss: 0.0123
env0_second_0:                 episode reward: 57.4000,                 loss: 0.0096
env1_first_0:                 episode reward: -51.9500,                 loss: nan
env1_second_0:                 episode reward: 51.9500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 6978.15,                last time consumption/overall running time: 7147.2888s / 480786.0613 s
env0_first_0:                 episode reward: -44.1000,                 loss: 0.0157
env0_second_0:                 episode reward: 44.1000,                 loss: 0.0113
env1_first_0:                 episode reward: -46.4000,                 loss: nan
env1_second_0:                 episode reward: 46.4000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 6783.65,                last time consumption/overall running time: 6955.4270s / 487741.4883 s
env0_first_0:                 episode reward: 49.0500,                 loss: 0.0158
env0_second_0:                 episode reward: -49.0500,                 loss: 0.0118
env1_first_0:                 episode reward: 45.4000,                 loss: nan
env1_second_0:                 episode reward: -45.4000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 7097.25,                last time consumption/overall running time: 7269.9983s / 495011.4866 s
env0_first_0:                 episode reward: 38.9000,                 loss: 0.0122
env0_second_0:                 episode reward: -38.9000,                 loss: 0.0221
env1_first_0:                 episode reward: 50.9500,                 loss: nan
env1_second_0:                 episode reward: -50.9500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 6510.6,                last time consumption/overall running time: 6662.3272s / 501673.8138 s
env0_first_0:                 episode reward: 24.4500,                 loss: 0.0147
env0_second_0:                 episode reward: -24.4500,                 loss: 0.0350
env1_first_0:                 episode reward: 24.4000,                 loss: nan
env1_second_0:                 episode reward: -24.4000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 5446.4,                last time consumption/overall running time: 5585.1367s / 507258.9505 s
env0_first_0:                 episode reward: -19.4500,                 loss: 0.0132
env0_second_0:                 episode reward: 19.4500,                 loss: 0.0204
env1_first_0:                 episode reward: -32.5500,                 loss: nan
env1_second_0:                 episode reward: 32.5500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 6688.65,                last time consumption/overall running time: 7011.7330s / 514270.6835 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0104
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0146
env1_first_0:                 episode reward: 23.4000,                 loss: nan
env1_second_0:                 episode reward: -23.4000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 7036.25,                last time consumption/overall running time: 7416.8342s / 521687.5176 s
env0_first_0:                 episode reward: 15.7500,                 loss: 0.0096
env0_second_0:                 episode reward: -15.7500,                 loss: 0.0107
env1_first_0:                 episode reward: 24.7000,                 loss: nan
env1_second_0:                 episode reward: -24.7000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 6873.6,                last time consumption/overall running time: 7043.9391s / 528731.4568 s
env0_first_0:                 episode reward: -48.9000,                 loss: 0.0099
env0_second_0:                 episode reward: 48.9000,                 loss: 0.0099
env1_first_0:                 episode reward: -34.5000,                 loss: nan
env1_second_0:                 episode reward: 34.5000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 4636.0,                last time consumption/overall running time: 4639.9783s / 533371.4351 s
env0_first_0:                 episode reward: 16.7500,                 loss: 0.0071
env0_second_0:                 episode reward: -16.7500,                 loss: 0.0078
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 6705.95,                last time consumption/overall running time: 6688.8926s / 540060.3277 s
env0_first_0:                 episode reward: 9.0500,                 loss: 0.0076
env0_second_0:                 episode reward: -9.0500,                 loss: 0.0078
env1_first_0:                 episode reward: 10.4000,                 loss: nan
env1_second_0:                 episode reward: -10.4000,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 5076.7,                last time consumption/overall running time: 5060.4885s / 545120.8162 s
env0_first_0:                 episode reward: 55.2500,                 loss: 0.0068
env0_second_0:                 episode reward: -55.2500,                 loss: 0.0089
env1_first_0:                 episode reward: 41.6000,                 loss: nan
env1_second_0:                 episode reward: -41.6000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 5409.25,                last time consumption/overall running time: 5385.8361s / 550506.6524 s
env0_first_0:                 episode reward: 32.4500,                 loss: 0.0054
env0_second_0:                 episode reward: -32.4500,                 loss: 0.0062
env1_first_0:                 episode reward: 36.2500,                 loss: nan
env1_second_0:                 episode reward: -36.2500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 8028.7,                last time consumption/overall running time: 7988.2361s / 558494.8885 s
env0_first_0:                 episode reward: -11.0000,                 loss: 0.0071
env0_second_0:                 episode reward: 11.0000,                 loss: 0.0079
env1_first_0:                 episode reward: 11.4000,                 loss: nan
env1_second_0:                 episode reward: -11.4000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 8484.25,                last time consumption/overall running time: 8424.8643s / 566919.7528 s
env0_first_0:                 episode reward: -25.9500,                 loss: 0.0113
env0_second_0:                 episode reward: 25.9500,                 loss: 0.0165
env1_first_0:                 episode reward: -25.5500,                 loss: nan
env1_second_0:                 episode reward: 25.5500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 6798.75,                last time consumption/overall running time: 6752.2984s / 573672.0512 s
env0_first_0:                 episode reward: 34.5000,                 loss: 0.0073
env0_second_0:                 episode reward: -34.5000,                 loss: 0.0290
env1_first_0:                 episode reward: 30.2000,                 loss: nan
env1_second_0:                 episode reward: -30.2000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 5255.95,                last time consumption/overall running time: 5221.3968s / 578893.4480 s
env0_first_0:                 episode reward: 37.7000,                 loss: 0.0055
env0_second_0:                 episode reward: -37.7000,                 loss: 0.0130
env1_first_0:                 episode reward: 30.3000,                 loss: nan
env1_second_0:                 episode reward: -30.3000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 4453.85,                last time consumption/overall running time: 4276.5309s / 583169.9789 s
env0_first_0:                 episode reward: 30.4500,                 loss: 0.0050
env0_second_0:                 episode reward: -30.4500,                 loss: 0.0060
env1_first_0:                 episode reward: 18.6500,                 loss: nan
env1_second_0:                 episode reward: -18.6500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 4492.65,                last time consumption/overall running time: 3992.5206s / 587162.4994 s
env0_first_0:                 episode reward: 13.8000,                 loss: 0.0051
env0_second_0:                 episode reward: -13.8000,                 loss: 0.0056
env1_first_0:                 episode reward: 14.5500,                 loss: nan
env1_second_0:                 episode reward: -14.5500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 6836.95,                last time consumption/overall running time: 6024.3926s / 593186.8920 s
env0_first_0:                 episode reward: -9.3000,                 loss: 0.0082
env0_second_0:                 episode reward: 9.3000,                 loss: 0.0065
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 5524.25,                last time consumption/overall running time: 4864.8345s / 598051.7265 s
env0_first_0:                 episode reward: 16.4500,                 loss: 0.0046
env0_second_0:                 episode reward: -16.4500,                 loss: 0.0046
env1_first_0:                 episode reward: 16.2000,                 loss: nan
env1_second_0:                 episode reward: -16.2000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 6034.75,                last time consumption/overall running time: 5298.3375s / 603350.0640 s
env0_first_0:                 episode reward: 9.3000,                 loss: 0.0053
env0_second_0:                 episode reward: -9.3000,                 loss: 0.0049
env1_first_0:                 episode reward: 7.8500,                 loss: nan
env1_second_0:                 episode reward: -7.8500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 5969.45,                last time consumption/overall running time: 5146.1944s / 608496.2584 s
env0_first_0:                 episode reward: 38.0000,                 loss: 0.0059
env0_second_0:                 episode reward: -38.0000,                 loss: 0.0058
env1_first_0:                 episode reward: 34.3000,                 loss: nan
env1_second_0:                 episode reward: -34.3000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 4670.65,                last time consumption/overall running time: 3853.5985s / 612349.8569 s
env0_first_0:                 episode reward: 25.1000,                 loss: 0.0052
env0_second_0:                 episode reward: -25.1000,                 loss: 0.0051
env1_first_0:                 episode reward: 18.2500,                 loss: nan
env1_second_0:                 episode reward: -18.2500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 4163.15,                last time consumption/overall running time: 3434.2690s / 615784.1258 s
env0_first_0:                 episode reward: 2.9500,                 loss: 0.0072
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0067
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 3963.85,                last time consumption/overall running time: 3264.2005s / 619048.3263 s
env0_first_0:                 episode reward: 17.9000,                 loss: 0.0044
env0_second_0:                 episode reward: -17.9000,                 loss: 0.0044
env1_first_0:                 episode reward: 24.5000,                 loss: nan
env1_second_0:                 episode reward: -24.5000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 4017.75,                last time consumption/overall running time: 3305.3713s / 622353.6977 s
env0_first_0:                 episode reward: 23.0000,                 loss: 0.0044
env0_second_0:                 episode reward: -23.0000,                 loss: 0.0041
env1_first_0:                 episode reward: 21.9500,                 loss: nan
env1_second_0:                 episode reward: -21.9500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 4909.5,                last time consumption/overall running time: 4036.2729s / 626389.9705 s
env0_first_0:                 episode reward: -13.3000,                 loss: 0.0061
env0_second_0:                 episode reward: 13.3000,                 loss: 0.0058
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 4392.2,                last time consumption/overall running time: 3610.8350s / 630000.8055 s
env0_first_0:                 episode reward: 12.4500,                 loss: 0.0049
env0_second_0:                 episode reward: -12.4500,                 loss: 0.0052
env1_first_0:                 episode reward: 11.3500,                 loss: nan
env1_second_0:                 episode reward: -11.3500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 4438.05,                last time consumption/overall running time: 3526.3408s / 633527.1463 s
env0_first_0:                 episode reward: 11.0500,                 loss: 0.0054
env0_second_0:                 episode reward: -11.0500,                 loss: 0.0053
env1_first_0:                 episode reward: 17.9000,                 loss: nan
env1_second_0:                 episode reward: -17.9000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 6483.35,                last time consumption/overall running time: 4737.2955s / 638264.4418 s
env0_first_0:                 episode reward: 2.8000,                 loss: 0.0079
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0073
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 7048.9,                last time consumption/overall running time: 5145.6065s / 643410.0483 s
env0_first_0:                 episode reward: -31.5000,                 loss: 0.0104
env0_second_0:                 episode reward: 31.5000,                 loss: 0.0089
env1_first_0:                 episode reward: -25.3500,                 loss: nan
env1_second_0:                 episode reward: 25.3500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 4498.35,                last time consumption/overall running time: 3054.6971s / 646464.7454 s
env0_first_0:                 episode reward: 23.3500,                 loss: 0.0079
env0_second_0:                 episode reward: -23.3500,                 loss: 0.0064
env1_first_0:                 episode reward: 34.6000,                 loss: nan
env1_second_0:                 episode reward: -34.6000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 4157.95,                last time consumption/overall running time: 2623.8185s / 649088.5638 s
env0_first_0:                 episode reward: 32.8000,                 loss: 0.0109
env0_second_0:                 episode reward: -32.8000,                 loss: 0.0083
env1_first_0:                 episode reward: 37.5000,                 loss: nan
env1_second_0:                 episode reward: -37.5000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 2826.85,                last time consumption/overall running time: 1782.0384s / 650870.6022 s
env0_first_0:                 episode reward: 20.4000,                 loss: 0.0081
env0_second_0:                 episode reward: -20.4000,                 loss: 0.0058
env1_first_0:                 episode reward: 22.2500,                 loss: nan
env1_second_0:                 episode reward: -22.2500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 3757.5,                last time consumption/overall running time: 2371.2339s / 653241.8361 s
env0_first_0:                 episode reward: 28.7500,                 loss: 0.0064
env0_second_0:                 episode reward: -28.7500,                 loss: 0.0054
env1_first_0:                 episode reward: 20.1500,                 loss: nan
env1_second_0:                 episode reward: -20.1500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 3428.0,                last time consumption/overall running time: 2155.8993s / 655397.7354 s
env0_first_0:                 episode reward: 20.0000,                 loss: 0.0052
env0_second_0:                 episode reward: -20.0000,                 loss: 0.0052
env1_first_0:                 episode reward: 19.1500,                 loss: nan
env1_second_0:                 episode reward: -19.1500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 3473.65,                last time consumption/overall running time: 2169.5476s / 657567.2831 s
env0_first_0:                 episode reward: 22.8000,                 loss: 0.0044
env0_second_0:                 episode reward: -22.8000,                 loss: 0.0049
env1_first_0:                 episode reward: 22.1500,                 loss: nan
env1_second_0:                 episode reward: -22.1500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 4107.3,                last time consumption/overall running time: 2420.3367s / 659987.6197 s
env0_first_0:                 episode reward: 26.9500,                 loss: 0.0050
env0_second_0:                 episode reward: -26.9500,                 loss: 0.0054
env1_first_0:                 episode reward: 25.3500,                 loss: nan
env1_second_0:                 episode reward: -25.3500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 4122.65,                last time consumption/overall running time: 2435.9264s / 662423.5461 s
env0_first_0:                 episode reward: 23.8500,                 loss: 0.0043
env0_second_0:                 episode reward: -23.8500,                 loss: 0.0045
env1_first_0:                 episode reward: 21.8500,                 loss: nan
env1_second_0:                 episode reward: -21.8500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 4800.8,                last time consumption/overall running time: 2831.5070s / 665255.0531 s
env0_first_0:                 episode reward: 4.3500,                 loss: 0.0044
env0_second_0:                 episode reward: -4.3500,                 loss: 0.0046
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 4257.45,                last time consumption/overall running time: 2508.7986s / 667763.8517 s
env0_first_0:                 episode reward: 12.5500,                 loss: 0.0063
env0_second_0:                 episode reward: -12.5500,                 loss: 0.0066
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 4378.2,                last time consumption/overall running time: 2507.5856s / 670271.4373 s
env0_first_0:                 episode reward: 12.9500,                 loss: 0.0056
env0_second_0:                 episode reward: -12.9500,                 loss: 0.0060
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 4867.9,                last time consumption/overall running time: 2678.5764s / 672950.0138 s
env0_first_0:                 episode reward: 5.6000,                 loss: 0.0065
env0_second_0:                 episode reward: -5.6000,                 loss: 0.0069
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 4794.7,                last time consumption/overall running time: 2568.0345s / 675518.0483 s
env0_first_0:                 episode reward: 19.3000,                 loss: 0.0056
env0_second_0:                 episode reward: -19.3000,                 loss: 0.0061
env1_first_0:                 episode reward: 26.8000,                 loss: nan
env1_second_0:                 episode reward: -26.8000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 4767.85,                last time consumption/overall running time: 2417.9899s / 677936.0382 s
env0_first_0:                 episode reward: 2.8000,                 loss: 0.0041
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0045
env1_first_0:                 episode reward: 20.4000,                 loss: nan
env1_second_0:                 episode reward: -20.4000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 4526.7,                last time consumption/overall running time: 2291.1331s / 680227.1713 s
env0_first_0:                 episode reward: 17.2500,                 loss: 0.0049
env0_second_0:                 episode reward: -17.2500,                 loss: 0.0051
env1_first_0:                 episode reward: 14.2000,                 loss: nan
env1_second_0:                 episode reward: -14.2000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 4455.15,                last time consumption/overall running time: 2268.8497s / 682496.0210 s
env0_first_0:                 episode reward: 27.3000,                 loss: 0.0043
env0_second_0:                 episode reward: -27.3000,                 loss: 0.0050
env1_first_0:                 episode reward: 28.2000,                 loss: nan
env1_second_0:                 episode reward: -28.2000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 5605.85,                last time consumption/overall running time: 2847.0249s / 685343.0459 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0054
env0_second_0:                 episode reward: -4.4500,                 loss: 0.0054
env1_first_0:                 episode reward: 15.3500,                 loss: nan
env1_second_0:                 episode reward: -15.3500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 8263.65,                last time consumption/overall running time: 4184.1966s / 689527.2424 s
env0_first_0:                 episode reward: -15.2500,                 loss: 0.0126
env0_second_0:                 episode reward: 15.2500,                 loss: 0.0069
env1_first_0:                 episode reward: -22.3000,                 loss: nan
env1_second_0:                 episode reward: 22.3000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 5061.35,                last time consumption/overall running time: 2557.3104s / 692084.5529 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0062
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0061
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 7182.7,                last time consumption/overall running time: 3630.9999s / 695715.5527 s
env0_first_0:                 episode reward: -66.3500,                 loss: 0.0084
env0_second_0:                 episode reward: 66.3500,                 loss: 0.0073
env1_first_0:                 episode reward: -51.2000,                 loss: nan
env1_second_0:                 episode reward: 51.2000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 5385.25,                last time consumption/overall running time: 2723.4515s / 698439.0043 s
env0_first_0:                 episode reward: 9.1000,                 loss: 0.0063
env0_second_0:                 episode reward: -9.1000,                 loss: 0.0058
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 5036.1,                last time consumption/overall running time: 2543.1216s / 700982.1259 s
env0_first_0:                 episode reward: 3.1500,                 loss: 0.0057
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0057
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 6744.45,                last time consumption/overall running time: 3406.2558s / 704388.3816 s
env0_first_0:                 episode reward: 36.1000,                 loss: 0.0071
env0_second_0:                 episode reward: -36.1000,                 loss: 0.0190
env1_first_0:                 episode reward: 45.5500,                 loss: nan
env1_second_0:                 episode reward: -45.5500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 9087.6,                last time consumption/overall running time: 4560.4446s / 708948.8262 s
env0_first_0:                 episode reward: 152.8500,                 loss: 0.0104
env0_second_0:                 episode reward: -152.8500,                 loss: 0.0289
env1_first_0:                 episode reward: 138.6500,                 loss: nan
env1_second_0:                 episode reward: -138.6500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 7729.4,                last time consumption/overall running time: 3876.0160s / 712824.8422 s
env0_first_0:                 episode reward: 140.7000,                 loss: 0.0098
env0_second_0:                 episode reward: -140.7000,                 loss: 0.0444
env1_first_0:                 episode reward: 140.8500,                 loss: nan
env1_second_0:                 episode reward: -140.8500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 7121.0,                last time consumption/overall running time: 3550.1958s / 716375.0380 s
env0_first_0:                 episode reward: 138.1000,                 loss: 0.0095
env0_second_0:                 episode reward: -138.1000,                 loss: 0.0510
env1_first_0:                 episode reward: 138.0000,                 loss: nan
env1_second_0:                 episode reward: -138.0000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 6653.0,                last time consumption/overall running time: 3313.9659s / 719689.0039 s
env0_first_0:                 episode reward: 111.1000,                 loss: 0.0099
env0_second_0:                 episode reward: -111.1000,                 loss: 0.0527
env1_first_0:                 episode reward: 96.2000,                 loss: nan
env1_second_0:                 episode reward: -96.2000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 6056.3,                last time consumption/overall running time: 3021.0299s / 722710.0339 s
env0_first_0:                 episode reward: 107.6500,                 loss: 0.0096
env0_second_0:                 episode reward: -107.6500,                 loss: 0.0497
env1_first_0:                 episode reward: 102.7500,                 loss: nan
env1_second_0:                 episode reward: -102.7500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 7020.85,                last time consumption/overall running time: 3498.3860s / 726208.4199 s
env0_first_0:                 episode reward: 127.3000,                 loss: 0.0099
env0_second_0:                 episode reward: -127.3000,                 loss: 0.0360
env1_first_0:                 episode reward: 128.1500,                 loss: nan
env1_second_0:                 episode reward: -128.1500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 8033.4,                last time consumption/overall running time: 3995.4281s / 730203.8479 s
env0_first_0:                 episode reward: 152.4000,                 loss: 0.0102
env0_second_0:                 episode reward: -152.4000,                 loss: 0.0429
env1_first_0:                 episode reward: 155.2500,                 loss: nan
env1_second_0:                 episode reward: -155.2500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 6769.5,                last time consumption/overall running time: 3363.2701s / 733567.1180 s
env0_first_0:                 episode reward: 124.6000,                 loss: 0.0099
env0_second_0:                 episode reward: -124.6000,                 loss: 0.0430
env1_first_0:                 episode reward: 133.9500,                 loss: nan
env1_second_0:                 episode reward: -133.9500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 5219.9,                last time consumption/overall running time: 2595.0260s / 736162.1441 s
env0_first_0:                 episode reward: 79.8000,                 loss: 0.0091
env0_second_0:                 episode reward: -79.8000,                 loss: 0.0324
env1_first_0:                 episode reward: 77.7000,                 loss: nan
env1_second_0:                 episode reward: -77.7000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 4300.15,                last time consumption/overall running time: 2137.5788s / 738299.7229 s
env0_first_0:                 episode reward: 34.6500,                 loss: 0.0085
env0_second_0:                 episode reward: -34.6500,                 loss: 0.0330
env1_first_0:                 episode reward: 55.5000,                 loss: nan
env1_second_0:                 episode reward: -55.5000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 5205.5,                last time consumption/overall running time: 2587.8384s / 740887.5612 s
env0_first_0:                 episode reward: 47.9000,                 loss: 0.0103
env0_second_0:                 episode reward: -47.9000,                 loss: 0.0573
env1_first_0:                 episode reward: 59.5000,                 loss: nan
env1_second_0:                 episode reward: -59.5000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 6839.25,                last time consumption/overall running time: 3390.9442s / 744278.5054 s
env0_first_0:                 episode reward: 118.0500,                 loss: 0.0104
env0_second_0:                 episode reward: -118.0500,                 loss: 0.0460
env1_first_0:                 episode reward: 111.3500,                 loss: nan
env1_second_0:                 episode reward: -111.3500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 6808.15,                last time consumption/overall running time: 3371.6750s / 747650.1805 s
env0_first_0:                 episode reward: 63.8000,                 loss: 0.0106
env0_second_0:                 episode reward: -63.8000,                 loss: 0.0426
env1_first_0:                 episode reward: 68.2500,                 loss: nan
env1_second_0:                 episode reward: -68.2500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 5793.25,                last time consumption/overall running time: 2729.7346s / 750379.9151 s
env0_first_0:                 episode reward: 73.3500,                 loss: 0.0109
env0_second_0:                 episode reward: -73.3500,                 loss: 0.0302
env1_first_0:                 episode reward: 75.8500,                 loss: nan
env1_second_0:                 episode reward: -75.8500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 4686.45,                last time consumption/overall running time: 2065.8883s / 752445.8034 s
env0_first_0:                 episode reward: 21.2500,                 loss: 0.0116
env0_second_0:                 episode reward: -21.2500,                 loss: 0.0270
env1_first_0:                 episode reward: 24.3500,                 loss: nan
env1_second_0:                 episode reward: -24.3500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 4324.55,                last time consumption/overall running time: 1902.3538s / 754348.1572 s
env0_first_0:                 episode reward: 47.3500,                 loss: 0.0104
env0_second_0:                 episode reward: -47.3500,                 loss: 0.0276
env1_first_0:                 episode reward: 50.4000,                 loss: nan
env1_second_0:                 episode reward: -50.4000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 4680.85,                last time consumption/overall running time: 2068.8404s / 756416.9976 s
env0_first_0:                 episode reward: 57.3500,                 loss: 0.0102
env0_second_0:                 episode reward: -57.3500,                 loss: 0.0303
env1_first_0:                 episode reward: 48.3500,                 loss: nan
env1_second_0:                 episode reward: -48.3500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 6388.85,                last time consumption/overall running time: 2822.6850s / 759239.6826 s
env0_first_0:                 episode reward: 102.1000,                 loss: 0.0109
env0_second_0:                 episode reward: -102.1000,                 loss: 0.0413
env1_first_0:                 episode reward: 111.6000,                 loss: nan
env1_second_0:                 episode reward: -111.6000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 5464.15,                last time consumption/overall running time: 2412.8279s / 761652.5105 s
env0_first_0:                 episode reward: 82.2500,                 loss: 0.0105
env0_second_0:                 episode reward: -82.2500,                 loss: 0.0460
env1_first_0:                 episode reward: 68.3500,                 loss: nan
env1_second_0:                 episode reward: -68.3500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 5805.65,                last time consumption/overall running time: 2567.5893s / 764220.0999 s
env0_first_0:                 episode reward: 99.0000,                 loss: 0.0103
env0_second_0:                 episode reward: -99.0000,                 loss: 0.0549
env1_first_0:                 episode reward: 104.2000,                 loss: nan
env1_second_0:                 episode reward: -104.2000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 8873.2,                last time consumption/overall running time: 3920.9038s / 768141.0037 s
env0_first_0:                 episode reward: 131.4500,                 loss: 0.0128
env0_second_0:                 episode reward: -131.4500,                 loss: 0.0463
env1_first_0:                 episode reward: 157.6000,                 loss: nan
env1_second_0:                 episode reward: -157.6000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 5744.2,                last time consumption/overall running time: 2542.1942s / 770683.1979 s
env0_first_0:                 episode reward: 91.0000,                 loss: 0.0241
env0_second_0:                 episode reward: -91.0000,                 loss: 0.0391
env1_first_0:                 episode reward: 84.9500,                 loss: nan
env1_second_0:                 episode reward: -84.9500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 6630.95,                last time consumption/overall running time: 3212.5330s / 773895.7309 s
env0_first_0:                 episode reward: 113.4500,                 loss: 0.0190
env0_second_0:                 episode reward: -113.4500,                 loss: 0.0340
env1_first_0:                 episode reward: 95.5500,                 loss: nan
env1_second_0:                 episode reward: -95.5500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 7357.45,                last time consumption/overall running time: 3559.0101s / 777454.7410 s
env0_first_0:                 episode reward: 90.2000,                 loss: 0.0219
env0_second_0:                 episode reward: -90.2000,                 loss: 0.0450
env1_first_0:                 episode reward: 108.9000,                 loss: nan
env1_second_0:                 episode reward: -108.9000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 6660.2,                last time consumption/overall running time: 3223.1151s / 780677.8561 s
env0_first_0:                 episode reward: 52.9500,                 loss: 0.0166
env0_second_0:                 episode reward: -52.9500,                 loss: 0.0225
env1_first_0:                 episode reward: 52.1000,                 loss: nan
env1_second_0:                 episode reward: -52.1000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 5300.35,                last time consumption/overall running time: 2421.7768s / 783099.6329 s
env0_first_0:                 episode reward: 47.9500,                 loss: 0.0118
env0_second_0:                 episode reward: -47.9500,                 loss: 0.0253
env1_first_0:                 episode reward: 66.2500,                 loss: nan
env1_second_0:                 episode reward: -66.2500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 4929.75,                last time consumption/overall running time: 2177.1048s / 785276.7377 s
env0_first_0:                 episode reward: 61.2000,                 loss: 0.0107
env0_second_0:                 episode reward: -61.2000,                 loss: 0.0177
env1_first_0:                 episode reward: 53.8000,                 loss: nan
env1_second_0:                 episode reward: -53.8000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 3265.2,                last time consumption/overall running time: 1440.9681s / 786717.7058 s
env0_first_0:                 episode reward: 16.2500,                 loss: 0.0093
env0_second_0:                 episode reward: -16.2500,                 loss: 0.0157
env1_first_0:                 episode reward: 23.0500,                 loss: nan
env1_second_0:                 episode reward: -23.0500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 4741.0,                last time consumption/overall running time: 2093.5099s / 788811.2157 s
env0_first_0:                 episode reward: 41.7000,                 loss: 0.0103
env0_second_0:                 episode reward: -41.7000,                 loss: 0.0126
env1_first_0:                 episode reward: 25.0500,                 loss: nan
env1_second_0:                 episode reward: -25.0500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 3638.4,                last time consumption/overall running time: 1465.4524s / 790276.6681 s
env0_first_0:                 episode reward: 20.3000,                 loss: 0.0099
env0_second_0:                 episode reward: -20.3000,                 loss: 0.0109
env1_first_0:                 episode reward: 31.9000,                 loss: nan
env1_second_0:                 episode reward: -31.9000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 2707.65,                last time consumption/overall running time: 1087.4479s / 791364.1160 s
env0_first_0:                 episode reward: 17.0000,                 loss: 0.0088
env0_second_0:                 episode reward: -17.0000,                 loss: 0.0118
env1_first_0:                 episode reward: 11.5000,                 loss: nan
env1_second_0:                 episode reward: -11.5000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 3317.35,                last time consumption/overall running time: 1333.6908s / 792697.8067 s
env0_first_0:                 episode reward: 17.8500,                 loss: 0.0102
env0_second_0:                 episode reward: -17.8500,                 loss: 0.0120
env1_first_0:                 episode reward: -10.1000,                 loss: nan
env1_second_0:                 episode reward: 10.1000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 3317.85,                last time consumption/overall running time: 1331.5120s / 794029.3187 s
env0_first_0:                 episode reward: 8.6500,                 loss: 0.0165
env0_second_0:                 episode reward: -8.6500,                 loss: 0.0150
env1_first_0:                 episode reward: 16.6500,                 loss: nan
env1_second_0:                 episode reward: -16.6500,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 3985.2,                last time consumption/overall running time: 1603.0498s / 795632.3685 s
env0_first_0:                 episode reward: -14.9500,                 loss: 0.0319
env0_second_0:                 episode reward: 14.9500,                 loss: 0.0133
env1_first_0:                 episode reward: -13.1000,                 loss: nan
env1_second_0:                 episode reward: 13.1000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 4751.8,                last time consumption/overall running time: 1910.7996s / 797543.1681 s
env0_first_0:                 episode reward: -30.4000,                 loss: 0.0587
env0_second_0:                 episode reward: 30.4000,                 loss: 0.0144
env1_first_0:                 episode reward: -11.7500,                 loss: nan
env1_second_0:                 episode reward: 11.7500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 4851.0,                last time consumption/overall running time: 1953.0540s / 799496.2222 s
env0_first_0:                 episode reward: -12.8500,                 loss: 0.0573
env0_second_0:                 episode reward: 12.8500,                 loss: 0.0135
env1_first_0:                 episode reward: -15.2000,                 loss: nan
env1_second_0:                 episode reward: 15.2000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 5712.05,                last time consumption/overall running time: 2307.4146s / 801803.6368 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0609
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0142
env1_first_0:                 episode reward: -11.4000,                 loss: nan
env1_second_0:                 episode reward: 11.4000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 3724.35,                last time consumption/overall running time: 1391.3436s / 803194.9803 s
env0_first_0:                 episode reward: 34.2500,                 loss: 0.0275
env0_second_0:                 episode reward: -34.2500,                 loss: 0.0129
env1_first_0:                 episode reward: 30.8000,                 loss: nan
env1_second_0:                 episode reward: -30.8000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 3944.75,                last time consumption/overall running time: 1433.7450s / 804628.7254 s
env0_first_0:                 episode reward: 24.4000,                 loss: 0.0287
env0_second_0:                 episode reward: -24.4000,                 loss: 0.0133
env1_first_0:                 episode reward: 22.3000,                 loss: nan
env1_second_0:                 episode reward: -22.3000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 2576.85,                last time consumption/overall running time: 933.3649s / 805562.0902 s
env0_first_0:                 episode reward: 21.5000,                 loss: 0.0200
env0_second_0:                 episode reward: -21.5000,                 loss: 0.0112
env1_first_0:                 episode reward: 26.0000,                 loss: nan