pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 3}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0521/pettingzoo_boxing_v1_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0521/pettingzoo_boxing_v1_nash_dqn_exploiter.
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 45.1224s / 45.1224 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0116
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0096
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 1633.8512s / 1678.9736 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0206
env0_second_0:                 episode reward: 0.6000,                 loss: 0.0207
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2054.8370s / 3733.8106 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0258
env0_second_0:                 episode reward: -3.8500,                 loss: 0.0263
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2167.5573s / 5901.3679 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0355
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0344
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2181.8329s / 8083.2008 s
env0_first_0:                 episode reward: -11.1000,                 loss: 0.0353
env0_second_0:                 episode reward: 11.1000,                 loss: 0.0347
env1_first_0:                 episode reward: -16.3000,                 loss: nan
env1_second_0:                 episode reward: 16.3000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2196.9292s / 10280.1299 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0561
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0581
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2187.1375s / 12467.2675 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0446
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0473
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2190.3675s / 14657.6350 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0392
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0443
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2183.1061s / 16840.7411 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0244
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0273
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2192.1622s / 19032.9033 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0237
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0238
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2173.5671s / 21206.4705 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0189
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0210
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2195.8712s / 23402.3417 s
env0_first_0:                 episode reward: -8.9500,                 loss: 0.0194
env0_second_0:                 episode reward: 8.9500,                 loss: 0.0203
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2192.4674s / 25594.8091 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.0122
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0117
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2191.9969s / 27786.8059 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0166
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0167
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2194.3139s / 29981.1198 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0172
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0172
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2202.3718s / 32183.4916 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.0108
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0105
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2184.6383s / 34368.1300 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0122
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0132
env1_first_0:                 episode reward: -9.9000,                 loss: nan
env1_second_0:                 episode reward: 9.9000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1762.2,                last time consumption/overall running time: 2171.1750s / 36539.3049 s
env0_first_0:                 episode reward: -25.1500,                 loss: 0.0293
env0_second_0:                 episode reward: 25.1500,                 loss: 0.0301
env1_first_0:                 episode reward: -30.0500,                 loss: nan
env1_second_0:                 episode reward: 30.0500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1735.95,                last time consumption/overall running time: 2338.3586s / 38877.6635 s
env0_first_0:                 episode reward: -31.1500,                 loss: 0.0535
env0_second_0:                 episode reward: 31.1500,                 loss: 0.0418
env1_first_0:                 episode reward: -23.0000,                 loss: nan
env1_second_0:                 episode reward: 23.0000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 902.5,                last time consumption/overall running time: 1320.5289s / 40198.1924 s
env0_first_0:                 episode reward: -66.2000,                 loss: 0.0594
env0_second_0:                 episode reward: 66.2000,                 loss: 0.0520
env1_first_0:                 episode reward: -60.0500,                 loss: nan
env1_second_0:                 episode reward: 60.0500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1237.45,                last time consumption/overall running time: 1793.4249s / 41991.6173 s
env0_first_0:                 episode reward: -45.1500,                 loss: 0.1059
env0_second_0:                 episode reward: 45.1500,                 loss: 0.0976
env1_first_0:                 episode reward: -44.0500,                 loss: nan
env1_second_0:                 episode reward: 44.0500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1360.35,                last time consumption/overall running time: 1983.9108s / 43975.5281 s
env0_first_0:                 episode reward: -44.5500,                 loss: 0.1929
env0_second_0:                 episode reward: 44.5500,                 loss: 0.1742
env1_first_0:                 episode reward: -47.5000,                 loss: nan
env1_second_0:                 episode reward: 47.5000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1106.3,                last time consumption/overall running time: 1623.1824s / 45598.7105 s
env0_first_0:                 episode reward: -65.7500,                 loss: 0.1813
env0_second_0:                 episode reward: 65.7500,                 loss: 0.1718
env1_first_0:                 episode reward: -71.1500,                 loss: nan
env1_second_0:                 episode reward: 71.1500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 452.35,                last time consumption/overall running time: 666.2555s / 46264.9660 s
env0_first_0:                 episode reward: -85.3000,                 loss: 0.1860
env0_second_0:                 episode reward: 85.3000,                 loss: 0.1857
env1_first_0:                 episode reward: -69.3500,                 loss: nan
env1_second_0:                 episode reward: 69.3500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 318.85,                last time consumption/overall running time: 469.6149s / 46734.5809 s
env0_first_0:                 episode reward: -75.0500,                 loss: 0.1876
env0_second_0:                 episode reward: 75.0500,                 loss: 0.1968
env1_first_0:                 episode reward: -84.9500,                 loss: nan
env1_second_0:                 episode reward: 84.9500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 348.1,                last time consumption/overall running time: 513.5769s / 47248.1577 s
env0_first_0:                 episode reward: -70.9500,                 loss: 0.2126
env0_second_0:                 episode reward: 70.9500,                 loss: 0.2236
env1_first_0:                 episode reward: -82.8500,                 loss: nan
env1_second_0:                 episode reward: 82.8500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 281.05,                last time consumption/overall running time: 413.9105s / 47662.0683 s
env0_first_0:                 episode reward: -86.1000,                 loss: 0.2319
env0_second_0:                 episode reward: 86.1000,                 loss: 0.2526
env1_first_0:                 episode reward: -80.9000,                 loss: nan
env1_second_0:                 episode reward: 80.9000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 278.5,                last time consumption/overall running time: 411.1975s / 48073.2657 s
env0_first_0:                 episode reward: -84.8500,                 loss: 0.2338
env0_second_0:                 episode reward: 84.8500,                 loss: 0.2676
env1_first_0:                 episode reward: -86.4500,                 loss: nan
env1_second_0:                 episode reward: 86.4500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 288.25,                last time consumption/overall running time: 426.6118s / 48499.8776 s
env0_first_0:                 episode reward: -83.6500,                 loss: 0.2605
env0_second_0:                 episode reward: 83.6500,                 loss: 0.3076
env1_first_0:                 episode reward: -90.1000,                 loss: nan
env1_second_0:                 episode reward: 90.1000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 267.8,                last time consumption/overall running time: 395.2826s / 48895.1601 s
env0_first_0:                 episode reward: -91.0500,                 loss: 0.2566
env0_second_0:                 episode reward: 91.0500,                 loss: 0.3081
env1_first_0:                 episode reward: -77.6000,                 loss: nan
env1_second_0:                 episode reward: 77.6000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 282.0,                last time consumption/overall running time: 415.1267s / 49310.2868 s
env0_first_0:                 episode reward: -84.3500,                 loss: 0.2908
env0_second_0:                 episode reward: 84.3500,                 loss: 0.3674
env1_first_0:                 episode reward: -84.0000,                 loss: nan
env1_second_0:                 episode reward: 84.0000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 280.2,                last time consumption/overall running time: 415.1480s / 49725.4348 s
env0_first_0:                 episode reward: -85.8000,                 loss: 0.2713
env0_second_0:                 episode reward: 85.8000,                 loss: 0.3346
env1_first_0:                 episode reward: -85.0500,                 loss: nan
env1_second_0:                 episode reward: 85.0500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 280.9,                last time consumption/overall running time: 417.5207s / 50142.9555 s
env0_first_0:                 episode reward: -90.4000,                 loss: 0.2864
env0_second_0:                 episode reward: 90.4000,                 loss: 0.3531
env1_first_0:                 episode reward: -82.3500,                 loss: nan
env1_second_0:                 episode reward: 82.3500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 277.35,                last time consumption/overall running time: 408.3458s / 50551.3013 s
env0_first_0:                 episode reward: -78.6000,                 loss: 0.2825
env0_second_0:                 episode reward: 78.6000,                 loss: 0.3750
env1_first_0:                 episode reward: -80.6000,                 loss: nan
env1_second_0:                 episode reward: 80.6000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 278.85,                last time consumption/overall running time: 411.2799s / 50962.5812 s
env0_first_0:                 episode reward: -82.3000,                 loss: 0.2589
env0_second_0:                 episode reward: 82.3000,                 loss: 0.3572
env1_first_0:                 episode reward: -82.1500,                 loss: nan
env1_second_0:                 episode reward: 82.1500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 303.5,                last time consumption/overall running time: 449.7303s / 51412.3115 s
env0_first_0:                 episode reward: -76.9500,                 loss: 0.2772
env0_second_0:                 episode reward: 76.9500,                 loss: 0.3692
env1_first_0:                 episode reward: -74.9000,                 loss: nan
env1_second_0:                 episode reward: 74.9000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 294.7,                last time consumption/overall running time: 433.7775s / 51846.0890 s
env0_first_0:                 episode reward: -86.3500,                 loss: 0.2786
env0_second_0:                 episode reward: 86.3500,                 loss: 0.3464
env1_first_0:                 episode reward: -73.1500,                 loss: nan
env1_second_0:                 episode reward: 73.1500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 266.2,                last time consumption/overall running time: 394.0823s / 52240.1713 s
env0_first_0:                 episode reward: -85.4000,                 loss: 0.2996
env0_second_0:                 episode reward: 85.4000,                 loss: 0.4130
env1_first_0:                 episode reward: -90.1500,                 loss: nan
env1_second_0:                 episode reward: 90.1500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 284.8,                last time consumption/overall running time: 420.9153s / 52661.0866 s
env0_first_0:                 episode reward: -84.6500,                 loss: 0.2997
env0_second_0:                 episode reward: 84.6500,                 loss: 0.3927
env1_first_0:                 episode reward: -88.7500,                 loss: nan
env1_second_0:                 episode reward: 88.7500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 259.55,                last time consumption/overall running time: 381.4509s / 53042.5374 s
env0_first_0:                 episode reward: -86.0500,                 loss: 0.3164
env0_second_0:                 episode reward: 86.0500,                 loss: 0.4323
env1_first_0:                 episode reward: -86.8500,                 loss: nan
env1_second_0:                 episode reward: 86.8500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 249.75,                last time consumption/overall running time: 368.7417s / 53411.2792 s
env0_first_0:                 episode reward: -75.4500,                 loss: 0.3193
env0_second_0:                 episode reward: 75.4500,                 loss: 0.4392
env1_first_0:                 episode reward: -87.6500,                 loss: nan
env1_second_0:                 episode reward: 87.6500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 253.25,                last time consumption/overall running time: 377.8071s / 53789.0863 s
env0_first_0:                 episode reward: -87.6000,                 loss: 0.3521
env0_second_0:                 episode reward: 87.6000,                 loss: 0.4897
env1_first_0:                 episode reward: -87.5000,                 loss: nan
env1_second_0:                 episode reward: 87.5000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 262.75,                last time consumption/overall running time: 390.6683s / 54179.7546 s
env0_first_0:                 episode reward: -81.7000,                 loss: 0.3362
env0_second_0:                 episode reward: 81.7000,                 loss: 0.5050
env1_first_0:                 episode reward: -84.9000,                 loss: nan
env1_second_0:                 episode reward: 84.9000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 246.45,                last time consumption/overall running time: 360.0959s / 54539.8505 s
env0_first_0:                 episode reward: -94.9500,                 loss: 0.3468
env0_second_0:                 episode reward: 94.9500,                 loss: 0.5022
env1_first_0:                 episode reward: -85.8500,                 loss: nan
env1_second_0:                 episode reward: 85.8500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 257.6,                last time consumption/overall running time: 379.2554s / 54919.1058 s
env0_first_0:                 episode reward: -83.7000,                 loss: 0.3797
env0_second_0:                 episode reward: 83.7000,                 loss: 0.5357
env1_first_0:                 episode reward: -87.4500,                 loss: nan
env1_second_0:                 episode reward: 87.4500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 257.3,                last time consumption/overall running time: 380.2221s / 55299.3280 s
env0_first_0:                 episode reward: -81.6500,                 loss: 0.4177
env0_second_0:                 episode reward: 81.6500,                 loss: 0.5581
env1_first_0:                 episode reward: -87.2000,                 loss: nan
env1_second_0:                 episode reward: 87.2000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 256.4,                last time consumption/overall running time: 378.8462s / 55678.1742 s
env0_first_0:                 episode reward: -77.9000,                 loss: 0.4217
env0_second_0:                 episode reward: 77.9000,                 loss: 0.6269
env1_first_0:                 episode reward: -88.5000,                 loss: nan
env1_second_0:                 episode reward: 88.5000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 261.8,                last time consumption/overall running time: 384.6036s / 56062.7778 s
env0_first_0:                 episode reward: -87.3000,                 loss: 0.4340
env0_second_0:                 episode reward: 87.3000,                 loss: 0.6274
env1_first_0:                 episode reward: -89.0000,                 loss: nan
env1_second_0:                 episode reward: 89.0000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 249.55,                last time consumption/overall running time: 363.0881s / 56425.8659 s
env0_first_0:                 episode reward: -93.5000,                 loss: 0.4649
env0_second_0:                 episode reward: 93.5000,                 loss: 0.6741
env1_first_0:                 episode reward: -87.0000,                 loss: nan
env1_second_0:                 episode reward: 87.0000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 250.55,                last time consumption/overall running time: 369.7197s / 56795.5856 s
env0_first_0:                 episode reward: -85.5000,                 loss: 0.4211
env0_second_0:                 episode reward: 85.5000,                 loss: 0.6225
env1_first_0:                 episode reward: -85.1000,                 loss: nan
env1_second_0:                 episode reward: 85.1000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 250.25,                last time consumption/overall running time: 370.6583s / 57166.2439 s
env0_first_0:                 episode reward: -92.3500,                 loss: 0.4389
env0_second_0:                 episode reward: 92.3500,                 loss: 0.6481
env1_first_0:                 episode reward: -93.8000,                 loss: nan
env1_second_0:                 episode reward: 93.8000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 253.5,                last time consumption/overall running time: 373.5985s / 57539.8424 s
env0_first_0:                 episode reward: -81.7500,                 loss: 0.4365
env0_second_0:                 episode reward: 81.7500,                 loss: 0.7071
env1_first_0:                 episode reward: -78.4000,                 loss: nan
env1_second_0:                 episode reward: 78.4000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 238.75,                last time consumption/overall running time: 349.4856s / 57889.3280 s
env0_first_0:                 episode reward: -91.4000,                 loss: 0.4492
env0_second_0:                 episode reward: 91.4000,                 loss: 0.6845
env1_first_0:                 episode reward: -93.8000,                 loss: nan
env1_second_0:                 episode reward: 93.8000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 244.1,                last time consumption/overall running time: 354.3982s / 58243.7261 s
env0_first_0:                 episode reward: -75.2000,                 loss: 0.4549
env0_second_0:                 episode reward: 75.2000,                 loss: 0.6408
env1_first_0:                 episode reward: -92.7500,                 loss: nan
env1_second_0:                 episode reward: 92.7500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 323.35,                last time consumption/overall running time: 473.9573s / 58717.6834 s
env0_first_0:                 episode reward: -77.8000,                 loss: 0.4690
env0_second_0:                 episode reward: 77.8000,                 loss: 0.6774
env1_first_0:                 episode reward: -79.5500,                 loss: nan
env1_second_0:                 episode reward: 79.5500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 239.45,                last time consumption/overall running time: 352.7811s / 59070.4645 s
env0_first_0:                 episode reward: -91.0500,                 loss: 0.4848
env0_second_0:                 episode reward: 91.0500,                 loss: 0.7019
env1_first_0:                 episode reward: -84.2500,                 loss: nan
env1_second_0:                 episode reward: 84.2500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 234.3,                last time consumption/overall running time: 342.9525s / 59413.4170 s
env0_first_0:                 episode reward: -90.6000,                 loss: 0.4773
env0_second_0:                 episode reward: 90.6000,                 loss: 0.6714
env1_first_0:                 episode reward: -92.8500,                 loss: nan
env1_second_0:                 episode reward: 92.8500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 238.9,                last time consumption/overall running time: 347.4462s / 59760.8631 s
env0_first_0:                 episode reward: -85.6500,                 loss: 0.4833
env0_second_0:                 episode reward: 85.6500,                 loss: 0.6467
env1_first_0:                 episode reward: -94.9500,                 loss: nan
env1_second_0:                 episode reward: 94.9500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 242.8,                last time consumption/overall running time: 359.1210s / 60119.9842 s
env0_first_0:                 episode reward: -90.3000,                 loss: 0.4995
env0_second_0:                 episode reward: 90.3000,                 loss: 0.7065
env1_first_0:                 episode reward: -82.2000,                 loss: nan
env1_second_0:                 episode reward: 82.2000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 240.8,                last time consumption/overall running time: 353.6680s / 60473.6521 s
env0_first_0:                 episode reward: -89.0500,                 loss: 0.5291
env0_second_0:                 episode reward: 89.0500,                 loss: 0.7190
env1_first_0:                 episode reward: -93.3500,                 loss: nan
env1_second_0:                 episode reward: 93.3500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 241.85,                last time consumption/overall running time: 357.6157s / 60831.2678 s
env0_first_0:                 episode reward: -92.4500,                 loss: 0.5115
env0_second_0:                 episode reward: 92.4500,                 loss: 0.7362
env1_first_0:                 episode reward: -93.6500,                 loss: nan
env1_second_0:                 episode reward: 93.6500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 241.05,                last time consumption/overall running time: 360.8565s / 61192.1243 s
env0_first_0:                 episode reward: -85.3500,                 loss: 0.5375
env0_second_0:                 episode reward: 85.3500,                 loss: 0.7477
env1_first_0:                 episode reward: -95.0000,                 loss: nan
env1_second_0:                 episode reward: 95.0000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 235.35,                last time consumption/overall running time: 350.6295s / 61542.7538 s
env0_first_0:                 episode reward: -88.0000,                 loss: 0.4608
env0_second_0:                 episode reward: 88.0000,                 loss: 0.7581
env1_first_0:                 episode reward: -93.8000,                 loss: nan
env1_second_0:                 episode reward: 93.8000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 238.8,                last time consumption/overall running time: 351.8655s / 61894.6193 s
env0_first_0:                 episode reward: -91.0500,                 loss: 0.5240
env0_second_0:                 episode reward: 91.0500,                 loss: 0.7378
env1_first_0:                 episode reward: -90.8000,                 loss: nan
env1_second_0:                 episode reward: 90.8000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 241.25,                last time consumption/overall running time: 359.7849s / 62254.4042 s
env0_first_0:                 episode reward: -91.6000,                 loss: 0.5596
env0_second_0:                 episode reward: 91.6000,                 loss: 0.7788
env1_first_0:                 episode reward: -90.7500,                 loss: nan
env1_second_0:                 episode reward: 90.7500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 242.05,                last time consumption/overall running time: 355.8883s / 62610.2925 s
env0_first_0:                 episode reward: -90.8500,                 loss: 0.5535
env0_second_0:                 episode reward: 90.8500,                 loss: 0.7581
env1_first_0:                 episode reward: -91.2000,                 loss: nan
env1_second_0:                 episode reward: 91.2000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 241.15,                last time consumption/overall running time: 352.0807s / 62962.3732 s
env0_first_0:                 episode reward: -90.7500,                 loss: 0.5155
env0_second_0:                 episode reward: 90.7500,                 loss: 0.7092
env1_first_0:                 episode reward: -92.2500,                 loss: nan
env1_second_0:                 episode reward: 92.2500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 240.3,                last time consumption/overall running time: 354.3383s / 63316.7116 s
env0_first_0:                 episode reward: -96.1500,                 loss: 0.5210
env0_second_0:                 episode reward: 96.1500,                 loss: 0.7802
env1_first_0:                 episode reward: -89.6000,                 loss: nan
env1_second_0:                 episode reward: 89.6000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 239.3,                last time consumption/overall running time: 352.3603s / 63669.0719 s
env0_first_0:                 episode reward: -95.9000,                 loss: 0.5525
env0_second_0:                 episode reward: 95.9000,                 loss: 0.7126
env1_first_0:                 episode reward: -92.2500,                 loss: nan
env1_second_0:                 episode reward: 92.2500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 238.85,                last time consumption/overall running time: 354.3228s / 64023.3946 s
env0_first_0:                 episode reward: -90.4500,                 loss: 0.5632
env0_second_0:                 episode reward: 90.4500,                 loss: 0.7621
env1_first_0:                 episode reward: -93.4000,                 loss: nan
env1_second_0:                 episode reward: 93.4000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 240.55,                last time consumption/overall running time: 352.9933s / 64376.3880 s
env0_first_0:                 episode reward: -93.0000,                 loss: 0.5548
env0_second_0:                 episode reward: 93.0000,                 loss: 0.7508
env1_first_0:                 episode reward: -91.1500,                 loss: nan
env1_second_0:                 episode reward: 91.1500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 235.95,                last time consumption/overall running time: 348.2515s / 64724.6395 s
env0_first_0:                 episode reward: -94.9500,                 loss: 0.5529
env0_second_0:                 episode reward: 94.9500,                 loss: 0.7996
env1_first_0:                 episode reward: -84.0500,                 loss: nan
env1_second_0:                 episode reward: 84.0500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 239.45,                last time consumption/overall running time: 352.5305s / 65077.1700 s
env0_first_0:                 episode reward: -92.4000,                 loss: 0.5144
env0_second_0:                 episode reward: 92.4000,                 loss: 0.7622
env1_first_0:                 episode reward: -86.5000,                 loss: nan
env1_second_0:                 episode reward: 86.5000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 244.5,                last time consumption/overall running time: 360.5880s / 65437.7581 s
env0_first_0:                 episode reward: -84.3000,                 loss: 0.5534
env0_second_0:                 episode reward: 84.3000,                 loss: 0.8344
env1_first_0:                 episode reward: -91.7000,                 loss: nan
env1_second_0:                 episode reward: 91.7000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 243.95,                last time consumption/overall running time: 359.4474s / 65797.2055 s
env0_first_0:                 episode reward: -89.7000,                 loss: 0.5752
env0_second_0:                 episode reward: 89.7000,                 loss: 0.8296
env1_first_0:                 episode reward: -89.4500,                 loss: nan
env1_second_0:                 episode reward: 89.4500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 230.75,                last time consumption/overall running time: 337.9779s / 66135.1834 s
env0_first_0:                 episode reward: -90.8500,                 loss: 0.5768
env0_second_0:                 episode reward: 90.8500,                 loss: 0.8161
env1_first_0:                 episode reward: -90.3500,                 loss: nan
env1_second_0:                 episode reward: 90.3500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 241.3,                last time consumption/overall running time: 356.1536s / 66491.3369 s
env0_first_0:                 episode reward: -85.8500,                 loss: 0.5759
env0_second_0:                 episode reward: 85.8500,                 loss: 0.8602
env1_first_0:                 episode reward: -92.7000,                 loss: nan
env1_second_0:                 episode reward: 92.7000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 243.1,                last time consumption/overall running time: 356.1448s / 66847.4817 s
env0_first_0:                 episode reward: -83.9000,                 loss: 0.7044
env0_second_0:                 episode reward: 83.9000,                 loss: 0.9539
env1_first_0:                 episode reward: -88.6500,                 loss: nan
env1_second_0:                 episode reward: 88.6500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 240.7,                last time consumption/overall running time: 353.9309s / 67201.4126 s
env0_first_0:                 episode reward: -84.9000,                 loss: 0.6679
env0_second_0:                 episode reward: 84.9000,                 loss: 0.9715
env1_first_0:                 episode reward: -89.2000,                 loss: nan
env1_second_0:                 episode reward: 89.2000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 232.75,                last time consumption/overall running time: 342.7936s / 67544.2063 s
env0_first_0:                 episode reward: -83.9000,                 loss: 0.6621
env0_second_0:                 episode reward: 83.9000,                 loss: 1.0788
env1_first_0:                 episode reward: -87.1500,                 loss: nan
env1_second_0:                 episode reward: 87.1500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 231.4,                last time consumption/overall running time: 339.8027s / 67884.0089 s
env0_first_0:                 episode reward: -74.5500,                 loss: 0.6302
env0_second_0:                 episode reward: 74.5500,                 loss: 0.9501
env1_first_0:                 episode reward: -97.9500,                 loss: nan
env1_second_0:                 episode reward: 97.9500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 226.75,                last time consumption/overall running time: 337.3737s / 68221.3827 s
env0_first_0:                 episode reward: -91.0000,                 loss: 0.6034
env0_second_0:                 episode reward: 91.0000,                 loss: 1.0046
env1_first_0:                 episode reward: -84.7000,                 loss: nan
env1_second_0:                 episode reward: 84.7000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 311.7,                last time consumption/overall running time: 460.3184s / 68681.7011 s
env0_first_0:                 episode reward: -70.7000,                 loss: 0.6590
env0_second_0:                 episode reward: 70.7000,                 loss: 0.9960
env1_first_0:                 episode reward: -92.6000,                 loss: nan
env1_second_0:                 episode reward: 92.6000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 242.25,                last time consumption/overall running time: 354.0120s / 69035.7130 s
env0_first_0:                 episode reward: -84.4000,                 loss: 0.6411
env0_second_0:                 episode reward: 84.4000,                 loss: 0.9716
env1_first_0:                 episode reward: -95.1000,                 loss: nan
env1_second_0:                 episode reward: 95.1000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 235.25,                last time consumption/overall running time: 345.2876s / 69381.0006 s
env0_first_0:                 episode reward: -87.9500,                 loss: 0.6375
env0_second_0:                 episode reward: 87.9500,                 loss: 1.1172
env1_first_0:                 episode reward: -87.6000,                 loss: nan
env1_second_0:                 episode reward: 87.6000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 255.7,                last time consumption/overall running time: 375.1950s / 69756.1955 s
env0_first_0:                 episode reward: -77.6500,                 loss: 0.6738
env0_second_0:                 episode reward: 77.6500,                 loss: 1.1041
env1_first_0:                 episode reward: -93.6500,                 loss: nan
env1_second_0:                 episode reward: 93.6500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 240.6,                last time consumption/overall running time: 353.0886s / 70109.2842 s
env0_first_0:                 episode reward: -88.4000,                 loss: 0.6858
env0_second_0:                 episode reward: 88.4000,                 loss: 1.0904
env1_first_0:                 episode reward: -90.8000,                 loss: nan
env1_second_0:                 episode reward: 90.8000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 238.7,                last time consumption/overall running time: 353.0729s / 70462.3571 s
env0_first_0:                 episode reward: -84.4500,                 loss: 0.6740
env0_second_0:                 episode reward: 84.4500,                 loss: 1.1160
env1_first_0:                 episode reward: -92.5500,                 loss: nan
env1_second_0:                 episode reward: 92.5500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 253.85,                last time consumption/overall running time: 372.5033s / 70834.8604 s
env0_first_0:                 episode reward: -89.7500,                 loss: 0.6512
env0_second_0:                 episode reward: 89.7500,                 loss: 1.0658
env1_first_0:                 episode reward: -81.3500,                 loss: nan
env1_second_0:                 episode reward: 81.3500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 223.8,                last time consumption/overall running time: 330.7399s / 71165.6003 s
env0_first_0:                 episode reward: -89.6500,                 loss: 0.6164
env0_second_0:                 episode reward: 89.6500,                 loss: 1.0861
env1_first_0:                 episode reward: -89.4500,                 loss: nan
env1_second_0:                 episode reward: 89.4500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 247.45,                last time consumption/overall running time: 364.1488s / 71529.7491 s
env0_first_0:                 episode reward: -89.4500,                 loss: 0.6000
env0_second_0:                 episode reward: 89.4500,                 loss: 1.0146
env1_first_0:                 episode reward: -82.7500,                 loss: nan
env1_second_0:                 episode reward: 82.7500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 240.2,                last time consumption/overall running time: 354.2987s / 71884.0479 s
env0_first_0:                 episode reward: -90.9000,                 loss: 0.5787
env0_second_0:                 episode reward: 90.9000,                 loss: 0.9683
env1_first_0:                 episode reward: -87.9000,                 loss: nan
env1_second_0:                 episode reward: 87.9000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 233.85,                last time consumption/overall running time: 343.4485s / 72227.4963 s
env0_first_0:                 episode reward: -80.5000,                 loss: 0.6197
env0_second_0:                 episode reward: 80.5000,                 loss: 0.9915
env1_first_0:                 episode reward: -89.8500,                 loss: nan
env1_second_0:                 episode reward: 89.8500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 239.2,                last time consumption/overall running time: 352.4390s / 72579.9353 s
env0_first_0:                 episode reward: -79.9500,                 loss: 0.5623
env0_second_0:                 episode reward: 79.9500,                 loss: 0.9648
env1_first_0:                 episode reward: -84.1000,                 loss: nan
env1_second_0:                 episode reward: 84.1000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 238.45,                last time consumption/overall running time: 355.0262s / 72934.9615 s
env0_first_0:                 episode reward: -83.7500,                 loss: 0.6138
env0_second_0:                 episode reward: 83.7500,                 loss: 1.0321
env1_first_0:                 episode reward: -86.1000,                 loss: nan
env1_second_0:                 episode reward: 86.1000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 232.0,                last time consumption/overall running time: 339.8829s / 73274.8444 s
env0_first_0:                 episode reward: -87.7500,                 loss: 0.5982
env0_second_0:                 episode reward: 87.7500,                 loss: 0.9896
env1_first_0:                 episode reward: -85.4500,                 loss: nan
env1_second_0:                 episode reward: 85.4500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 228.1,                last time consumption/overall running time: 336.6195s / 73611.4639 s
env0_first_0:                 episode reward: -89.6000,                 loss: 0.6018
env0_second_0:                 episode reward: 89.6000,                 loss: 1.0021
env1_first_0:                 episode reward: -92.4500,                 loss: nan
env1_second_0:                 episode reward: 92.4500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 237.85,                last time consumption/overall running time: 351.7505s / 73963.2144 s
env0_first_0:                 episode reward: -77.7000,                 loss: 0.5675
env0_second_0:                 episode reward: 77.7000,                 loss: 1.0137
env1_first_0:                 episode reward: -96.2500,                 loss: nan
env1_second_0:                 episode reward: 96.2500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 236.0,                last time consumption/overall running time: 344.8110s / 74308.0254 s
env0_first_0:                 episode reward: -86.4500,                 loss: 0.5962
env0_second_0:                 episode reward: 86.4500,                 loss: 1.0491
env1_first_0:                 episode reward: -80.6500,                 loss: nan
env1_second_0:                 episode reward: 80.6500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 240.95,                last time consumption/overall running time: 359.3621s / 74667.3875 s
env0_first_0:                 episode reward: -85.2500,                 loss: 0.5639
env0_second_0:                 episode reward: 85.2500,                 loss: 1.0778
env1_first_0:                 episode reward: -89.6500,                 loss: nan
env1_second_0:                 episode reward: 89.6500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 344.75,                last time consumption/overall running time: 502.9788s / 75170.3663 s
env0_first_0:                 episode reward: -81.7000,                 loss: 0.5815
env0_second_0:                 episode reward: 81.7000,                 loss: 1.1339
env1_first_0:                 episode reward: -61.5000,                 loss: nan
env1_second_0:                 episode reward: 61.5000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 244.45,                last time consumption/overall running time: 358.5550s / 75528.9213 s
env0_first_0:                 episode reward: -79.0000,                 loss: 0.6543
env0_second_0:                 episode reward: 79.0000,                 loss: 1.1111
env1_first_0:                 episode reward: -86.9500,                 loss: nan
env1_second_0:                 episode reward: 86.9500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 313.25,                last time consumption/overall running time: 459.9982s / 75988.9195 s
env0_first_0:                 episode reward: -86.6500,                 loss: 0.6132
env0_second_0:                 episode reward: 86.6500,                 loss: 1.0461
env1_first_0:                 episode reward: -82.3000,                 loss: nan
env1_second_0:                 episode reward: 82.3000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 265.25,                last time consumption/overall running time: 392.3450s / 76381.2645 s
env0_first_0:                 episode reward: -88.6000,                 loss: 0.5968
env0_second_0:                 episode reward: 88.6000,                 loss: 1.0863
env1_first_0:                 episode reward: -77.3000,                 loss: nan
env1_second_0:                 episode reward: 77.3000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 238.05,                last time consumption/overall running time: 352.1208s / 76733.3853 s
env0_first_0:                 episode reward: -86.4500,                 loss: 0.6434
env0_second_0:                 episode reward: 86.4500,                 loss: 1.1354
env1_first_0:                 episode reward: -79.0500,                 loss: nan
env1_second_0:                 episode reward: 79.0500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 231.35,                last time consumption/overall running time: 341.8632s / 77075.2485 s
env0_first_0:                 episode reward: -86.7500,                 loss: 0.6127
env0_second_0:                 episode reward: 86.7500,                 loss: 1.0876
env1_first_0:                 episode reward: -88.7000,                 loss: nan
env1_second_0:                 episode reward: 88.7000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 227.4,                last time consumption/overall running time: 336.0814s / 77411.3299 s
env0_first_0:                 episode reward: -94.4500,                 loss: 0.5580
env0_second_0:                 episode reward: 94.4500,                 loss: 1.0350
env1_first_0:                 episode reward: -91.5500,                 loss: nan
env1_second_0:                 episode reward: 91.5500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 241.55,                last time consumption/overall running time: 354.9286s / 77766.2584 s
env0_first_0:                 episode reward: -74.7000,                 loss: 0.6029
env0_second_0:                 episode reward: 74.7000,                 loss: 0.9926
env1_first_0:                 episode reward: -94.6500,                 loss: nan
env1_second_0:                 episode reward: 94.6500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 233.2,                last time consumption/overall running time: 342.8034s / 78109.0618 s
env0_first_0:                 episode reward: -85.4000,                 loss: 0.5952
env0_second_0:                 episode reward: 85.4000,                 loss: 1.0324
env1_first_0:                 episode reward: -84.7000,                 loss: nan
env1_second_0:                 episode reward: 84.7000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 311.5,                last time consumption/overall running time: 456.0394s / 78565.1012 s
env0_first_0:                 episode reward: -73.1000,                 loss: 0.5960
env0_second_0:                 episode reward: 73.1000,                 loss: 0.8461
env1_first_0:                 episode reward: -72.4500,                 loss: nan
env1_second_0:                 episode reward: 72.4500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 299.95,                last time consumption/overall running time: 437.7272s / 79002.8285 s
env0_first_0:                 episode reward: -85.2500,                 loss: 0.5497
env0_second_0:                 episode reward: 85.2500,                 loss: 0.9392
env1_first_0:                 episode reward: -76.0500,                 loss: nan
env1_second_0:                 episode reward: 76.0500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 222.5,                last time consumption/overall running time: 329.5241s / 79332.3525 s
env0_first_0:                 episode reward: -92.2500,                 loss: 0.5081
env0_second_0:                 episode reward: 92.2500,                 loss: 0.8346
env1_first_0:                 episode reward: -79.7500,                 loss: nan
env1_second_0:                 episode reward: 79.7500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 227.55,                last time consumption/overall running time: 340.5848s / 79672.9374 s
env0_first_0:                 episode reward: -85.7000,                 loss: 0.5393
env0_second_0:                 episode reward: 85.7000,                 loss: 0.8971
env1_first_0:                 episode reward: -92.6500,                 loss: nan
env1_second_0:                 episode reward: 92.6500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 228.2,                last time consumption/overall running time: 336.4912s / 80009.4286 s
env0_first_0:                 episode reward: -85.9500,                 loss: 0.5217
env0_second_0:                 episode reward: 85.9500,                 loss: 0.9035
env1_first_0:                 episode reward: -94.5000,                 loss: nan
env1_second_0:                 episode reward: 94.5000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 230.8,                last time consumption/overall running time: 337.9975s / 80347.4261 s
env0_first_0:                 episode reward: -85.1000,                 loss: 0.5345
env0_second_0:                 episode reward: 85.1000,                 loss: 0.8442
env1_first_0:                 episode reward: -93.5000,                 loss: nan
env1_second_0:                 episode reward: 93.5000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 331.45,                last time consumption/overall running time: 487.9884s / 80835.4145 s
env0_first_0:                 episode reward: -81.8500,                 loss: 0.5033
env0_second_0:                 episode reward: 81.8500,                 loss: 0.8158
env1_first_0:                 episode reward: -68.1000,                 loss: nan
env1_second_0:                 episode reward: 68.1000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 241.65,                last time consumption/overall running time: 357.6856s / 81193.1001 s
env0_first_0:                 episode reward: -69.9500,                 loss: 0.4510
env0_second_0:                 episode reward: 69.9500,                 loss: 0.7791
env1_first_0:                 episode reward: -82.0000,                 loss: nan
env1_second_0:                 episode reward: 82.0000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 381.45,                last time consumption/overall running time: 560.6294s / 81753.7296 s
env0_first_0:                 episode reward: -74.8500,                 loss: 0.5374
env0_second_0:                 episode reward: 74.8500,                 loss: 0.8246
env1_first_0:                 episode reward: -75.8000,                 loss: nan
env1_second_0:                 episode reward: 75.8000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 567.75,                last time consumption/overall running time: 835.5764s / 82589.3060 s
env0_first_0:                 episode reward: -63.0000,                 loss: 0.4307
env0_second_0:                 episode reward: 63.0000,                 loss: 0.7595
env1_first_0:                 episode reward: -66.5000,                 loss: nan
env1_second_0:                 episode reward: 66.5000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 626.45,                last time consumption/overall running time: 918.2883s / 83507.5943 s
env0_first_0:                 episode reward: -71.7500,                 loss: 0.3981
env0_second_0:                 episode reward: 71.7500,                 loss: 0.6697
env1_first_0:                 episode reward: -69.8000,                 loss: nan
env1_second_0:                 episode reward: 69.8000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 429.8,                last time consumption/overall running time: 636.1268s / 84143.7211 s
env0_first_0:                 episode reward: -69.0500,                 loss: 0.3211
env0_second_0:                 episode reward: 69.0500,                 loss: 0.6183
env1_first_0:                 episode reward: -81.0000,                 loss: nan
env1_second_0:                 episode reward: 81.0000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 309.6,                last time consumption/overall running time: 453.3637s / 84597.0848 s
env0_first_0:                 episode reward: -88.7500,                 loss: 0.3369
env0_second_0:                 episode reward: 88.7500,                 loss: 0.5455
env1_first_0:                 episode reward: -85.6000,                 loss: nan
env1_second_0:                 episode reward: 85.6000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 393.45,                last time consumption/overall running time: 578.6700s / 85175.7548 s
env0_first_0:                 episode reward: -80.6000,                 loss: 0.3043
env0_second_0:                 episode reward: 80.6000,                 loss: 0.4834
env1_first_0:                 episode reward: -84.2000,                 loss: nan
env1_second_0:                 episode reward: 84.2000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 622.15,                last time consumption/overall running time: 910.8106s / 86086.5654 s
env0_first_0:                 episode reward: -70.9500,                 loss: 0.2983
env0_second_0:                 episode reward: 70.9500,                 loss: 0.4457
env1_first_0:                 episode reward: -65.9500,                 loss: nan
env1_second_0:                 episode reward: 65.9500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 299.35,                last time consumption/overall running time: 442.6846s / 86529.2500 s
env0_first_0:                 episode reward: -83.5000,                 loss: 0.3136
env0_second_0:                 episode reward: 83.5000,                 loss: 0.4493
env1_first_0:                 episode reward: -79.0500,                 loss: nan
env1_second_0:                 episode reward: 79.0500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 391.9,                last time consumption/overall running time: 578.1155s / 87107.3655 s
env0_first_0:                 episode reward: -77.9000,                 loss: 0.3430
env0_second_0:                 episode reward: 77.9000,                 loss: 0.4937
env1_first_0:                 episode reward: -84.2500,                 loss: nan
env1_second_0:                 episode reward: 84.2500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 404.8,                last time consumption/overall running time: 596.4312s / 87703.7967 s
env0_first_0:                 episode reward: -75.1500,                 loss: 0.3587
env0_second_0:                 episode reward: 75.1500,                 loss: 0.5268
env1_first_0:                 episode reward: -80.0500,                 loss: nan
env1_second_0:                 episode reward: 80.0500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 309.0,                last time consumption/overall running time: 455.2518s / 88159.0485 s
env0_first_0:                 episode reward: -87.7000,                 loss: 0.3832
env0_second_0:                 episode reward: 87.7000,                 loss: 0.5805
env1_first_0:                 episode reward: -78.7000,                 loss: nan
env1_second_0:                 episode reward: 78.7000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 316.4,                last time consumption/overall running time: 466.6329s / 88625.6814 s
env0_first_0:                 episode reward: -92.0500,                 loss: 0.3602
env0_second_0:                 episode reward: 92.0500,                 loss: 0.5227
env1_first_0:                 episode reward: -94.5000,                 loss: nan
env1_second_0:                 episode reward: 94.5000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 249.9,                last time consumption/overall running time: 367.8787s / 88993.5601 s
env0_first_0:                 episode reward: -92.2000,                 loss: 0.3722
env0_second_0:                 episode reward: 92.2000,                 loss: 0.5910
env1_first_0:                 episode reward: -88.2000,                 loss: nan
env1_second_0:                 episode reward: 88.2000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 544.15,                last time consumption/overall running time: 797.0444s / 89790.6045 s
env0_first_0:                 episode reward: -87.7000,                 loss: 0.4233
env0_second_0:                 episode reward: 87.7000,                 loss: 0.6191
env1_first_0:                 episode reward: -76.5000,                 loss: nan
env1_second_0:                 episode reward: 76.5000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 624.95,                last time consumption/overall running time: 917.7593s / 90708.3638 s
env0_first_0:                 episode reward: -84.4000,                 loss: 0.3820
env0_second_0:                 episode reward: 84.4000,                 loss: 0.5625
env1_first_0:                 episode reward: -83.2000,                 loss: nan
env1_second_0:                 episode reward: 83.2000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 478.9,                last time consumption/overall running time: 702.7248s / 91411.0886 s
env0_first_0:                 episode reward: -92.4500,                 loss: 0.3580
env0_second_0:                 episode reward: 92.4500,                 loss: 0.5069
env1_first_0:                 episode reward: -79.3000,                 loss: nan
env1_second_0:                 episode reward: 79.3000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 229.75,                last time consumption/overall running time: 341.8611s / 91752.9497 s
env0_first_0:                 episode reward: -81.0500,                 loss: 0.3535
env0_second_0:                 episode reward: 81.0500,                 loss: 0.4903
env1_first_0:                 episode reward: -91.7500,                 loss: nan
env1_second_0:                 episode reward: 91.7500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 244.15,                last time consumption/overall running time: 362.1455s / 92115.0952 s
env0_first_0:                 episode reward: -78.2500,                 loss: 0.3916
env0_second_0:                 episode reward: 78.2500,                 loss: 0.5364
env1_first_0:                 episode reward: -91.4500,                 loss: nan
env1_second_0:                 episode reward: 91.4500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 240.9,                last time consumption/overall running time: 361.1589s / 92476.2540 s
env0_first_0:                 episode reward: -91.6500,                 loss: 0.3939
env0_second_0:                 episode reward: 91.6500,                 loss: 0.5380
env1_first_0:                 episode reward: -83.5500,                 loss: nan
env1_second_0:                 episode reward: 83.5500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 231.75,                last time consumption/overall running time: 343.5314s / 92819.7854 s
env0_first_0:                 episode reward: -93.1500,                 loss: 0.4300
env0_second_0:                 episode reward: 93.1500,                 loss: 0.5895
env1_first_0:                 episode reward: -84.3500,                 loss: nan
env1_second_0:                 episode reward: 84.3500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 319.5,                last time consumption/overall running time: 474.8581s / 93294.6435 s
env0_first_0:                 episode reward: -88.0000,                 loss: 0.3899
env0_second_0:                 episode reward: 88.0000,                 loss: 0.5659
env1_first_0:                 episode reward: -89.9000,                 loss: nan
env1_second_0:                 episode reward: 89.9000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 233.7,                last time consumption/overall running time: 345.9890s / 93640.6326 s
env0_first_0:                 episode reward: -88.0500,                 loss: 0.4697
env0_second_0:                 episode reward: 88.0500,                 loss: 0.6611
env1_first_0:                 episode reward: -82.3500,                 loss: nan
env1_second_0:                 episode reward: 82.3500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 230.4,                last time consumption/overall running time: 341.8172s / 93982.4498 s
env0_first_0:                 episode reward: -90.1000,                 loss: 0.5729
env0_second_0:                 episode reward: 90.1000,                 loss: 0.7310
env1_first_0:                 episode reward: -71.7500,                 loss: nan
env1_second_0:                 episode reward: 71.7500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 319.4,                last time consumption/overall running time: 471.6389s / 94454.0887 s
env0_first_0:                 episode reward: -90.4000,                 loss: 0.5693
env0_second_0:                 episode reward: 90.4000,                 loss: 0.8062
env1_first_0:                 episode reward: -80.3000,                 loss: nan
env1_second_0:                 episode reward: 80.3000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 313.65,                last time consumption/overall running time: 461.4882s / 94915.5769 s
env0_first_0:                 episode reward: -86.5000,                 loss: 0.5954
env0_second_0:                 episode reward: 86.5000,                 loss: 0.8616
env1_first_0:                 episode reward: -88.8000,                 loss: nan
env1_second_0:                 episode reward: 88.8000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 230.4,                last time consumption/overall running time: 344.1334s / 95259.7103 s
env0_first_0:                 episode reward: -86.6500,                 loss: 0.6103
env0_second_0:                 episode reward: 86.6500,                 loss: 0.8486
env1_first_0:                 episode reward: -87.5000,                 loss: nan
env1_second_0:                 episode reward: 87.5000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 236.4,                last time consumption/overall running time: 348.1017s / 95607.8119 s
env0_first_0:                 episode reward: -85.3500,                 loss: 0.6286
env0_second_0:                 episode reward: 85.3500,                 loss: 0.9042
env1_first_0:                 episode reward: -83.9500,                 loss: nan
env1_second_0:                 episode reward: 83.9500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 221.85,                last time consumption/overall running time: 325.4557s / 95933.2677 s
env0_first_0:                 episode reward: -92.9000,                 loss: 0.5721
env0_second_0:                 episode reward: 92.9000,                 loss: 0.9112
env1_first_0:                 episode reward: -76.4500,                 loss: nan
env1_second_0:                 episode reward: 76.4500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 220.3,                last time consumption/overall running time: 324.7490s / 96258.0167 s
env0_first_0:                 episode reward: -91.1000,                 loss: 0.5696
env0_second_0:                 episode reward: 91.1000,                 loss: 0.8905
env1_first_0:                 episode reward: -85.0500,                 loss: nan
env1_second_0:                 episode reward: 85.0500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 259.6,                last time consumption/overall running time: 379.6639s / 96637.6806 s
env0_first_0:                 episode reward: -73.9000,                 loss: 0.5806
env0_second_0:                 episode reward: 73.9000,                 loss: 0.8424
env1_first_0:                 episode reward: -91.0000,                 loss: nan
env1_second_0:                 episode reward: 91.0000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 221.75,                last time consumption/overall running time: 326.0239s / 96963.7045 s
env0_first_0:                 episode reward: -84.4000,                 loss: 0.5669
env0_second_0:                 episode reward: 84.4000,                 loss: 0.8542
env1_first_0:                 episode reward: -91.5000,                 loss: nan
env1_second_0:                 episode reward: 91.5000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 381.0,                last time consumption/overall running time: 560.0361s / 97523.7406 s
env0_first_0:                 episode reward: -82.1500,                 loss: 0.5565
env0_second_0:                 episode reward: 82.1500,                 loss: 0.8529
env1_first_0:                 episode reward: -88.3000,                 loss: nan
env1_second_0:                 episode reward: 88.3000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 300.4,                last time consumption/overall running time: 440.0593s / 97963.7999 s
env0_first_0:                 episode reward: -82.2000,                 loss: 0.5215
env0_second_0:                 episode reward: 82.2000,                 loss: 0.7866
env1_first_0:                 episode reward: -85.2000,                 loss: nan
env1_second_0:                 episode reward: 85.2000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 330.1,                last time consumption/overall running time: 474.4513s / 98438.2512 s
env0_first_0:                 episode reward: -77.6500,                 loss: 0.5489
env0_second_0:                 episode reward: 77.6500,                 loss: 0.7860
env1_first_0:                 episode reward: -86.0000,                 loss: nan
env1_second_0:                 episode reward: 86.0000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 239.85,                last time consumption/overall running time: 346.0897s / 98784.3409 s
env0_first_0:                 episode reward: -87.5000,                 loss: 0.5205
env0_second_0:                 episode reward: 87.5000,                 loss: 0.7920
env1_first_0:                 episode reward: -86.6500,                 loss: nan
env1_second_0:                 episode reward: 86.6500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 307.45,                last time consumption/overall running time: 444.5569s / 99228.8977 s
env0_first_0:                 episode reward: -87.0500,                 loss: 0.5533
env0_second_0:                 episode reward: 87.0500,                 loss: 0.8138
env1_first_0:                 episode reward: -90.3000,                 loss: nan
env1_second_0:                 episode reward: 90.3000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 469.0,                last time consumption/overall running time: 676.2857s / 99905.1835 s
env0_first_0:                 episode reward: -88.9000,                 loss: 0.4506
env0_second_0:                 episode reward: 88.9000,                 loss: 0.6731
env1_first_0:                 episode reward: -81.4500,                 loss: nan
env1_second_0:                 episode reward: 81.4500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 404.35,                last time consumption/overall running time: 582.3153s / 100487.4988 s
env0_first_0:                 episode reward: -89.0000,                 loss: 0.4324
env0_second_0:                 episode reward: 89.0000,                 loss: 0.6290
env1_first_0:                 episode reward: -92.8000,                 loss: nan
env1_second_0:                 episode reward: 92.8000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 890.7,                last time consumption/overall running time: 1285.4077s / 101772.9064 s
env0_first_0:                 episode reward: -78.3000,                 loss: 0.3528
env0_second_0:                 episode reward: 78.3000,                 loss: 0.5157
env1_first_0:                 episode reward: -81.3000,                 loss: nan
env1_second_0:                 episode reward: 81.3000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 416.1,                last time consumption/overall running time: 599.8016s / 102372.7080 s
env0_first_0:                 episode reward: -88.8000,                 loss: 0.3219
env0_second_0:                 episode reward: 88.8000,                 loss: 0.4445
env1_first_0:                 episode reward: -91.1500,                 loss: nan
env1_second_0:                 episode reward: 91.1500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 712.6,                last time consumption/overall running time: 1033.3561s / 103406.0642 s
env0_first_0:                 episode reward: -72.3000,                 loss: 0.3300
env0_second_0:                 episode reward: 72.3000,                 loss: 0.4282
env1_first_0:                 episode reward: -74.1000,                 loss: nan
env1_second_0:                 episode reward: 74.1000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 406.3,                last time consumption/overall running time: 591.2028s / 103997.2669 s
env0_first_0:                 episode reward: -89.0000,                 loss: 0.3261
env0_second_0:                 episode reward: 89.0000,                 loss: 0.4760
env1_first_0:                 episode reward: -91.5000,                 loss: nan
env1_second_0:                 episode reward: 91.5000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 479.45,                last time consumption/overall running time: 689.2362s / 104686.5031 s
env0_first_0:                 episode reward: -95.1500,                 loss: 0.3370
env0_second_0:                 episode reward: 95.1500,                 loss: 0.4817
env1_first_0:                 episode reward: -78.6000,                 loss: nan
env1_second_0:                 episode reward: 78.6000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 616.1,                last time consumption/overall running time: 892.7006s / 105579.2038 s
env0_first_0:                 episode reward: -86.4000,                 loss: 0.3512
env0_second_0:                 episode reward: 86.4000,                 loss: 0.4864
env1_first_0:                 episode reward: -85.8500,                 loss: nan
env1_second_0:                 episode reward: 85.8500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 852.75,                last time consumption/overall running time: 1227.2167s / 106806.4205 s
env0_first_0:                 episode reward: -91.0500,                 loss: 0.3316
env0_second_0:                 episode reward: 91.0500,                 loss: 0.4416
env1_first_0:                 episode reward: -90.7000,                 loss: nan
env1_second_0:                 episode reward: 90.7000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 860.65,                last time consumption/overall running time: 1236.3987s / 108042.8191 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.2569
env0_second_0:                 episode reward: 88.5500,                 loss: 0.4126
env1_first_0:                 episode reward: -88.5000,                 loss: nan
env1_second_0:                 episode reward: 88.5000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 523.15,                last time consumption/overall running time: 762.0371s / 108804.8562 s
env0_first_0:                 episode reward: -80.9500,                 loss: 0.3037
env0_second_0:                 episode reward: 80.9500,                 loss: 0.4032
env1_first_0:                 episode reward: -84.2000,                 loss: nan
env1_second_0:                 episode reward: 84.2000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 523.6,                last time consumption/overall running time: 755.9716s / 109560.8278 s
env0_first_0:                 episode reward: -85.7000,                 loss: 0.3368
env0_second_0:                 episode reward: 85.7000,                 loss: 0.4897
env1_first_0:                 episode reward: -86.9500,                 loss: nan
env1_second_0:                 episode reward: 86.9500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 513.2,                last time consumption/overall running time: 742.0582s / 110302.8860 s
env0_first_0:                 episode reward: -91.6000,                 loss: 0.3637
env0_second_0:                 episode reward: 91.6000,                 loss: 0.5061
env1_first_0:                 episode reward: -84.4500,                 loss: nan
env1_second_0:                 episode reward: 84.4500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 393.85,                last time consumption/overall running time: 569.4137s / 110872.2997 s
env0_first_0:                 episode reward: -74.8000,                 loss: 0.4007
env0_second_0:                 episode reward: 74.8000,                 loss: 0.5365
env1_first_0:                 episode reward: -85.4500,                 loss: nan
env1_second_0:                 episode reward: 85.4500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 1094.35,                last time consumption/overall running time: 1574.9837s / 112447.2833 s
env0_first_0:                 episode reward: -87.0000,                 loss: 0.3816
env0_second_0:                 episode reward: 87.0000,                 loss: 0.5423
env1_first_0:                 episode reward: -93.8000,                 loss: nan
env1_second_0:                 episode reward: 93.8000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1477.85,                last time consumption/overall running time: 2128.8593s / 114576.1426 s
env0_first_0:                 episode reward: -85.7000,                 loss: 0.2492
env0_second_0:                 episode reward: 85.7000,                 loss: 0.3412
env1_first_0:                 episode reward: -86.2000,                 loss: nan
env1_second_0:                 episode reward: 86.2000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1101.3,                last time consumption/overall running time: 1584.6317s / 116160.7743 s
env0_first_0:                 episode reward: -85.9500,                 loss: 0.1518
env0_second_0:                 episode reward: 85.9500,                 loss: 0.2135
env1_first_0:                 episode reward: -92.5000,                 loss: nan
env1_second_0:                 episode reward: 92.5000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 902.35,                last time consumption/overall running time: 1303.6345s / 117464.4088 s
env0_first_0:                 episode reward: -77.9500,                 loss: 0.1865
env0_second_0:                 episode reward: 77.9500,                 loss: 0.2767
env1_first_0:                 episode reward: -69.7000,                 loss: nan
env1_second_0:                 episode reward: 69.7000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 443.25,                last time consumption/overall running time: 643.2921s / 118107.7009 s
env0_first_0:                 episode reward: -75.4000,                 loss: 0.2166
env0_second_0:                 episode reward: 75.4000,                 loss: 0.3568
env1_first_0:                 episode reward: -79.7500,                 loss: nan
env1_second_0:                 episode reward: 79.7500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 587.3,                last time consumption/overall running time: 844.0927s / 118951.7936 s
env0_first_0:                 episode reward: -75.3500,                 loss: 0.2684
env0_second_0:                 episode reward: 75.3500,                 loss: 0.4413
env1_first_0:                 episode reward: -83.6500,                 loss: nan
env1_second_0:                 episode reward: 83.6500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1298.55,                last time consumption/overall running time: 1875.0798s / 120826.8733 s
env0_first_0:                 episode reward: -83.8000,                 loss: 0.2704
env0_second_0:                 episode reward: 83.8000,                 loss: 0.4524
env1_first_0:                 episode reward: -90.1000,                 loss: nan
env1_second_0:                 episode reward: 90.1000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 910.6,                last time consumption/overall running time: 1305.8355s / 122132.7089 s
env0_first_0:                 episode reward: -93.2000,                 loss: 0.2929
env0_second_0:                 episode reward: 93.2000,                 loss: 0.5170
env1_first_0:                 episode reward: -88.3500,                 loss: nan
env1_second_0:                 episode reward: 88.3500,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1359.65,                last time consumption/overall running time: 1958.5364s / 124091.2452 s
env0_first_0:                 episode reward: -92.5000,                 loss: 0.2569
env0_second_0:                 episode reward: 92.5000,                 loss: 0.4366
env1_first_0:                 episode reward: -91.1000,                 loss: nan
env1_second_0:                 episode reward: 91.1000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1065.7,                last time consumption/overall running time: 1529.8063s / 125621.0515 s
env0_first_0:                 episode reward: -86.8500,                 loss: 0.2204
env0_second_0:                 episode reward: 86.8500,                 loss: 0.3831
env1_first_0:                 episode reward: -84.8000,                 loss: nan
env1_second_0:                 episode reward: 84.8000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 447.35,                last time consumption/overall running time: 637.2161s / 126258.2676 s
env0_first_0:                 episode reward: -86.4000,                 loss: 0.2361
env0_second_0:                 episode reward: 86.4000,                 loss: 0.3936
env1_first_0:                 episode reward: -69.0500,                 loss: nan
env1_second_0:                 episode reward: 69.0500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 398.4,                last time consumption/overall running time: 576.9282s / 126835.1958 s
env0_first_0:                 episode reward: -80.8500,                 loss: 0.3022
env0_second_0:                 episode reward: 80.8500,                 loss: 0.4914
env1_first_0:                 episode reward: -84.9000,                 loss: nan
env1_second_0:                 episode reward: 84.9000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 432.1,                last time consumption/overall running time: 625.4984s / 127460.6942 s
env0_first_0:                 episode reward: -93.3000,                 loss: 0.3488
env0_second_0:                 episode reward: 93.3000,                 loss: 0.5809
env1_first_0:                 episode reward: -74.0500,                 loss: nan
env1_second_0:                 episode reward: 74.0500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 647.55,                last time consumption/overall running time: 924.6497s / 128385.3439 s
env0_first_0:                 episode reward: -82.9500,                 loss: 0.3916
env0_second_0:                 episode reward: 82.9500,                 loss: 0.6211
env1_first_0:                 episode reward: -93.1000,                 loss: nan
env1_second_0:                 episode reward: 93.1000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 934.2,                last time consumption/overall running time: 1346.4174s / 129731.7613 s
env0_first_0:                 episode reward: -91.6500,                 loss: 0.4146
env0_second_0:                 episode reward: 91.6500,                 loss: 0.6353
env1_first_0:                 episode reward: -82.8500,                 loss: nan
env1_second_0:                 episode reward: 82.8500,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1369.1,                last time consumption/overall running time: 1968.3582s / 131700.1195 s
env0_first_0:                 episode reward: -83.7000,                 loss: 0.3136
env0_second_0:                 episode reward: 83.7000,                 loss: 0.4780
env1_first_0:                 episode reward: -86.9000,                 loss: nan
env1_second_0:                 episode reward: 86.9000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1471.4,                last time consumption/overall running time: 2106.5766s / 133806.6961 s
env0_first_0:                 episode reward: -92.1500,                 loss: 0.1911
env0_second_0:                 episode reward: 92.1500,                 loss: 0.3073
env1_first_0:                 episode reward: -93.9500,                 loss: nan
env1_second_0:                 episode reward: 93.9500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 491.9,                last time consumption/overall running time: 702.8819s / 134509.5780 s
env0_first_0:                 episode reward: -85.7500,                 loss: 0.1967
env0_second_0:                 episode reward: 85.7500,                 loss: 0.3429
env1_first_0:                 episode reward: -78.3500,                 loss: nan
env1_second_0:                 episode reward: 78.3500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 824.75,                last time consumption/overall running time: 1192.2886s / 135701.8666 s
env0_first_0:                 episode reward: -70.1000,                 loss: 0.2822
env0_second_0:                 episode reward: 70.1000,                 loss: 0.4400
env1_first_0:                 episode reward: -65.7000,                 loss: nan
env1_second_0:                 episode reward: 65.7000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1549.75,                last time consumption/overall running time: 2224.4082s / 137926.2748 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.2592
env0_second_0:                 episode reward: 6.8500,                 loss: 0.4353
env1_first_0:                 episode reward: -16.8000,                 loss: nan
env1_second_0:                 episode reward: 16.8000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 549.7,                last time consumption/overall running time: 792.3122s / 138718.5870 s
env0_first_0:                 episode reward: -70.6000,                 loss: 0.2009
env0_second_0:                 episode reward: 70.6000,                 loss: 0.3517
env1_first_0:                 episode reward: -77.3500,                 loss: nan
env1_second_0:                 episode reward: 77.3500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 277.85,                last time consumption/overall running time: 396.5876s / 139115.1746 s
env0_first_0:                 episode reward: -78.2500,                 loss: 0.2243
env0_second_0:                 episode reward: 78.2500,                 loss: 0.3638
env1_first_0:                 episode reward: -77.2000,                 loss: nan
env1_second_0:                 episode reward: 77.2000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 404.2,                last time consumption/overall running time: 577.2231s / 139692.3978 s
env0_first_0:                 episode reward: -78.0500,                 loss: 0.2984
env0_second_0:                 episode reward: 78.0500,                 loss: 0.5051
env1_first_0:                 episode reward: -86.4500,                 loss: nan
env1_second_0:                 episode reward: 86.4500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 565.8,                last time consumption/overall running time: 812.7343s / 140505.1321 s
env0_first_0:                 episode reward: -93.3500,                 loss: 0.3492
env0_second_0:                 episode reward: 93.3500,                 loss: 0.6380
env1_first_0:                 episode reward: -78.6500,                 loss: nan
env1_second_0:                 episode reward: 78.6500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 292.25,                last time consumption/overall running time: 421.5154s / 140926.6475 s
env0_first_0:                 episode reward: -75.1000,                 loss: 0.3977
env0_second_0:                 episode reward: 75.1000,                 loss: 0.5907
env1_first_0:                 episode reward: -93.2500,                 loss: nan
env1_second_0:                 episode reward: 93.2500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 256.95,                last time consumption/overall running time: 375.1530s / 141301.8006 s
env0_first_0:                 episode reward: -92.9500,                 loss: 0.4638
env0_second_0:                 episode reward: 92.9500,                 loss: 0.7265
env1_first_0:                 episode reward: -83.7000,                 loss: nan
env1_second_0:                 episode reward: 83.7000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 287.6,                last time consumption/overall running time: 417.2913s / 141719.0919 s
env0_first_0:                 episode reward: -80.3500,                 loss: 0.4950
env0_second_0:                 episode reward: 80.3500,                 loss: 0.7261
env1_first_0:                 episode reward: -79.2500,                 loss: nan
env1_second_0:                 episode reward: 79.2500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 226.7,                last time consumption/overall running time: 326.9609s / 142046.0528 s
env0_first_0:                 episode reward: -94.4500,                 loss: 0.5119
env0_second_0:                 episode reward: 94.4500,                 loss: 0.7353
env1_first_0:                 episode reward: -85.8500,                 loss: nan
env1_second_0:                 episode reward: 85.8500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 315.65,                last time consumption/overall running time: 457.1908s / 142503.2436 s
env0_first_0:                 episode reward: -89.7500,                 loss: 0.5551
env0_second_0:                 episode reward: 89.7500,                 loss: 0.7763
env1_first_0:                 episode reward: -83.3500,                 loss: nan
env1_second_0:                 episode reward: 83.3500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 475.55,                last time consumption/overall running time: 685.8576s / 143189.1012 s
env0_first_0:                 episode reward: -80.0000,                 loss: 0.5188
env0_second_0:                 episode reward: 80.0000,                 loss: 0.7398
env1_first_0:                 episode reward: -62.7000,                 loss: nan
env1_second_0:                 episode reward: 62.7000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 291.85,                last time consumption/overall running time: 421.2277s / 143610.3290 s
env0_first_0:                 episode reward: -87.5500,                 loss: 0.5543
env0_second_0:                 episode reward: 87.5500,                 loss: 0.7615
env1_first_0:                 episode reward: -88.0500,                 loss: nan
env1_second_0:                 episode reward: 88.0500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 370.3,                last time consumption/overall running time: 533.1681s / 144143.4971 s
env0_first_0:                 episode reward: -83.8500,                 loss: 0.5415
env0_second_0:                 episode reward: 83.8500,                 loss: 0.7866
env1_first_0:                 episode reward: -89.3500,                 loss: nan
env1_second_0:                 episode reward: 89.3500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 257.95,                last time consumption/overall running time: 370.4731s / 144513.9702 s
env0_first_0:                 episode reward: -76.7500,                 loss: 0.6133
env0_second_0:                 episode reward: 76.7500,                 loss: 0.8121
env1_first_0:                 episode reward: -84.0500,                 loss: nan
env1_second_0:                 episode reward: 84.0500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 552.8,                last time consumption/overall running time: 791.1122s / 145305.0824 s
env0_first_0:                 episode reward: -87.3500,                 loss: 0.5748
env0_second_0:                 episode reward: 87.3500,                 loss: 0.7832
env1_first_0:                 episode reward: -83.2000,                 loss: nan
env1_second_0:                 episode reward: 83.2000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 457.4,                last time consumption/overall running time: 646.5118s / 145951.5942 s
env0_first_0:                 episode reward: -86.1000,                 loss: 0.5003
env0_second_0:                 episode reward: 86.1000,                 loss: 0.6635
env1_first_0:                 episode reward: -85.8000,                 loss: nan
env1_second_0:                 episode reward: 85.8000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 669.45,                last time consumption/overall running time: 948.9863s / 146900.5805 s
env0_first_0:                 episode reward: -87.9000,                 loss: 0.4821
env0_second_0:                 episode reward: 87.9000,                 loss: 0.6411
env1_first_0:                 episode reward: -78.3000,                 loss: nan
env1_second_0:                 episode reward: 78.3000,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1108.3,                last time consumption/overall running time: 1545.7305s / 148446.3110 s
env0_first_0:                 episode reward: -84.2500,                 loss: 0.3481
env0_second_0:                 episode reward: 84.2500,                 loss: 0.4730
env1_first_0:                 episode reward: -86.8000,                 loss: nan
env1_second_0:                 episode reward: 86.8000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 418.25,                last time consumption/overall running time: 586.6870s / 149032.9979 s
env0_first_0:                 episode reward: -78.2500,                 loss: 0.2778
env0_second_0:                 episode reward: 78.2500,                 loss: 0.3145
env1_first_0:                 episode reward: -84.0000,                 loss: nan
env1_second_0:                 episode reward: 84.0000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1160.4,                last time consumption/overall running time: 1628.1095s / 150661.1074 s
env0_first_0:                 episode reward: -81.5500,                 loss: 0.2578
env0_second_0:                 episode reward: 81.5500,                 loss: 0.3246
env1_first_0:                 episode reward: -90.3500,                 loss: nan
env1_second_0:                 episode reward: 90.3500,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 1332.25,                last time consumption/overall running time: 1860.4351s / 152521.5425 s
env0_first_0:                 episode reward: -85.8500,                 loss: 0.2701
env0_second_0:                 episode reward: 85.8500,                 loss: 0.3293
env1_first_0:                 episode reward: -84.5000,                 loss: nan
env1_second_0:                 episode reward: 84.5000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1097.15,                last time consumption/overall running time: 1535.6446s / 154057.1872 s
env0_first_0:                 episode reward: -82.1500,                 loss: 0.2756
env0_second_0:                 episode reward: 82.1500,                 loss: 0.3530
env1_first_0:                 episode reward: -79.0500,                 loss: nan
env1_second_0:                 episode reward: 79.0500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 712.2,                last time consumption/overall running time: 985.9301s / 155043.1172 s
env0_first_0:                 episode reward: -86.6000,                 loss: 0.3379
env0_second_0:                 episode reward: 86.6000,                 loss: 0.4129
env1_first_0:                 episode reward: -88.5000,                 loss: nan
env1_second_0:                 episode reward: 88.5000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 393.05,                last time consumption/overall running time: 548.7508s / 155591.8680 s
env0_first_0:                 episode reward: -86.0500,                 loss: 0.3863
env0_second_0:                 episode reward: 86.0500,                 loss: 0.4907
env1_first_0:                 episode reward: -83.1000,                 loss: nan
env1_second_0:                 episode reward: 83.1000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 571.45,                last time consumption/overall running time: 793.9745s / 156385.8425 s
env0_first_0:                 episode reward: -86.4500,                 loss: 0.3986
env0_second_0:                 episode reward: 86.4500,                 loss: 0.5353
env1_first_0:                 episode reward: -95.1000,                 loss: nan
env1_second_0:                 episode reward: 95.1000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 767.3,                last time consumption/overall running time: 1064.4358s / 157450.2783 s
env0_first_0:                 episode reward: -81.0500,                 loss: 0.3799
env0_second_0:                 episode reward: 81.0500,                 loss: 0.5162
env1_first_0:                 episode reward: -82.6000,                 loss: nan
env1_second_0:                 episode reward: 82.6000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 392.15,                last time consumption/overall running time: 544.3716s / 157994.6499 s
env0_first_0:                 episode reward: -93.5500,                 loss: 0.3438
env0_second_0:                 episode reward: 93.5500,                 loss: 0.4709
env1_first_0:                 episode reward: -75.8000,                 loss: nan
env1_second_0:                 episode reward: 75.8000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 442.15,                last time consumption/overall running time: 618.8453s / 158613.4951 s
env0_first_0:                 episode reward: -73.2000,                 loss: 0.3555
env0_second_0:                 episode reward: 73.2000,                 loss: 0.5557
env1_first_0:                 episode reward: -82.7000,                 loss: nan
env1_second_0:                 episode reward: 82.7000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 391.45,                last time consumption/overall running time: 545.3046s / 159158.7998 s
env0_first_0:                 episode reward: -91.1500,                 loss: 0.3857
env0_second_0:                 episode reward: 91.1500,                 loss: 0.5889
env1_first_0:                 episode reward: -79.3500,                 loss: nan
env1_second_0:                 episode reward: 79.3500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 417.35,                last time consumption/overall running time: 583.7024s / 159742.5022 s
env0_first_0:                 episode reward: -89.1000,                 loss: 0.4199
env0_second_0:                 episode reward: 89.1000,                 loss: 0.5811
env1_first_0:                 episode reward: -86.5500,                 loss: nan
env1_second_0:                 episode reward: 86.5500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 384.0,                last time consumption/overall running time: 541.6262s / 160284.1284 s
env0_first_0:                 episode reward: -85.7000,                 loss: 0.4692
env0_second_0:                 episode reward: 85.7000,                 loss: 0.6553
env1_first_0:                 episode reward: -75.5500,                 loss: nan
env1_second_0:                 episode reward: 75.5500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 551.15,                last time consumption/overall running time: 767.6058s / 161051.7342 s
env0_first_0:                 episode reward: -78.5000,                 loss: 0.4761
env0_second_0:                 episode reward: 78.5000,                 loss: 0.6679
env1_first_0:                 episode reward: -84.8500,                 loss: nan
env1_second_0:                 episode reward: 84.8500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1018.6,                last time consumption/overall running time: 1405.5150s / 162457.2492 s
env0_first_0:                 episode reward: -87.3500,                 loss: 0.4646
env0_second_0:                 episode reward: 87.3500,                 loss: 0.6454
env1_first_0:                 episode reward: -86.4000,                 loss: nan
env1_second_0:                 episode reward: 86.4000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 814.55,                last time consumption/overall running time: 1127.2559s / 163584.5051 s
env0_first_0:                 episode reward: -79.0000,                 loss: 0.3748
env0_second_0:                 episode reward: 79.0000,                 loss: 0.5183
env1_first_0:                 episode reward: -71.6000,                 loss: nan
env1_second_0:                 episode reward: 71.6000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 772.85,                last time consumption/overall running time: 1062.9591s / 164647.4642 s
env0_first_0:                 episode reward: -72.7000,                 loss: 0.3539
env0_second_0:                 episode reward: 72.7000,                 loss: 0.4811
env1_first_0:                 episode reward: -83.2500,                 loss: nan
env1_second_0:                 episode reward: 83.2500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 917.7,                last time consumption/overall running time: 1265.3032s / 165912.7674 s
env0_first_0:                 episode reward: -86.7500,                 loss: 0.3579
env0_second_0:                 episode reward: 86.7500,                 loss: 0.4544
env1_first_0:                 episode reward: -80.2500,                 loss: nan
env1_second_0:                 episode reward: 80.2500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 471.55,                last time consumption/overall running time: 649.1137s / 166561.8811 s
env0_first_0:                 episode reward: -76.9500,                 loss: 0.4129
env0_second_0:                 episode reward: 76.9500,                 loss: 0.5203
env1_first_0:                 episode reward: -89.7000,                 loss: nan
env1_second_0:                 episode reward: 89.7000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 1313.45,                last time consumption/overall running time: 1813.2147s / 168375.0958 s
env0_first_0:                 episode reward: -75.8000,                 loss: 0.3588
env0_second_0:                 episode reward: 75.8000,                 loss: 0.4398
env1_first_0:                 episode reward: -84.2000,                 loss: nan
env1_second_0:                 episode reward: 84.2000,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1073.0,                last time consumption/overall running time: 1478.5576s / 169853.6534 s
env0_first_0:                 episode reward: -89.9000,                 loss: 0.3576
env0_second_0:                 episode reward: 89.9000,                 loss: 0.4454
env1_first_0:                 episode reward: -78.7000,                 loss: nan
env1_second_0:                 episode reward: 78.7000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1099.1,                last time consumption/overall running time: 1500.6273s / 171354.2807 s
env0_first_0:                 episode reward: -74.8500,                 loss: 0.3849
env0_second_0:                 episode reward: 74.8500,                 loss: 0.4666
env1_first_0:                 episode reward: -61.4000,                 loss: nan
env1_second_0:                 episode reward: 61.4000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 741.9,                last time consumption/overall running time: 1019.2589s / 172373.5396 s
env0_first_0:                 episode reward: -90.4000,                 loss: 0.3955
env0_second_0:                 episode reward: 90.4000,                 loss: 0.4677
env1_first_0:                 episode reward: -85.6500,                 loss: nan
env1_second_0:                 episode reward: 85.6500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 890.85,                last time consumption/overall running time: 1218.9244s / 173592.4640 s
env0_first_0:                 episode reward: -81.0000,                 loss: 0.3846
env0_second_0:                 episode reward: 81.0000,                 loss: 0.4773
env1_first_0:                 episode reward: -81.9000,                 loss: nan
env1_second_0:                 episode reward: 81.9000,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 880.75,                last time consumption/overall running time: 1208.8713s / 174801.3354 s
env0_first_0:                 episode reward: -93.0500,                 loss: 0.3717
env0_second_0:                 episode reward: 93.0500,                 loss: 0.4656
env1_first_0:                 episode reward: -85.9000,                 loss: nan
env1_second_0:                 episode reward: 85.9000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 539.7,                last time consumption/overall running time: 740.1603s / 175541.4956 s
env0_first_0:                 episode reward: -89.2000,                 loss: 0.4122
env0_second_0:                 episode reward: 89.2000,                 loss: 0.4950
env1_first_0:                 episode reward: -89.1000,                 loss: nan
env1_second_0:                 episode reward: 89.1000,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 349.7,                last time consumption/overall running time: 479.6268s / 176021.1224 s
env0_first_0:                 episode reward: -82.9000,                 loss: 0.4190
env0_second_0:                 episode reward: 82.9000,                 loss: 0.4618
env1_first_0:                 episode reward: -76.8000,                 loss: nan
env1_second_0:                 episode reward: 76.8000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 331.05,                last time consumption/overall running time: 456.4139s / 176477.5363 s
env0_first_0:                 episode reward: -94.7000,                 loss: 0.4630
env0_second_0:                 episode reward: 94.7000,                 loss: 0.4485
env1_first_0:                 episode reward: -78.0000,                 loss: nan
env1_second_0:                 episode reward: 78.0000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 315.75,                last time consumption/overall running time: 437.4139s / 176914.9502 s
env0_first_0:                 episode reward: -74.7500,                 loss: 0.4769
env0_second_0:                 episode reward: 74.7500,                 loss: 0.5178
env1_first_0:                 episode reward: -91.5000,                 loss: nan
env1_second_0:                 episode reward: 91.5000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 569.95,                last time consumption/overall running time: 780.3866s / 177695.3368 s
env0_first_0:                 episode reward: -90.1500,                 loss: 0.4973
env0_second_0:                 episode reward: 90.1500,                 loss: 0.5152
env1_first_0:                 episode reward: -86.9000,                 loss: nan
env1_second_0:                 episode reward: 86.9000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 341.65,                last time consumption/overall running time: 467.9382s / 178163.2749 s
env0_first_0:                 episode reward: -86.2000,                 loss: 0.4517
env0_second_0:                 episode reward: 86.2000,                 loss: 0.5175
env1_first_0:                 episode reward: -84.4000,                 loss: nan
env1_second_0:                 episode reward: 84.4000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 455.4,                last time consumption/overall running time: 615.3050s / 178778.5800 s
env0_first_0:                 episode reward: -86.9000,                 loss: 0.4596
env0_second_0:                 episode reward: 86.9000,                 loss: 0.5449
env1_first_0:                 episode reward: -92.5500,                 loss: nan
env1_second_0:                 episode reward: 92.5500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 474.05,                last time consumption/overall running time: 648.2400s / 179426.8200 s
env0_first_0:                 episode reward: -92.9000,                 loss: 0.4473
env0_second_0:                 episode reward: 92.9000,                 loss: 0.5674
env1_first_0:                 episode reward: -90.9000,                 loss: nan
env1_second_0:                 episode reward: 90.9000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 1110.2,                last time consumption/overall running time: 1509.5154s / 180936.3354 s
env0_first_0:                 episode reward: -88.4500,                 loss: 0.3584
env0_second_0:                 episode reward: 88.4500,                 loss: 0.4441
env1_first_0:                 episode reward: -83.6500,                 loss: nan
env1_second_0:                 episode reward: 83.6500,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 878.9,                last time consumption/overall running time: 1191.6361s / 182127.9715 s
env0_first_0:                 episode reward: -87.4500,                 loss: 0.2632
env0_second_0:                 episode reward: 87.4500,                 loss: 0.3613
env1_first_0:                 episode reward: -88.2500,                 loss: nan
env1_second_0:                 episode reward: 88.2500,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 871.7,                last time consumption/overall running time: 1184.9979s / 183312.9695 s
env0_first_0:                 episode reward: -87.6500,                 loss: 0.2119
env0_second_0:                 episode reward: 87.6500,                 loss: 0.2918
env1_first_0:                 episode reward: -83.0500,                 loss: nan
env1_second_0:                 episode reward: 83.0500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 808.05,                last time consumption/overall running time: 1097.8585s / 184410.8279 s
env0_first_0:                 episode reward: -88.3500,                 loss: 0.2139
env0_second_0:                 episode reward: 88.3500,                 loss: 0.2719
env1_first_0:                 episode reward: -82.0500,                 loss: nan
env1_second_0:                 episode reward: 82.0500,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 749.5,                last time consumption/overall running time: 1020.0184s / 185430.8464 s
env0_first_0:                 episode reward: -88.8000,                 loss: 0.2356
env0_second_0:                 episode reward: 88.8000,                 loss: 0.3360
env1_first_0:                 episode reward: -73.3000,                 loss: nan
env1_second_0:                 episode reward: 73.3000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 299.9,                last time consumption/overall running time: 407.5184s / 185838.3647 s
env0_first_0:                 episode reward: -78.2000,                 loss: 0.2681
env0_second_0:                 episode reward: 78.2000,                 loss: 0.3349
env1_first_0:                 episode reward: -89.3500,                 loss: nan
env1_second_0:                 episode reward: 89.3500,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 438.15,                last time consumption/overall running time: 598.3245s / 186436.6892 s
env0_first_0:                 episode reward: -87.2500,                 loss: 0.2837
env0_second_0:                 episode reward: 87.2500,                 loss: 0.3528
env1_first_0:                 episode reward: -86.2500,                 loss: nan
env1_second_0:                 episode reward: 86.2500,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 549.05,                last time consumption/overall running time: 740.2032s / 187176.8924 s
env0_first_0:                 episode reward: -84.0500,                 loss: 0.3515
env0_second_0:                 episode reward: 84.0500,                 loss: 0.4310
env1_first_0:                 episode reward: -82.0500,                 loss: nan
env1_second_0:                 episode reward: 82.0500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 368.0,                last time consumption/overall running time: 494.1432s / 187671.0355 s
env0_first_0:                 episode reward: -92.8500,                 loss: 0.3506
env0_second_0:                 episode reward: 92.8500,                 loss: 0.4017
env1_first_0:                 episode reward: -89.0000,                 loss: nan
env1_second_0:                 episode reward: 89.0000,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 243.7,                last time consumption/overall running time: 330.3546s / 188001.3902 s
env0_first_0:                 episode reward: -90.5500,                 loss: 0.3428
env0_second_0:                 episode reward: 90.5500,                 loss: 0.4411
env1_first_0:                 episode reward: -79.6000,                 loss: nan
env1_second_0:                 episode reward: 79.6000,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 544.7,                last time consumption/overall running time: 731.6299s / 188733.0200 s
env0_first_0:                 episode reward: -91.3500,                 loss: 0.3265
env0_second_0:                 episode reward: 91.3500,                 loss: 0.4229
env1_first_0:                 episode reward: -85.6000,                 loss: nan
env1_second_0:                 episode reward: 85.6000,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1446.3,                last time consumption/overall running time: 1955.2901s / 190688.3101 s
env0_first_0:                 episode reward: -84.9500,                 loss: 0.3049
env0_second_0:                 episode reward: 84.9500,                 loss: 0.3449
env1_first_0:                 episode reward: -86.2500,                 loss: nan
env1_second_0:                 episode reward: 86.2500,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 1069.55,                last time consumption/overall running time: 1428.9177s / 192117.2277 s
env0_first_0:                 episode reward: -86.1000,                 loss: 0.1937
env0_second_0:                 episode reward: 86.1000,                 loss: 0.2375
env1_first_0:                 episode reward: -79.1500,                 loss: nan
env1_second_0:                 episode reward: 79.1500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 715.3,                last time consumption/overall running time: 942.5814s / 193059.8092 s
env0_first_0:                 episode reward: -88.2000,                 loss: 0.2023
env0_second_0:                 episode reward: 88.2000,                 loss: 0.2793
env1_first_0:                 episode reward: -83.8500,                 loss: nan
env1_second_0:                 episode reward: 83.8500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 601.75,                last time consumption/overall running time: 796.1428s / 193855.9520 s
env0_first_0:                 episode reward: -85.5000,                 loss: 0.2219
env0_second_0:                 episode reward: 85.5000,                 loss: 0.2836
env1_first_0:                 episode reward: -84.1500,                 loss: nan
env1_second_0:                 episode reward: 84.1500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 449.1,                last time consumption/overall running time: 595.2956s / 194451.2476 s
env0_first_0:                 episode reward: -78.1000,                 loss: 0.2902
env0_second_0:                 episode reward: 78.1000,                 loss: 0.3535
env1_first_0:                 episode reward: -93.3500,                 loss: nan
env1_second_0:                 episode reward: 93.3500,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 306.55,                last time consumption/overall running time: 405.8672s / 194857.1148 s
env0_first_0:                 episode reward: -89.7500,                 loss: 0.3165
env0_second_0:                 episode reward: 89.7500,                 loss: 0.4176
env1_first_0:                 episode reward: -85.1500,                 loss: nan
env1_second_0:                 episode reward: 85.1500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 269.1,                last time consumption/overall running time: 356.9325s / 195214.0473 s
env0_first_0:                 episode reward: -91.1000,                 loss: 0.3379
env0_second_0:                 episode reward: 91.1000,                 loss: 0.4606
env1_first_0:                 episode reward: -76.7000,                 loss: nan
env1_second_0:                 episode reward: 76.7000,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 394.65,                last time consumption/overall running time: 515.4175s / 195729.4648 s
env0_first_0:                 episode reward: -94.2000,                 loss: 0.3556
env0_second_0:                 episode reward: 94.2000,                 loss: 0.4810
env1_first_0:                 episode reward: -85.4000,                 loss: nan
env1_second_0:                 episode reward: 85.4000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 649.05,                last time consumption/overall running time: 849.3725s / 196578.8372 s
env0_first_0:                 episode reward: -92.7500,                 loss: 0.3139
env0_second_0:                 episode reward: 92.7500,                 loss: 0.4628
env1_first_0:                 episode reward: -92.9000,                 loss: nan
env1_second_0:                 episode reward: 92.9000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 959.25,                last time consumption/overall running time: 1265.4763s / 197844.3135 s
env0_first_0:                 episode reward: -90.0000,                 loss: 0.3284
env0_second_0:                 episode reward: 90.0000,                 loss: 0.4417
env1_first_0:                 episode reward: -78.7000,                 loss: nan
env1_second_0:                 episode reward: 78.7000,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 805.45,                last time consumption/overall running time: 1051.7948s / 198896.1082 s
env0_first_0:                 episode reward: -84.8000,                 loss: 0.2941
env0_second_0:                 episode reward: 84.8000,                 loss: 0.4323
env1_first_0:                 episode reward: -86.4000,                 loss: nan
env1_second_0:                 episode reward: 86.4000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 731.55,                last time consumption/overall running time: 950.9682s / 199847.0764 s
env0_first_0:                 episode reward: -84.7500,                 loss: 0.3037
env0_second_0:                 episode reward: 84.7500,                 loss: 0.4051
env1_first_0:                 episode reward: -87.2000,                 loss: nan
env1_second_0:                 episode reward: 87.2000,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 486.25,                last time consumption/overall running time: 617.5858s / 200464.6623 s
env0_first_0:                 episode reward: -87.3000,                 loss: 0.3192
env0_second_0:                 episode reward: 87.3000,                 loss: 0.4426
env1_first_0:                 episode reward: -88.3500,                 loss: nan
env1_second_0:                 episode reward: 88.3500,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 578.85,                last time consumption/overall running time: 733.7679s / 201198.4302 s
env0_first_0:                 episode reward: -91.6500,                 loss: 0.3148
env0_second_0:                 episode reward: 91.6500,                 loss: 0.4618
env1_first_0:                 episode reward: -87.4000,                 loss: nan
env1_second_0:                 episode reward: 87.4000,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 665.45,                last time consumption/overall running time: 848.3805s / 202046.8107 s
env0_first_0:                 episode reward: -84.4000,                 loss: 0.3068
env0_second_0:                 episode reward: 84.4000,                 loss: 0.3999
env1_first_0:                 episode reward: -89.5500,                 loss: nan
env1_second_0:                 episode reward: 89.5500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 877.25,                last time consumption/overall running time: 1107.9105s / 203154.7212 s
env0_first_0:                 episode reward: -78.5000,                 loss: 0.2981
env0_second_0:                 episode reward: 78.5000,                 loss: 0.4481
env1_first_0:                 episode reward: -81.8500,                 loss: nan
env1_second_0:                 episode reward: 81.8500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 712.0,                last time consumption/overall running time: 915.2929s / 204070.0142 s
env0_first_0:                 episode reward: -62.8500,                 loss: 0.2679
env0_second_0:                 episode reward: 62.8500,                 loss: 0.4416
env1_first_0:                 episode reward: -73.6500,                 loss: nan
env1_second_0:                 episode reward: 73.6500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 300.95,                last time consumption/overall running time: 386.0811s / 204456.0953 s
env0_first_0:                 episode reward: -81.0000,                 loss: 0.3234
env0_second_0:                 episode reward: 81.0000,                 loss: 0.4283
env1_first_0:                 episode reward: -78.3000,                 loss: nan
env1_second_0:                 episode reward: 78.3000,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 311.75,                last time consumption/overall running time: 395.7730s / 204851.8683 s
env0_first_0:                 episode reward: -74.1000,                 loss: 0.3403
env0_second_0:                 episode reward: 74.1000,                 loss: 0.4990
env1_first_0:                 episode reward: -80.9500,                 loss: nan
env1_second_0:                 episode reward: 80.9500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 304.75,                last time consumption/overall running time: 389.8619s / 205241.7301 s
env0_first_0:                 episode reward: -91.8500,                 loss: 0.3649
env0_second_0:                 episode reward: 91.8500,                 loss: 0.5493
env1_first_0:                 episode reward: -79.2500,                 loss: nan
env1_second_0:                 episode reward: 79.2500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 514.2,                last time consumption/overall running time: 655.6511s / 205897.3812 s
env0_first_0:                 episode reward: -79.5000,                 loss: 0.3693
env0_second_0:                 episode reward: 79.5000,                 loss: 0.6154
env1_first_0:                 episode reward: -76.1000,                 loss: nan
env1_second_0:                 episode reward: 76.1000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 467.25,                last time consumption/overall running time: 594.1818s / 206491.5630 s
env0_first_0:                 episode reward: -82.1000,                 loss: 0.4276
env0_second_0:                 episode reward: 82.1000,                 loss: 0.5979
env1_first_0:                 episode reward: -90.2500,                 loss: nan
env1_second_0:                 episode reward: 90.2500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 449.55,                last time consumption/overall running time: 569.8868s / 207061.4498 s
env0_first_0:                 episode reward: -92.4000,                 loss: 0.5060
env0_second_0:                 episode reward: 92.4000,                 loss: 0.5698
env1_first_0:                 episode reward: -81.7500,                 loss: nan
env1_second_0:                 episode reward: 81.7500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 231.0,                last time consumption/overall running time: 295.2725s / 207356.7223 s
env0_first_0:                 episode reward: -90.4000,                 loss: 0.4316
env0_second_0:                 episode reward: 90.4000,                 loss: 0.5830
env1_first_0:                 episode reward: -96.8500,                 loss: nan
env1_second_0:                 episode reward: 96.8500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 378.25,                last time consumption/overall running time: 485.3654s / 207842.0877 s
env0_first_0:                 episode reward: -79.6000,                 loss: 0.4316
env0_second_0:                 episode reward: 79.6000,                 loss: 0.5264
env1_first_0:                 episode reward: -93.4000,                 loss: nan
env1_second_0:                 episode reward: 93.4000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 780.4,                last time consumption/overall running time: 993.2789s / 208835.3666 s
env0_first_0:                 episode reward: -90.6500,                 loss: 0.4061
env0_second_0:                 episode reward: 90.6500,                 loss: 0.4636
env1_first_0:                 episode reward: -88.2000,                 loss: nan
env1_second_0:                 episode reward: 88.2000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 447.4,                last time consumption/overall running time: 572.4250s / 209407.7916 s
env0_first_0:                 episode reward: -94.9000,                 loss: 0.3407
env0_second_0:                 episode reward: 94.9000,                 loss: 0.4013
env1_first_0:                 episode reward: -77.2500,                 loss: nan
env1_second_0:                 episode reward: 77.2500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 227.45,                last time consumption/overall running time: 292.6916s / 209700.4832 s
env0_first_0:                 episode reward: -82.7000,                 loss: 0.3312
env0_second_0:                 episode reward: 82.7000,                 loss: 0.3928
env1_first_0:                 episode reward: -92.1500,                 loss: nan
env1_second_0:                 episode reward: 92.1500,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 281.85,                last time consumption/overall running time: 359.5915s / 210060.0747 s
env0_first_0:                 episode reward: -79.3500,                 loss: 0.3120
env0_second_0:                 episode reward: 79.3500,                 loss: 0.4351
env1_first_0:                 episode reward: -71.8000,                 loss: nan
env1_second_0:                 episode reward: 71.8000,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 520.05,                last time consumption/overall running time: 657.5563s / 210717.6311 s
env0_first_0:                 episode reward: -82.4500,                 loss: 0.3209
env0_second_0:                 episode reward: 82.4500,                 loss: 0.4157
env1_first_0:                 episode reward: -79.0000,                 loss: nan
env1_second_0:                 episode reward: 79.0000,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 362.05,                last time consumption/overall running time: 458.0788s / 211175.7099 s
env0_first_0:                 episode reward: -69.4500,                 loss: 0.3135
env0_second_0:                 episode reward: 69.4500,                 loss: 0.3997
env1_first_0:                 episode reward: -96.6500,                 loss: nan
env1_second_0:                 episode reward: 96.6500,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 390.8,                last time consumption/overall running time: 499.4721s / 211675.1820 s
env0_first_0:                 episode reward: -92.4500,                 loss: 0.3632
env0_second_0:                 episode reward: 92.4500,                 loss: 0.4661
env1_first_0:                 episode reward: -93.8500,                 loss: nan
env1_second_0:                 episode reward: 93.8500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 254.75,                last time consumption/overall running time: 325.3800s / 212000.5619 s
env0_first_0:                 episode reward: -90.9500,                 loss: 0.3203
env0_second_0:                 episode reward: 90.9500,                 loss: 0.4783
env1_first_0:                 episode reward: -92.8500,                 loss: nan
env1_second_0:                 episode reward: 92.8500,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 320.9,                last time consumption/overall running time: 407.7897s / 212408.3516 s
env0_first_0:                 episode reward: -86.8500,                 loss: 0.3799
env0_second_0:                 episode reward: 86.8500,                 loss: 0.4642
env1_first_0:                 episode reward: -94.0000,                 loss: nan
env1_second_0:                 episode reward: 94.0000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 406.5,                last time consumption/overall running time: 519.1717s / 212927.5233 s
env0_first_0:                 episode reward: -78.4000,                 loss: 0.3242
env0_second_0:                 episode reward: 78.4000,                 loss: 0.4121
env1_first_0:                 episode reward: -97.3500,                 loss: nan
env1_second_0:                 episode reward: 97.3500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 261.05,                last time consumption/overall running time: 335.3441s / 213262.8675 s
env0_first_0:                 episode reward: -87.5500,                 loss: 0.3190
env0_second_0:                 episode reward: 87.5500,                 loss: 0.3874
env1_first_0:                 episode reward: -79.4000,                 loss: nan
env1_second_0:                 episode reward: 79.4000,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 301.15,                last time consumption/overall running time: 383.5179s / 213646.3854 s