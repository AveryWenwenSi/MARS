pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 10000, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nfsp', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'eta': 0.1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0521/pettingzoo_boxing_v1_nfsp. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0521/pettingzoo_boxing_v1_nfsp.
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 5.4170s / 5.4170 s
env0_first_0:                 episode reward: -3.0000,                 loss: nan
env0_second_0:                 episode reward: 3.0000,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 737.3050s / 742.7220 s
env0_first_0:                 episode reward: 3.3500,                 loss: 0.0060
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0057
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 1335.1900s / 2077.9119 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0216
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0190
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1728.75,                last time consumption/overall running time: 1314.1484s / 3392.0604 s
env0_first_0:                 episode reward: 17.1000,                 loss: 0.0436
env0_second_0:                 episode reward: -17.1000,                 loss: 0.0385
env1_first_0:                 episode reward: 16.8500,                 loss: nan
env1_second_0:                 episode reward: -16.8500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1402.45,                last time consumption/overall running time: 1070.8269s / 4462.8873 s
env0_first_0:                 episode reward: 49.5000,                 loss: 0.0820
env0_second_0:                 episode reward: -49.5000,                 loss: 0.0569
env1_first_0:                 episode reward: 40.6500,                 loss: nan
env1_second_0:                 episode reward: -40.6500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1021.9,                last time consumption/overall running time: 782.8621s / 5245.7493 s
env0_first_0:                 episode reward: 57.6000,                 loss: 0.1221
env0_second_0:                 episode reward: -57.6000,                 loss: 0.0874
env1_first_0:                 episode reward: 46.4500,                 loss: nan
env1_second_0:                 episode reward: -46.4500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 471.5,                last time consumption/overall running time: 363.5152s / 5609.2645 s
env0_first_0:                 episode reward: 67.9000,                 loss: 0.1566
env0_second_0:                 episode reward: -67.9000,                 loss: 0.1107
env1_first_0:                 episode reward: 55.1000,                 loss: nan
env1_second_0:                 episode reward: -55.1000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 519.25,                last time consumption/overall running time: 397.3788s / 6006.6433 s
env0_first_0:                 episode reward: 59.5000,                 loss: 0.1702
env0_second_0:                 episode reward: -59.5000,                 loss: 0.1169
env1_first_0:                 episode reward: 57.8500,                 loss: nan
env1_second_0:                 episode reward: -57.8500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 768.95,                last time consumption/overall running time: 588.4550s / 6595.0983 s
env0_first_0:                 episode reward: 37.0500,                 loss: 0.1848
env0_second_0:                 episode reward: -37.0500,                 loss: 0.1226
env1_first_0:                 episode reward: 56.0500,                 loss: nan
env1_second_0:                 episode reward: -56.0500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 842.4,                last time consumption/overall running time: 646.9489s / 7242.0472 s
env0_first_0:                 episode reward: 35.1500,                 loss: 0.2061
env0_second_0:                 episode reward: -35.1500,                 loss: 0.1402
env1_first_0:                 episode reward: 53.8000,                 loss: nan
env1_second_0:                 episode reward: -53.8000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 756.5,                last time consumption/overall running time: 579.8840s / 7821.9312 s
env0_first_0:                 episode reward: 37.1500,                 loss: 0.2538
env0_second_0:                 episode reward: -37.1500,                 loss: 0.1458
env1_first_0:                 episode reward: 38.5000,                 loss: nan
env1_second_0:                 episode reward: -38.5000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1056.45,                last time consumption/overall running time: 812.8707s / 8634.8019 s
env0_first_0:                 episode reward: 13.8000,                 loss: 0.2989
env0_second_0:                 episode reward: -13.8000,                 loss: 0.1805
env1_first_0:                 episode reward: 53.0500,                 loss: nan
env1_second_0:                 episode reward: -53.0500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 497.65,                last time consumption/overall running time: 382.7268s / 9017.5287 s
env0_first_0:                 episode reward: 26.5000,                 loss: 0.3351
env0_second_0:                 episode reward: -26.5000,                 loss: 0.2456
env1_first_0:                 episode reward: 31.3000,                 loss: nan
env1_second_0:                 episode reward: -31.3000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 345.75,                last time consumption/overall running time: 264.5535s / 9282.0822 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.4346
env0_second_0:                 episode reward: 6.7500,                 loss: 0.2849
env1_first_0:                 episode reward: -17.5500,                 loss: nan
env1_second_0:                 episode reward: 17.5500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 318.2,                last time consumption/overall running time: 244.4305s / 9526.5127 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.5203
env0_second_0:                 episode reward: 7.1500,                 loss: 0.3345
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 429.25,                last time consumption/overall running time: 330.4131s / 9856.9259 s
env0_first_0:                 episode reward: 1.7000,                 loss: 0.4879
env0_second_0:                 episode reward: -1.7000,                 loss: 0.3260
env1_first_0:                 episode reward: 13.9500,                 loss: nan
env1_second_0:                 episode reward: -13.9500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 505.9,                last time consumption/overall running time: 389.9458s / 10246.8717 s
env0_first_0:                 episode reward: 31.5500,                 loss: 0.5287
env0_second_0:                 episode reward: -31.5500,                 loss: 0.3413
env1_first_0:                 episode reward: 18.9000,                 loss: nan
env1_second_0:                 episode reward: -18.9000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 306.0,                last time consumption/overall running time: 235.6537s / 10482.5254 s
env0_first_0:                 episode reward: 23.0500,                 loss: 0.5970
env0_second_0:                 episode reward: -23.0500,                 loss: 0.4213
env1_first_0:                 episode reward: 13.7000,                 loss: nan
env1_second_0:                 episode reward: -13.7000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 283.8,                last time consumption/overall running time: 217.6434s / 10700.1688 s
env0_first_0:                 episode reward: 40.8500,                 loss: 0.6349
env0_second_0:                 episode reward: -40.8500,                 loss: 0.4953
env1_first_0:                 episode reward: 41.5000,                 loss: nan
env1_second_0:                 episode reward: -41.5000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 337.15,                last time consumption/overall running time: 259.5467s / 10959.7155 s
env0_first_0:                 episode reward: 19.7000,                 loss: 0.6535
env0_second_0:                 episode reward: -19.7000,                 loss: 0.5210
env1_first_0:                 episode reward: 38.0500,                 loss: nan
env1_second_0:                 episode reward: -38.0500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 395.9,                last time consumption/overall running time: 305.2935s / 11265.0090 s
env0_first_0:                 episode reward: 13.2000,                 loss: 0.6590
env0_second_0:                 episode reward: -13.2000,                 loss: 0.5395
env1_first_0:                 episode reward: 20.6500,                 loss: nan
env1_second_0:                 episode reward: -20.6500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 304.8,                last time consumption/overall running time: 235.8303s / 11500.8394 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.6756
env0_second_0:                 episode reward: 3.6500,                 loss: 0.5233
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 293.05,                last time consumption/overall running time: 226.9033s / 11727.7427 s
env0_first_0:                 episode reward: 23.0000,                 loss: 0.7194
env0_second_0:                 episode reward: -23.0000,                 loss: 0.5652
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 348.3,                last time consumption/overall running time: 266.8314s / 11994.5741 s
env0_first_0:                 episode reward: 2.9500,                 loss: 0.7713
env0_second_0:                 episode reward: -2.9500,                 loss: 0.6024
env1_first_0:                 episode reward: 8.5000,                 loss: nan
env1_second_0:                 episode reward: -8.5000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 310.35,                last time consumption/overall running time: 238.9966s / 12233.5707 s
env0_first_0:                 episode reward: -7.3000,                 loss: 0.8136
env0_second_0:                 episode reward: 7.3000,                 loss: 0.5800
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 500.3,                last time consumption/overall running time: 384.1206s / 12617.6913 s
env0_first_0:                 episode reward: -27.5500,                 loss: 0.7790
env0_second_0:                 episode reward: 27.5500,                 loss: 0.5548
env1_first_0:                 episode reward: -23.6000,                 loss: nan
env1_second_0:                 episode reward: 23.6000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 268.5,                last time consumption/overall running time: 206.2758s / 12823.9671 s
env0_first_0:                 episode reward: -18.6000,                 loss: 0.7617
env0_second_0:                 episode reward: 18.6000,                 loss: 0.5659
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 335.1,                last time consumption/overall running time: 257.1657s / 13081.1328 s
env0_first_0:                 episode reward: -35.1000,                 loss: 0.8006
env0_second_0:                 episode reward: 35.1000,                 loss: 0.6160
env1_first_0:                 episode reward: -25.4000,                 loss: nan
env1_second_0:                 episode reward: 25.4000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 325.65,                last time consumption/overall running time: 250.5250s / 13331.6578 s
env0_first_0:                 episode reward: -42.2000,                 loss: 0.8466
env0_second_0:                 episode reward: 42.2000,                 loss: 0.6675
env1_first_0:                 episode reward: -45.3500,                 loss: nan
env1_second_0:                 episode reward: 45.3500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 449.85,                last time consumption/overall running time: 346.5642s / 13678.2220 s
env0_first_0:                 episode reward: -21.3500,                 loss: 0.8888
env0_second_0:                 episode reward: 21.3500,                 loss: 0.7484
env1_first_0:                 episode reward: 21.1000,                 loss: nan
env1_second_0:                 episode reward: -21.1000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 377.6,                last time consumption/overall running time: 291.2132s / 13969.4352 s
env0_first_0:                 episode reward: 20.9500,                 loss: 0.9110
env0_second_0:                 episode reward: -20.9500,                 loss: 0.7457
env1_first_0:                 episode reward: 39.7000,                 loss: nan
env1_second_0:                 episode reward: -39.7000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 318.5,                last time consumption/overall running time: 246.4069s / 14215.8421 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.9388
env0_second_0:                 episode reward: -3.3000,                 loss: 0.7474
env1_first_0:                 episode reward: 10.7500,                 loss: nan
env1_second_0:                 episode reward: -10.7500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 280.4,                last time consumption/overall running time: 215.9951s / 14431.8372 s
env0_first_0:                 episode reward: 25.9500,                 loss: 0.9706
env0_second_0:                 episode reward: -25.9500,                 loss: 0.7562
env1_first_0:                 episode reward: 19.9500,                 loss: nan
env1_second_0:                 episode reward: -19.9500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 321.05,                last time consumption/overall running time: 246.3506s / 14678.1879 s
env0_first_0:                 episode reward: 15.8000,                 loss: 0.9677
env0_second_0:                 episode reward: -15.8000,                 loss: 0.8285
env1_first_0:                 episode reward: 24.2500,                 loss: nan
env1_second_0:                 episode reward: -24.2500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 280.95,                last time consumption/overall running time: 214.9328s / 14893.1207 s
env0_first_0:                 episode reward: 12.3000,                 loss: 0.9245
env0_second_0:                 episode reward: -12.3000,                 loss: 0.8023
env1_first_0:                 episode reward: 21.2000,                 loss: nan
env1_second_0:                 episode reward: -21.2000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 291.7,                last time consumption/overall running time: 225.0821s / 15118.2028 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.9149
env0_second_0:                 episode reward: 5.3500,                 loss: 0.7865
env1_first_0:                 episode reward: 17.8500,                 loss: nan
env1_second_0:                 episode reward: -17.8500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 288.65,                last time consumption/overall running time: 221.8230s / 15340.0257 s
env0_first_0:                 episode reward: 14.6500,                 loss: 0.9413
env0_second_0:                 episode reward: -14.6500,                 loss: 0.7572
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 256.8,                last time consumption/overall running time: 197.0312s / 15537.0569 s
env0_first_0:                 episode reward: 13.0500,                 loss: 0.9632
env0_second_0:                 episode reward: -13.0500,                 loss: 0.7630
env1_first_0:                 episode reward: 17.9500,                 loss: nan
env1_second_0:                 episode reward: -17.9500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 303.65,                last time consumption/overall running time: 233.6084s / 15770.6653 s
env0_first_0:                 episode reward: 30.2000,                 loss: 0.9398
env0_second_0:                 episode reward: -30.2000,                 loss: 0.7346
env1_first_0:                 episode reward: 16.9000,                 loss: nan
env1_second_0:                 episode reward: -16.9000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 351.2,                last time consumption/overall running time: 270.8253s / 16041.4906 s
env0_first_0:                 episode reward: -18.4000,                 loss: 0.9753
env0_second_0:                 episode reward: 18.4000,                 loss: 0.7622
env1_first_0:                 episode reward: 8.4000,                 loss: nan
env1_second_0:                 episode reward: -8.4000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 345.95,                last time consumption/overall running time: 266.8846s / 16308.3753 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.9893
env0_second_0:                 episode reward: 7.1500,                 loss: 0.8041
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 299.5,                last time consumption/overall running time: 231.2819s / 16539.6571 s
env0_first_0:                 episode reward: -22.7000,                 loss: 1.0711
env0_second_0:                 episode reward: 22.7000,                 loss: 0.8901
env1_first_0:                 episode reward: -31.2000,                 loss: nan
env1_second_0:                 episode reward: 31.2000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 341.05,                last time consumption/overall running time: 260.8735s / 16800.5306 s
env0_first_0:                 episode reward: -29.1500,                 loss: 1.1711
env0_second_0:                 episode reward: 29.1500,                 loss: 0.9523
env1_first_0:                 episode reward: -45.1000,                 loss: nan
env1_second_0:                 episode reward: 45.1000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 283.3,                last time consumption/overall running time: 216.7315s / 17017.2621 s
env0_first_0:                 episode reward: 17.3000,                 loss: 1.2412
env0_second_0:                 episode reward: -17.3000,                 loss: 0.9865
env1_first_0:                 episode reward: 11.7500,                 loss: nan
env1_second_0:                 episode reward: -11.7500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 313.55,                last time consumption/overall running time: 240.6742s / 17257.9362 s
env0_first_0:                 episode reward: -25.8000,                 loss: 1.2375
env0_second_0:                 episode reward: 25.8000,                 loss: 0.9920
env1_first_0:                 episode reward: -32.6500,                 loss: nan
env1_second_0:                 episode reward: 32.6500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 309.85,                last time consumption/overall running time: 238.5723s / 17496.5086 s
env0_first_0:                 episode reward: -32.1000,                 loss: 1.2011
env0_second_0:                 episode reward: 32.1000,                 loss: 1.0282
env1_first_0:                 episode reward: -21.2000,                 loss: nan
env1_second_0:                 episode reward: 21.2000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 320.45,                last time consumption/overall running time: 246.6487s / 17743.1573 s
env0_first_0:                 episode reward: -40.7000,                 loss: 1.2003
env0_second_0:                 episode reward: 40.7000,                 loss: 1.0243
env1_first_0:                 episode reward: -24.5000,                 loss: nan
env1_second_0:                 episode reward: 24.5000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 391.95,                last time consumption/overall running time: 303.0951s / 18046.2524 s
env0_first_0:                 episode reward: -54.4500,                 loss: 1.2138
env0_second_0:                 episode reward: 54.4500,                 loss: 1.0026
env1_first_0:                 episode reward: -26.1500,                 loss: nan
env1_second_0:                 episode reward: 26.1500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 433.55,                last time consumption/overall running time: 334.4231s / 18380.6754 s
env0_first_0:                 episode reward: -53.0000,                 loss: 1.1053
env0_second_0:                 episode reward: 53.0000,                 loss: 0.9075
env1_first_0:                 episode reward: -52.9500,                 loss: nan
env1_second_0:                 episode reward: 52.9500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 459.45,                last time consumption/overall running time: 353.4355s / 18734.1110 s
env0_first_0:                 episode reward: -27.9500,                 loss: 0.9906
env0_second_0:                 episode reward: 27.9500,                 loss: 0.7963
env1_first_0:                 episode reward: -66.1500,                 loss: nan
env1_second_0:                 episode reward: 66.1500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 362.6,                last time consumption/overall running time: 279.7127s / 19013.8237 s
env0_first_0:                 episode reward: -31.2500,                 loss: 0.9467
env0_second_0:                 episode reward: 31.2500,                 loss: 0.6903
env1_first_0:                 episode reward: -42.3500,                 loss: nan
env1_second_0:                 episode reward: 42.3500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 303.1,                last time consumption/overall running time: 232.6627s / 19246.4864 s
env0_first_0:                 episode reward: -24.8000,                 loss: 1.0026
env0_second_0:                 episode reward: 24.8000,                 loss: 0.6760
env1_first_0:                 episode reward: -46.5500,                 loss: nan
env1_second_0:                 episode reward: 46.5500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 278.3,                last time consumption/overall running time: 215.4393s / 19461.9257 s
env0_first_0:                 episode reward: -51.6500,                 loss: 1.0291
env0_second_0:                 episode reward: 51.6500,                 loss: 0.7202
env1_first_0:                 episode reward: -45.7000,                 loss: nan
env1_second_0:                 episode reward: 45.7000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 271.4,                last time consumption/overall running time: 210.3701s / 19672.2958 s
env0_first_0:                 episode reward: -56.8500,                 loss: 1.0744
env0_second_0:                 episode reward: 56.8500,                 loss: 0.7754
env1_first_0:                 episode reward: -51.3500,                 loss: nan
env1_second_0:                 episode reward: 51.3500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 315.8,                last time consumption/overall running time: 243.5398s / 19915.8356 s
env0_first_0:                 episode reward: -38.3500,                 loss: 1.1035
env0_second_0:                 episode reward: 38.3500,                 loss: 0.8223
env1_first_0:                 episode reward: -31.0500,                 loss: nan
env1_second_0:                 episode reward: 31.0500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 411.55,                last time consumption/overall running time: 316.2698s / 20232.1054 s
env0_first_0:                 episode reward: -87.2500,                 loss: 1.0787
env0_second_0:                 episode reward: 87.2500,                 loss: 0.8442
env1_first_0:                 episode reward: -47.4000,                 loss: nan
env1_second_0:                 episode reward: 47.4000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 359.2,                last time consumption/overall running time: 275.4399s / 20507.5453 s
env0_first_0:                 episode reward: -59.5000,                 loss: 1.1119
env0_second_0:                 episode reward: 59.5000,                 loss: 0.8015
env1_first_0:                 episode reward: -75.0000,                 loss: nan
env1_second_0:                 episode reward: 75.0000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 327.25,                last time consumption/overall running time: 252.7767s / 20760.3220 s
env0_first_0:                 episode reward: -63.1000,                 loss: 1.1700
env0_second_0:                 episode reward: 63.1000,                 loss: 0.8279
env1_first_0:                 episode reward: -44.0000,                 loss: nan
env1_second_0:                 episode reward: 44.0000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 299.1,                last time consumption/overall running time: 230.8484s / 20991.1704 s
env0_first_0:                 episode reward: -65.5500,                 loss: 1.2324
env0_second_0:                 episode reward: 65.5500,                 loss: 0.8443
env1_first_0:                 episode reward: -72.2500,                 loss: nan
env1_second_0:                 episode reward: 72.2500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 621.45,                last time consumption/overall running time: 477.3372s / 21468.5076 s
env0_first_0:                 episode reward: -53.6000,                 loss: 1.0943
env0_second_0:                 episode reward: 53.6000,                 loss: 0.7933
env1_first_0:                 episode reward: -33.0500,                 loss: nan
env1_second_0:                 episode reward: 33.0500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 339.7,                last time consumption/overall running time: 262.2983s / 21730.8059 s
env0_first_0:                 episode reward: -38.2500,                 loss: 1.0246
env0_second_0:                 episode reward: 38.2500,                 loss: 0.6915
env1_first_0:                 episode reward: -22.1500,                 loss: nan
env1_second_0:                 episode reward: 22.1500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 290.4,                last time consumption/overall running time: 223.4978s / 21954.3037 s
env0_first_0:                 episode reward: -31.9500,                 loss: 0.9968
env0_second_0:                 episode reward: 31.9500,                 loss: 0.6666
env1_first_0:                 episode reward: -37.6000,                 loss: nan
env1_second_0:                 episode reward: 37.6000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 255.8,                last time consumption/overall running time: 196.7963s / 22151.1000 s
env0_first_0:                 episode reward: 8.7500,                 loss: 1.1227
env0_second_0:                 episode reward: -8.7500,                 loss: 0.6897
env1_first_0:                 episode reward: -9.8000,                 loss: nan
env1_second_0:                 episode reward: 9.8000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 297.0,                last time consumption/overall running time: 228.4705s / 22379.5705 s
env0_first_0:                 episode reward: 0.7500,                 loss: 1.1735
env0_second_0:                 episode reward: -0.7500,                 loss: 0.7078
env1_first_0:                 episode reward: -14.1500,                 loss: nan
env1_second_0:                 episode reward: 14.1500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 348.95,                last time consumption/overall running time: 267.1862s / 22646.7567 s
env0_first_0:                 episode reward: -37.1000,                 loss: 1.2783
env0_second_0:                 episode reward: 37.1000,                 loss: 0.7600
env1_first_0:                 episode reward: -51.4000,                 loss: nan
env1_second_0:                 episode reward: 51.4000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 321.85,                last time consumption/overall running time: 247.3327s / 22894.0893 s
env0_first_0:                 episode reward: -37.8000,                 loss: 1.1925
env0_second_0:                 episode reward: 37.8000,                 loss: 0.7663
env1_first_0:                 episode reward: -49.6000,                 loss: nan
env1_second_0:                 episode reward: 49.6000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1193.15,                last time consumption/overall running time: 913.9161s / 23808.0054 s
env0_first_0:                 episode reward: -14.8500,                 loss: 1.0238
env0_second_0:                 episode reward: 14.8500,                 loss: 0.6729
env1_first_0:                 episode reward: -29.8500,                 loss: nan
env1_second_0:                 episode reward: 29.8500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 469.05,                last time consumption/overall running time: 360.5028s / 24168.5081 s
env0_first_0:                 episode reward: -80.0500,                 loss: 0.6730
env0_second_0:                 episode reward: 80.0500,                 loss: 0.4475
env1_first_0:                 episode reward: -55.8000,                 loss: nan
env1_second_0:                 episode reward: 55.8000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 334.75,                last time consumption/overall running time: 258.0052s / 24426.5133 s
env0_first_0:                 episode reward: -50.1500,                 loss: 0.6662
env0_second_0:                 episode reward: 50.1500,                 loss: 0.4450
env1_first_0:                 episode reward: -62.9000,                 loss: nan
env1_second_0:                 episode reward: 62.9000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 367.75,                last time consumption/overall running time: 283.0993s / 24709.6126 s
env0_first_0:                 episode reward: -23.6500,                 loss: 0.6616
env0_second_0:                 episode reward: 23.6500,                 loss: 0.4446
env1_first_0:                 episode reward: -45.8000,                 loss: nan
env1_second_0:                 episode reward: 45.8000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 303.4,                last time consumption/overall running time: 233.2087s / 24942.8213 s
env0_first_0:                 episode reward: -27.8500,                 loss: 0.7221
env0_second_0:                 episode reward: 27.8500,                 loss: 0.4643
env1_first_0:                 episode reward: -33.7500,                 loss: nan
env1_second_0:                 episode reward: 33.7500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 432.65,                last time consumption/overall running time: 333.3549s / 25276.1762 s
env0_first_0:                 episode reward: -34.5000,                 loss: 0.7943
env0_second_0:                 episode reward: 34.5000,                 loss: 0.5091
env1_first_0:                 episode reward: -76.3000,                 loss: nan
env1_second_0:                 episode reward: 76.3000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 366.65,                last time consumption/overall running time: 281.2621s / 25557.4383 s
env0_first_0:                 episode reward: -39.0500,                 loss: 0.8837
env0_second_0:                 episode reward: 39.0500,                 loss: 0.6012
env1_first_0:                 episode reward: -33.8000,                 loss: nan
env1_second_0:                 episode reward: 33.8000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 256.35,                last time consumption/overall running time: 198.1382s / 25755.5765 s
env0_first_0:                 episode reward: -59.8000,                 loss: 0.9634
env0_second_0:                 episode reward: 59.8000,                 loss: 0.6839
env1_first_0:                 episode reward: -60.6500,                 loss: nan
env1_second_0:                 episode reward: 60.6500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 266.15,                last time consumption/overall running time: 205.2670s / 25960.8435 s
env0_first_0:                 episode reward: -51.4500,                 loss: 1.0652
env0_second_0:                 episode reward: 51.4500,                 loss: 0.7554
env1_first_0:                 episode reward: -40.5000,                 loss: nan
env1_second_0:                 episode reward: 40.5000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 271.6,                last time consumption/overall running time: 208.9813s / 26169.8248 s
env0_first_0:                 episode reward: -57.6500,                 loss: 1.0688
env0_second_0:                 episode reward: 57.6500,                 loss: 0.8210
env1_first_0:                 episode reward: -25.0000,                 loss: nan
env1_second_0:                 episode reward: 25.0000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 267.8,                last time consumption/overall running time: 206.0563s / 26375.8811 s
env0_first_0:                 episode reward: -4.1500,                 loss: 1.0935
env0_second_0:                 episode reward: 4.1500,                 loss: 0.8694
env1_first_0:                 episode reward: -15.4500,                 loss: nan
env1_second_0:                 episode reward: 15.4500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 273.15,                last time consumption/overall running time: 210.0974s / 26585.9786 s
env0_first_0:                 episode reward: -30.7000,                 loss: 1.0648
env0_second_0:                 episode reward: 30.7000,                 loss: 0.8430
env1_first_0:                 episode reward: -26.2500,                 loss: nan
env1_second_0:                 episode reward: 26.2500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 258.4,                last time consumption/overall running time: 198.4494s / 26784.4280 s
env0_first_0:                 episode reward: -25.3000,                 loss: 1.0124
env0_second_0:                 episode reward: 25.3000,                 loss: 0.8318
env1_first_0:                 episode reward: -64.8000,                 loss: nan
env1_second_0:                 episode reward: 64.8000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 515.85,                last time consumption/overall running time: 396.3079s / 27180.7359 s
env0_first_0:                 episode reward: -24.2500,                 loss: 1.0775
env0_second_0:                 episode reward: 24.2500,                 loss: 0.8145
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 314.2,                last time consumption/overall running time: 241.0491s / 27421.7850 s
env0_first_0:                 episode reward: -75.4500,                 loss: 1.1880
env0_second_0:                 episode reward: 75.4500,                 loss: 0.8633
env1_first_0:                 episode reward: -46.3500,                 loss: nan
env1_second_0:                 episode reward: 46.3500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 501.6,                last time consumption/overall running time: 384.7106s / 27806.4956 s
env0_first_0:                 episode reward: -50.5500,                 loss: 1.1097
env0_second_0:                 episode reward: 50.5500,                 loss: 0.8170
env1_first_0:                 episode reward: -63.1500,                 loss: nan
env1_second_0:                 episode reward: 63.1500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 287.0,                last time consumption/overall running time: 221.4808s / 28027.9764 s
env0_first_0:                 episode reward: -63.6000,                 loss: 1.0566
env0_second_0:                 episode reward: 63.6000,                 loss: 0.7654
env1_first_0:                 episode reward: -55.8000,                 loss: nan
env1_second_0:                 episode reward: 55.8000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 246.8,                last time consumption/overall running time: 190.6237s / 28218.6001 s
env0_first_0:                 episode reward: -79.8500,                 loss: 1.0702
env0_second_0:                 episode reward: 79.8500,                 loss: 0.7449
env1_first_0:                 episode reward: -95.2500,                 loss: nan
env1_second_0:                 episode reward: 95.2500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 246.95,                last time consumption/overall running time: 191.2866s / 28409.8868 s
env0_first_0:                 episode reward: -90.0000,                 loss: 1.0251
env0_second_0:                 episode reward: 90.0000,                 loss: 0.6778
env1_first_0:                 episode reward: -79.8000,                 loss: nan
env1_second_0:                 episode reward: 79.8000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 230.25,                last time consumption/overall running time: 177.6678s / 28587.5546 s
env0_first_0:                 episode reward: -84.8500,                 loss: 1.0498
env0_second_0:                 episode reward: 84.8500,                 loss: 0.6680
env1_first_0:                 episode reward: -76.2500,                 loss: nan
env1_second_0:                 episode reward: 76.2500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 285.75,                last time consumption/overall running time: 220.1848s / 28807.7394 s
env0_first_0:                 episode reward: -76.9500,                 loss: 1.0561
env0_second_0:                 episode reward: 76.9500,                 loss: 0.6477
env1_first_0:                 episode reward: -83.8000,                 loss: nan
env1_second_0:                 episode reward: 83.8000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 259.95,                last time consumption/overall running time: 201.3956s / 29009.1350 s
env0_first_0:                 episode reward: -80.8000,                 loss: 1.1202
env0_second_0:                 episode reward: 80.8000,                 loss: 0.6489
env1_first_0:                 episode reward: -75.3500,                 loss: nan
env1_second_0:                 episode reward: 75.3500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 363.35,                last time consumption/overall running time: 281.1162s / 29290.2512 s
env0_first_0:                 episode reward: -60.4000,                 loss: 1.1409
env0_second_0:                 episode reward: 60.4000,                 loss: 0.6218
env1_first_0:                 episode reward: -67.4500,                 loss: nan
env1_second_0:                 episode reward: 67.4500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 488.25,                last time consumption/overall running time: 378.0116s / 29668.2628 s
env0_first_0:                 episode reward: -82.5500,                 loss: 1.1770
env0_second_0:                 episode reward: 82.5500,                 loss: 0.6262
env1_first_0:                 episode reward: -61.0000,                 loss: nan
env1_second_0:                 episode reward: 61.0000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 334.0,                last time consumption/overall running time: 256.9479s / 29925.2107 s
env0_first_0:                 episode reward: -66.5000,                 loss: 1.1468
env0_second_0:                 episode reward: 66.5000,                 loss: 0.6606
env1_first_0:                 episode reward: -50.5000,                 loss: nan
env1_second_0:                 episode reward: 50.5000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 263.05,                last time consumption/overall running time: 203.2545s / 30128.4652 s
env0_first_0:                 episode reward: -78.7000,                 loss: 1.1501
env0_second_0:                 episode reward: 78.7000,                 loss: 0.6646
env1_first_0:                 episode reward: -88.5000,                 loss: nan
env1_second_0:                 episode reward: 88.5000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 281.55,                last time consumption/overall running time: 217.0414s / 30345.5065 s
env0_first_0:                 episode reward: -56.7000,                 loss: 1.1408
env0_second_0:                 episode reward: 56.7000,                 loss: 0.7503
env1_first_0:                 episode reward: -63.0500,                 loss: nan
env1_second_0:                 episode reward: 63.0500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 258.55,                last time consumption/overall running time: 199.4067s / 30544.9133 s
env0_first_0:                 episode reward: -68.6500,                 loss: 1.2113
env0_second_0:                 episode reward: 68.6500,                 loss: 0.8224
env1_first_0:                 episode reward: -66.9500,                 loss: nan
env1_second_0:                 episode reward: 66.9500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 319.45,                last time consumption/overall running time: 245.5100s / 30790.4233 s
env0_first_0:                 episode reward: -62.3000,                 loss: 1.2528
env0_second_0:                 episode reward: 62.3000,                 loss: 0.8428
env1_first_0:                 episode reward: -68.6000,                 loss: nan
env1_second_0:                 episode reward: 68.6000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 289.65,                last time consumption/overall running time: 222.7841s / 31013.2074 s
env0_first_0:                 episode reward: -28.3000,                 loss: 1.3225
env0_second_0:                 episode reward: 28.3000,                 loss: 0.9096
env1_first_0:                 episode reward: -42.7500,                 loss: nan
env1_second_0:                 episode reward: 42.7500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 310.0,                last time consumption/overall running time: 239.7339s / 31252.9413 s
env0_first_0:                 episode reward: -37.2500,                 loss: 1.4548
env0_second_0:                 episode reward: 37.2500,                 loss: 0.9813
env1_first_0:                 episode reward: -47.6500,                 loss: nan
env1_second_0:                 episode reward: 47.6500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 321.25,                last time consumption/overall running time: 247.6440s / 31500.5854 s
env0_first_0:                 episode reward: -47.3000,                 loss: 1.6251
env0_second_0:                 episode reward: 47.3000,                 loss: 1.1358