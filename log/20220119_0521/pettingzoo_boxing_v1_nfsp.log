pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 10000, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nfsp', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'eta': 0.1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0521/pettingzoo_boxing_v1_nfsp. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0521/pettingzoo_boxing_v1_nfsp.
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 5.4170s / 5.4170 s
env0_first_0:                 episode reward: -3.0000,                 loss: nan
env0_second_0:                 episode reward: 3.0000,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 737.3050s / 742.7220 s
env0_first_0:                 episode reward: 3.3500,                 loss: 0.0060
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0057
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 1335.1900s / 2077.9119 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0216
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0190
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1728.75,                last time consumption/overall running time: 1314.1484s / 3392.0604 s
env0_first_0:                 episode reward: 17.1000,                 loss: 0.0436
env0_second_0:                 episode reward: -17.1000,                 loss: 0.0385
env1_first_0:                 episode reward: 16.8500,                 loss: nan
env1_second_0:                 episode reward: -16.8500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1402.45,                last time consumption/overall running time: 1070.8269s / 4462.8873 s
env0_first_0:                 episode reward: 49.5000,                 loss: 0.0820
env0_second_0:                 episode reward: -49.5000,                 loss: 0.0569
env1_first_0:                 episode reward: 40.6500,                 loss: nan
env1_second_0:                 episode reward: -40.6500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1021.9,                last time consumption/overall running time: 782.8621s / 5245.7493 s
env0_first_0:                 episode reward: 57.6000,                 loss: 0.1221
env0_second_0:                 episode reward: -57.6000,                 loss: 0.0874
env1_first_0:                 episode reward: 46.4500,                 loss: nan
env1_second_0:                 episode reward: -46.4500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 471.5,                last time consumption/overall running time: 363.5152s / 5609.2645 s
env0_first_0:                 episode reward: 67.9000,                 loss: 0.1566
env0_second_0:                 episode reward: -67.9000,                 loss: 0.1107
env1_first_0:                 episode reward: 55.1000,                 loss: nan
env1_second_0:                 episode reward: -55.1000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 519.25,                last time consumption/overall running time: 397.3788s / 6006.6433 s
env0_first_0:                 episode reward: 59.5000,                 loss: 0.1702
env0_second_0:                 episode reward: -59.5000,                 loss: 0.1169
env1_first_0:                 episode reward: 57.8500,                 loss: nan
env1_second_0:                 episode reward: -57.8500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 768.95,                last time consumption/overall running time: 588.4550s / 6595.0983 s
env0_first_0:                 episode reward: 37.0500,                 loss: 0.1848
env0_second_0:                 episode reward: -37.0500,                 loss: 0.1226
env1_first_0:                 episode reward: 56.0500,                 loss: nan
env1_second_0:                 episode reward: -56.0500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 842.4,                last time consumption/overall running time: 646.9489s / 7242.0472 s
env0_first_0:                 episode reward: 35.1500,                 loss: 0.2061
env0_second_0:                 episode reward: -35.1500,                 loss: 0.1402
env1_first_0:                 episode reward: 53.8000,                 loss: nan
env1_second_0:                 episode reward: -53.8000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 756.5,                last time consumption/overall running time: 579.8840s / 7821.9312 s
env0_first_0:                 episode reward: 37.1500,                 loss: 0.2538
env0_second_0:                 episode reward: -37.1500,                 loss: 0.1458
env1_first_0:                 episode reward: 38.5000,                 loss: nan
env1_second_0:                 episode reward: -38.5000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1056.45,                last time consumption/overall running time: 812.8707s / 8634.8019 s
env0_first_0:                 episode reward: 13.8000,                 loss: 0.2989
env0_second_0:                 episode reward: -13.8000,                 loss: 0.1805
env1_first_0:                 episode reward: 53.0500,                 loss: nan
env1_second_0:                 episode reward: -53.0500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 497.65,                last time consumption/overall running time: 382.7268s / 9017.5287 s
env0_first_0:                 episode reward: 26.5000,                 loss: 0.3351
env0_second_0:                 episode reward: -26.5000,                 loss: 0.2456
env1_first_0:                 episode reward: 31.3000,                 loss: nan
env1_second_0:                 episode reward: -31.3000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 345.75,                last time consumption/overall running time: 264.5535s / 9282.0822 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.4346
env0_second_0:                 episode reward: 6.7500,                 loss: 0.2849
env1_first_0:                 episode reward: -17.5500,                 loss: nan
env1_second_0:                 episode reward: 17.5500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 318.2,                last time consumption/overall running time: 244.4305s / 9526.5127 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.5203
env0_second_0:                 episode reward: 7.1500,                 loss: 0.3345
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 429.25,                last time consumption/overall running time: 330.4131s / 9856.9259 s
env0_first_0:                 episode reward: 1.7000,                 loss: 0.4879
env0_second_0:                 episode reward: -1.7000,                 loss: 0.3260
env1_first_0:                 episode reward: 13.9500,                 loss: nan
env1_second_0:                 episode reward: -13.9500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 505.9,                last time consumption/overall running time: 389.9458s / 10246.8717 s
env0_first_0:                 episode reward: 31.5500,                 loss: 0.5287
env0_second_0:                 episode reward: -31.5500,                 loss: 0.3413
env1_first_0:                 episode reward: 18.9000,                 loss: nan
env1_second_0:                 episode reward: -18.9000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 306.0,                last time consumption/overall running time: 235.6537s / 10482.5254 s
env0_first_0:                 episode reward: 23.0500,                 loss: 0.5970
env0_second_0:                 episode reward: -23.0500,                 loss: 0.4213
env1_first_0:                 episode reward: 13.7000,                 loss: nan
env1_second_0:                 episode reward: -13.7000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 283.8,                last time consumption/overall running time: 217.6434s / 10700.1688 s
env0_first_0:                 episode reward: 40.8500,                 loss: 0.6349
env0_second_0:                 episode reward: -40.8500,                 loss: 0.4953
env1_first_0:                 episode reward: 41.5000,                 loss: nan
env1_second_0:                 episode reward: -41.5000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 337.15,                last time consumption/overall running time: 259.5467s / 10959.7155 s
env0_first_0:                 episode reward: 19.7000,                 loss: 0.6535
env0_second_0:                 episode reward: -19.7000,                 loss: 0.5210
env1_first_0:                 episode reward: 38.0500,                 loss: nan
env1_second_0:                 episode reward: -38.0500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 395.9,                last time consumption/overall running time: 305.2935s / 11265.0090 s
env0_first_0:                 episode reward: 13.2000,                 loss: 0.6590
env0_second_0:                 episode reward: -13.2000,                 loss: 0.5395
env1_first_0:                 episode reward: 20.6500,                 loss: nan
env1_second_0:                 episode reward: -20.6500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 304.8,                last time consumption/overall running time: 235.8303s / 11500.8394 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.6756
env0_second_0:                 episode reward: 3.6500,                 loss: 0.5233
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 293.05,                last time consumption/overall running time: 226.9033s / 11727.7427 s
env0_first_0:                 episode reward: 23.0000,                 loss: 0.7194
env0_second_0:                 episode reward: -23.0000,                 loss: 0.5652
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 348.3,                last time consumption/overall running time: 266.8314s / 11994.5741 s
env0_first_0:                 episode reward: 2.9500,                 loss: 0.7713
env0_second_0:                 episode reward: -2.9500,                 loss: 0.6024
env1_first_0:                 episode reward: 8.5000,                 loss: nan
env1_second_0:                 episode reward: -8.5000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 310.35,                last time consumption/overall running time: 238.9966s / 12233.5707 s
env0_first_0:                 episode reward: -7.3000,                 loss: 0.8136
env0_second_0:                 episode reward: 7.3000,                 loss: 0.5800
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 500.3,                last time consumption/overall running time: 384.1206s / 12617.6913 s
env0_first_0:                 episode reward: -27.5500,                 loss: 0.7790
env0_second_0:                 episode reward: 27.5500,                 loss: 0.5548
env1_first_0:                 episode reward: -23.6000,                 loss: nan
env1_second_0:                 episode reward: 23.6000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 268.5,                last time consumption/overall running time: 206.2758s / 12823.9671 s
env0_first_0:                 episode reward: -18.6000,                 loss: 0.7617
env0_second_0:                 episode reward: 18.6000,                 loss: 0.5659
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 335.1,                last time consumption/overall running time: 257.1657s / 13081.1328 s
env0_first_0:                 episode reward: -35.1000,                 loss: 0.8006
env0_second_0:                 episode reward: 35.1000,                 loss: 0.6160
env1_first_0:                 episode reward: -25.4000,                 loss: nan
env1_second_0:                 episode reward: 25.4000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 325.65,                last time consumption/overall running time: 250.5250s / 13331.6578 s
env0_first_0:                 episode reward: -42.2000,                 loss: 0.8466
env0_second_0:                 episode reward: 42.2000,                 loss: 0.6675
env1_first_0:                 episode reward: -45.3500,                 loss: nan
env1_second_0:                 episode reward: 45.3500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 449.85,                last time consumption/overall running time: 346.5642s / 13678.2220 s
env0_first_0:                 episode reward: -21.3500,                 loss: 0.8888
env0_second_0:                 episode reward: 21.3500,                 loss: 0.7484
env1_first_0:                 episode reward: 21.1000,                 loss: nan
env1_second_0:                 episode reward: -21.1000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 377.6,                last time consumption/overall running time: 291.2132s / 13969.4352 s
env0_first_0:                 episode reward: 20.9500,                 loss: 0.9110
env0_second_0:                 episode reward: -20.9500,                 loss: 0.7457
env1_first_0:                 episode reward: 39.7000,                 loss: nan
env1_second_0:                 episode reward: -39.7000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 318.5,                last time consumption/overall running time: 246.4069s / 14215.8421 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.9388
env0_second_0:                 episode reward: -3.3000,                 loss: 0.7474
env1_first_0:                 episode reward: 10.7500,                 loss: nan
env1_second_0:                 episode reward: -10.7500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 280.4,                last time consumption/overall running time: 215.9951s / 14431.8372 s
env0_first_0:                 episode reward: 25.9500,                 loss: 0.9706
env0_second_0:                 episode reward: -25.9500,                 loss: 0.7562
env1_first_0:                 episode reward: 19.9500,                 loss: nan
env1_second_0:                 episode reward: -19.9500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 321.05,                last time consumption/overall running time: 246.3506s / 14678.1879 s
env0_first_0:                 episode reward: 15.8000,                 loss: 0.9677
env0_second_0:                 episode reward: -15.8000,                 loss: 0.8285
env1_first_0:                 episode reward: 24.2500,                 loss: nan
env1_second_0:                 episode reward: -24.2500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 280.95,                last time consumption/overall running time: 214.9328s / 14893.1207 s
env0_first_0:                 episode reward: 12.3000,                 loss: 0.9245
env0_second_0:                 episode reward: -12.3000,                 loss: 0.8023
env1_first_0:                 episode reward: 21.2000,                 loss: nan
env1_second_0:                 episode reward: -21.2000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 291.7,                last time consumption/overall running time: 225.0821s / 15118.2028 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.9149
env0_second_0:                 episode reward: 5.3500,                 loss: 0.7865
env1_first_0:                 episode reward: 17.8500,                 loss: nan
env1_second_0:                 episode reward: -17.8500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 288.65,                last time consumption/overall running time: 221.8230s / 15340.0257 s
env0_first_0:                 episode reward: 14.6500,                 loss: 0.9413
env0_second_0:                 episode reward: -14.6500,                 loss: 0.7572
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 256.8,                last time consumption/overall running time: 197.0312s / 15537.0569 s
env0_first_0:                 episode reward: 13.0500,                 loss: 0.9632
env0_second_0:                 episode reward: -13.0500,                 loss: 0.7630
env1_first_0:                 episode reward: 17.9500,                 loss: nan
env1_second_0:                 episode reward: -17.9500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 303.65,                last time consumption/overall running time: 233.6084s / 15770.6653 s
env0_first_0:                 episode reward: 30.2000,                 loss: 0.9398
env0_second_0:                 episode reward: -30.2000,                 loss: 0.7346
env1_first_0:                 episode reward: 16.9000,                 loss: nan
env1_second_0:                 episode reward: -16.9000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 351.2,                last time consumption/overall running time: 270.8253s / 16041.4906 s
env0_first_0:                 episode reward: -18.4000,                 loss: 0.9753
env0_second_0:                 episode reward: 18.4000,                 loss: 0.7622
env1_first_0:                 episode reward: 8.4000,                 loss: nan
env1_second_0:                 episode reward: -8.4000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 345.95,                last time consumption/overall running time: 266.8846s / 16308.3753 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.9893
env0_second_0:                 episode reward: 7.1500,                 loss: 0.8041
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 299.5,                last time consumption/overall running time: 231.2819s / 16539.6571 s
env0_first_0:                 episode reward: -22.7000,                 loss: 1.0711
env0_second_0:                 episode reward: 22.7000,                 loss: 0.8901
env1_first_0:                 episode reward: -31.2000,                 loss: nan
env1_second_0:                 episode reward: 31.2000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 341.05,                last time consumption/overall running time: 260.8735s / 16800.5306 s
env0_first_0:                 episode reward: -29.1500,                 loss: 1.1711
env0_second_0:                 episode reward: 29.1500,                 loss: 0.9523
env1_first_0:                 episode reward: -45.1000,                 loss: nan
env1_second_0:                 episode reward: 45.1000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 283.3,                last time consumption/overall running time: 216.7315s / 17017.2621 s
env0_first_0:                 episode reward: 17.3000,                 loss: 1.2412
env0_second_0:                 episode reward: -17.3000,                 loss: 0.9865
env1_first_0:                 episode reward: 11.7500,                 loss: nan
env1_second_0:                 episode reward: -11.7500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 313.55,                last time consumption/overall running time: 240.6742s / 17257.9362 s
env0_first_0:                 episode reward: -25.8000,                 loss: 1.2375
env0_second_0:                 episode reward: 25.8000,                 loss: 0.9920
env1_first_0:                 episode reward: -32.6500,                 loss: nan
env1_second_0:                 episode reward: 32.6500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 309.85,                last time consumption/overall running time: 238.5723s / 17496.5086 s
env0_first_0:                 episode reward: -32.1000,                 loss: 1.2011
env0_second_0:                 episode reward: 32.1000,                 loss: 1.0282
env1_first_0:                 episode reward: -21.2000,                 loss: nan
env1_second_0:                 episode reward: 21.2000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 320.45,                last time consumption/overall running time: 246.6487s / 17743.1573 s
env0_first_0:                 episode reward: -40.7000,                 loss: 1.2003
env0_second_0:                 episode reward: 40.7000,                 loss: 1.0243
env1_first_0:                 episode reward: -24.5000,                 loss: nan
env1_second_0:                 episode reward: 24.5000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 391.95,                last time consumption/overall running time: 303.0951s / 18046.2524 s
env0_first_0:                 episode reward: -54.4500,                 loss: 1.2138
env0_second_0:                 episode reward: 54.4500,                 loss: 1.0026
env1_first_0:                 episode reward: -26.1500,                 loss: nan
env1_second_0:                 episode reward: 26.1500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 433.55,                last time consumption/overall running time: 334.4231s / 18380.6754 s
env0_first_0:                 episode reward: -53.0000,                 loss: 1.1053
env0_second_0:                 episode reward: 53.0000,                 loss: 0.9075
env1_first_0:                 episode reward: -52.9500,                 loss: nan
env1_second_0:                 episode reward: 52.9500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 459.45,                last time consumption/overall running time: 353.4355s / 18734.1110 s
env0_first_0:                 episode reward: -27.9500,                 loss: 0.9906
env0_second_0:                 episode reward: 27.9500,                 loss: 0.7963
env1_first_0:                 episode reward: -66.1500,                 loss: nan
env1_second_0:                 episode reward: 66.1500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 362.6,                last time consumption/overall running time: 279.7127s / 19013.8237 s
env0_first_0:                 episode reward: -31.2500,                 loss: 0.9467
env0_second_0:                 episode reward: 31.2500,                 loss: 0.6903
env1_first_0:                 episode reward: -42.3500,                 loss: nan
env1_second_0:                 episode reward: 42.3500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 303.1,                last time consumption/overall running time: 232.6627s / 19246.4864 s
env0_first_0:                 episode reward: -24.8000,                 loss: 1.0026
env0_second_0:                 episode reward: 24.8000,                 loss: 0.6760
env1_first_0:                 episode reward: -46.5500,                 loss: nan
env1_second_0:                 episode reward: 46.5500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 278.3,                last time consumption/overall running time: 215.4393s / 19461.9257 s
env0_first_0:                 episode reward: -51.6500,                 loss: 1.0291
env0_second_0:                 episode reward: 51.6500,                 loss: 0.7202
env1_first_0:                 episode reward: -45.7000,                 loss: nan
env1_second_0:                 episode reward: 45.7000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 271.4,                last time consumption/overall running time: 210.3701s / 19672.2958 s
env0_first_0:                 episode reward: -56.8500,                 loss: 1.0744
env0_second_0:                 episode reward: 56.8500,                 loss: 0.7754
env1_first_0:                 episode reward: -51.3500,                 loss: nan
env1_second_0:                 episode reward: 51.3500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 315.8,                last time consumption/overall running time: 243.5398s / 19915.8356 s
env0_first_0:                 episode reward: -38.3500,                 loss: 1.1035
env0_second_0:                 episode reward: 38.3500,                 loss: 0.8223
env1_first_0:                 episode reward: -31.0500,                 loss: nan
env1_second_0:                 episode reward: 31.0500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 411.55,                last time consumption/overall running time: 316.2698s / 20232.1054 s
env0_first_0:                 episode reward: -87.2500,                 loss: 1.0787
env0_second_0:                 episode reward: 87.2500,                 loss: 0.8442
env1_first_0:                 episode reward: -47.4000,                 loss: nan
env1_second_0:                 episode reward: 47.4000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 359.2,                last time consumption/overall running time: 275.4399s / 20507.5453 s
env0_first_0:                 episode reward: -59.5000,                 loss: 1.1119
env0_second_0:                 episode reward: 59.5000,                 loss: 0.8015
env1_first_0:                 episode reward: -75.0000,                 loss: nan
env1_second_0:                 episode reward: 75.0000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 327.25,                last time consumption/overall running time: 252.7767s / 20760.3220 s
env0_first_0:                 episode reward: -63.1000,                 loss: 1.1700
env0_second_0:                 episode reward: 63.1000,                 loss: 0.8279
env1_first_0:                 episode reward: -44.0000,                 loss: nan
env1_second_0:                 episode reward: 44.0000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 299.1,                last time consumption/overall running time: 230.8484s / 20991.1704 s
env0_first_0:                 episode reward: -65.5500,                 loss: 1.2324
env0_second_0:                 episode reward: 65.5500,                 loss: 0.8443
env1_first_0:                 episode reward: -72.2500,                 loss: nan
env1_second_0:                 episode reward: 72.2500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 621.45,                last time consumption/overall running time: 477.3372s / 21468.5076 s
env0_first_0:                 episode reward: -53.6000,                 loss: 1.0943
env0_second_0:                 episode reward: 53.6000,                 loss: 0.7933
env1_first_0:                 episode reward: -33.0500,                 loss: nan
env1_second_0:                 episode reward: 33.0500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 339.7,                last time consumption/overall running time: 262.2983s / 21730.8059 s
env0_first_0:                 episode reward: -38.2500,                 loss: 1.0246
env0_second_0:                 episode reward: 38.2500,                 loss: 0.6915
env1_first_0:                 episode reward: -22.1500,                 loss: nan
env1_second_0:                 episode reward: 22.1500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 290.4,                last time consumption/overall running time: 223.4978s / 21954.3037 s
env0_first_0:                 episode reward: -31.9500,                 loss: 0.9968
env0_second_0:                 episode reward: 31.9500,                 loss: 0.6666
env1_first_0:                 episode reward: -37.6000,                 loss: nan
env1_second_0:                 episode reward: 37.6000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 255.8,                last time consumption/overall running time: 196.7963s / 22151.1000 s
env0_first_0:                 episode reward: 8.7500,                 loss: 1.1227
env0_second_0:                 episode reward: -8.7500,                 loss: 0.6897
env1_first_0:                 episode reward: -9.8000,                 loss: nan
env1_second_0:                 episode reward: 9.8000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 297.0,                last time consumption/overall running time: 228.4705s / 22379.5705 s
env0_first_0:                 episode reward: 0.7500,                 loss: 1.1735
env0_second_0:                 episode reward: -0.7500,                 loss: 0.7078
env1_first_0:                 episode reward: -14.1500,                 loss: nan
env1_second_0:                 episode reward: 14.1500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 348.95,                last time consumption/overall running time: 267.1862s / 22646.7567 s
env0_first_0:                 episode reward: -37.1000,                 loss: 1.2783
env0_second_0:                 episode reward: 37.1000,                 loss: 0.7600
env1_first_0:                 episode reward: -51.4000,                 loss: nan
env1_second_0:                 episode reward: 51.4000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 321.85,                last time consumption/overall running time: 247.3327s / 22894.0893 s
env0_first_0:                 episode reward: -37.8000,                 loss: 1.1925
env0_second_0:                 episode reward: 37.8000,                 loss: 0.7663
env1_first_0:                 episode reward: -49.6000,                 loss: nan
env1_second_0:                 episode reward: 49.6000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1193.15,                last time consumption/overall running time: 913.9161s / 23808.0054 s
env0_first_0:                 episode reward: -14.8500,                 loss: 1.0238
env0_second_0:                 episode reward: 14.8500,                 loss: 0.6729
env1_first_0:                 episode reward: -29.8500,                 loss: nan
env1_second_0:                 episode reward: 29.8500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 469.05,                last time consumption/overall running time: 360.5028s / 24168.5081 s
env0_first_0:                 episode reward: -80.0500,                 loss: 0.6730
env0_second_0:                 episode reward: 80.0500,                 loss: 0.4475
env1_first_0:                 episode reward: -55.8000,                 loss: nan
env1_second_0:                 episode reward: 55.8000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 334.75,                last time consumption/overall running time: 258.0052s / 24426.5133 s
env0_first_0:                 episode reward: -50.1500,                 loss: 0.6662
env0_second_0:                 episode reward: 50.1500,                 loss: 0.4450
env1_first_0:                 episode reward: -62.9000,                 loss: nan
env1_second_0:                 episode reward: 62.9000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 367.75,                last time consumption/overall running time: 283.0993s / 24709.6126 s
env0_first_0:                 episode reward: -23.6500,                 loss: 0.6616
env0_second_0:                 episode reward: 23.6500,                 loss: 0.4446
env1_first_0:                 episode reward: -45.8000,                 loss: nan
env1_second_0:                 episode reward: 45.8000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 303.4,                last time consumption/overall running time: 233.2087s / 24942.8213 s
env0_first_0:                 episode reward: -27.8500,                 loss: 0.7221
env0_second_0:                 episode reward: 27.8500,                 loss: 0.4643
env1_first_0:                 episode reward: -33.7500,                 loss: nan
env1_second_0:                 episode reward: 33.7500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 432.65,                last time consumption/overall running time: 333.3549s / 25276.1762 s
env0_first_0:                 episode reward: -34.5000,                 loss: 0.7943
env0_second_0:                 episode reward: 34.5000,                 loss: 0.5091
env1_first_0:                 episode reward: -76.3000,                 loss: nan
env1_second_0:                 episode reward: 76.3000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 366.65,                last time consumption/overall running time: 281.2621s / 25557.4383 s
env0_first_0:                 episode reward: -39.0500,                 loss: 0.8837
env0_second_0:                 episode reward: 39.0500,                 loss: 0.6012
env1_first_0:                 episode reward: -33.8000,                 loss: nan
env1_second_0:                 episode reward: 33.8000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 256.35,                last time consumption/overall running time: 198.1382s / 25755.5765 s
env0_first_0:                 episode reward: -59.8000,                 loss: 0.9634
env0_second_0:                 episode reward: 59.8000,                 loss: 0.6839
env1_first_0:                 episode reward: -60.6500,                 loss: nan
env1_second_0:                 episode reward: 60.6500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 266.15,                last time consumption/overall running time: 205.2670s / 25960.8435 s
env0_first_0:                 episode reward: -51.4500,                 loss: 1.0652
env0_second_0:                 episode reward: 51.4500,                 loss: 0.7554
env1_first_0:                 episode reward: -40.5000,                 loss: nan
env1_second_0:                 episode reward: 40.5000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 271.6,                last time consumption/overall running time: 208.9813s / 26169.8248 s
env0_first_0:                 episode reward: -57.6500,                 loss: 1.0688
env0_second_0:                 episode reward: 57.6500,                 loss: 0.8210
env1_first_0:                 episode reward: -25.0000,                 loss: nan
env1_second_0:                 episode reward: 25.0000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 267.8,                last time consumption/overall running time: 206.0563s / 26375.8811 s
env0_first_0:                 episode reward: -4.1500,                 loss: 1.0935
env0_second_0:                 episode reward: 4.1500,                 loss: 0.8694
env1_first_0:                 episode reward: -15.4500,                 loss: nan
env1_second_0:                 episode reward: 15.4500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 273.15,                last time consumption/overall running time: 210.0974s / 26585.9786 s
env0_first_0:                 episode reward: -30.7000,                 loss: 1.0648
env0_second_0:                 episode reward: 30.7000,                 loss: 0.8430
env1_first_0:                 episode reward: -26.2500,                 loss: nan
env1_second_0:                 episode reward: 26.2500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 258.4,                last time consumption/overall running time: 198.4494s / 26784.4280 s
env0_first_0:                 episode reward: -25.3000,                 loss: 1.0124
env0_second_0:                 episode reward: 25.3000,                 loss: 0.8318
env1_first_0:                 episode reward: -64.8000,                 loss: nan
env1_second_0:                 episode reward: 64.8000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 515.85,                last time consumption/overall running time: 396.3079s / 27180.7359 s
env0_first_0:                 episode reward: -24.2500,                 loss: 1.0775
env0_second_0:                 episode reward: 24.2500,                 loss: 0.8145
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 314.2,                last time consumption/overall running time: 241.0491s / 27421.7850 s
env0_first_0:                 episode reward: -75.4500,                 loss: 1.1880
env0_second_0:                 episode reward: 75.4500,                 loss: 0.8633
env1_first_0:                 episode reward: -46.3500,                 loss: nan
env1_second_0:                 episode reward: 46.3500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 501.6,                last time consumption/overall running time: 384.7106s / 27806.4956 s
env0_first_0:                 episode reward: -50.5500,                 loss: 1.1097
env0_second_0:                 episode reward: 50.5500,                 loss: 0.8170
env1_first_0:                 episode reward: -63.1500,                 loss: nan
env1_second_0:                 episode reward: 63.1500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 287.0,                last time consumption/overall running time: 221.4808s / 28027.9764 s
env0_first_0:                 episode reward: -63.6000,                 loss: 1.0566
env0_second_0:                 episode reward: 63.6000,                 loss: 0.7654
env1_first_0:                 episode reward: -55.8000,                 loss: nan
env1_second_0:                 episode reward: 55.8000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 246.8,                last time consumption/overall running time: 190.6237s / 28218.6001 s
env0_first_0:                 episode reward: -79.8500,                 loss: 1.0702
env0_second_0:                 episode reward: 79.8500,                 loss: 0.7449
env1_first_0:                 episode reward: -95.2500,                 loss: nan
env1_second_0:                 episode reward: 95.2500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 246.95,                last time consumption/overall running time: 191.2866s / 28409.8868 s
env0_first_0:                 episode reward: -90.0000,                 loss: 1.0251
env0_second_0:                 episode reward: 90.0000,                 loss: 0.6778
env1_first_0:                 episode reward: -79.8000,                 loss: nan
env1_second_0:                 episode reward: 79.8000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 230.25,                last time consumption/overall running time: 177.6678s / 28587.5546 s
env0_first_0:                 episode reward: -84.8500,                 loss: 1.0498
env0_second_0:                 episode reward: 84.8500,                 loss: 0.6680
env1_first_0:                 episode reward: -76.2500,                 loss: nan
env1_second_0:                 episode reward: 76.2500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 285.75,                last time consumption/overall running time: 220.1848s / 28807.7394 s
env0_first_0:                 episode reward: -76.9500,                 loss: 1.0561
env0_second_0:                 episode reward: 76.9500,                 loss: 0.6477
env1_first_0:                 episode reward: -83.8000,                 loss: nan
env1_second_0:                 episode reward: 83.8000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 259.95,                last time consumption/overall running time: 201.3956s / 29009.1350 s
env0_first_0:                 episode reward: -80.8000,                 loss: 1.1202
env0_second_0:                 episode reward: 80.8000,                 loss: 0.6489
env1_first_0:                 episode reward: -75.3500,                 loss: nan
env1_second_0:                 episode reward: 75.3500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 363.35,                last time consumption/overall running time: 281.1162s / 29290.2512 s
env0_first_0:                 episode reward: -60.4000,                 loss: 1.1409
env0_second_0:                 episode reward: 60.4000,                 loss: 0.6218
env1_first_0:                 episode reward: -67.4500,                 loss: nan
env1_second_0:                 episode reward: 67.4500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 488.25,                last time consumption/overall running time: 378.0116s / 29668.2628 s
env0_first_0:                 episode reward: -82.5500,                 loss: 1.1770
env0_second_0:                 episode reward: 82.5500,                 loss: 0.6262
env1_first_0:                 episode reward: -61.0000,                 loss: nan
env1_second_0:                 episode reward: 61.0000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 334.0,                last time consumption/overall running time: 256.9479s / 29925.2107 s
env0_first_0:                 episode reward: -66.5000,                 loss: 1.1468
env0_second_0:                 episode reward: 66.5000,                 loss: 0.6606
env1_first_0:                 episode reward: -50.5000,                 loss: nan
env1_second_0:                 episode reward: 50.5000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 263.05,                last time consumption/overall running time: 203.2545s / 30128.4652 s
env0_first_0:                 episode reward: -78.7000,                 loss: 1.1501
env0_second_0:                 episode reward: 78.7000,                 loss: 0.6646
env1_first_0:                 episode reward: -88.5000,                 loss: nan
env1_second_0:                 episode reward: 88.5000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 281.55,                last time consumption/overall running time: 217.0414s / 30345.5065 s
env0_first_0:                 episode reward: -56.7000,                 loss: 1.1408
env0_second_0:                 episode reward: 56.7000,                 loss: 0.7503
env1_first_0:                 episode reward: -63.0500,                 loss: nan
env1_second_0:                 episode reward: 63.0500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 258.55,                last time consumption/overall running time: 199.4067s / 30544.9133 s
env0_first_0:                 episode reward: -68.6500,                 loss: 1.2113
env0_second_0:                 episode reward: 68.6500,                 loss: 0.8224
env1_first_0:                 episode reward: -66.9500,                 loss: nan
env1_second_0:                 episode reward: 66.9500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 319.45,                last time consumption/overall running time: 245.5100s / 30790.4233 s
env0_first_0:                 episode reward: -62.3000,                 loss: 1.2528
env0_second_0:                 episode reward: 62.3000,                 loss: 0.8428
env1_first_0:                 episode reward: -68.6000,                 loss: nan
env1_second_0:                 episode reward: 68.6000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 289.65,                last time consumption/overall running time: 222.7841s / 31013.2074 s
env0_first_0:                 episode reward: -28.3000,                 loss: 1.3225
env0_second_0:                 episode reward: 28.3000,                 loss: 0.9096
env1_first_0:                 episode reward: -42.7500,                 loss: nan
env1_second_0:                 episode reward: 42.7500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 310.0,                last time consumption/overall running time: 239.7339s / 31252.9413 s
env0_first_0:                 episode reward: -37.2500,                 loss: 1.4548
env0_second_0:                 episode reward: 37.2500,                 loss: 0.9813
env1_first_0:                 episode reward: -47.6500,                 loss: nan
env1_second_0:                 episode reward: 47.6500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 321.25,                last time consumption/overall running time: 247.6440s / 31500.5854 s
env0_first_0:                 episode reward: -47.3000,                 loss: 1.6251
env0_second_0:                 episode reward: 47.3000,                 loss: 1.1358
env1_first_0:                 episode reward: -57.3500,                 loss: nan
env1_second_0:                 episode reward: 57.3500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 288.6,                last time consumption/overall running time: 221.8036s / 31722.3890 s
env0_first_0:                 episode reward: -66.7000,                 loss: 1.7772
env0_second_0:                 episode reward: 66.7000,                 loss: 1.1670
env1_first_0:                 episode reward: -24.9000,                 loss: nan
env1_second_0:                 episode reward: 24.9000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 320.65,                last time consumption/overall running time: 245.5747s / 31967.9636 s
env0_first_0:                 episode reward: -56.8000,                 loss: 1.7946
env0_second_0:                 episode reward: 56.8000,                 loss: 1.1751
env1_first_0:                 episode reward: -61.4500,                 loss: nan
env1_second_0:                 episode reward: 61.4500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 246.75,                last time consumption/overall running time: 189.6729s / 32157.6365 s
env0_first_0:                 episode reward: -85.4500,                 loss: 1.7685
env0_second_0:                 episode reward: 85.4500,                 loss: 1.1650
env1_first_0:                 episode reward: -61.8500,                 loss: nan
env1_second_0:                 episode reward: 61.8500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 264.75,                last time consumption/overall running time: 204.2550s / 32361.8915 s
env0_first_0:                 episode reward: -66.5500,                 loss: 1.6991
env0_second_0:                 episode reward: 66.5500,                 loss: 1.0806
env1_first_0:                 episode reward: -88.5500,                 loss: nan
env1_second_0:                 episode reward: 88.5500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 252.65,                last time consumption/overall running time: 194.7511s / 32556.6426 s
env0_first_0:                 episode reward: -64.1000,                 loss: 1.5981
env0_second_0:                 episode reward: 64.1000,                 loss: 1.0199
env1_first_0:                 episode reward: -84.3500,                 loss: nan
env1_second_0:                 episode reward: 84.3500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 271.35,                last time consumption/overall running time: 209.1743s / 32765.8169 s
env0_first_0:                 episode reward: -77.5000,                 loss: 1.5778
env0_second_0:                 episode reward: 77.5000,                 loss: 1.0283
env1_first_0:                 episode reward: -80.5000,                 loss: nan
env1_second_0:                 episode reward: 80.5000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 345.05,                last time consumption/overall running time: 266.3404s / 33032.1573 s
env0_first_0:                 episode reward: -58.9500,                 loss: 1.5510
env0_second_0:                 episode reward: 58.9500,                 loss: 0.9999
env1_first_0:                 episode reward: -50.8500,                 loss: nan
env1_second_0:                 episode reward: 50.8500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 283.3,                last time consumption/overall running time: 218.5769s / 33250.7343 s
env0_first_0:                 episode reward: -74.2500,                 loss: 1.5455
env0_second_0:                 episode reward: 74.2500,                 loss: 0.9698
env1_first_0:                 episode reward: -36.6500,                 loss: nan
env1_second_0:                 episode reward: 36.6500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 367.2,                last time consumption/overall running time: 283.0049s / 33533.7392 s
env0_first_0:                 episode reward: -25.6000,                 loss: 1.4840
env0_second_0:                 episode reward: 25.6000,                 loss: 0.9267
env1_first_0:                 episode reward: -47.9000,                 loss: nan
env1_second_0:                 episode reward: 47.9000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 343.3,                last time consumption/overall running time: 264.1832s / 33797.9224 s
env0_first_0:                 episode reward: -29.1000,                 loss: 1.4762
env0_second_0:                 episode reward: 29.1000,                 loss: 0.8802
env1_first_0:                 episode reward: -47.8000,                 loss: nan
env1_second_0:                 episode reward: 47.8000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 274.35,                last time consumption/overall running time: 211.7977s / 34009.7200 s
env0_first_0:                 episode reward: -52.3000,                 loss: 1.4641
env0_second_0:                 episode reward: 52.3000,                 loss: 0.9819
env1_first_0:                 episode reward: -41.6000,                 loss: nan
env1_second_0:                 episode reward: 41.6000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 268.15,                last time consumption/overall running time: 206.7592s / 34216.4792 s
env0_first_0:                 episode reward: -44.1500,                 loss: 1.4868
env0_second_0:                 episode reward: 44.1500,                 loss: 1.0023
env1_first_0:                 episode reward: -77.6500,                 loss: nan
env1_second_0:                 episode reward: 77.6500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 235.3,                last time consumption/overall running time: 181.8706s / 34398.3498 s
env0_first_0:                 episode reward: -87.7500,                 loss: 1.5731
env0_second_0:                 episode reward: 87.7500,                 loss: 1.0268
env1_first_0:                 episode reward: -62.7000,                 loss: nan
env1_second_0:                 episode reward: 62.7000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 265.25,                last time consumption/overall running time: 204.7344s / 34603.0843 s
env0_first_0:                 episode reward: -54.2500,                 loss: 1.6004
env0_second_0:                 episode reward: 54.2500,                 loss: 0.9997
env1_first_0:                 episode reward: -79.2000,                 loss: nan
env1_second_0:                 episode reward: 79.2000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 574.35,                last time consumption/overall running time: 441.9296s / 35045.0138 s
env0_first_0:                 episode reward: -60.7000,                 loss: 1.3790
env0_second_0:                 episode reward: 60.7000,                 loss: 0.8823
env1_first_0:                 episode reward: -57.7500,                 loss: nan
env1_second_0:                 episode reward: 57.7500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 467.4,                last time consumption/overall running time: 358.3967s / 35403.4105 s
env0_first_0:                 episode reward: -38.3500,                 loss: 1.1723
env0_second_0:                 episode reward: 38.3500,                 loss: 0.7556
env1_first_0:                 episode reward: -49.1500,                 loss: nan
env1_second_0:                 episode reward: 49.1500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 280.0,                last time consumption/overall running time: 215.9207s / 35619.3312 s
env0_first_0:                 episode reward: -54.7000,                 loss: 1.1876
env0_second_0:                 episode reward: 54.7000,                 loss: 0.7378
env1_first_0:                 episode reward: -76.1500,                 loss: nan
env1_second_0:                 episode reward: 76.1500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 269.2,                last time consumption/overall running time: 207.7416s / 35827.0728 s
env0_first_0:                 episode reward: -72.6500,                 loss: 1.1581
env0_second_0:                 episode reward: 72.6500,                 loss: 0.7326
env1_first_0:                 episode reward: -58.7000,                 loss: nan
env1_second_0:                 episode reward: 58.7000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 435.05,                last time consumption/overall running time: 334.1491s / 36161.2219 s
env0_first_0:                 episode reward: -38.7500,                 loss: 1.1537
env0_second_0:                 episode reward: 38.7500,                 loss: 0.7323
env1_first_0:                 episode reward: -62.8000,                 loss: nan
env1_second_0:                 episode reward: 62.8000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 367.75,                last time consumption/overall running time: 282.0403s / 36443.2621 s
env0_first_0:                 episode reward: -33.7000,                 loss: 1.1528
env0_second_0:                 episode reward: 33.7000,                 loss: 0.7285
env1_first_0:                 episode reward: -56.0500,                 loss: nan
env1_second_0:                 episode reward: 56.0500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 254.95,                last time consumption/overall running time: 197.3886s / 36640.6507 s
env0_first_0:                 episode reward: -56.2000,                 loss: 1.0973
env0_second_0:                 episode reward: 56.2000,                 loss: 0.7407
env1_first_0:                 episode reward: -84.5000,                 loss: nan
env1_second_0:                 episode reward: 84.5000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 267.85,                last time consumption/overall running time: 207.7942s / 36848.4448 s
env0_first_0:                 episode reward: -45.6000,                 loss: 1.1558
env0_second_0:                 episode reward: 45.6000,                 loss: 0.8031
env1_first_0:                 episode reward: -52.1500,                 loss: nan
env1_second_0:                 episode reward: 52.1500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 251.2,                last time consumption/overall running time: 193.7566s / 37042.2015 s
env0_first_0:                 episode reward: -75.1500,                 loss: 1.2114
env0_second_0:                 episode reward: 75.1500,                 loss: 0.8115
env1_first_0:                 episode reward: -67.5500,                 loss: nan
env1_second_0:                 episode reward: 67.5500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 416.7,                last time consumption/overall running time: 321.9989s / 37364.2004 s
env0_first_0:                 episode reward: -21.6000,                 loss: 1.2532
env0_second_0:                 episode reward: 21.6000,                 loss: 0.8360
env1_first_0:                 episode reward: -29.1500,                 loss: nan
env1_second_0:                 episode reward: 29.1500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 287.35,                last time consumption/overall running time: 245.5309s / 37609.7312 s
env0_first_0:                 episode reward: -29.8000,                 loss: 1.2217
env0_second_0:                 episode reward: 29.8000,                 loss: 0.7790
env1_first_0:                 episode reward: -35.0500,                 loss: nan
env1_second_0:                 episode reward: 35.0500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 266.7,                last time consumption/overall running time: 236.5013s / 37846.2325 s
env0_first_0:                 episode reward: -66.0500,                 loss: 1.1312
env0_second_0:                 episode reward: 66.0500,                 loss: 0.8003
env1_first_0:                 episode reward: -52.4000,                 loss: nan
env1_second_0:                 episode reward: 52.4000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 269.1,                last time consumption/overall running time: 249.6019s / 38095.8344 s
env0_first_0:                 episode reward: -82.6000,                 loss: 1.0890
env0_second_0:                 episode reward: 82.6000,                 loss: 0.7828
env1_first_0:                 episode reward: -46.9500,                 loss: nan
env1_second_0:                 episode reward: 46.9500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 307.3,                last time consumption/overall running time: 286.1831s / 38382.0175 s
env0_first_0:                 episode reward: -48.9000,                 loss: 1.1271
env0_second_0:                 episode reward: 48.9000,                 loss: 0.8674
env1_first_0:                 episode reward: -63.7000,                 loss: nan
env1_second_0:                 episode reward: 63.7000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 318.4,                last time consumption/overall running time: 294.1764s / 38676.1938 s
env0_first_0:                 episode reward: -58.9000,                 loss: 1.2164
env0_second_0:                 episode reward: 58.9000,                 loss: 0.9428
env1_first_0:                 episode reward: -58.5000,                 loss: nan
env1_second_0:                 episode reward: 58.5000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 412.2,                last time consumption/overall running time: 379.3090s / 39055.5029 s
env0_first_0:                 episode reward: -33.2500,                 loss: 1.3097
env0_second_0:                 episode reward: 33.2500,                 loss: 1.0345
env1_first_0:                 episode reward: -33.5500,                 loss: nan
env1_second_0:                 episode reward: 33.5500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 405.5,                last time consumption/overall running time: 371.0195s / 39426.5223 s
env0_first_0:                 episode reward: -24.1500,                 loss: 1.3241
env0_second_0:                 episode reward: 24.1500,                 loss: 1.0571
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 305.45,                last time consumption/overall running time: 280.1637s / 39706.6861 s
env0_first_0:                 episode reward: -77.4000,                 loss: 1.4388
env0_second_0:                 episode reward: 77.4000,                 loss: 1.0653
env1_first_0:                 episode reward: -64.1500,                 loss: nan
env1_second_0:                 episode reward: 64.1500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 264.1,                last time consumption/overall running time: 241.7115s / 39948.3976 s
env0_first_0:                 episode reward: -56.3000,                 loss: 1.5673
env0_second_0:                 episode reward: 56.3000,                 loss: 1.0791
env1_first_0:                 episode reward: -79.5000,                 loss: nan
env1_second_0:                 episode reward: 79.5000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 280.9,                last time consumption/overall running time: 257.5962s / 40205.9938 s
env0_first_0:                 episode reward: -66.1500,                 loss: 1.4502
env0_second_0:                 episode reward: 66.1500,                 loss: 1.1154
env1_first_0:                 episode reward: -54.9500,                 loss: nan
env1_second_0:                 episode reward: 54.9500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 288.15,                last time consumption/overall running time: 263.5059s / 40469.4996 s
env0_first_0:                 episode reward: -53.8500,                 loss: 1.3659
env0_second_0:                 episode reward: 53.8500,                 loss: 1.1431
env1_first_0:                 episode reward: -48.7500,                 loss: nan
env1_second_0:                 episode reward: 48.7500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 323.4,                last time consumption/overall running time: 295.8081s / 40765.3077 s
env0_first_0:                 episode reward: -59.1000,                 loss: 1.4123
env0_second_0:                 episode reward: 59.1000,                 loss: 1.1451
env1_first_0:                 episode reward: -40.1500,                 loss: nan
env1_second_0:                 episode reward: 40.1500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 733.3,                last time consumption/overall running time: 670.8150s / 41436.1227 s
env0_first_0:                 episode reward: -17.7500,                 loss: 1.3321
env0_second_0:                 episode reward: 17.7500,                 loss: 1.0736
env1_first_0:                 episode reward: -31.8500,                 loss: nan
env1_second_0:                 episode reward: 31.8500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 414.6,                last time consumption/overall running time: 379.2577s / 41815.3805 s
env0_first_0:                 episode reward: -30.2000,                 loss: 1.2557
env0_second_0:                 episode reward: 30.2000,                 loss: 1.0162
env1_first_0:                 episode reward: -19.0500,                 loss: nan
env1_second_0:                 episode reward: 19.0500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 374.3,                last time consumption/overall running time: 342.2180s / 42157.5984 s
env0_first_0:                 episode reward: -36.1000,                 loss: 1.3764
env0_second_0:                 episode reward: 36.1000,                 loss: 1.0673
env1_first_0:                 episode reward: -40.2500,                 loss: nan
env1_second_0:                 episode reward: 40.2500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 341.95,                last time consumption/overall running time: 312.5994s / 42470.1979 s
env0_first_0:                 episode reward: -13.7000,                 loss: 1.5293
env0_second_0:                 episode reward: 13.7000,                 loss: 1.1605
env1_first_0:                 episode reward: -28.7500,                 loss: nan
env1_second_0:                 episode reward: 28.7500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 334.2,                last time consumption/overall running time: 305.3209s / 42775.5187 s
env0_first_0:                 episode reward: 0.7500,                 loss: 1.6053
env0_second_0:                 episode reward: -0.7500,                 loss: 1.2408
env1_first_0:                 episode reward: -39.6500,                 loss: nan
env1_second_0:                 episode reward: 39.6500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 276.05,                last time consumption/overall running time: 253.0069s / 43028.5256 s
env0_first_0:                 episode reward: -34.2500,                 loss: 1.5898
env0_second_0:                 episode reward: 34.2500,                 loss: 1.2000
env1_first_0:                 episode reward: -53.0500,                 loss: nan
env1_second_0:                 episode reward: 53.0500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 287.25,                last time consumption/overall running time: 263.2520s / 43291.7776 s
env0_first_0:                 episode reward: -43.4000,                 loss: 1.5984
env0_second_0:                 episode reward: 43.4000,                 loss: 1.2472
env1_first_0:                 episode reward: -39.8000,                 loss: nan
env1_second_0:                 episode reward: 39.8000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 300.7,                last time consumption/overall running time: 275.3498s / 43567.1273 s
env0_first_0:                 episode reward: -55.5000,                 loss: 1.7869
env0_second_0:                 episode reward: 55.5000,                 loss: 1.4292
env1_first_0:                 episode reward: -58.1500,                 loss: nan
env1_second_0:                 episode reward: 58.1500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 281.4,                last time consumption/overall running time: 258.1404s / 43825.2677 s
env0_first_0:                 episode reward: -49.6000,                 loss: 1.8868
env0_second_0:                 episode reward: 49.6000,                 loss: 1.4369
env1_first_0:                 episode reward: -51.0500,                 loss: nan
env1_second_0:                 episode reward: 51.0500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 278.25,                last time consumption/overall running time: 254.8332s / 44080.1009 s
env0_first_0:                 episode reward: -46.5000,                 loss: 1.7844
env0_second_0:                 episode reward: 46.5000,                 loss: 1.4111
env1_first_0:                 episode reward: -53.4500,                 loss: nan
env1_second_0:                 episode reward: 53.4500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 281.1,                last time consumption/overall running time: 258.8327s / 44338.9336 s
env0_first_0:                 episode reward: -54.9000,                 loss: 1.6508
env0_second_0:                 episode reward: 54.9000,                 loss: 1.3797
env1_first_0:                 episode reward: -86.4500,                 loss: nan
env1_second_0:                 episode reward: 86.4500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 276.0,                last time consumption/overall running time: 253.6987s / 44592.6322 s
env0_first_0:                 episode reward: -78.5000,                 loss: 1.5556
env0_second_0:                 episode reward: 78.5000,                 loss: 1.2555
env1_first_0:                 episode reward: -62.7500,                 loss: nan
env1_second_0:                 episode reward: 62.7500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 251.65,                last time consumption/overall running time: 231.0969s / 44823.7291 s
env0_first_0:                 episode reward: -80.4000,                 loss: 1.7061
env0_second_0:                 episode reward: 80.4000,                 loss: 1.2517
env1_first_0:                 episode reward: -75.1000,                 loss: nan
env1_second_0:                 episode reward: 75.1000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 317.25,                last time consumption/overall running time: 289.7378s / 45113.4670 s
env0_first_0:                 episode reward: -28.7500,                 loss: 1.6499
env0_second_0:                 episode reward: 28.7500,                 loss: 1.1614
env1_first_0:                 episode reward: -45.0000,                 loss: nan
env1_second_0:                 episode reward: 45.0000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 279.75,                last time consumption/overall running time: 255.5949s / 45369.0618 s
env0_first_0:                 episode reward: -29.6000,                 loss: 1.6024
env0_second_0:                 episode reward: 29.6000,                 loss: 1.1315
env1_first_0:                 episode reward: -39.0500,                 loss: nan
env1_second_0:                 episode reward: 39.0500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 359.6,                last time consumption/overall running time: 328.4030s / 45697.4649 s
env0_first_0:                 episode reward: -46.9500,                 loss: 1.5977
env0_second_0:                 episode reward: 46.9500,                 loss: 1.0439
env1_first_0:                 episode reward: -29.1500,                 loss: nan
env1_second_0:                 episode reward: 29.1500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 323.95,                last time consumption/overall running time: 296.0631s / 45993.5279 s
env0_first_0:                 episode reward: -22.6500,                 loss: 1.6258
env0_second_0:                 episode reward: 22.6500,                 loss: 1.0896
env1_first_0:                 episode reward: -52.4000,                 loss: nan
env1_second_0:                 episode reward: 52.4000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 345.75,                last time consumption/overall running time: 316.0513s / 46309.5792 s
env0_first_0:                 episode reward: -21.4000,                 loss: 1.6422
env0_second_0:                 episode reward: 21.4000,                 loss: 1.1792
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 319.25,                last time consumption/overall running time: 291.9117s / 46601.4909 s
env0_first_0:                 episode reward: -33.0500,                 loss: 1.6805
env0_second_0:                 episode reward: 33.0500,                 loss: 1.1961
env1_first_0:                 episode reward: -46.4500,                 loss: nan
env1_second_0:                 episode reward: 46.4500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 334.15,                last time consumption/overall running time: 305.4554s / 46906.9463 s
env0_first_0:                 episode reward: -53.1500,                 loss: 1.6479
env0_second_0:                 episode reward: 53.1500,                 loss: 1.1952
env1_first_0:                 episode reward: -51.0500,                 loss: nan
env1_second_0:                 episode reward: 51.0500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 375.55,                last time consumption/overall running time: 343.1299s / 47250.0762 s
env0_first_0:                 episode reward: -44.0000,                 loss: 1.6150
env0_second_0:                 episode reward: 44.0000,                 loss: 1.2209
env1_first_0:                 episode reward: -28.2000,                 loss: nan
env1_second_0:                 episode reward: 28.2000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 315.9,                last time consumption/overall running time: 288.9259s / 47539.0021 s
env0_first_0:                 episode reward: -31.4500,                 loss: 1.6363
env0_second_0:                 episode reward: 31.4500,                 loss: 1.2368
env1_first_0:                 episode reward: -46.8000,                 loss: nan
env1_second_0:                 episode reward: 46.8000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 360.5,                last time consumption/overall running time: 328.8654s / 47867.8675 s
env0_first_0:                 episode reward: -19.2000,                 loss: 1.6773
env0_second_0:                 episode reward: 19.2000,                 loss: 1.2645
env1_first_0:                 episode reward: -38.0000,                 loss: nan
env1_second_0:                 episode reward: 38.0000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 318.35,                last time consumption/overall running time: 291.5005s / 48159.3679 s
env0_first_0:                 episode reward: -38.5000,                 loss: 1.6328
env0_second_0:                 episode reward: 38.5000,                 loss: 1.2474
env1_first_0:                 episode reward: -63.3000,                 loss: nan
env1_second_0:                 episode reward: 63.3000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 279.8,                last time consumption/overall running time: 255.5154s / 48414.8833 s
env0_first_0:                 episode reward: -69.0500,                 loss: 1.6094
env0_second_0:                 episode reward: 69.0500,                 loss: 1.1950
env1_first_0:                 episode reward: -22.9500,                 loss: nan
env1_second_0:                 episode reward: 22.9500,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 301.45,                last time consumption/overall running time: 275.7351s / 48690.6184 s
env0_first_0:                 episode reward: -32.3000,                 loss: 1.6162
env0_second_0:                 episode reward: 32.3000,                 loss: 1.2225
env1_first_0:                 episode reward: -55.9000,                 loss: nan
env1_second_0:                 episode reward: 55.9000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 291.7,                last time consumption/overall running time: 267.3288s / 48957.9472 s
env0_first_0:                 episode reward: -57.2500,                 loss: 1.7110
env0_second_0:                 episode reward: 57.2500,                 loss: 1.2203
env1_first_0:                 episode reward: -30.7500,                 loss: nan
env1_second_0:                 episode reward: 30.7500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 299.15,                last time consumption/overall running time: 273.4026s / 49231.3498 s
env0_first_0:                 episode reward: -8.2500,                 loss: 1.7345
env0_second_0:                 episode reward: 8.2500,                 loss: 1.2349
env1_first_0:                 episode reward: -12.3000,                 loss: nan
env1_second_0:                 episode reward: 12.3000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 302.7,                last time consumption/overall running time: 276.9400s / 49508.2898 s
env0_first_0:                 episode reward: -21.4500,                 loss: 1.7719
env0_second_0:                 episode reward: 21.4500,                 loss: 1.2944
env1_first_0:                 episode reward: -19.2500,                 loss: nan
env1_second_0:                 episode reward: 19.2500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 310.7,                last time consumption/overall running time: 283.8866s / 49792.1764 s
env0_first_0:                 episode reward: -10.5500,                 loss: 1.8835
env0_second_0:                 episode reward: 10.5500,                 loss: 1.3846
env1_first_0:                 episode reward: -11.9000,                 loss: nan
env1_second_0:                 episode reward: 11.9000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 293.25,                last time consumption/overall running time: 268.7114s / 50060.8878 s
env0_first_0:                 episode reward: -32.8000,                 loss: 1.8705
env0_second_0:                 episode reward: 32.8000,                 loss: 1.4530
env1_first_0:                 episode reward: -49.1000,                 loss: nan
env1_second_0:                 episode reward: 49.1000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 339.35,                last time consumption/overall running time: 309.7144s / 50370.6022 s
env0_first_0:                 episode reward: -23.4000,                 loss: 1.7788
env0_second_0:                 episode reward: 23.4000,                 loss: 1.3791
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 544.15,                last time consumption/overall running time: 498.5203s / 50869.1225 s
env0_first_0:                 episode reward: 7.6500,                 loss: 1.7324
env0_second_0:                 episode reward: -7.6500,                 loss: 1.3791
env1_first_0:                 episode reward: -8.8000,                 loss: nan
env1_second_0:                 episode reward: 8.8000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 383.15,                last time consumption/overall running time: 349.3950s / 51218.5175 s
env0_first_0:                 episode reward: -36.1500,                 loss: 1.5359
env0_second_0:                 episode reward: 36.1500,                 loss: 1.3667
env1_first_0:                 episode reward: -24.4000,                 loss: nan
env1_second_0:                 episode reward: 24.4000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 367.5,                last time consumption/overall running time: 336.1221s / 51554.6396 s
env0_first_0:                 episode reward: -14.6000,                 loss: 1.5187
env0_second_0:                 episode reward: 14.6000,                 loss: 1.3250
env1_first_0:                 episode reward: -54.5000,                 loss: nan
env1_second_0:                 episode reward: 54.5000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 294.15,                last time consumption/overall running time: 269.2561s / 51823.8957 s
env0_first_0:                 episode reward: -33.0000,                 loss: 1.5427
env0_second_0:                 episode reward: 33.0000,                 loss: 1.1676
env1_first_0:                 episode reward: -35.4000,                 loss: nan
env1_second_0:                 episode reward: 35.4000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 321.7,                last time consumption/overall running time: 294.0204s / 52117.9160 s
env0_first_0:                 episode reward: -41.6000,                 loss: 1.6031
env0_second_0:                 episode reward: 41.6000,                 loss: 1.2108
env1_first_0:                 episode reward: -38.9500,                 loss: nan
env1_second_0:                 episode reward: 38.9500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 292.6,                last time consumption/overall running time: 267.4005s / 52385.3165 s
env0_first_0:                 episode reward: -58.2000,                 loss: 1.6122
env0_second_0:                 episode reward: 58.2000,                 loss: 1.2428
env1_first_0:                 episode reward: -41.4000,                 loss: nan
env1_second_0:                 episode reward: 41.4000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 335.45,                last time consumption/overall running time: 306.6753s / 52691.9918 s
env0_first_0:                 episode reward: -25.9000,                 loss: 1.7299
env0_second_0:                 episode reward: 25.9000,                 loss: 1.1825
env1_first_0:                 episode reward: -55.8500,                 loss: nan
env1_second_0:                 episode reward: 55.8500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 311.0,                last time consumption/overall running time: 283.6908s / 52975.6826 s
env0_first_0:                 episode reward: -43.5500,                 loss: 1.7356
env0_second_0:                 episode reward: 43.5500,                 loss: 1.2001
env1_first_0:                 episode reward: -57.5000,                 loss: nan
env1_second_0:                 episode reward: 57.5000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 315.05,                last time consumption/overall running time: 287.5344s / 53263.2170 s
env0_first_0:                 episode reward: -47.8500,                 loss: 1.7842
env0_second_0:                 episode reward: 47.8500,                 loss: 1.1889
env1_first_0:                 episode reward: -54.5000,                 loss: nan
env1_second_0:                 episode reward: 54.5000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 369.55,                last time consumption/overall running time: 337.3606s / 53600.5776 s
env0_first_0:                 episode reward: -16.9500,                 loss: 1.8178
env0_second_0:                 episode reward: 16.9500,                 loss: 1.2529
env1_first_0:                 episode reward: -40.7000,                 loss: nan
env1_second_0:                 episode reward: 40.7000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 311.5,                last time consumption/overall running time: 284.8099s / 53885.3875 s
env0_first_0:                 episode reward: -38.1500,                 loss: 1.8328
env0_second_0:                 episode reward: 38.1500,                 loss: 1.3953
env1_first_0:                 episode reward: -34.6500,                 loss: nan
env1_second_0:                 episode reward: 34.6500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 439.9,                last time consumption/overall running time: 400.5580s / 54285.9454 s
env0_first_0:                 episode reward: -50.3000,                 loss: 1.7614
env0_second_0:                 episode reward: 50.3000,                 loss: 1.4020
env1_first_0:                 episode reward: -47.2500,                 loss: nan
env1_second_0:                 episode reward: 47.2500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 584.85,                last time consumption/overall running time: 533.4594s / 54819.4049 s
env0_first_0:                 episode reward: -43.3000,                 loss: 1.5372
env0_second_0:                 episode reward: 43.3000,                 loss: 1.2879
env1_first_0:                 episode reward: -43.4000,                 loss: nan
env1_second_0:                 episode reward: 43.4000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 468.2,                last time consumption/overall running time: 428.6908s / 55248.0956 s
env0_first_0:                 episode reward: -5.4500,                 loss: 1.3661
env0_second_0:                 episode reward: 5.4500,                 loss: 1.0832
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 361.9,                last time consumption/overall running time: 331.0450s / 55579.1407 s
env0_first_0:                 episode reward: -56.8500,                 loss: 1.3288
env0_second_0:                 episode reward: 56.8500,                 loss: 1.0155
env1_first_0:                 episode reward: -28.5000,                 loss: nan
env1_second_0:                 episode reward: 28.5000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 738.4,                last time consumption/overall running time: 673.8961s / 56253.0368 s
env0_first_0:                 episode reward: -26.9000,                 loss: 1.1119
env0_second_0:                 episode reward: 26.9000,                 loss: 0.7643
env1_first_0:                 episode reward: -29.8000,                 loss: nan
env1_second_0:                 episode reward: 29.8000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 237.95,                last time consumption/overall running time: 217.4553s / 56470.4921 s
env0_first_0:                 episode reward: -66.5500,                 loss: 0.8673
env0_second_0:                 episode reward: 66.5500,                 loss: 0.6031
env1_first_0:                 episode reward: -91.2500,                 loss: nan
env1_second_0:                 episode reward: 91.2500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 270.7,                last time consumption/overall running time: 246.9915s / 56717.4836 s
env0_first_0:                 episode reward: -70.7000,                 loss: 0.9540
env0_second_0:                 episode reward: 70.7000,                 loss: 0.6529
env1_first_0:                 episode reward: -82.2000,                 loss: nan
env1_second_0:                 episode reward: 82.2000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 298.5,                last time consumption/overall running time: 272.1563s / 56989.6399 s
env0_first_0:                 episode reward: -65.3000,                 loss: 1.0789
env0_second_0:                 episode reward: 65.3000,                 loss: 0.7451
env1_first_0:                 episode reward: -63.1500,                 loss: nan
env1_second_0:                 episode reward: 63.1500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 338.15,                last time consumption/overall running time: 308.0728s / 57297.7126 s
env0_first_0:                 episode reward: -53.8000,                 loss: 1.2006
env0_second_0:                 episode reward: 53.8000,                 loss: 0.8504
env1_first_0:                 episode reward: -67.7000,                 loss: nan
env1_second_0:                 episode reward: 67.7000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 316.8,                last time consumption/overall running time: 289.2016s / 57586.9143 s
env0_first_0:                 episode reward: -41.1500,                 loss: 1.2643
env0_second_0:                 episode reward: 41.1500,                 loss: 0.9168
env1_first_0:                 episode reward: -36.3000,                 loss: nan
env1_second_0:                 episode reward: 36.3000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 384.3,                last time consumption/overall running time: 350.7314s / 57937.6457 s
env0_first_0:                 episode reward: -28.7000,                 loss: 1.3438
env0_second_0:                 episode reward: 28.7000,                 loss: 0.8350
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 301.6,                last time consumption/overall running time: 274.6577s / 58212.3034 s
env0_first_0:                 episode reward: -43.5500,                 loss: 1.4502
env0_second_0:                 episode reward: 43.5500,                 loss: 0.8773
env1_first_0:                 episode reward: -56.4000,                 loss: nan
env1_second_0:                 episode reward: 56.4000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 267.75,                last time consumption/overall running time: 244.6828s / 58456.9861 s
env0_first_0:                 episode reward: -78.4000,                 loss: 1.6412
env0_second_0:                 episode reward: 78.4000,                 loss: 0.9475
env1_first_0:                 episode reward: -82.9500,                 loss: nan
env1_second_0:                 episode reward: 82.9500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 243.6,                last time consumption/overall running time: 222.5538s / 58679.5399 s
env0_first_0:                 episode reward: -80.4000,                 loss: 1.6531
env0_second_0:                 episode reward: 80.4000,                 loss: 0.9614
env1_first_0:                 episode reward: -70.4500,                 loss: nan
env1_second_0:                 episode reward: 70.4500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 254.5,                last time consumption/overall running time: 232.0059s / 58911.5458 s
env0_first_0:                 episode reward: -77.3000,                 loss: 1.6859
env0_second_0:                 episode reward: 77.3000,                 loss: 0.9295
env1_first_0:                 episode reward: -83.1500,                 loss: nan
env1_second_0:                 episode reward: 83.1500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 259.45,                last time consumption/overall running time: 237.4403s / 59148.9861 s
env0_first_0:                 episode reward: -58.8500,                 loss: 1.6824
env0_second_0:                 episode reward: 58.8500,                 loss: 0.9285
env1_first_0:                 episode reward: -37.7500,                 loss: nan
env1_second_0:                 episode reward: 37.7500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 270.65,                last time consumption/overall running time: 247.0394s / 59396.0255 s
env0_first_0:                 episode reward: -71.1000,                 loss: 1.6888
env0_second_0:                 episode reward: 71.1000,                 loss: 0.9335
env1_first_0:                 episode reward: -68.2500,                 loss: nan
env1_second_0:                 episode reward: 68.2500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 273.2,                last time consumption/overall running time: 249.9341s / 59645.9596 s
env0_first_0:                 episode reward: -64.5500,                 loss: 1.6115
env0_second_0:                 episode reward: 64.5500,                 loss: 0.9288
env1_first_0:                 episode reward: -26.2000,                 loss: nan
env1_second_0:                 episode reward: 26.2000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 324.85,                last time consumption/overall running time: 296.2232s / 59942.1828 s
env0_first_0:                 episode reward: -15.6500,                 loss: 1.6117
env0_second_0:                 episode reward: 15.6500,                 loss: 0.9011
env1_first_0:                 episode reward: -31.9500,                 loss: nan
env1_second_0:                 episode reward: 31.9500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 317.35,                last time consumption/overall running time: 289.4879s / 60231.6708 s
env0_first_0:                 episode reward: -53.5000,                 loss: 1.5661
env0_second_0:                 episode reward: 53.5000,                 loss: 0.8848
env1_first_0:                 episode reward: -48.2000,                 loss: nan
env1_second_0:                 episode reward: 48.2000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 306.45,                last time consumption/overall running time: 279.7627s / 60511.4334 s
env0_first_0:                 episode reward: -40.7500,                 loss: 1.4882
env0_second_0:                 episode reward: 40.7500,                 loss: 0.8710
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 354.4,                last time consumption/overall running time: 323.0911s / 60834.5246 s
env0_first_0:                 episode reward: -35.6500,                 loss: 1.4920
env0_second_0:                 episode reward: 35.6500,                 loss: 0.8369
env1_first_0:                 episode reward: -60.8000,                 loss: nan
env1_second_0:                 episode reward: 60.8000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 292.35,                last time consumption/overall running time: 266.5290s / 61101.0536 s
env0_first_0:                 episode reward: -50.0500,                 loss: 1.5036
env0_second_0:                 episode reward: 50.0500,                 loss: 0.8719
env1_first_0:                 episode reward: -57.2500,                 loss: nan
env1_second_0:                 episode reward: 57.2500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 291.35,                last time consumption/overall running time: 265.7134s / 61366.7669 s
env0_first_0:                 episode reward: -51.1500,                 loss: 1.5314
env0_second_0:                 episode reward: 51.1500,                 loss: 0.9713
env1_first_0:                 episode reward: -26.7000,                 loss: nan
env1_second_0:                 episode reward: 26.7000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 292.95,                last time consumption/overall running time: 268.0105s / 61634.7775 s
env0_first_0:                 episode reward: -50.5000,                 loss: 1.6347
env0_second_0:                 episode reward: 50.5000,                 loss: 1.0620
env1_first_0:                 episode reward: -43.6500,                 loss: nan
env1_second_0:                 episode reward: 43.6500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 282.95,                last time consumption/overall running time: 257.9791s / 61892.7566 s
env0_first_0:                 episode reward: -67.8000,                 loss: 1.6892
env0_second_0:                 episode reward: 67.8000,                 loss: 1.0527
env1_first_0:                 episode reward: -70.9000,                 loss: nan
env1_second_0:                 episode reward: 70.9000,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 278.7,                last time consumption/overall running time: 253.7481s / 62146.5047 s
env0_first_0:                 episode reward: -59.1000,                 loss: 1.7536
env0_second_0:                 episode reward: 59.1000,                 loss: 1.0767
env1_first_0:                 episode reward: -43.2000,                 loss: nan
env1_second_0:                 episode reward: 43.2000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 327.4,                last time consumption/overall running time: 298.4794s / 62444.9841 s
env0_first_0:                 episode reward: -19.7500,                 loss: 1.6921
env0_second_0:                 episode reward: 19.7500,                 loss: 1.1088
env1_first_0:                 episode reward: -44.6000,                 loss: nan
env1_second_0:                 episode reward: 44.6000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 317.55,                last time consumption/overall running time: 289.7446s / 62734.7286 s
env0_first_0:                 episode reward: -58.2000,                 loss: 1.6539
env0_second_0:                 episode reward: 58.2000,                 loss: 1.1502
env1_first_0:                 episode reward: -37.8000,                 loss: nan
env1_second_0:                 episode reward: 37.8000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 284.55,                last time consumption/overall running time: 259.9429s / 62994.6716 s
env0_first_0:                 episode reward: -32.4000,                 loss: 1.6701
env0_second_0:                 episode reward: 32.4000,                 loss: 1.1750
env1_first_0:                 episode reward: -38.8000,                 loss: nan
env1_second_0:                 episode reward: 38.8000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 267.75,                last time consumption/overall running time: 244.4885s / 63239.1601 s
env0_first_0:                 episode reward: -60.1500,                 loss: 1.8499
env0_second_0:                 episode reward: 60.1500,                 loss: 1.2086
env1_first_0:                 episode reward: -54.7500,                 loss: nan
env1_second_0:                 episode reward: 54.7500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 276.05,                last time consumption/overall running time: 252.4369s / 63491.5970 s
env0_first_0:                 episode reward: -48.7000,                 loss: 1.8532
env0_second_0:                 episode reward: 48.7000,                 loss: 1.1630
env1_first_0:                 episode reward: -70.4500,                 loss: nan
env1_second_0:                 episode reward: 70.4500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 252.0,                last time consumption/overall running time: 230.5396s / 63722.1366 s
env0_first_0:                 episode reward: -61.4500,                 loss: 1.7511
env0_second_0:                 episode reward: 61.4500,                 loss: 1.0906
env1_first_0:                 episode reward: -48.0500,                 loss: nan
env1_second_0:                 episode reward: 48.0500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 266.35,                last time consumption/overall running time: 243.7177s / 63965.8543 s
env0_first_0:                 episode reward: -74.1500,                 loss: 1.6877
env0_second_0:                 episode reward: 74.1500,                 loss: 1.0356
env1_first_0:                 episode reward: -67.4000,                 loss: nan
env1_second_0:                 episode reward: 67.4000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 270.95,                last time consumption/overall running time: 247.3904s / 64213.2447 s
env0_first_0:                 episode reward: -59.8000,                 loss: 1.6165
env0_second_0:                 episode reward: 59.8000,                 loss: 0.9929
env1_first_0:                 episode reward: -57.2500,                 loss: nan
env1_second_0:                 episode reward: 57.2500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 276.3,                last time consumption/overall running time: 252.8950s / 64466.1397 s
env0_first_0:                 episode reward: -23.7500,                 loss: 1.5505
env0_second_0:                 episode reward: 23.7500,                 loss: 1.0068
env1_first_0:                 episode reward: -40.7000,                 loss: nan
env1_second_0:                 episode reward: 40.7000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 271.0,                last time consumption/overall running time: 248.2003s / 64714.3400 s
env0_first_0:                 episode reward: -69.1000,                 loss: 1.5910
env0_second_0:                 episode reward: 69.1000,                 loss: 1.0016
env1_first_0:                 episode reward: -76.9500,                 loss: nan
env1_second_0:                 episode reward: 76.9500,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 307.8,                last time consumption/overall running time: 281.3320s / 64995.6720 s
env0_first_0:                 episode reward: -45.8000,                 loss: 1.6339
env0_second_0:                 episode reward: 45.8000,                 loss: 1.0434
env1_first_0:                 episode reward: -60.9000,                 loss: nan
env1_second_0:                 episode reward: 60.9000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 286.45,                last time consumption/overall running time: 262.2204s / 65257.8924 s
env0_first_0:                 episode reward: -35.8000,                 loss: 1.7275
env0_second_0:                 episode reward: 35.8000,                 loss: 1.1008
env1_first_0:                 episode reward: -13.4500,                 loss: nan
env1_second_0:                 episode reward: 13.4500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 312.1,                last time consumption/overall running time: 284.1477s / 65542.0400 s
env0_first_0:                 episode reward: -28.0500,                 loss: 1.7466
env0_second_0:                 episode reward: 28.0500,                 loss: 1.1106
env1_first_0:                 episode reward: -45.7000,                 loss: nan
env1_second_0:                 episode reward: 45.7000,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 357.6,                last time consumption/overall running time: 326.8326s / 65868.8727 s
env0_first_0:                 episode reward: -8.7000,                 loss: 1.8016
env0_second_0:                 episode reward: 8.7000,                 loss: 1.2377
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 336.4,                last time consumption/overall running time: 306.8986s / 66175.7713 s
env0_first_0:                 episode reward: -40.6000,                 loss: 1.9196
env0_second_0:                 episode reward: 40.6000,                 loss: 1.3474
env1_first_0:                 episode reward: -18.8000,                 loss: nan
env1_second_0:                 episode reward: 18.8000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 294.8,                last time consumption/overall running time: 269.2606s / 66445.0318 s
env0_first_0:                 episode reward: -47.7500,                 loss: 1.9978
env0_second_0:                 episode reward: 47.7500,                 loss: 1.3465
env1_first_0:                 episode reward: -26.8500,                 loss: nan
env1_second_0:                 episode reward: 26.8500,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 294.85,                last time consumption/overall running time: 268.5098s / 66713.5416 s
env0_first_0:                 episode reward: -53.4000,                 loss: 2.0053
env0_second_0:                 episode reward: 53.4000,                 loss: 1.3310
env1_first_0:                 episode reward: -52.8500,                 loss: nan
env1_second_0:                 episode reward: 52.8500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 293.9,                last time consumption/overall running time: 267.9996s / 66981.5412 s
env0_first_0:                 episode reward: -49.8500,                 loss: 1.9934
env0_second_0:                 episode reward: 49.8500,                 loss: 1.4250
env1_first_0:                 episode reward: -32.8000,                 loss: nan
env1_second_0:                 episode reward: 32.8000,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 450.35,                last time consumption/overall running time: 410.2656s / 67391.8068 s
env0_first_0:                 episode reward: -40.3000,                 loss: 1.8954
env0_second_0:                 episode reward: 40.3000,                 loss: 1.4033
env1_first_0:                 episode reward: -66.8000,                 loss: nan
env1_second_0:                 episode reward: 66.8000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 363.95,                last time consumption/overall running time: 332.1073s / 67723.9141 s
env0_first_0:                 episode reward: -57.0500,                 loss: 1.7946
env0_second_0:                 episode reward: 57.0500,                 loss: 1.3013
env1_first_0:                 episode reward: -53.1000,                 loss: nan
env1_second_0:                 episode reward: 53.1000,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 331.45,                last time consumption/overall running time: 302.3152s / 68026.2293 s
env0_first_0:                 episode reward: -76.8500,                 loss: 1.7067
env0_second_0:                 episode reward: 76.8500,                 loss: 1.2228
env1_first_0:                 episode reward: -63.1000,                 loss: nan
env1_second_0:                 episode reward: 63.1000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 259.6,                last time consumption/overall running time: 237.4118s / 68263.6411 s
env0_first_0:                 episode reward: -55.5000,                 loss: 1.5477
env0_second_0:                 episode reward: 55.5000,                 loss: 1.0810
env1_first_0:                 episode reward: -88.0000,                 loss: nan
env1_second_0:                 episode reward: 88.0000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 266.35,                last time consumption/overall running time: 243.6342s / 68507.2752 s
env0_first_0:                 episode reward: -45.0000,                 loss: 1.4673
env0_second_0:                 episode reward: 45.0000,                 loss: 1.0155
env1_first_0:                 episode reward: -39.0500,                 loss: nan
env1_second_0:                 episode reward: 39.0500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 266.7,                last time consumption/overall running time: 244.1139s / 68751.3892 s
env0_first_0:                 episode reward: -60.3000,                 loss: 1.3903
env0_second_0:                 episode reward: 60.3000,                 loss: 0.9823
env1_first_0:                 episode reward: -56.1000,                 loss: nan
env1_second_0:                 episode reward: 56.1000,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 243.85,                last time consumption/overall running time: 222.9079s / 68974.2971 s
env0_first_0:                 episode reward: -65.1500,                 loss: 1.3901
env0_second_0:                 episode reward: 65.1500,                 loss: 0.9944
env1_first_0:                 episode reward: -64.9500,                 loss: nan
env1_second_0:                 episode reward: 64.9500,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 249.7,                last time consumption/overall running time: 227.8869s / 69202.1840 s
env0_first_0:                 episode reward: -53.8000,                 loss: 1.4259
env0_second_0:                 episode reward: 53.8000,                 loss: 0.9404
env1_first_0:                 episode reward: -57.5500,                 loss: nan
env1_second_0:                 episode reward: 57.5500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 284.4,                last time consumption/overall running time: 259.4363s / 69461.6203 s
env0_first_0:                 episode reward: -80.9000,                 loss: 1.3030
env0_second_0:                 episode reward: 80.9000,                 loss: 0.9477
env1_first_0:                 episode reward: -66.1000,                 loss: nan
env1_second_0:                 episode reward: 66.1000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 276.8,                last time consumption/overall running time: 251.8056s / 69713.4259 s
env0_first_0:                 episode reward: -73.7500,                 loss: 1.3402
env0_second_0:                 episode reward: 73.7500,                 loss: 0.9853
env1_first_0:                 episode reward: -67.9000,                 loss: nan
env1_second_0:                 episode reward: 67.9000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 259.3,                last time consumption/overall running time: 236.1264s / 69949.5522 s
env0_first_0:                 episode reward: -46.8000,                 loss: 1.4333
env0_second_0:                 episode reward: 46.8000,                 loss: 1.0377
env1_first_0:                 episode reward: -47.3000,                 loss: nan
env1_second_0:                 episode reward: 47.3000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 280.75,                last time consumption/overall running time: 256.2417s / 70205.7939 s
env0_first_0:                 episode reward: -20.5500,                 loss: 1.4312
env0_second_0:                 episode reward: 20.5500,                 loss: 1.0508