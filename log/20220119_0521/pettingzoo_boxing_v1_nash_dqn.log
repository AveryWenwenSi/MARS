pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0521/pettingzoo_boxing_v1_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0521/pettingzoo_boxing_v1_nash_dqn.
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 43.4883s / 43.4883 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0056
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0056
env1_first_0:                 episode reward: -13.0000,                 loss: nan
env1_second_0:                 episode reward: 13.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 1558.1818s / 1601.6701 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0175
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0169
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 1953.9673s / 3555.6374 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0178
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0179
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2066.7230s / 5622.3603 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0182
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0180
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2103.7477s / 7726.1080 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0157
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0160
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2098.1921s / 9824.3001 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0150
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0152
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2102.5478s / 11926.8479 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0163
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0166
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2117.6597s / 14044.5076 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0161
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0160
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2110.4252s / 16154.9329 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0160
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0164
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2100.2476s / 18255.1805 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0178
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0179
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2111.4073s / 20366.5878 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0166
env0_second_0:                 episode reward: 1.9500,                 loss: 0.0163
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2101.4292s / 22468.0170 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0140
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0145
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2100.5032s / 24568.5202 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0153
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0155
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2089.0678s / 26657.5880 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0145
env0_second_0:                 episode reward: 1.6500,                 loss: 0.0150
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2097.2665s / 28754.8545 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.0160
env0_second_0:                 episode reward: 3.9000,                 loss: 0.0159
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2099.2551s / 30854.1096 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0181
env0_second_0:                 episode reward: 2.2000,                 loss: 0.0185
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2104.9084s / 32959.0180 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0201
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0194
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2101.3930s / 35060.4110 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0191
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0186
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2102.0210s / 37162.4320 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0172
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0177
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2425.9119s / 39588.3439 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0172
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0173
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2462.6242s / 42050.9682 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0197
env0_second_0:                 episode reward: 2.5000,                 loss: 0.0187
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2468.3536s / 44519.3218 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0196
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0189
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2445.8250s / 46965.1468 s
env0_first_0:                 episode reward: -4.6500,                 loss: 0.0190
env0_second_0:                 episode reward: 4.6500,                 loss: 0.0187
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2438.6540s / 49403.8009 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0173
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0168
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2440.8089s / 51844.6097 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0179
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0174
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2436.2443s / 54280.8540 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0195
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0178
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2449.7406s / 56730.5947 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0192
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0182
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2456.7939s / 59187.3886 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.0198
env0_second_0:                 episode reward: 3.9000,                 loss: 0.0200
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2441.4561s / 61628.8447 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0190
env0_second_0:                 episode reward: 2.5500,                 loss: 0.0187
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2449.0226s / 64077.8673 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0206
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0195
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2455.8460s / 66533.7133 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0207
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0207
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2449.2513s / 68982.9646 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0227
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0216
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2442.9743s / 71425.9389 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0273
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0247
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2445.3184s / 73871.2573 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0285
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0270
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2439.7074s / 76310.9647 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0273
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0246
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2455.2306s / 78766.1953 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0345
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0285
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2451.9642s / 81218.1595 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0449
env0_second_0:                 episode reward: 7.7000,                 loss: 0.0369
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2448.1335s / 83666.2930 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0500
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0418
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2451.0295s / 86117.3225 s
env0_first_0:                 episode reward: -7.4500,                 loss: 0.0435
env0_second_0:                 episode reward: 7.4500,                 loss: 0.0379
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2450.1106s / 88567.4331 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0420
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0370
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2445.8349s / 91013.2680 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0471
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0445
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2439.0153s / 93452.2833 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0466
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0450
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2442.2221s / 95894.5054 s
env0_first_0:                 episode reward: -8.5000,                 loss: 0.0448
env0_second_0:                 episode reward: 8.5000,                 loss: 0.0447
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2420.8180s / 98315.3234 s
env0_first_0:                 episode reward: -8.7500,                 loss: 0.0466
env0_second_0:                 episode reward: 8.7500,                 loss: 0.0423
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2413.4675s / 100728.7909 s
env0_first_0:                 episode reward: -8.7500,                 loss: 0.0616
env0_second_0:                 episode reward: 8.7500,                 loss: 0.0531
env1_first_0:                 episode reward: -7.9500,                 loss: nan
env1_second_0:                 episode reward: 7.9500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2405.6622s / 103134.4531 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.0557
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0552
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2411.2469s / 105545.6999 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0470
env0_second_0:                 episode reward: 4.4500,                 loss: 0.0458
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2412.6546s / 107958.3545 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0542
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0476
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2406.3972s / 110364.7517 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0428
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0453
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2407.7048s / 112772.4565 s
env0_first_0:                 episode reward: -10.9000,                 loss: 0.0518
env0_second_0:                 episode reward: 10.9000,                 loss: 0.0479
env1_first_0:                 episode reward: -10.6000,                 loss: nan
env1_second_0:                 episode reward: 10.6000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2408.1066s / 115180.5631 s
env0_first_0:                 episode reward: -10.6500,                 loss: 0.0642
env0_second_0:                 episode reward: 10.6500,                 loss: 0.0571
env1_first_0:                 episode reward: -8.3000,                 loss: nan
env1_second_0:                 episode reward: 8.3000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2413.1882s / 117593.7512 s
env0_first_0:                 episode reward: -8.9000,                 loss: 0.0757
env0_second_0:                 episode reward: 8.9000,                 loss: 0.0676
env1_first_0:                 episode reward: -10.8500,                 loss: nan
env1_second_0:                 episode reward: 10.8500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2412.3702s / 120006.1215 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0729
env0_second_0:                 episode reward: 2.1000,                 loss: 0.0644
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2416.8817s / 122423.0032 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0675
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0576
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2418.0195s / 124841.0228 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0610
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0677
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2403.7901s / 127244.8128 s
env0_first_0:                 episode reward: -4.6500,                 loss: 0.0666
env0_second_0:                 episode reward: 4.6500,                 loss: 0.0669
env1_first_0:                 episode reward: -12.9000,                 loss: nan
env1_second_0:                 episode reward: 12.9000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1783.45,                last time consumption/overall running time: 2412.2552s / 129657.0680 s
env0_first_0:                 episode reward: -20.4000,                 loss: 0.0781
env0_second_0:                 episode reward: 20.4000,                 loss: 0.0700
env1_first_0:                 episode reward: -18.0500,                 loss: nan
env1_second_0:                 episode reward: 18.0500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1782.85,                last time consumption/overall running time: 2410.1322s / 132067.2002 s
env0_first_0:                 episode reward: -11.1000,                 loss: 0.0840
env0_second_0:                 episode reward: 11.1000,                 loss: 0.0875
env1_first_0:                 episode reward: -11.0500,                 loss: nan
env1_second_0:                 episode reward: 11.0500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2415.2645s / 134482.4647 s
env0_first_0:                 episode reward: -13.3500,                 loss: 0.0762
env0_second_0:                 episode reward: 13.3500,                 loss: 0.0811
env1_first_0:                 episode reward: -13.9500,                 loss: nan
env1_second_0:                 episode reward: 13.9500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2426.2237s / 136908.6884 s
env0_first_0:                 episode reward: -9.1500,                 loss: 0.0700
env0_second_0:                 episode reward: 9.1500,                 loss: 0.0827
env1_first_0:                 episode reward: -14.0500,                 loss: nan
env1_second_0:                 episode reward: 14.0500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2421.7592s / 139330.4476 s
env0_first_0:                 episode reward: -17.4000,                 loss: 0.0845
env0_second_0:                 episode reward: 17.4000,                 loss: 0.0926
env1_first_0:                 episode reward: -16.5500,                 loss: nan
env1_second_0:                 episode reward: 16.5500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2418.9771s / 141749.4247 s
env0_first_0:                 episode reward: -20.1000,                 loss: 0.0859
env0_second_0:                 episode reward: 20.1000,                 loss: 0.0820
env1_first_0:                 episode reward: -13.0500,                 loss: nan
env1_second_0:                 episode reward: 13.0500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1778.8,                last time consumption/overall running time: 2411.8213s / 144161.2460 s
env0_first_0:                 episode reward: -18.9500,                 loss: 0.0746
env0_second_0:                 episode reward: 18.9500,                 loss: 0.0722
env1_first_0:                 episode reward: -14.6500,                 loss: nan
env1_second_0:                 episode reward: 14.6500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1757.55,                last time consumption/overall running time: 2342.3140s / 146503.5600 s
env0_first_0:                 episode reward: -21.7000,                 loss: 0.0858
env0_second_0:                 episode reward: 21.7000,                 loss: 0.0783
env1_first_0:                 episode reward: -26.6500,                 loss: nan
env1_second_0:                 episode reward: 26.6500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1713.9,                last time consumption/overall running time: 2272.3444s / 148775.9044 s
env0_first_0:                 episode reward: -28.6500,                 loss: 0.1097
env0_second_0:                 episode reward: 28.6500,                 loss: 0.0981
env1_first_0:                 episode reward: -24.3000,                 loss: nan
env1_second_0:                 episode reward: 24.3000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1704.75,                last time consumption/overall running time: 2251.5663s / 151027.4707 s
env0_first_0:                 episode reward: -26.4000,                 loss: 0.1259
env0_second_0:                 episode reward: 26.4000,                 loss: 0.1003
env1_first_0:                 episode reward: -25.7500,                 loss: nan
env1_second_0:                 episode reward: 25.7500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1721.9,                last time consumption/overall running time: 2278.9516s / 153306.4223 s
env0_first_0:                 episode reward: -22.4000,                 loss: 0.1169
env0_second_0:                 episode reward: 22.4000,                 loss: 0.1115
env1_first_0:                 episode reward: -26.6500,                 loss: nan
env1_second_0:                 episode reward: 26.6500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1616.85,                last time consumption/overall running time: 2137.2557s / 155443.6781 s
env0_first_0:                 episode reward: -29.8500,                 loss: 0.1428
env0_second_0:                 episode reward: 29.8500,                 loss: 0.1295
env1_first_0:                 episode reward: -39.9000,                 loss: nan
env1_second_0:                 episode reward: 39.9000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1464.65,                last time consumption/overall running time: 1934.2253s / 157377.9033 s
env0_first_0:                 episode reward: -44.0000,                 loss: 0.1769
env0_second_0:                 episode reward: 44.0000,                 loss: 0.1645
env1_first_0:                 episode reward: -35.2000,                 loss: nan
env1_second_0:                 episode reward: 35.2000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1265.05,                last time consumption/overall running time: 1674.8724s / 159052.7757 s
env0_first_0:                 episode reward: -46.3500,                 loss: 0.2044
env0_second_0:                 episode reward: 46.3500,                 loss: 0.1670
env1_first_0:                 episode reward: -40.8500,                 loss: nan
env1_second_0:                 episode reward: 40.8500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1141.6,                last time consumption/overall running time: 1503.6871s / 160556.4629 s
env0_first_0:                 episode reward: -52.7500,                 loss: 0.2059
env0_second_0:                 episode reward: 52.7500,                 loss: 0.1756
env1_first_0:                 episode reward: -48.4500,                 loss: nan
env1_second_0:                 episode reward: 48.4500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 906.25,                last time consumption/overall running time: 1185.4364s / 161741.8992 s
env0_first_0:                 episode reward: -49.8500,                 loss: 0.2361
env0_second_0:                 episode reward: 49.8500,                 loss: 0.2042
env1_first_0:                 episode reward: -59.1500,                 loss: nan
env1_second_0:                 episode reward: 59.1500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 1086.95,                last time consumption/overall running time: 1420.5174s / 163162.4167 s
env0_first_0:                 episode reward: -49.5000,                 loss: 0.2707
env0_second_0:                 episode reward: 49.5000,                 loss: 0.2469
env1_first_0:                 episode reward: -50.2000,                 loss: nan
env1_second_0:                 episode reward: 50.2000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 960.65,                last time consumption/overall running time: 1258.8063s / 164421.2229 s
env0_first_0:                 episode reward: -42.1000,                 loss: 0.3397
env0_second_0:                 episode reward: 42.1000,                 loss: 0.2621
env1_first_0:                 episode reward: -60.7000,                 loss: nan
env1_second_0:                 episode reward: 60.7000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 833.35,                last time consumption/overall running time: 1083.7953s / 165505.0183 s
env0_first_0:                 episode reward: -66.8000,                 loss: 0.3537
env0_second_0:                 episode reward: 66.8000,                 loss: 0.2917
env1_first_0:                 episode reward: -51.2000,                 loss: nan
env1_second_0:                 episode reward: 51.2000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 704.7,                last time consumption/overall running time: 927.0093s / 166432.0276 s
env0_first_0:                 episode reward: -69.8000,                 loss: 0.3796
env0_second_0:                 episode reward: 69.8000,                 loss: 0.2962
env1_first_0:                 episode reward: -52.8000,                 loss: nan
env1_second_0:                 episode reward: 52.8000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 767.65,                last time consumption/overall running time: 1004.6692s / 167436.6968 s
env0_first_0:                 episode reward: -59.0500,                 loss: 0.3799
env0_second_0:                 episode reward: 59.0500,                 loss: 0.2890
env1_first_0:                 episode reward: -47.5000,                 loss: nan
env1_second_0:                 episode reward: 47.5000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 642.05,                last time consumption/overall running time: 840.2134s / 168276.9102 s
env0_first_0:                 episode reward: -41.1000,                 loss: 0.4208
env0_second_0:                 episode reward: 41.1000,                 loss: 0.3232
env1_first_0:                 episode reward: -67.7500,                 loss: nan
env1_second_0:                 episode reward: 67.7500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 664.45,                last time consumption/overall running time: 860.3474s / 169137.2576 s
env0_first_0:                 episode reward: -59.7500,                 loss: 0.4432
env0_second_0:                 episode reward: 59.7500,                 loss: 0.3344
env1_first_0:                 episode reward: -63.0000,                 loss: nan
env1_second_0:                 episode reward: 63.0000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 523.45,                last time consumption/overall running time: 683.7015s / 169820.9591 s
env0_first_0:                 episode reward: -64.5000,                 loss: 0.5051
env0_second_0:                 episode reward: 64.5000,                 loss: 0.3901
env1_first_0:                 episode reward: -66.8000,                 loss: nan
env1_second_0:                 episode reward: 66.8000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 624.75,                last time consumption/overall running time: 813.0377s / 170633.9968 s
env0_first_0:                 episode reward: -65.8000,                 loss: 0.5717
env0_second_0:                 episode reward: 65.8000,                 loss: 0.4548
env1_first_0:                 episode reward: -59.8500,                 loss: nan
env1_second_0:                 episode reward: 59.8500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 584.85,                last time consumption/overall running time: 763.0599s / 171397.0567 s
env0_first_0:                 episode reward: -60.0000,                 loss: 0.5976
env0_second_0:                 episode reward: 60.0000,                 loss: 0.4876
env1_first_0:                 episode reward: -66.1500,                 loss: nan
env1_second_0:                 episode reward: 66.1500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 530.25,                last time consumption/overall running time: 689.2730s / 172086.3297 s
env0_first_0:                 episode reward: -51.0000,                 loss: 0.6191
env0_second_0:                 episode reward: 51.0000,                 loss: 0.5360
env1_first_0:                 episode reward: -67.9000,                 loss: nan
env1_second_0:                 episode reward: 67.9000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 537.85,                last time consumption/overall running time: 699.7719s / 172786.1017 s
env0_first_0:                 episode reward: -60.4500,                 loss: 0.6233
env0_second_0:                 episode reward: 60.4500,                 loss: 0.5682
env1_first_0:                 episode reward: -63.2500,                 loss: nan
env1_second_0:                 episode reward: 63.2500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 567.05,                last time consumption/overall running time: 737.1385s / 173523.2401 s
env0_first_0:                 episode reward: -52.8500,                 loss: 0.6988
env0_second_0:                 episode reward: 52.8500,                 loss: 0.5330
env1_first_0:                 episode reward: -65.1500,                 loss: nan
env1_second_0:                 episode reward: 65.1500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 503.3,                last time consumption/overall running time: 659.0368s / 174182.2769 s
env0_first_0:                 episode reward: -59.3500,                 loss: 0.6706
env0_second_0:                 episode reward: 59.3500,                 loss: 0.5751
env1_first_0:                 episode reward: -72.7000,                 loss: nan
env1_second_0:                 episode reward: 72.7000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 530.55,                last time consumption/overall running time: 690.3467s / 174872.6236 s
env0_first_0:                 episode reward: -60.7000,                 loss: 0.6461
env0_second_0:                 episode reward: 60.7000,                 loss: 0.6032
env1_first_0:                 episode reward: -62.2500,                 loss: nan
env1_second_0:                 episode reward: 62.2500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 537.75,                last time consumption/overall running time: 705.7052s / 175578.3288 s
env0_first_0:                 episode reward: -53.7500,                 loss: 0.6529
env0_second_0:                 episode reward: 53.7500,                 loss: 0.6323
env1_first_0:                 episode reward: -67.4000,                 loss: nan
env1_second_0:                 episode reward: 67.4000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 452.05,                last time consumption/overall running time: 592.8030s / 176171.1318 s
env0_first_0:                 episode reward: -56.8500,                 loss: 0.6712
env0_second_0:                 episode reward: 56.8500,                 loss: 0.6585
env1_first_0:                 episode reward: -64.4500,                 loss: nan
env1_second_0:                 episode reward: 64.4500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 406.3,                last time consumption/overall running time: 530.4816s / 176701.6134 s
env0_first_0:                 episode reward: -64.9500,                 loss: 0.7000
env0_second_0:                 episode reward: 64.9500,                 loss: 0.7170
env1_first_0:                 episode reward: -70.0000,                 loss: nan
env1_second_0:                 episode reward: 70.0000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 480.7,                last time consumption/overall running time: 627.9838s / 177329.5972 s
env0_first_0:                 episode reward: -57.0000,                 loss: 0.7263
env0_second_0:                 episode reward: 57.0000,                 loss: 0.6892
env1_first_0:                 episode reward: -59.5500,                 loss: nan
env1_second_0:                 episode reward: 59.5500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 467.2,                last time consumption/overall running time: 609.2251s / 177938.8224 s
env0_first_0:                 episode reward: -73.4000,                 loss: 0.7155
env0_second_0:                 episode reward: 73.4000,                 loss: 0.7554
env1_first_0:                 episode reward: -67.9500,                 loss: nan
env1_second_0:                 episode reward: 67.9500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 422.1,                last time consumption/overall running time: 546.1498s / 178484.9722 s
env0_first_0:                 episode reward: -66.6500,                 loss: 0.7230
env0_second_0:                 episode reward: 66.6500,                 loss: 0.7524
env1_first_0:                 episode reward: -71.3500,                 loss: nan
env1_second_0:                 episode reward: 71.3500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 474.25,                last time consumption/overall running time: 611.9487s / 179096.9209 s
env0_first_0:                 episode reward: -72.6500,                 loss: 0.8278
env0_second_0:                 episode reward: 72.6500,                 loss: 0.7683
env1_first_0:                 episode reward: -68.3500,                 loss: nan
env1_second_0:                 episode reward: 68.3500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 476.25,                last time consumption/overall running time: 619.3912s / 179716.3120 s
env0_first_0:                 episode reward: -69.5500,                 loss: 0.8276
env0_second_0:                 episode reward: 69.5500,                 loss: 0.7760
env1_first_0:                 episode reward: -62.9500,                 loss: nan
env1_second_0:                 episode reward: 62.9500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 503.0,                last time consumption/overall running time: 653.4358s / 180369.7478 s
env0_first_0:                 episode reward: -66.4000,                 loss: 0.8244
env0_second_0:                 episode reward: 66.4000,                 loss: 0.7737
env1_first_0:                 episode reward: -58.1500,                 loss: nan
env1_second_0:                 episode reward: 58.1500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 528.0,                last time consumption/overall running time: 685.3223s / 181055.0701 s
env0_first_0:                 episode reward: -65.4500,                 loss: 0.8011
env0_second_0:                 episode reward: 65.4500,                 loss: 0.7458
env1_first_0:                 episode reward: -59.0500,                 loss: nan
env1_second_0:                 episode reward: 59.0500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 464.7,                last time consumption/overall running time: 596.9917s / 181652.0619 s
env0_first_0:                 episode reward: -71.8500,                 loss: 0.8381
env0_second_0:                 episode reward: 71.8500,                 loss: 0.7710
env1_first_0:                 episode reward: -54.5500,                 loss: nan
env1_second_0:                 episode reward: 54.5500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 387.55,                last time consumption/overall running time: 497.9601s / 182150.0220 s
env0_first_0:                 episode reward: -50.5500,                 loss: 0.8542
env0_second_0:                 episode reward: 50.5500,                 loss: 0.7771
env1_first_0:                 episode reward: -71.4000,                 loss: nan
env1_second_0:                 episode reward: 71.4000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 493.6,                last time consumption/overall running time: 640.9523s / 182790.9744 s
env0_first_0:                 episode reward: -77.6000,                 loss: 0.8693
env0_second_0:                 episode reward: 77.6000,                 loss: 0.7445
env1_first_0:                 episode reward: -52.1000,                 loss: nan
env1_second_0:                 episode reward: 52.1000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 415.0,                last time consumption/overall running time: 530.8331s / 183321.8075 s
env0_first_0:                 episode reward: -71.1500,                 loss: 0.8553
env0_second_0:                 episode reward: 71.1500,                 loss: 0.7574
env1_first_0:                 episode reward: -56.3500,                 loss: nan
env1_second_0:                 episode reward: 56.3500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 402.75,                last time consumption/overall running time: 521.9456s / 183843.7530 s
env0_first_0:                 episode reward: -51.7000,                 loss: 0.9066
env0_second_0:                 episode reward: 51.7000,                 loss: 0.7864
env1_first_0:                 episode reward: -68.8000,                 loss: nan
env1_second_0:                 episode reward: 68.8000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 510.7,                last time consumption/overall running time: 659.0866s / 184502.8397 s
env0_first_0:                 episode reward: -56.3500,                 loss: 1.0288
env0_second_0:                 episode reward: 56.3500,                 loss: 0.8484
env1_first_0:                 episode reward: -64.1500,                 loss: nan
env1_second_0:                 episode reward: 64.1500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 451.55,                last time consumption/overall running time: 576.8619s / 185079.7015 s
env0_first_0:                 episode reward: -65.0000,                 loss: 1.0999
env0_second_0:                 episode reward: 65.0000,                 loss: 0.8824
env1_first_0:                 episode reward: -65.7000,                 loss: nan
env1_second_0:                 episode reward: 65.7000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 537.3,                last time consumption/overall running time: 696.7978s / 185776.4994 s
env0_first_0:                 episode reward: -59.7500,                 loss: 1.1153
env0_second_0:                 episode reward: 59.7500,                 loss: 0.8849
env1_first_0:                 episode reward: -65.3000,                 loss: nan
env1_second_0:                 episode reward: 65.3000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 487.65,                last time consumption/overall running time: 622.9831s / 186399.4825 s
env0_first_0:                 episode reward: -63.7500,                 loss: 1.0517
env0_second_0:                 episode reward: 63.7500,                 loss: 0.8319
env1_first_0:                 episode reward: -45.7500,                 loss: nan
env1_second_0:                 episode reward: 45.7500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 645.55,                last time consumption/overall running time: 825.3888s / 187224.8713 s
env0_first_0:                 episode reward: -51.9000,                 loss: 1.0494
env0_second_0:                 episode reward: 51.9000,                 loss: 0.7760
env1_first_0:                 episode reward: -55.2500,                 loss: nan
env1_second_0:                 episode reward: 55.2500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 426.65,                last time consumption/overall running time: 547.9260s / 187772.7973 s
env0_first_0:                 episode reward: -79.4500,                 loss: 1.0055
env0_second_0:                 episode reward: 79.4500,                 loss: 0.7846
env1_first_0:                 episode reward: -55.7000,                 loss: nan
env1_second_0:                 episode reward: 55.7000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 515.7,                last time consumption/overall running time: 660.0925s / 188432.8898 s
env0_first_0:                 episode reward: -76.5000,                 loss: 0.9080
env0_second_0:                 episode reward: 76.5000,                 loss: 0.7278
env1_first_0:                 episode reward: -40.3000,                 loss: nan
env1_second_0:                 episode reward: 40.3000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 577.65,                last time consumption/overall running time: 735.5887s / 189168.4785 s
env0_first_0:                 episode reward: -46.9500,                 loss: 0.9175
env0_second_0:                 episode reward: 46.9500,                 loss: 0.7494
env1_first_0:                 episode reward: -66.0500,                 loss: nan
env1_second_0:                 episode reward: 66.0500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 557.6,                last time consumption/overall running time: 712.8958s / 189881.3742 s
env0_first_0:                 episode reward: -47.2500,                 loss: 0.9533
env0_second_0:                 episode reward: 47.2500,                 loss: 0.7575
env1_first_0:                 episode reward: -62.6500,                 loss: nan
env1_second_0:                 episode reward: 62.6500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 562.95,                last time consumption/overall running time: 720.6727s / 190602.0470 s
env0_first_0:                 episode reward: -51.4000,                 loss: 1.0017
env0_second_0:                 episode reward: 51.4000,                 loss: 0.7869
env1_first_0:                 episode reward: -70.8000,                 loss: nan
env1_second_0:                 episode reward: 70.8000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 500.55,                last time consumption/overall running time: 638.1305s / 191240.1775 s
env0_first_0:                 episode reward: -57.7500,                 loss: 1.0718
env0_second_0:                 episode reward: 57.7500,                 loss: 0.8292
env1_first_0:                 episode reward: -54.6500,                 loss: nan
env1_second_0:                 episode reward: 54.6500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 549.1,                last time consumption/overall running time: 702.1933s / 191942.3708 s
env0_first_0:                 episode reward: -64.9500,                 loss: 1.0195
env0_second_0:                 episode reward: 64.9500,                 loss: 0.7961
env1_first_0:                 episode reward: -61.1000,                 loss: nan
env1_second_0:                 episode reward: 61.1000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 534.3,                last time consumption/overall running time: 678.5928s / 192620.9636 s
env0_first_0:                 episode reward: -67.2000,                 loss: 0.9882
env0_second_0:                 episode reward: 67.2000,                 loss: 0.7962
env1_first_0:                 episode reward: -51.4000,                 loss: nan
env1_second_0:                 episode reward: 51.4000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 459.8,                last time consumption/overall running time: 580.2348s / 193201.1985 s
env0_first_0:                 episode reward: -65.0500,                 loss: 0.9799
env0_second_0:                 episode reward: 65.0500,                 loss: 0.8064
env1_first_0:                 episode reward: -52.6000,                 loss: nan
env1_second_0:                 episode reward: 52.6000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 590.05,                last time consumption/overall running time: 742.1732s / 193943.3717 s
env0_first_0:                 episode reward: -71.9000,                 loss: 1.0272
env0_second_0:                 episode reward: 71.9000,                 loss: 0.7646
env1_first_0:                 episode reward: -50.4500,                 loss: nan
env1_second_0:                 episode reward: 50.4500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 445.9,                last time consumption/overall running time: 564.0918s / 194507.4635 s
env0_first_0:                 episode reward: -69.7000,                 loss: 1.1042
env0_second_0:                 episode reward: 69.7000,                 loss: 0.7981
env1_first_0:                 episode reward: -49.1500,                 loss: nan
env1_second_0:                 episode reward: 49.1500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 473.65,                last time consumption/overall running time: 599.6379s / 195107.1014 s
env0_first_0:                 episode reward: -48.8000,                 loss: 1.1318
env0_second_0:                 episode reward: 48.8000,                 loss: 0.7928
env1_first_0:                 episode reward: -62.9500,                 loss: nan
env1_second_0:                 episode reward: 62.9500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 561.3,                last time consumption/overall running time: 704.9367s / 195812.0381 s
env0_first_0:                 episode reward: -61.5000,                 loss: 1.1814
env0_second_0:                 episode reward: 61.5000,                 loss: 0.7391
env1_first_0:                 episode reward: -62.1000,                 loss: nan
env1_second_0:                 episode reward: 62.1000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 582.6,                last time consumption/overall running time: 741.6609s / 196553.6989 s
env0_first_0:                 episode reward: -63.7500,                 loss: 1.2656
env0_second_0:                 episode reward: 63.7500,                 loss: 0.7550
env1_first_0:                 episode reward: -51.6500,                 loss: nan
env1_second_0:                 episode reward: 51.6500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 382.25,                last time consumption/overall running time: 480.0064s / 197033.7054 s
env0_first_0:                 episode reward: -63.2000,                 loss: 1.2560
env0_second_0:                 episode reward: 63.2000,                 loss: 0.8050
env1_first_0:                 episode reward: -73.7500,                 loss: nan
env1_second_0:                 episode reward: 73.7500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 494.25,                last time consumption/overall running time: 621.5686s / 197655.2739 s
env0_first_0:                 episode reward: -59.1000,                 loss: 1.3534
env0_second_0:                 episode reward: 59.1000,                 loss: 0.8541
env1_first_0:                 episode reward: -61.3500,                 loss: nan
env1_second_0:                 episode reward: 61.3500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 433.3,                last time consumption/overall running time: 546.6986s / 198201.9725 s
env0_first_0:                 episode reward: -57.5000,                 loss: 1.3269
env0_second_0:                 episode reward: 57.5000,                 loss: 0.7669
env1_first_0:                 episode reward: -72.0000,                 loss: nan
env1_second_0:                 episode reward: 72.0000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 430.55,                last time consumption/overall running time: 542.7125s / 198744.6849 s
env0_first_0:                 episode reward: -67.9000,                 loss: 1.3895
env0_second_0:                 episode reward: 67.9000,                 loss: 0.7970
env1_first_0:                 episode reward: -51.7500,                 loss: nan
env1_second_0:                 episode reward: 51.7500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 411.3,                last time consumption/overall running time: 510.8403s / 199255.5253 s
env0_first_0:                 episode reward: -69.7500,                 loss: 1.3630
env0_second_0:                 episode reward: 69.7500,                 loss: 0.8181
env1_first_0:                 episode reward: -49.8500,                 loss: nan
env1_second_0:                 episode reward: 49.8500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 462.45,                last time consumption/overall running time: 576.2272s / 199831.7524 s
env0_first_0:                 episode reward: -59.5000,                 loss: 1.3301
env0_second_0:                 episode reward: 59.5000,                 loss: 0.7720
env1_first_0:                 episode reward: -56.8500,                 loss: nan
env1_second_0:                 episode reward: 56.8500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 538.05,                last time consumption/overall running time: 667.5055s / 200499.2579 s
env0_first_0:                 episode reward: -58.1000,                 loss: 1.3463
env0_second_0:                 episode reward: 58.1000,                 loss: 0.7347
env1_first_0:                 episode reward: -54.8500,                 loss: nan
env1_second_0:                 episode reward: 54.8500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 545.4,                last time consumption/overall running time: 675.7465s / 201175.0044 s
env0_first_0:                 episode reward: -58.0500,                 loss: 1.3659
env0_second_0:                 episode reward: 58.0500,                 loss: 0.7748
env1_first_0:                 episode reward: -64.7500,                 loss: nan
env1_second_0:                 episode reward: 64.7500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 488.4,                last time consumption/overall running time: 603.2965s / 201778.3009 s
env0_first_0:                 episode reward: -69.9500,                 loss: 1.3325
env0_second_0:                 episode reward: 69.9500,                 loss: 0.7133
env1_first_0:                 episode reward: -52.5000,                 loss: nan
env1_second_0:                 episode reward: 52.5000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 416.25,                last time consumption/overall running time: 518.0812s / 202296.3821 s
env0_first_0:                 episode reward: -57.4500,                 loss: 1.3077
env0_second_0:                 episode reward: 57.4500,                 loss: 0.6927
env1_first_0:                 episode reward: -69.2000,                 loss: nan
env1_second_0:                 episode reward: 69.2000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 555.7,                last time consumption/overall running time: 683.8548s / 202980.2369 s
env0_first_0:                 episode reward: -41.8500,                 loss: 1.2152
env0_second_0:                 episode reward: 41.8500,                 loss: 0.6427
env1_first_0:                 episode reward: -70.9500,                 loss: nan
env1_second_0:                 episode reward: 70.9500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 484.05,                last time consumption/overall running time: 593.6476s / 203573.8845 s
env0_first_0:                 episode reward: -63.7500,                 loss: 1.1395
env0_second_0:                 episode reward: 63.7500,                 loss: 0.6510
env1_first_0:                 episode reward: -55.0500,                 loss: nan
env1_second_0:                 episode reward: 55.0500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 517.55,                last time consumption/overall running time: 636.6301s / 204210.5146 s
env0_first_0:                 episode reward: -58.6000,                 loss: 1.1700
env0_second_0:                 episode reward: 58.6000,                 loss: 0.7147
env1_first_0:                 episode reward: -51.3500,                 loss: nan
env1_second_0:                 episode reward: 51.3500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 411.45,                last time consumption/overall running time: 504.0408s / 204714.5554 s
env0_first_0:                 episode reward: -64.1000,                 loss: 1.1572
env0_second_0:                 episode reward: 64.1000,                 loss: 0.7607
env1_first_0:                 episode reward: -45.3000,                 loss: nan
env1_second_0:                 episode reward: 45.3000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 420.9,                last time consumption/overall running time: 517.2368s / 205231.7922 s
env0_first_0:                 episode reward: -58.5500,                 loss: 1.2281
env0_second_0:                 episode reward: 58.5500,                 loss: 0.7305
env1_first_0:                 episode reward: -65.8500,                 loss: nan
env1_second_0:                 episode reward: 65.8500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 448.05,                last time consumption/overall running time: 547.2673s / 205779.0595 s
env0_first_0:                 episode reward: -61.1000,                 loss: 1.1964
env0_second_0:                 episode reward: 61.1000,                 loss: 0.6936
env1_first_0:                 episode reward: -55.6000,                 loss: nan
env1_second_0:                 episode reward: 55.6000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 445.3,                last time consumption/overall running time: 545.0272s / 206324.0867 s
env0_first_0:                 episode reward: -54.3500,                 loss: 1.2872
env0_second_0:                 episode reward: 54.3500,                 loss: 0.7540
env1_first_0:                 episode reward: -58.2000,                 loss: nan
env1_second_0:                 episode reward: 58.2000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 494.45,                last time consumption/overall running time: 608.5780s / 206932.6647 s
env0_first_0:                 episode reward: -67.1500,                 loss: 1.2485
env0_second_0:                 episode reward: 67.1500,                 loss: 0.7996
env1_first_0:                 episode reward: -57.1000,                 loss: nan
env1_second_0:                 episode reward: 57.1000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 490.65,                last time consumption/overall running time: 603.3080s / 207535.9727 s
env0_first_0:                 episode reward: -77.5000,                 loss: 1.2237
env0_second_0:                 episode reward: 77.5000,                 loss: 0.8082
env1_first_0:                 episode reward: -48.2000,                 loss: nan
env1_second_0:                 episode reward: 48.2000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 423.65,                last time consumption/overall running time: 520.4496s / 208056.4223 s
env0_first_0:                 episode reward: -57.2000,                 loss: 1.2367
env0_second_0:                 episode reward: 57.2000,                 loss: 0.7835
env1_first_0:                 episode reward: -68.8000,                 loss: nan
env1_second_0:                 episode reward: 68.8000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 429.35,                last time consumption/overall running time: 526.5178s / 208582.9401 s
env0_first_0:                 episode reward: -65.3500,                 loss: 1.2358
env0_second_0:                 episode reward: 65.3500,                 loss: 0.8065
env1_first_0:                 episode reward: -59.6500,                 loss: nan
env1_second_0:                 episode reward: 59.6500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 461.95,                last time consumption/overall running time: 569.7636s / 209152.7037 s
env0_first_0:                 episode reward: -53.4500,                 loss: 1.3194
env0_second_0:                 episode reward: 53.4500,                 loss: 0.8568
env1_first_0:                 episode reward: -65.2000,                 loss: nan
env1_second_0:                 episode reward: 65.2000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 492.9,                last time consumption/overall running time: 607.8094s / 209760.5130 s
env0_first_0:                 episode reward: -59.0500,                 loss: 1.3484
env0_second_0:                 episode reward: 59.0500,                 loss: 0.8549
env1_first_0:                 episode reward: -50.7000,                 loss: nan
env1_second_0:                 episode reward: 50.7000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 427.85,                last time consumption/overall running time: 526.4888s / 210287.0019 s
env0_first_0:                 episode reward: -70.8000,                 loss: 1.2688
env0_second_0:                 episode reward: 70.8000,                 loss: 0.7565
env1_first_0:                 episode reward: -63.6000,                 loss: nan
env1_second_0:                 episode reward: 63.6000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 502.0,                last time consumption/overall running time: 617.0681s / 210904.0700 s
env0_first_0:                 episode reward: -60.2500,                 loss: 1.3067
env0_second_0:                 episode reward: 60.2500,                 loss: 0.7361
env1_first_0:                 episode reward: -40.0500,                 loss: nan
env1_second_0:                 episode reward: 40.0500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 443.75,                last time consumption/overall running time: 543.4716s / 211447.5416 s
env0_first_0:                 episode reward: -82.7000,                 loss: 1.2615
env0_second_0:                 episode reward: 82.7000,                 loss: 0.7300
env1_first_0:                 episode reward: -47.1500,                 loss: nan
env1_second_0:                 episode reward: 47.1500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 525.85,                last time consumption/overall running time: 648.1452s / 212095.6868 s
env0_first_0:                 episode reward: -72.9500,                 loss: 1.2380
env0_second_0:                 episode reward: 72.9500,                 loss: 0.7931
env1_first_0:                 episode reward: -51.0000,                 loss: nan
env1_second_0:                 episode reward: 51.0000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 464.5,                last time consumption/overall running time: 569.8175s / 212665.5042 s
env0_first_0:                 episode reward: -56.1500,                 loss: 1.1979
env0_second_0:                 episode reward: 56.1500,                 loss: 0.8008
env1_first_0:                 episode reward: -72.3500,                 loss: nan
env1_second_0:                 episode reward: 72.3500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 445.45,                last time consumption/overall running time: 551.4109s / 213216.9151 s
env0_first_0:                 episode reward: -61.5500,                 loss: 1.1985
env0_second_0:                 episode reward: 61.5500,                 loss: 0.7993
env1_first_0:                 episode reward: -77.1500,                 loss: nan
env1_second_0:                 episode reward: 77.1500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 564.9,                last time consumption/overall running time: 698.6038s / 213915.5189 s
env0_first_0:                 episode reward: -62.9500,                 loss: 1.2260
env0_second_0:                 episode reward: 62.9500,                 loss: 0.7868
env1_first_0:                 episode reward: -60.0500,                 loss: nan
env1_second_0:                 episode reward: 60.0500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 564.55,                last time consumption/overall running time: 682.4656s / 214597.9845 s
env0_first_0:                 episode reward: -53.2000,                 loss: 1.2042
env0_second_0:                 episode reward: 53.2000,                 loss: 0.8326
env1_first_0:                 episode reward: -65.4000,                 loss: nan
env1_second_0:                 episode reward: 65.4000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 447.75,                last time consumption/overall running time: 543.7897s / 215141.7742 s
env0_first_0:                 episode reward: -71.8500,                 loss: 1.1878
env0_second_0:                 episode reward: 71.8500,                 loss: 0.8188
env1_first_0:                 episode reward: -57.4000,                 loss: nan
env1_second_0:                 episode reward: 57.4000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 435.0,                last time consumption/overall running time: 527.9816s / 215669.7557 s
env0_first_0:                 episode reward: -61.7000,                 loss: 1.1576
env0_second_0:                 episode reward: 61.7000,                 loss: 0.8483
env1_first_0:                 episode reward: -62.8500,                 loss: nan
env1_second_0:                 episode reward: 62.8500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 526.05,                last time consumption/overall running time: 638.9806s / 216308.7363 s
env0_first_0:                 episode reward: -61.7500,                 loss: 1.1566
env0_second_0:                 episode reward: 61.7500,                 loss: 0.8019
env1_first_0:                 episode reward: -59.9500,                 loss: nan
env1_second_0:                 episode reward: 59.9500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 507.1,                last time consumption/overall running time: 611.3720s / 216920.1083 s
env0_first_0:                 episode reward: -45.1000,                 loss: 1.2253
env0_second_0:                 episode reward: 45.1000,                 loss: 0.8654
env1_first_0:                 episode reward: -63.4500,                 loss: nan
env1_second_0:                 episode reward: 63.4500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 560.25,                last time consumption/overall running time: 682.4882s / 217602.5965 s
env0_first_0:                 episode reward: -67.7500,                 loss: 1.3400
env0_second_0:                 episode reward: 67.7500,                 loss: 0.8249
env1_first_0:                 episode reward: -51.0000,                 loss: nan
env1_second_0:                 episode reward: 51.0000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 518.1,                last time consumption/overall running time: 622.8159s / 218225.4124 s
env0_first_0:                 episode reward: -66.9000,                 loss: 1.2680
env0_second_0:                 episode reward: 66.9000,                 loss: 0.9039
env1_first_0:                 episode reward: -55.7000,                 loss: nan
env1_second_0:                 episode reward: 55.7000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 476.0,                last time consumption/overall running time: 571.8615s / 218797.2739 s
env0_first_0:                 episode reward: -61.2000,                 loss: 1.2882
env0_second_0:                 episode reward: 61.2000,                 loss: 0.8682
env1_first_0:                 episode reward: -54.2000,                 loss: nan
env1_second_0:                 episode reward: 54.2000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 439.9,                last time consumption/overall running time: 532.5829s / 219329.8568 s
env0_first_0:                 episode reward: -55.4000,                 loss: 1.3361
env0_second_0:                 episode reward: 55.4000,                 loss: 0.8756
env1_first_0:                 episode reward: -63.6500,                 loss: nan
env1_second_0:                 episode reward: 63.6500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 573.2,                last time consumption/overall running time: 693.3330s / 220023.1899 s
env0_first_0:                 episode reward: -51.1500,                 loss: 1.2915
env0_second_0:                 episode reward: 51.1500,                 loss: 0.8649
env1_first_0:                 episode reward: -71.0000,                 loss: nan
env1_second_0:                 episode reward: 71.0000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 603.4,                last time consumption/overall running time: 729.0601s / 220752.2499 s
env0_first_0:                 episode reward: -56.7000,                 loss: 1.2363
env0_second_0:                 episode reward: 56.7000,                 loss: 0.8466
env1_first_0:                 episode reward: -56.9500,                 loss: nan
env1_second_0:                 episode reward: 56.9500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 528.05,                last time consumption/overall running time: 631.1951s / 221383.4450 s
env0_first_0:                 episode reward: -48.2000,                 loss: 1.3278
env0_second_0:                 episode reward: 48.2000,                 loss: 0.8250
env1_first_0:                 episode reward: -67.9000,                 loss: nan
env1_second_0:                 episode reward: 67.9000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 435.0,                last time consumption/overall running time: 523.2190s / 221906.6640 s
env0_first_0:                 episode reward: -60.4000,                 loss: 1.3322
env0_second_0:                 episode reward: 60.4000,                 loss: 0.8715
env1_first_0:                 episode reward: -59.3000,                 loss: nan
env1_second_0:                 episode reward: 59.3000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 514.8,                last time consumption/overall running time: 733.1761s / 222639.8402 s
env0_first_0:                 episode reward: -50.4000,                 loss: 1.2631
env0_second_0:                 episode reward: 50.4000,                 loss: 0.8745
env1_first_0:                 episode reward: -69.7000,                 loss: nan
env1_second_0:                 episode reward: 69.7000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 703.6,                last time consumption/overall running time: 851.6062s / 223491.4464 s