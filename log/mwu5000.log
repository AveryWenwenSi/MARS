pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7fdb0c0590d0>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 640, 'max_episodes': 101000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2.
Episode: 1/101000 (0.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4397s / 4.4397 s
agent0:                 episode reward: 1.1570,                 loss: nan
agent1:                 episode reward: -1.1570,                 loss: nan
Episode: 21/101000 (0.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1506s / 4.5903 s
agent0:                 episode reward: 0.3433,                 loss: nan
agent1:                 episode reward: -0.3433,                 loss: nan
Episode: 41/101000 (0.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2729s / 4.8632 s
agent0:                 episode reward: -0.2217,                 loss: nan
agent1:                 episode reward: 0.2217,                 loss: nan
Episode: 61/101000 (0.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1859s / 5.0490 s
agent0:                 episode reward: 0.1959,                 loss: nan
agent1:                 episode reward: -0.1959,                 loss: nan
Episode: 81/101000 (0.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1158s / 5.1648 s
agent0:                 episode reward: 0.7555,                 loss: nan
agent1:                 episode reward: -0.7555,                 loss: nan
Episode: 101/101000 (0.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2594s / 5.4243 s
agent0:                 episode reward: 0.0226,                 loss: nan
agent1:                 episode reward: -0.0226,                 loss: nan
Episode: 121/101000 (0.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3244s / 5.7487 s
agent0:                 episode reward: 0.0641,                 loss: nan
agent1:                 episode reward: -0.0641,                 loss: nan
Episode: 141/101000 (0.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2383s / 5.9870 s
agent0:                 episode reward: 0.4002,                 loss: nan
agent1:                 episode reward: -0.4002,                 loss: nan
Episode: 161/101000 (0.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3255s / 6.3125 s
agent0:                 episode reward: -0.1196,                 loss: nan
agent1:                 episode reward: 0.1196,                 loss: nan
Episode: 181/101000 (0.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1664s / 6.4789 s
agent0:                 episode reward: -0.0733,                 loss: nan
agent1:                 episode reward: 0.0733,                 loss: nan
Episode: 201/101000 (0.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2211s / 6.7000 s
agent0:                 episode reward: -0.1402,                 loss: nan
agent1:                 episode reward: 0.1402,                 loss: nan
Episode: 221/101000 (0.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 0.5414s / 7.2414 s
agent0:                 episode reward: 0.3195,                 loss: 0.1740
agent1:                 episode reward: -0.3195,                 loss: nan
Episode: 241/101000 (0.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1839s / 8.4254 s
agent0:                 episode reward: 0.0965,                 loss: 0.1514
agent1:                 episode reward: -0.0965,                 loss: nan
Episode: 261/101000 (0.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2926s / 9.7179 s
agent0:                 episode reward: 0.1477,                 loss: 0.1507
agent1:                 episode reward: -0.1477,                 loss: nan
Episode: 281/101000 (0.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1963s / 10.9142 s
agent0:                 episode reward: 0.5094,                 loss: 0.1471
agent1:                 episode reward: -0.5094,                 loss: nan
Score delta: 1.6058842784349179, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/62_0.
Episode: 301/101000 (0.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1724s / 11.0866 s
agent0:                 episode reward: 0.2074,                 loss: nan
agent1:                 episode reward: -0.2074,                 loss: nan
Episode: 321/101000 (0.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2085s / 11.2952 s
agent0:                 episode reward: 0.0733,                 loss: nan
agent1:                 episode reward: -0.0733,                 loss: nan
Episode: 341/101000 (0.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2510s / 11.5462 s
agent0:                 episode reward: -0.2328,                 loss: nan
agent1:                 episode reward: 0.2328,                 loss: nan
Episode: 361/101000 (0.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1938s / 11.7400 s
agent0:                 episode reward: -0.0523,                 loss: nan
agent1:                 episode reward: 0.0523,                 loss: nan
Episode: 381/101000 (0.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1594s / 11.8994 s
agent0:                 episode reward: 0.1722,                 loss: nan
agent1:                 episode reward: -0.1722,                 loss: nan
Episode: 401/101000 (0.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2566s / 12.1560 s
agent0:                 episode reward: -0.1183,                 loss: nan
agent1:                 episode reward: 0.1183,                 loss: nan
Episode: 421/101000 (0.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1314s / 12.2874 s
agent0:                 episode reward: 0.4455,                 loss: nan
agent1:                 episode reward: -0.4455,                 loss: nan
Episode: 441/101000 (0.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1079s / 12.3953 s
agent0:                 episode reward: -0.1424,                 loss: nan
agent1:                 episode reward: 0.1424,                 loss: nan
Episode: 461/101000 (0.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2029s / 12.5982 s
agent0:                 episode reward: -0.0350,                 loss: nan
agent1:                 episode reward: 0.0350,                 loss: nan
Episode: 481/101000 (0.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0876s / 12.6859 s
agent0:                 episode reward: 0.0250,                 loss: nan
agent1:                 episode reward: -0.0250,                 loss: nan
Episode: 501/101000 (0.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0171s / 13.7029 s
agent0:                 episode reward: 0.2399,                 loss: nan
agent1:                 episode reward: -0.2399,                 loss: 0.1968
Episode: 521/101000 (0.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4579s / 15.1608 s
agent0:                 episode reward: -0.1245,                 loss: nan
agent1:                 episode reward: 0.1245,                 loss: 0.1788
Episode: 541/101000 (0.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2442s / 16.4050 s
agent0:                 episode reward: -0.0657,                 loss: nan
agent1:                 episode reward: 0.0657,                 loss: 0.1689
Episode: 561/101000 (0.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2627s / 17.6677 s
agent0:                 episode reward: -0.1610,                 loss: nan
agent1:                 episode reward: 0.1610,                 loss: 0.1606
Episode: 581/101000 (0.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1216s / 18.7894 s
agent0:                 episode reward: -0.0251,                 loss: nan
agent1:                 episode reward: 0.0251,                 loss: 0.1529
Episode: 601/101000 (0.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0560s / 19.8454 s
agent0:                 episode reward: -0.2036,                 loss: nan
agent1:                 episode reward: 0.2036,                 loss: 0.1481
Episode: 621/101000 (0.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 0.9041s / 20.7495 s
agent0:                 episode reward: -0.0297,                 loss: nan
agent1:                 episode reward: 0.0297,                 loss: 0.1426
Episode: 641/101000 (0.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0813s / 21.8308 s
agent0:                 episode reward: 0.1312,                 loss: nan
agent1:                 episode reward: -0.1312,                 loss: 0.1380
Episode: 661/101000 (0.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3050s / 23.1358 s
agent0:                 episode reward: -0.0469,                 loss: nan
agent1:                 episode reward: 0.0469,                 loss: 0.1354
Episode: 681/101000 (0.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8107s / 24.9465 s
agent0:                 episode reward: -0.0518,                 loss: nan
agent1:                 episode reward: 0.0518,                 loss: 0.1331
Episode: 701/101000 (0.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3993s / 26.3458 s
agent0:                 episode reward: 0.0854,                 loss: nan
agent1:                 episode reward: -0.0854,                 loss: 0.1317
Episode: 721/101000 (0.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1544s / 27.5002 s
agent0:                 episode reward: -0.6676,                 loss: nan
agent1:                 episode reward: 0.6676,                 loss: 0.1300
Score delta: 1.6813993502849052, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/295_1.
Episode: 741/101000 (0.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 0.7455s / 28.2457 s
agent0:                 episode reward: -0.7947,                 loss: 0.1499
agent1:                 episode reward: 0.7947,                 loss: nan
Episode: 761/101000 (0.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3451s / 29.5907 s
agent0:                 episode reward: -0.8786,                 loss: 0.1539
agent1:                 episode reward: 0.8786,                 loss: nan
Episode: 781/101000 (0.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2904s / 30.8811 s
agent0:                 episode reward: -0.1662,                 loss: 0.1528
agent1:                 episode reward: 0.1662,                 loss: nan
Episode: 801/101000 (0.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4176s / 32.2987 s
agent0:                 episode reward: -0.9345,                 loss: 0.1527
agent1:                 episode reward: 0.9345,                 loss: nan
Episode: 821/101000 (0.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0716s / 33.3704 s
agent0:                 episode reward: -0.7442,                 loss: 0.1527
agent1:                 episode reward: 0.7442,                 loss: nan
Episode: 841/101000 (0.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1785s / 34.5489 s
agent0:                 episode reward: -0.8577,                 loss: 0.1515
agent1:                 episode reward: 0.8577,                 loss: nan
Episode: 861/101000 (0.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1026s / 35.6515 s
agent0:                 episode reward: -0.5918,                 loss: 0.1495
agent1:                 episode reward: 0.5918,                 loss: nan
Episode: 881/101000 (0.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1664s / 36.8179 s
agent0:                 episode reward: -0.6189,                 loss: 0.1485
agent1:                 episode reward: 0.6189,                 loss: nan
Episode: 901/101000 (0.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3107s / 38.1286 s
agent0:                 episode reward: -0.9659,                 loss: 0.1460
agent1:                 episode reward: 0.9659,                 loss: nan
Episode: 921/101000 (0.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3401s / 39.4687 s
agent0:                 episode reward: -0.2858,                 loss: 0.1447
agent1:                 episode reward: 0.2858,                 loss: nan
Episode: 941/101000 (0.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2701s / 40.7389 s
agent0:                 episode reward: -0.4417,                 loss: 0.1429
agent1:                 episode reward: 0.4417,                 loss: nan
Episode: 961/101000 (0.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1854s / 41.9243 s
agent0:                 episode reward: -0.8227,                 loss: 0.1422
agent1:                 episode reward: 0.8227,                 loss: nan
Episode: 981/101000 (0.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3369s / 43.2612 s
agent0:                 episode reward: -0.5713,                 loss: 0.1394
agent1:                 episode reward: 0.5713,                 loss: nan
Episode: 1001/101000 (0.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6224s / 44.8836 s
agent0:                 episode reward: -0.2062,                 loss: 0.1524
agent1:                 episode reward: 0.2062,                 loss: nan
Episode: 1021/101000 (1.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5547s / 46.4382 s
agent0:                 episode reward: -0.6304,                 loss: 0.1707
agent1:                 episode reward: 0.6304,                 loss: nan
Episode: 1041/101000 (1.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2943s / 47.7325 s
agent0:                 episode reward: -0.3451,                 loss: 0.1679
agent1:                 episode reward: 0.3451,                 loss: nan
Episode: 1061/101000 (1.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 0.9172s / 48.6497 s
agent0:                 episode reward: -0.0697,                 loss: 0.1669
agent1:                 episode reward: 0.0697,                 loss: nan
Episode: 1081/101000 (1.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3368s / 49.9865 s
agent0:                 episode reward: -0.6736,                 loss: 0.1665
agent1:                 episode reward: 0.6736,                 loss: nan
Episode: 1101/101000 (1.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5292s / 51.5156 s
agent0:                 episode reward: -0.1643,                 loss: 0.1660
agent1:                 episode reward: 0.1643,                 loss: nan
Episode: 1121/101000 (1.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4705s / 52.9862 s
agent0:                 episode reward: -0.4242,                 loss: 0.1645
agent1:                 episode reward: 0.4242,                 loss: nan
Episode: 1141/101000 (1.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 0.7447s / 53.7308 s
agent0:                 episode reward: -1.1232,                 loss: 0.1618
agent1:                 episode reward: 1.1232,                 loss: nan
Episode: 1161/101000 (1.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4908s / 55.2216 s
agent0:                 episode reward: -0.6400,                 loss: 0.1615
agent1:                 episode reward: 0.6400,                 loss: nan
Episode: 1181/101000 (1.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2664s / 56.4880 s
agent0:                 episode reward: -0.3682,                 loss: 0.1642
agent1:                 episode reward: 0.3682,                 loss: nan
Episode: 1201/101000 (1.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6675s / 58.1555 s
agent0:                 episode reward: -0.7537,                 loss: 0.1619
agent1:                 episode reward: 0.7537,                 loss: nan
Episode: 1221/101000 (1.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2389s / 59.3944 s
agent0:                 episode reward: -0.2091,                 loss: 0.1615
agent1:                 episode reward: 0.2091,                 loss: nan
Episode: 1241/101000 (1.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2685s / 60.6629 s
agent0:                 episode reward: -0.9479,                 loss: 0.1600
agent1:                 episode reward: 0.9479,                 loss: nan
Episode: 1261/101000 (1.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4548s / 62.1176 s
agent0:                 episode reward: -0.2982,                 loss: 0.1588
agent1:                 episode reward: 0.2982,                 loss: nan
Episode: 1281/101000 (1.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2854s / 63.4031 s
agent0:                 episode reward: -0.5636,                 loss: 0.1572
agent1:                 episode reward: 0.5636,                 loss: nan
Episode: 1301/101000 (1.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2121s / 64.6152 s
agent0:                 episode reward: -0.6232,                 loss: 0.1563
agent1:                 episode reward: 0.6232,                 loss: nan
Episode: 1321/101000 (1.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1858s / 65.8009 s
agent0:                 episode reward: -0.1001,                 loss: 0.1573
agent1:                 episode reward: 0.1001,                 loss: nan
Episode: 1341/101000 (1.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2234s / 67.0244 s
agent0:                 episode reward: -0.5272,                 loss: 0.1855
agent1:                 episode reward: 0.5272,                 loss: nan
Episode: 1361/101000 (1.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1454s / 68.1698 s
agent0:                 episode reward: -0.5438,                 loss: 0.1948
agent1:                 episode reward: 0.5438,                 loss: nan
Episode: 1381/101000 (1.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1501s / 69.3199 s
agent0:                 episode reward: -0.1162,                 loss: 0.1944
agent1:                 episode reward: 0.1162,                 loss: nan
Episode: 1401/101000 (1.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 0.9886s / 70.3085 s
agent0:                 episode reward: -0.1670,                 loss: 0.1939
agent1:                 episode reward: 0.1670,                 loss: nan
Episode: 1421/101000 (1.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2700s / 71.5786 s
agent0:                 episode reward: -0.8877,                 loss: 0.1917
agent1:                 episode reward: 0.8877,                 loss: nan
Episode: 1441/101000 (1.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 0.7538s / 72.3323 s
agent0:                 episode reward: -0.3188,                 loss: 0.1923
agent1:                 episode reward: 0.3188,                 loss: nan
Episode: 1461/101000 (1.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4105s / 73.7429 s
agent0:                 episode reward: -0.5644,                 loss: 0.1914
agent1:                 episode reward: 0.5644,                 loss: nan
Episode: 1481/101000 (1.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4835s / 75.2264 s
agent0:                 episode reward: -0.2995,                 loss: 0.1917
agent1:                 episode reward: 0.2995,                 loss: nan
Episode: 1501/101000 (1.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2479s / 76.4743 s
agent0:                 episode reward: -0.4529,                 loss: 0.1893
agent1:                 episode reward: 0.4529,                 loss: nan
Episode: 1521/101000 (1.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3187s / 77.7930 s
agent0:                 episode reward: -0.3737,                 loss: 0.1897
agent1:                 episode reward: 0.3737,                 loss: nan
Episode: 1541/101000 (1.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3603s / 79.1532 s
agent0:                 episode reward: -0.4140,                 loss: 0.1902
agent1:                 episode reward: 0.4140,                 loss: nan
Episode: 1561/101000 (1.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7862s / 80.9394 s
agent0:                 episode reward: -0.1478,                 loss: 0.1892
agent1:                 episode reward: 0.1478,                 loss: nan
Episode: 1581/101000 (1.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0658s / 82.0052 s
agent0:                 episode reward: -0.3792,                 loss: 0.1886
agent1:                 episode reward: 0.3792,                 loss: nan
Episode: 1601/101000 (1.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6323s / 83.6375 s
agent0:                 episode reward: -0.5319,                 loss: 0.1900
agent1:                 episode reward: 0.5319,                 loss: nan
Episode: 1621/101000 (1.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6008s / 85.2383 s
agent0:                 episode reward: -0.6651,                 loss: 0.1890
agent1:                 episode reward: 0.6651,                 loss: nan
Episode: 1641/101000 (1.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4145s / 86.6528 s
agent0:                 episode reward: -0.3965,                 loss: 0.1901
agent1:                 episode reward: 0.3965,                 loss: nan
Episode: 1661/101000 (1.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1192s / 87.7720 s
agent0:                 episode reward: -0.5852,                 loss: 0.1913
agent1:                 episode reward: 0.5852,                 loss: nan
Episode: 1681/101000 (1.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3448s / 89.1168 s
agent0:                 episode reward: 0.0537,                 loss: 0.2055
agent1:                 episode reward: -0.0537,                 loss: nan
Episode: 1701/101000 (1.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3341s / 90.4509 s
agent0:                 episode reward: -0.7589,                 loss: 0.2000
agent1:                 episode reward: 0.7589,                 loss: nan
Episode: 1721/101000 (1.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4825s / 91.9333 s
agent0:                 episode reward: -0.8051,                 loss: 0.2040
agent1:                 episode reward: 0.8051,                 loss: nan
Episode: 1741/101000 (1.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4694s / 93.4027 s
agent0:                 episode reward: -0.2788,                 loss: 0.2035
agent1:                 episode reward: 0.2788,                 loss: nan
Episode: 1761/101000 (1.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0397s / 95.4424 s
agent0:                 episode reward: -0.2505,                 loss: 0.2017
agent1:                 episode reward: 0.2505,                 loss: nan
Episode: 1781/101000 (1.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1713s / 96.6137 s
agent0:                 episode reward: -0.7092,                 loss: 0.2010
agent1:                 episode reward: 0.7092,                 loss: nan
Episode: 1801/101000 (1.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0810s / 97.6947 s
agent0:                 episode reward: -0.1083,                 loss: 0.2009
agent1:                 episode reward: 0.1083,                 loss: nan
Episode: 1821/101000 (1.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1337s / 98.8284 s
agent0:                 episode reward: -0.3977,                 loss: 0.2015
agent1:                 episode reward: 0.3977,                 loss: nan
Episode: 1841/101000 (1.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4043s / 100.2326 s
agent0:                 episode reward: -0.8264,                 loss: 0.2008
agent1:                 episode reward: 0.8264,                 loss: nan
Episode: 1861/101000 (1.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3622s / 101.5949 s
agent0:                 episode reward: -0.6390,                 loss: 0.1982
agent1:                 episode reward: 0.6390,                 loss: nan
Episode: 1881/101000 (1.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 0.9502s / 102.5450 s
agent0:                 episode reward: -0.1987,                 loss: 0.1982
agent1:                 episode reward: 0.1987,                 loss: nan
Episode: 1901/101000 (1.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6084s / 104.1534 s
agent0:                 episode reward: -0.6976,                 loss: 0.1981
agent1:                 episode reward: 0.6976,                 loss: nan
Episode: 1921/101000 (1.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3354s / 105.4888 s
agent0:                 episode reward: -0.5337,                 loss: 0.1968
agent1:                 episode reward: 0.5337,                 loss: nan
Episode: 1941/101000 (1.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3068s / 106.7956 s
agent0:                 episode reward: -0.5214,                 loss: 0.1983
agent1:                 episode reward: 0.5214,                 loss: nan
Episode: 1961/101000 (1.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1597s / 107.9553 s
agent0:                 episode reward: -0.2117,                 loss: 0.1977
agent1:                 episode reward: 0.2117,                 loss: nan
Episode: 1981/101000 (1.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3711s / 109.3264 s
agent0:                 episode reward: -0.3580,                 loss: 0.1967
agent1:                 episode reward: 0.3580,                 loss: nan
Episode: 2001/101000 (1.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7506s / 111.0769 s
agent0:                 episode reward: -0.7240,                 loss: 0.2014
agent1:                 episode reward: 0.7240,                 loss: nan
Episode: 2021/101000 (2.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6122s / 112.6892 s
agent0:                 episode reward: -0.5033,                 loss: 0.2074
agent1:                 episode reward: 0.5033,                 loss: nan
Episode: 2041/101000 (2.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4270s / 114.1162 s
agent0:                 episode reward: -0.4908,                 loss: 0.2087
agent1:                 episode reward: 0.4908,                 loss: nan
Episode: 2061/101000 (2.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3565s / 115.4727 s
agent0:                 episode reward: -0.5434,                 loss: 0.2121
agent1:                 episode reward: 0.5434,                 loss: nan
Episode: 2081/101000 (2.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4048s / 116.8775 s
agent0:                 episode reward: -0.0817,                 loss: 0.2106
agent1:                 episode reward: 0.0817,                 loss: nan
Episode: 2101/101000 (2.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5786s / 118.4561 s
agent0:                 episode reward: -0.1995,                 loss: 0.2073
agent1:                 episode reward: 0.1995,                 loss: nan
Episode: 2121/101000 (2.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2837s / 119.7399 s
agent0:                 episode reward: -0.1083,                 loss: 0.2069
agent1:                 episode reward: 0.1083,                 loss: nan
Episode: 2141/101000 (2.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5104s / 121.2503 s
agent0:                 episode reward: -0.4420,                 loss: 0.2097
agent1:                 episode reward: 0.4420,                 loss: nan
Episode: 2161/101000 (2.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5189s / 122.7692 s
agent0:                 episode reward: -0.4074,                 loss: 0.2094
agent1:                 episode reward: 0.4074,                 loss: nan
Episode: 2181/101000 (2.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4148s / 124.1840 s
agent0:                 episode reward: -0.5892,                 loss: 0.2060
agent1:                 episode reward: 0.5892,                 loss: nan
Episode: 2201/101000 (2.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3396s / 125.5236 s
agent0:                 episode reward: -0.2451,                 loss: 0.2091
agent1:                 episode reward: 0.2451,                 loss: nan
Episode: 2221/101000 (2.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2843s / 126.8079 s
agent0:                 episode reward: -0.4992,                 loss: 0.2069
agent1:                 episode reward: 0.4992,                 loss: nan
Episode: 2241/101000 (2.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1460s / 127.9539 s
agent0:                 episode reward: -0.2050,                 loss: 0.2055
agent1:                 episode reward: 0.2050,                 loss: nan
Episode: 2261/101000 (2.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0727s / 130.0266 s
agent0:                 episode reward: -0.2753,                 loss: 0.2050
agent1:                 episode reward: 0.2753,                 loss: nan
Episode: 2281/101000 (2.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4401s / 131.4667 s
agent0:                 episode reward: -0.3652,                 loss: 0.2057
agent1:                 episode reward: 0.3652,                 loss: nan
Episode: 2301/101000 (2.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8118s / 133.2785 s
agent0:                 episode reward: -0.1687,                 loss: 0.2052
agent1:                 episode reward: 0.1687,                 loss: nan
Episode: 2321/101000 (2.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2822s / 134.5607 s
agent0:                 episode reward: -0.8322,                 loss: 0.2056
agent1:                 episode reward: 0.8322,                 loss: nan
Episode: 2341/101000 (2.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0735s / 135.6343 s
agent0:                 episode reward: -0.5675,                 loss: 0.2098
agent1:                 episode reward: 0.5675,                 loss: nan
Episode: 2361/101000 (2.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3083s / 136.9425 s
agent0:                 episode reward: -0.4446,                 loss: 0.2125
agent1:                 episode reward: 0.4446,                 loss: nan
Episode: 2381/101000 (2.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2385s / 138.1810 s
agent0:                 episode reward: -0.8620,                 loss: 0.2099
agent1:                 episode reward: 0.8620,                 loss: nan
Episode: 2401/101000 (2.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5969s / 139.7779 s
agent0:                 episode reward: -0.0209,                 loss: 0.2099
agent1:                 episode reward: 0.0209,                 loss: 0.1293
Score delta: 1.5389327529376584, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/1963_0.
Episode: 2421/101000 (2.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0927s / 140.8706 s
agent0:                 episode reward: 0.4124,                 loss: nan
agent1:                 episode reward: -0.4124,                 loss: 0.1278
Episode: 2441/101000 (2.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5944s / 142.4650 s
agent0:                 episode reward: -0.2944,                 loss: nan
agent1:                 episode reward: 0.2944,                 loss: 0.1295
Episode: 2461/101000 (2.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4318s / 143.8968 s
agent0:                 episode reward: -0.4500,                 loss: 0.2031
agent1:                 episode reward: 0.4500,                 loss: 0.1288
Score delta: 1.550608044938685, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/2032_1.
Episode: 2481/101000 (2.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4122s / 145.3090 s
agent0:                 episode reward: -0.6727,                 loss: 0.1723
agent1:                 episode reward: 0.6727,                 loss: nan
Episode: 2501/101000 (2.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3750s / 146.6840 s
agent0:                 episode reward: -0.8207,                 loss: 0.1619
agent1:                 episode reward: 0.8207,                 loss: nan
Episode: 2521/101000 (2.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0371s / 147.7211 s
agent0:                 episode reward: 0.4075,                 loss: 0.1574
agent1:                 episode reward: -0.4075,                 loss: nan
Episode: 2541/101000 (2.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4995s / 149.2205 s
agent0:                 episode reward: -0.5339,                 loss: 0.1532
agent1:                 episode reward: 0.5339,                 loss: nan
Episode: 2561/101000 (2.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4525s / 150.6730 s
agent0:                 episode reward: 0.0931,                 loss: 0.1516
agent1:                 episode reward: -0.0931,                 loss: nan
Episode: 2581/101000 (2.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5471s / 152.2201 s
agent0:                 episode reward: -0.3355,                 loss: 0.1508
agent1:                 episode reward: 0.3355,                 loss: nan
Episode: 2601/101000 (2.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2435s / 153.4636 s
agent0:                 episode reward: -0.4228,                 loss: 0.1453
agent1:                 episode reward: 0.4228,                 loss: nan
Episode: 2621/101000 (2.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1038s / 154.5674 s
agent0:                 episode reward: -0.4645,                 loss: 0.1429
agent1:                 episode reward: 0.4645,                 loss: nan
Episode: 2641/101000 (2.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6068s / 156.1742 s
agent0:                 episode reward: -0.1702,                 loss: 0.1391
agent1:                 episode reward: 0.1702,                 loss: nan
Episode: 2661/101000 (2.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5189s / 157.6931 s
agent0:                 episode reward: -0.5617,                 loss: 0.1376
agent1:                 episode reward: 0.5617,                 loss: nan
Episode: 2681/101000 (2.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5220s / 159.2151 s
agent0:                 episode reward: -0.3611,                 loss: 0.1356
agent1:                 episode reward: 0.3611,                 loss: nan
Episode: 2701/101000 (2.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5727s / 160.7878 s
agent0:                 episode reward: 0.0140,                 loss: 0.1328
agent1:                 episode reward: -0.0140,                 loss: nan
Episode: 2721/101000 (2.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2697s / 162.0575 s
agent0:                 episode reward: -0.2517,                 loss: 0.1311
agent1:                 episode reward: 0.2517,                 loss: nan
Episode: 2741/101000 (2.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6206s / 163.6781 s
agent0:                 episode reward: -0.7405,                 loss: 0.1501
agent1:                 episode reward: 0.7405,                 loss: nan
Episode: 2761/101000 (2.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3711s / 165.0492 s
agent0:                 episode reward: -0.1292,                 loss: 0.1565
agent1:                 episode reward: 0.1292,                 loss: nan
Episode: 2781/101000 (2.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6772s / 166.7263 s
agent0:                 episode reward: -0.0991,                 loss: 0.1556
agent1:                 episode reward: 0.0991,                 loss: nan
Episode: 2801/101000 (2.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6456s / 168.3719 s
agent0:                 episode reward: -0.5934,                 loss: 0.1536
agent1:                 episode reward: 0.5934,                 loss: nan
Episode: 2821/101000 (2.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3542s / 169.7262 s
agent0:                 episode reward: -0.5550,                 loss: 0.1519
agent1:                 episode reward: 0.5550,                 loss: nan
Episode: 2841/101000 (2.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2335s / 170.9597 s
agent0:                 episode reward: -0.4525,                 loss: 0.1529
agent1:                 episode reward: 0.4525,                 loss: nan
Episode: 2861/101000 (2.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3573s / 172.3170 s
agent0:                 episode reward: -0.0556,                 loss: 0.1510
agent1:                 episode reward: 0.0556,                 loss: nan
Score delta: 1.576593634150648, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/2435_0.
Episode: 2881/101000 (2.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4236s / 173.7406 s
agent0:                 episode reward: 0.2206,                 loss: nan
agent1:                 episode reward: -0.2206,                 loss: 0.1290
Episode: 2901/101000 (2.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6430s / 175.3837 s
agent0:                 episode reward: -0.0582,                 loss: nan
agent1:                 episode reward: 0.0582,                 loss: 0.1363
Episode: 2921/101000 (2.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3238s / 176.7074 s
agent0:                 episode reward: -0.1679,                 loss: nan
agent1:                 episode reward: 0.1679,                 loss: 0.1359
Episode: 2941/101000 (2.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3859s / 178.0933 s
agent0:                 episode reward: -0.8286,                 loss: 0.1874
agent1:                 episode reward: 0.8286,                 loss: 0.1371
Score delta: 1.784581497369739, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/2503_1.
Episode: 2961/101000 (2.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3252s / 179.4185 s
agent0:                 episode reward: -0.2308,                 loss: 0.1664
agent1:                 episode reward: 0.2308,                 loss: nan
Episode: 2981/101000 (2.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5232s / 180.9416 s
agent0:                 episode reward: -0.2465,                 loss: 0.1578
agent1:                 episode reward: 0.2465,                 loss: nan
Episode: 3001/101000 (2.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4757s / 182.4174 s
agent0:                 episode reward: -0.5883,                 loss: 0.1540
agent1:                 episode reward: 0.5883,                 loss: nan
Episode: 3021/101000 (2.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3498s / 183.7672 s
agent0:                 episode reward: -0.8318,                 loss: 0.1505
agent1:                 episode reward: 0.8318,                 loss: nan
Episode: 3041/101000 (3.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7063s / 185.4735 s
agent0:                 episode reward: -0.4125,                 loss: 0.1499
agent1:                 episode reward: 0.4125,                 loss: nan
Episode: 3061/101000 (3.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9421s / 187.4156 s
agent0:                 episode reward: 0.0225,                 loss: 0.1457
agent1:                 episode reward: -0.0225,                 loss: nan
Episode: 3081/101000 (3.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4562s / 188.8718 s
agent0:                 episode reward: -0.2860,                 loss: 0.1440
agent1:                 episode reward: 0.2860,                 loss: nan
Episode: 3101/101000 (3.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1814s / 190.0532 s
agent0:                 episode reward: -0.5430,                 loss: 0.1401
agent1:                 episode reward: 0.5430,                 loss: nan
Episode: 3121/101000 (3.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5537s / 191.6069 s
agent0:                 episode reward: -0.0532,                 loss: 0.1391
agent1:                 episode reward: 0.0532,                 loss: nan
Episode: 3141/101000 (3.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2194s / 192.8263 s
agent0:                 episode reward: -0.2247,                 loss: 0.1492
agent1:                 episode reward: 0.2247,                 loss: nan
Episode: 3161/101000 (3.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4348s / 194.2610 s
agent0:                 episode reward: -0.0285,                 loss: 0.1536
agent1:                 episode reward: 0.0285,                 loss: nan
Episode: 3181/101000 (3.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6157s / 195.8768 s
agent0:                 episode reward: -0.7133,                 loss: 0.1510
agent1:                 episode reward: 0.7133,                 loss: nan
Episode: 3201/101000 (3.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7352s / 197.6120 s
agent0:                 episode reward: -0.2834,                 loss: 0.1469
agent1:                 episode reward: 0.2834,                 loss: nan
Episode: 3221/101000 (3.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4251s / 199.0371 s
agent0:                 episode reward: -0.3416,                 loss: 0.1458
agent1:                 episode reward: 0.3416,                 loss: nan
Episode: 3241/101000 (3.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7043s / 200.7414 s
agent0:                 episode reward: -0.5994,                 loss: 0.1427
agent1:                 episode reward: 0.5994,                 loss: nan
Episode: 3261/101000 (3.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5045s / 202.2460 s
agent0:                 episode reward: -0.6093,                 loss: 0.1433
agent1:                 episode reward: 0.6093,                 loss: nan
Episode: 3281/101000 (3.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6267s / 203.8727 s
agent0:                 episode reward: -0.2707,                 loss: 0.1402
agent1:                 episode reward: 0.2707,                 loss: nan
Episode: 3301/101000 (3.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7393s / 205.6120 s
agent0:                 episode reward: 0.0048,                 loss: 0.1401
agent1:                 episode reward: -0.0048,                 loss: nan
Episode: 3321/101000 (3.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5520s / 207.1640 s
agent0:                 episode reward: -0.6569,                 loss: 0.1379
agent1:                 episode reward: 0.6569,                 loss: nan
Episode: 3341/101000 (3.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 0.6740s / 207.8380 s
agent0:                 episode reward: 0.1427,                 loss: 0.1383
agent1:                 episode reward: -0.1427,                 loss: nan
Episode: 3361/101000 (3.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3754s / 209.2134 s
agent0:                 episode reward: -0.4517,                 loss: 0.1382
agent1:                 episode reward: 0.4517,                 loss: nan
Episode: 3381/101000 (3.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5576s / 210.7709 s
agent0:                 episode reward: -0.8424,                 loss: 0.1389
agent1:                 episode reward: 0.8424,                 loss: nan
Episode: 3401/101000 (3.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3558s / 212.1268 s
agent0:                 episode reward: -0.6450,                 loss: 0.1369
agent1:                 episode reward: 0.6450,                 loss: nan
Episode: 3421/101000 (3.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6412s / 213.7680 s
agent0:                 episode reward: -0.0019,                 loss: 0.1359
agent1:                 episode reward: 0.0019,                 loss: nan
Episode: 3441/101000 (3.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4782s / 215.2462 s
agent0:                 episode reward: -0.6528,                 loss: 0.1368
agent1:                 episode reward: 0.6528,                 loss: nan
Episode: 3461/101000 (3.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2604s / 216.5066 s
agent0:                 episode reward: -0.3129,                 loss: 0.1345
agent1:                 episode reward: 0.3129,                 loss: nan
Episode: 3481/101000 (3.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0906s / 217.5972 s
agent0:                 episode reward: -0.3957,                 loss: 0.1785
agent1:                 episode reward: 0.3957,                 loss: nan
Episode: 3501/101000 (3.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6182s / 219.2154 s
agent0:                 episode reward: -0.3052,                 loss: 0.1815
agent1:                 episode reward: 0.3052,                 loss: nan
Episode: 3521/101000 (3.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7028s / 220.9182 s
agent0:                 episode reward: 0.0246,                 loss: 0.1831
agent1:                 episode reward: -0.0246,                 loss: nan
Episode: 3541/101000 (3.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7518s / 222.6700 s
agent0:                 episode reward: -0.3224,                 loss: 0.1853
agent1:                 episode reward: 0.3224,                 loss: nan
Episode: 3561/101000 (3.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4347s / 224.1048 s
agent0:                 episode reward: -0.3420,                 loss: 0.1823
agent1:                 episode reward: 0.3420,                 loss: nan
Episode: 3581/101000 (3.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8159s / 225.9207 s
agent0:                 episode reward: -0.5024,                 loss: 0.1818
agent1:                 episode reward: 0.5024,                 loss: nan
Episode: 3601/101000 (3.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6053s / 227.5259 s
agent0:                 episode reward: -0.2208,                 loss: 0.1826
agent1:                 episode reward: 0.2208,                 loss: nan
Episode: 3621/101000 (3.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5954s / 229.1213 s
agent0:                 episode reward: -0.3312,                 loss: 0.1792
agent1:                 episode reward: 0.3312,                 loss: nan
Episode: 3641/101000 (3.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6648s / 230.7862 s
agent0:                 episode reward: -0.3644,                 loss: 0.1809
agent1:                 episode reward: 0.3644,                 loss: nan
Episode: 3661/101000 (3.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3522s / 232.1383 s
agent0:                 episode reward: -0.3825,                 loss: 0.1797
agent1:                 episode reward: 0.3825,                 loss: nan
Episode: 3681/101000 (3.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7192s / 233.8575 s
agent0:                 episode reward: -0.8421,                 loss: 0.1806
agent1:                 episode reward: 0.8421,                 loss: nan
Episode: 3701/101000 (3.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0402s / 234.8977 s
agent0:                 episode reward: -0.1593,                 loss: 0.1822
agent1:                 episode reward: 0.1593,                 loss: nan
Episode: 3721/101000 (3.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6078s / 236.5055 s
agent0:                 episode reward: -0.6059,                 loss: 0.1802
agent1:                 episode reward: 0.6059,                 loss: nan
Episode: 3741/101000 (3.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5230s / 238.0285 s
agent0:                 episode reward: 0.1515,                 loss: 0.1797
agent1:                 episode reward: -0.1515,                 loss: 0.1386
Score delta: 1.7277498395242978, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/3305_0.
Episode: 3761/101000 (3.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5239s / 239.5524 s
agent0:                 episode reward: 0.0609,                 loss: nan
agent1:                 episode reward: -0.0609,                 loss: 0.1396
Episode: 3781/101000 (3.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7366s / 241.2890 s
agent0:                 episode reward: -0.4748,                 loss: 0.1484
agent1:                 episode reward: 0.4748,                 loss: 0.1443
Score delta: 1.516757973227484, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/3340_1.
Episode: 3801/101000 (3.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5973s / 242.8863 s
agent0:                 episode reward: -0.7638,                 loss: 0.1474
agent1:                 episode reward: 0.7638,                 loss: nan
Episode: 3821/101000 (3.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6976s / 244.5839 s
agent0:                 episode reward: -0.3523,                 loss: 0.1464
agent1:                 episode reward: 0.3523,                 loss: nan
Episode: 3841/101000 (3.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4651s / 246.0490 s
agent0:                 episode reward: -0.6391,                 loss: 0.1693
agent1:                 episode reward: 0.6391,                 loss: nan
Episode: 3861/101000 (3.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7668s / 247.8157 s
agent0:                 episode reward: -0.1847,                 loss: 0.1912
agent1:                 episode reward: 0.1847,                 loss: nan
Episode: 3881/101000 (3.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8852s / 249.7010 s
agent0:                 episode reward: -0.5168,                 loss: 0.1871
agent1:                 episode reward: 0.5168,                 loss: nan
Episode: 3901/101000 (3.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8448s / 251.5457 s
agent0:                 episode reward: -0.1631,                 loss: 0.1889
agent1:                 episode reward: 0.1631,                 loss: nan
Episode: 3921/101000 (3.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9222s / 253.4680 s
agent0:                 episode reward: -0.5308,                 loss: 0.1876
agent1:                 episode reward: 0.5308,                 loss: nan
Episode: 3941/101000 (3.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6989s / 255.1669 s
agent0:                 episode reward: -0.7963,                 loss: 0.1875
agent1:                 episode reward: 0.7963,                 loss: nan
Episode: 3961/101000 (3.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6998s / 256.8667 s
agent0:                 episode reward: -0.4655,                 loss: 0.1882
agent1:                 episode reward: 0.4655,                 loss: nan
Episode: 3981/101000 (3.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6210s / 258.4877 s
agent0:                 episode reward: -0.2645,                 loss: 0.1872
agent1:                 episode reward: 0.2645,                 loss: nan
Episode: 4001/101000 (3.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3926s / 259.8803 s
agent0:                 episode reward: -0.0307,                 loss: 0.1862
agent1:                 episode reward: 0.0307,                 loss: nan
Episode: 4021/101000 (3.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2414s / 262.1216 s
agent0:                 episode reward: -0.3289,                 loss: 0.1873
agent1:                 episode reward: 0.3289,                 loss: nan
Episode: 4041/101000 (4.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5636s / 263.6853 s
agent0:                 episode reward: -0.0492,                 loss: 0.1861
agent1:                 episode reward: 0.0492,                 loss: nan
Episode: 4061/101000 (4.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6822s / 265.3675 s
agent0:                 episode reward: -0.4584,                 loss: 0.1855
agent1:                 episode reward: 0.4584,                 loss: nan
Episode: 4081/101000 (4.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4936s / 266.8611 s
agent0:                 episode reward: -0.5276,                 loss: 0.1841
agent1:                 episode reward: 0.5276,                 loss: nan
Episode: 4101/101000 (4.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6314s / 268.4925 s
agent0:                 episode reward: -0.0050,                 loss: 0.1873
agent1:                 episode reward: 0.0050,                 loss: nan
Episode: 4121/101000 (4.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3751s / 269.8676 s
agent0:                 episode reward: -0.3231,                 loss: 0.1866
agent1:                 episode reward: 0.3231,                 loss: nan
Episode: 4141/101000 (4.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8337s / 271.7013 s
agent0:                 episode reward: -0.2729,                 loss: 0.1875
agent1:                 episode reward: 0.2729,                 loss: nan
Episode: 4161/101000 (4.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8330s / 273.5342 s
agent0:                 episode reward: -0.4348,                 loss: 0.1862
agent1:                 episode reward: 0.4348,                 loss: nan
Episode: 4181/101000 (4.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9473s / 275.4815 s
agent0:                 episode reward: -0.8012,                 loss: 0.1974
agent1:                 episode reward: 0.8012,                 loss: nan
Episode: 4201/101000 (4.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6728s / 277.1544 s
agent0:                 episode reward: 0.0887,                 loss: 0.2009
agent1:                 episode reward: -0.0887,                 loss: nan
Episode: 4221/101000 (4.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1948s / 279.3492 s
agent0:                 episode reward: -0.5044,                 loss: 0.2004
agent1:                 episode reward: 0.5044,                 loss: nan
Episode: 4241/101000 (4.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7986s / 281.1478 s
agent0:                 episode reward: 0.2829,                 loss: 0.1994
agent1:                 episode reward: -0.2829,                 loss: 0.1427
Score delta: 1.6821824456856416, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/3811_0.
Episode: 4261/101000 (4.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6034s / 282.7511 s
agent0:                 episode reward: -0.1788,                 loss: nan
agent1:                 episode reward: 0.1788,                 loss: 0.1445
Episode: 4281/101000 (4.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6885s / 284.4396 s
agent0:                 episode reward: -0.0192,                 loss: nan
agent1:                 episode reward: 0.0192,                 loss: 0.1475
Episode: 4301/101000 (4.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6874s / 286.1270 s
agent0:                 episode reward: -0.0585,                 loss: nan
agent1:                 episode reward: 0.0585,                 loss: 0.1493
Episode: 4321/101000 (4.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4776s / 287.6047 s
agent0:                 episode reward: -0.2072,                 loss: nan
agent1:                 episode reward: 0.2072,                 loss: 0.1501
Episode: 4341/101000 (4.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7898s / 289.3945 s
agent0:                 episode reward: 0.0131,                 loss: nan
agent1:                 episode reward: -0.0131,                 loss: 0.1512
Episode: 4361/101000 (4.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6588s / 291.0533 s
agent0:                 episode reward: 0.0112,                 loss: nan
agent1:                 episode reward: -0.0112,                 loss: 0.1535
Episode: 4381/101000 (4.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9353s / 292.9886 s
agent0:                 episode reward: 0.2177,                 loss: nan
agent1:                 episode reward: -0.2177,                 loss: 0.1547
Episode: 4401/101000 (4.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9266s / 294.9152 s
agent0:                 episode reward: -0.2532,                 loss: nan
agent1:                 episode reward: 0.2532,                 loss: 0.1562
Episode: 4421/101000 (4.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8646s / 296.7798 s
agent0:                 episode reward: -0.4020,                 loss: nan
agent1:                 episode reward: 0.4020,                 loss: 0.1596
Episode: 4441/101000 (4.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6707s / 298.4505 s
agent0:                 episode reward: -0.3103,                 loss: 0.1979
agent1:                 episode reward: 0.3103,                 loss: 0.1604
Score delta: 1.932850166799416, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/4013_1.
Episode: 4461/101000 (4.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6857s / 300.1362 s
agent0:                 episode reward: -0.3066,                 loss: 0.2026
agent1:                 episode reward: 0.3066,                 loss: nan
Episode: 4481/101000 (4.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6925s / 302.8287 s
agent0:                 episode reward: -0.1390,                 loss: 0.1993
agent1:                 episode reward: 0.1390,                 loss: nan
Episode: 4501/101000 (4.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7087s / 304.5374 s
agent0:                 episode reward: -0.2195,                 loss: 0.1999
agent1:                 episode reward: 0.2195,                 loss: nan
Episode: 4521/101000 (4.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4947s / 306.0320 s
agent0:                 episode reward: 0.0794,                 loss: 0.2001
agent1:                 episode reward: -0.0794,                 loss: nan
Episode: 4541/101000 (4.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1980s / 308.2301 s
agent0:                 episode reward: -0.4470,                 loss: 0.1994
agent1:                 episode reward: 0.4470,                 loss: nan
Episode: 4561/101000 (4.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9800s / 310.2101 s
agent0:                 episode reward: -0.6866,                 loss: 0.1985
agent1:                 episode reward: 0.6866,                 loss: nan
Episode: 4581/101000 (4.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0174s / 312.2275 s
agent0:                 episode reward: -0.1203,                 loss: 0.2004
agent1:                 episode reward: 0.1203,                 loss: nan
Episode: 4601/101000 (4.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1366s / 314.3641 s
agent0:                 episode reward: -0.1295,                 loss: 0.1988
agent1:                 episode reward: 0.1295,                 loss: nan
Episode: 4621/101000 (4.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8206s / 316.1847 s
agent0:                 episode reward: -0.2545,                 loss: 0.1988
agent1:                 episode reward: 0.2545,                 loss: nan
Episode: 4641/101000 (4.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7223s / 317.9070 s
agent0:                 episode reward: -0.3530,                 loss: 0.1998
agent1:                 episode reward: 0.3530,                 loss: nan
Episode: 4661/101000 (4.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7577s / 319.6647 s
agent0:                 episode reward: -0.1665,                 loss: 0.1989
agent1:                 episode reward: 0.1665,                 loss: nan
Episode: 4681/101000 (4.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7085s / 321.3732 s
agent0:                 episode reward: -0.5882,                 loss: 0.1994
agent1:                 episode reward: 0.5882,                 loss: nan
Episode: 4701/101000 (4.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1045s / 323.4777 s
agent0:                 episode reward: -0.3889,                 loss: 0.2008
agent1:                 episode reward: 0.3889,                 loss: nan
Episode: 4721/101000 (4.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0895s / 325.5672 s
agent0:                 episode reward: -0.2089,                 loss: 0.2058
agent1:                 episode reward: 0.2089,                 loss: nan
Episode: 4741/101000 (4.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9910s / 327.5582 s
agent0:                 episode reward: -0.5162,                 loss: 0.2026
agent1:                 episode reward: 0.5162,                 loss: nan
Episode: 4761/101000 (4.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6833s / 329.2415 s
agent0:                 episode reward: -0.3700,                 loss: 0.2072
agent1:                 episode reward: 0.3700,                 loss: nan
Episode: 4781/101000 (4.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6201s / 330.8616 s
agent0:                 episode reward: -0.5949,                 loss: 0.2043
agent1:                 episode reward: 0.5949,                 loss: nan
Episode: 4801/101000 (4.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7825s / 332.6442 s
agent0:                 episode reward: -0.3615,                 loss: 0.2057
agent1:                 episode reward: 0.3615,                 loss: nan
Episode: 4821/101000 (4.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7895s / 334.4337 s
agent0:                 episode reward: -0.3849,                 loss: 0.2049
agent1:                 episode reward: 0.3849,                 loss: nan
Episode: 4841/101000 (4.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9782s / 336.4118 s
agent0:                 episode reward: -0.4079,                 loss: 0.2033
agent1:                 episode reward: 0.4079,                 loss: nan
Episode: 4861/101000 (4.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8439s / 338.2557 s
agent0:                 episode reward: -0.5604,                 loss: 0.2020
agent1:                 episode reward: 0.5604,                 loss: nan
Episode: 4881/101000 (4.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6539s / 339.9096 s
agent0:                 episode reward: 0.0863,                 loss: 0.2038
agent1:                 episode reward: -0.0863,                 loss: nan
Episode: 4901/101000 (4.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9744s / 341.8840 s
agent0:                 episode reward: -0.0101,                 loss: 0.2060
agent1:                 episode reward: 0.0101,                 loss: nan
Episode: 4921/101000 (4.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6507s / 343.5347 s
agent0:                 episode reward: -0.3169,                 loss: 0.2010
agent1:                 episode reward: 0.3169,                 loss: nan
Episode: 4941/101000 (4.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8508s / 345.3855 s
agent0:                 episode reward: -0.2026,                 loss: 0.2010
agent1:                 episode reward: 0.2026,                 loss: nan
Episode: 4961/101000 (4.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7152s / 347.1007 s
agent0:                 episode reward: -0.6712,                 loss: 0.2021
agent1:                 episode reward: 0.6712,                 loss: nan
Episode: 4981/101000 (4.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5969s / 348.6976 s
agent0:                 episode reward: -0.1914,                 loss: 0.2036
agent1:                 episode reward: 0.1914,                 loss: nan
Episode: 5001/101000 (4.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9267s / 350.6243 s
agent0:                 episode reward: -0.5307,                 loss: 0.2016
agent1:                 episode reward: 0.5307,                 loss: nan
Episode: 5021/101000 (4.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1407s / 352.7650 s
agent0:                 episode reward: -0.3284,                 loss: 0.2033
agent1:                 episode reward: 0.3284,                 loss: nan
Episode: 5041/101000 (4.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3160s / 354.0809 s
agent0:                 episode reward: -0.4160,                 loss: 0.2087
agent1:                 episode reward: 0.4160,                 loss: nan
Episode: 5061/101000 (5.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9498s / 356.0307 s
agent0:                 episode reward: -0.4547,                 loss: 0.2182
agent1:                 episode reward: 0.4547,                 loss: nan
Episode: 5081/101000 (5.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4253s / 357.4560 s
agent0:                 episode reward: -0.4842,                 loss: 0.2187
agent1:                 episode reward: 0.4842,                 loss: nan
Episode: 5101/101000 (5.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9126s / 359.3687 s
agent0:                 episode reward: -0.4021,                 loss: 0.2177
agent1:                 episode reward: 0.4021,                 loss: nan
Episode: 5121/101000 (5.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7333s / 361.1020 s
agent0:                 episode reward: -0.4470,                 loss: 0.2186
agent1:                 episode reward: 0.4470,                 loss: nan
Episode: 5141/101000 (5.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9951s / 363.0971 s
agent0:                 episode reward: -0.5674,                 loss: 0.2168
agent1:                 episode reward: 0.5674,                 loss: nan
Episode: 5161/101000 (5.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8676s / 364.9647 s
agent0:                 episode reward: -0.1392,                 loss: 0.2189
agent1:                 episode reward: 0.1392,                 loss: nan
Episode: 5181/101000 (5.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7659s / 366.7307 s
agent0:                 episode reward: 0.2616,                 loss: 0.2136
agent1:                 episode reward: -0.2616,                 loss: nan
Episode: 5201/101000 (5.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4667s / 368.1973 s
agent0:                 episode reward: 0.2487,                 loss: 0.2123
agent1:                 episode reward: -0.2487,                 loss: 0.1554
Score delta: 1.650075839919856, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/4756_0.
Episode: 5221/101000 (5.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5393s / 369.7366 s
agent0:                 episode reward: -0.6183,                 loss: 0.1468
agent1:                 episode reward: 0.6183,                 loss: 0.1537
Score delta: 1.5126934516451311, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/4789_1.
Episode: 5241/101000 (5.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7475s / 371.4841 s
agent0:                 episode reward: -0.6155,                 loss: 0.1483
agent1:                 episode reward: 0.6155,                 loss: nan
Episode: 5261/101000 (5.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6967s / 373.1808 s
agent0:                 episode reward: -0.7202,                 loss: 0.1477
agent1:                 episode reward: 0.7202,                 loss: nan
Episode: 5281/101000 (5.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7621s / 374.9429 s
agent0:                 episode reward: 0.2258,                 loss: 0.1466
agent1:                 episode reward: -0.2258,                 loss: nan
Episode: 5301/101000 (5.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8833s / 376.8262 s
agent0:                 episode reward: -0.3721,                 loss: 0.1469
agent1:                 episode reward: 0.3721,                 loss: nan
Episode: 5321/101000 (5.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4749s / 378.3011 s
agent0:                 episode reward: -0.3609,                 loss: 0.1443
agent1:                 episode reward: 0.3609,                 loss: nan
Episode: 5341/101000 (5.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8450s / 380.1461 s
agent0:                 episode reward: -0.1583,                 loss: 0.1454
agent1:                 episode reward: 0.1583,                 loss: nan
Episode: 5361/101000 (5.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2173s / 382.3635 s
agent0:                 episode reward: -0.2812,                 loss: 0.1448
agent1:                 episode reward: 0.2812,                 loss: nan
Episode: 5381/101000 (5.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6395s / 384.0029 s
agent0:                 episode reward: -0.4052,                 loss: 0.1449
agent1:                 episode reward: 0.4052,                 loss: nan
Episode: 5401/101000 (5.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7136s / 385.7165 s
agent0:                 episode reward: -0.7592,                 loss: 0.1486
agent1:                 episode reward: 0.7592,                 loss: nan
Episode: 5421/101000 (5.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7759s / 387.4924 s
agent0:                 episode reward: -0.5198,                 loss: 0.1987
agent1:                 episode reward: 0.5198,                 loss: nan
Episode: 5441/101000 (5.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5516s / 389.0440 s
agent0:                 episode reward: -0.7417,                 loss: 0.1968
agent1:                 episode reward: 0.7417,                 loss: nan
Episode: 5461/101000 (5.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6545s / 390.6985 s
agent0:                 episode reward: -0.3905,                 loss: 0.1959
agent1:                 episode reward: 0.3905,                 loss: nan
Episode: 5481/101000 (5.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6202s / 392.3187 s
agent0:                 episode reward: -0.1396,                 loss: 0.1974
agent1:                 episode reward: 0.1396,                 loss: nan
Episode: 5501/101000 (5.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3988s / 393.7175 s
agent0:                 episode reward: -0.9435,                 loss: 0.1956
agent1:                 episode reward: 0.9435,                 loss: nan
Episode: 5521/101000 (5.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6160s / 395.3335 s
agent0:                 episode reward: -0.1680,                 loss: 0.1961
agent1:                 episode reward: 0.1680,                 loss: nan
Episode: 5541/101000 (5.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8715s / 397.2050 s
agent0:                 episode reward: 0.1316,                 loss: 0.1955
agent1:                 episode reward: -0.1316,                 loss: nan
Episode: 5561/101000 (5.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4964s / 398.7015 s
agent0:                 episode reward: -0.1519,                 loss: 0.1954
agent1:                 episode reward: 0.1519,                 loss: nan
Episode: 5581/101000 (5.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0058s / 400.7072 s
agent0:                 episode reward: -0.3630,                 loss: 0.1977
agent1:                 episode reward: 0.3630,                 loss: nan
Episode: 5601/101000 (5.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4295s / 402.1367 s
agent0:                 episode reward: -0.1467,                 loss: 0.1958
agent1:                 episode reward: 0.1467,                 loss: nan
Episode: 5621/101000 (5.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6445s / 403.7813 s
agent0:                 episode reward: -0.1601,                 loss: 0.1956
agent1:                 episode reward: 0.1601,                 loss: 0.1529
Score delta: 1.5078116839998554, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/5180_0.
Episode: 5641/101000 (5.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9962s / 405.7775 s
agent0:                 episode reward: -0.1153,                 loss: nan
agent1:                 episode reward: 0.1153,                 loss: 0.1648
Episode: 5661/101000 (5.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9238s / 407.7013 s
agent0:                 episode reward: -0.0783,                 loss: nan
agent1:                 episode reward: 0.0783,                 loss: 0.1747
Episode: 5681/101000 (5.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8266s / 409.5279 s
agent0:                 episode reward: -0.1516,                 loss: 0.2018
agent1:                 episode reward: 0.1516,                 loss: 0.1781
Score delta: 1.6385205553840236, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/5245_1.
Episode: 5701/101000 (5.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9356s / 411.4634 s
agent0:                 episode reward: -0.7932,                 loss: 0.1982
agent1:                 episode reward: 0.7932,                 loss: nan
Episode: 5721/101000 (5.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8063s / 413.2697 s
agent0:                 episode reward: -0.6816,                 loss: 0.2006
agent1:                 episode reward: 0.6816,                 loss: nan
Episode: 5741/101000 (5.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3383s / 414.6080 s
agent0:                 episode reward: -0.2584,                 loss: 0.2021
agent1:                 episode reward: 0.2584,                 loss: nan
Episode: 5761/101000 (5.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0241s / 416.6321 s
agent0:                 episode reward: 0.0024,                 loss: 0.2000
agent1:                 episode reward: -0.0024,                 loss: nan
Episode: 5781/101000 (5.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1029s / 418.7350 s
agent0:                 episode reward: -0.3986,                 loss: 0.2012
agent1:                 episode reward: 0.3986,                 loss: nan
Episode: 5801/101000 (5.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8706s / 420.6056 s
agent0:                 episode reward: 0.1058,                 loss: 0.2058
agent1:                 episode reward: -0.1058,                 loss: nan
Episode: 5821/101000 (5.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3091s / 421.9147 s
agent0:                 episode reward: -0.0940,                 loss: 0.2171
agent1:                 episode reward: 0.0940,                 loss: nan
Episode: 5841/101000 (5.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1530s / 423.0678 s
agent0:                 episode reward: -0.2743,                 loss: 0.2175
agent1:                 episode reward: 0.2743,                 loss: nan
Episode: 5861/101000 (5.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6701s / 424.7379 s
agent0:                 episode reward: 0.1138,                 loss: 0.2145
agent1:                 episode reward: -0.1138,                 loss: nan
Episode: 5881/101000 (5.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6845s / 426.4224 s
agent0:                 episode reward: -0.1218,                 loss: 0.2174
agent1:                 episode reward: 0.1218,                 loss: nan
Episode: 5901/101000 (5.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5510s / 427.9734 s
agent0:                 episode reward: -0.4081,                 loss: 0.2171
agent1:                 episode reward: 0.4081,                 loss: nan
Episode: 5921/101000 (5.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3830s / 429.3564 s
agent0:                 episode reward: -0.1572,                 loss: 0.2160
agent1:                 episode reward: 0.1572,                 loss: nan
Episode: 5941/101000 (5.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7907s / 431.1471 s
agent0:                 episode reward: -0.9772,                 loss: 0.2147
agent1:                 episode reward: 0.9772,                 loss: nan
Episode: 5961/101000 (5.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6555s / 432.8026 s
agent0:                 episode reward: -0.6300,                 loss: 0.2197
agent1:                 episode reward: 0.6300,                 loss: nan
Episode: 5981/101000 (5.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7529s / 434.5555 s
agent0:                 episode reward: -0.1183,                 loss: 0.2214
agent1:                 episode reward: 0.1183,                 loss: nan
Episode: 6001/101000 (5.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4354s / 435.9909 s
agent0:                 episode reward: -0.5817,                 loss: 0.2206
agent1:                 episode reward: 0.5817,                 loss: nan
Episode: 6021/101000 (5.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0073s / 437.9983 s
agent0:                 episode reward: -0.4917,                 loss: 0.2180
agent1:                 episode reward: 0.4917,                 loss: nan
Episode: 6041/101000 (5.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4197s / 439.4179 s
agent0:                 episode reward: -0.7860,                 loss: 0.2167
agent1:                 episode reward: 0.7860,                 loss: nan
Episode: 6061/101000 (6.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2547s / 440.6726 s
agent0:                 episode reward: -0.1696,                 loss: 0.2188
agent1:                 episode reward: 0.1696,                 loss: nan
Episode: 6081/101000 (6.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9180s / 442.5906 s
agent0:                 episode reward: -0.6643,                 loss: 0.2185
agent1:                 episode reward: 0.6643,                 loss: nan
Episode: 6101/101000 (6.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6874s / 444.2781 s
agent0:                 episode reward: -0.5455,                 loss: 0.2198
agent1:                 episode reward: 0.5455,                 loss: nan
Episode: 6121/101000 (6.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7725s / 446.0506 s
agent0:                 episode reward: -0.5440,                 loss: 0.2151
agent1:                 episode reward: 0.5440,                 loss: nan
Episode: 6141/101000 (6.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5892s / 447.6398 s
agent0:                 episode reward: -0.3863,                 loss: 0.2239
agent1:                 episode reward: 0.3863,                 loss: nan
Episode: 6161/101000 (6.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6298s / 449.2696 s
agent0:                 episode reward: -0.1724,                 loss: 0.2306
agent1:                 episode reward: 0.1724,                 loss: nan
Episode: 6181/101000 (6.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7500s / 451.0195 s
agent0:                 episode reward: -0.5267,                 loss: 0.2313
agent1:                 episode reward: 0.5267,                 loss: nan
Episode: 6201/101000 (6.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9422s / 452.9618 s
agent0:                 episode reward: -0.5337,                 loss: 0.2316
agent1:                 episode reward: 0.5337,                 loss: nan
Episode: 6221/101000 (6.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7515s / 454.7132 s
agent0:                 episode reward: -0.4990,                 loss: 0.2321
agent1:                 episode reward: 0.4990,                 loss: nan
Episode: 6241/101000 (6.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4888s / 456.2020 s
agent0:                 episode reward: -0.5622,                 loss: 0.2290
agent1:                 episode reward: 0.5622,                 loss: nan
Episode: 6261/101000 (6.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8823s / 458.0843 s
agent0:                 episode reward: -0.2023,                 loss: 0.2318
agent1:                 episode reward: 0.2023,                 loss: nan
Episode: 6281/101000 (6.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7161s / 459.8004 s
agent0:                 episode reward: -0.0085,                 loss: 0.2322
agent1:                 episode reward: 0.0085,                 loss: nan
Episode: 6301/101000 (6.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4880s / 461.2884 s
agent0:                 episode reward: -0.7777,                 loss: 0.2332
agent1:                 episode reward: 0.7777,                 loss: nan
Episode: 6321/101000 (6.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1651s / 463.4535 s
agent0:                 episode reward: -0.5321,                 loss: 0.2299
agent1:                 episode reward: 0.5321,                 loss: nan
Episode: 6341/101000 (6.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4717s / 464.9252 s
agent0:                 episode reward: -0.4995,                 loss: 0.2327
agent1:                 episode reward: 0.4995,                 loss: nan
Episode: 6361/101000 (6.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6950s / 466.6202 s
agent0:                 episode reward: -0.2111,                 loss: 0.2308
agent1:                 episode reward: 0.2111,                 loss: nan
Episode: 6381/101000 (6.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8829s / 468.5031 s
agent0:                 episode reward: -0.8142,                 loss: 0.2310
agent1:                 episode reward: 0.8142,                 loss: nan
Episode: 6401/101000 (6.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0808s / 470.5839 s
agent0:                 episode reward: -0.1786,                 loss: 0.2311
agent1:                 episode reward: 0.1786,                 loss: nan
Episode: 6421/101000 (6.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0919s / 472.6759 s
agent0:                 episode reward: 0.1181,                 loss: 0.2327
agent1:                 episode reward: -0.1181,                 loss: nan
Episode: 6441/101000 (6.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7055s / 474.3814 s
agent0:                 episode reward: -0.4729,                 loss: 0.2339
agent1:                 episode reward: 0.4729,                 loss: nan
Episode: 6461/101000 (6.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9480s / 476.3293 s
agent0:                 episode reward: -0.2744,                 loss: 0.2304
agent1:                 episode reward: 0.2744,                 loss: nan
Episode: 6481/101000 (6.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8998s / 478.2291 s
agent0:                 episode reward: -0.4959,                 loss: 0.2300
agent1:                 episode reward: 0.4959,                 loss: nan
Episode: 6501/101000 (6.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5238s / 479.7529 s
agent0:                 episode reward: -0.3165,                 loss: 0.2314
agent1:                 episode reward: 0.3165,                 loss: nan
Episode: 6521/101000 (6.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8667s / 481.6196 s
agent0:                 episode reward: 0.1383,                 loss: 0.2321
agent1:                 episode reward: -0.1383,                 loss: nan
Episode: 6541/101000 (6.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8486s / 483.4682 s
agent0:                 episode reward: -0.1664,                 loss: 0.2310
agent1:                 episode reward: 0.1664,                 loss: nan
Episode: 6561/101000 (6.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3330s / 485.8012 s
agent0:                 episode reward: -0.5102,                 loss: 0.2308
agent1:                 episode reward: 0.5102,                 loss: nan
Episode: 6581/101000 (6.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6517s / 487.4529 s
agent0:                 episode reward: -0.3842,                 loss: 0.2294
agent1:                 episode reward: 0.3842,                 loss: nan
Episode: 6601/101000 (6.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5724s / 489.0253 s
agent0:                 episode reward: -1.0030,                 loss: 0.2281
agent1:                 episode reward: 1.0030,                 loss: nan
Episode: 6621/101000 (6.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6414s / 490.6666 s
agent0:                 episode reward: -0.4476,                 loss: 0.2298
agent1:                 episode reward: 0.4476,                 loss: nan
Episode: 6641/101000 (6.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9008s / 492.5674 s
agent0:                 episode reward: -0.5612,                 loss: 0.2304
agent1:                 episode reward: 0.5612,                 loss: nan
Episode: 6661/101000 (6.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8625s / 494.4299 s
agent0:                 episode reward: 0.0463,                 loss: 0.2326
agent1:                 episode reward: -0.0463,                 loss: nan
Episode: 6681/101000 (6.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8562s / 496.2861 s
agent0:                 episode reward: -0.3433,                 loss: 0.2318
agent1:                 episode reward: 0.3433,                 loss: nan
Episode: 6701/101000 (6.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8232s / 498.1094 s
agent0:                 episode reward: -0.5826,                 loss: 0.2314
agent1:                 episode reward: 0.5826,                 loss: nan
Episode: 6721/101000 (6.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3455s / 499.4548 s
agent0:                 episode reward: -0.3898,                 loss: 0.2325
agent1:                 episode reward: 0.3898,                 loss: nan
Episode: 6741/101000 (6.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8934s / 501.3482 s
agent0:                 episode reward: -0.0224,                 loss: 0.2329
agent1:                 episode reward: 0.0224,                 loss: nan
Episode: 6761/101000 (6.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9735s / 503.3217 s
agent0:                 episode reward: -0.1177,                 loss: 0.2342
agent1:                 episode reward: 0.1177,                 loss: nan
Episode: 6781/101000 (6.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4474s / 504.7691 s
agent0:                 episode reward: -0.4499,                 loss: 0.2296
agent1:                 episode reward: 0.4499,                 loss: nan
Episode: 6801/101000 (6.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3645s / 506.1336 s
agent0:                 episode reward: -0.0210,                 loss: 0.2328
agent1:                 episode reward: 0.0210,                 loss: nan
Episode: 6821/101000 (6.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7951s / 507.9287 s
agent0:                 episode reward: -0.5522,                 loss: 0.2345
agent1:                 episode reward: 0.5522,                 loss: nan
Episode: 6841/101000 (6.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5012s / 509.4298 s
agent0:                 episode reward: -0.8000,                 loss: 0.2335
agent1:                 episode reward: 0.8000,                 loss: nan
Episode: 6861/101000 (6.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5149s / 510.9448 s
agent0:                 episode reward: -0.4800,                 loss: 0.2334
agent1:                 episode reward: 0.4800,                 loss: nan
Episode: 6881/101000 (6.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1502s / 513.0950 s
agent0:                 episode reward: 0.1593,                 loss: 0.2307
agent1:                 episode reward: -0.1593,                 loss: nan
Episode: 6901/101000 (6.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0392s / 515.1341 s
agent0:                 episode reward: 0.0356,                 loss: 0.2334
agent1:                 episode reward: -0.0356,                 loss: nan
Episode: 6921/101000 (6.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8560s / 516.9901 s
agent0:                 episode reward: -0.1005,                 loss: 0.2347
agent1:                 episode reward: 0.1005,                 loss: nan
Episode: 6941/101000 (6.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9512s / 518.9414 s
agent0:                 episode reward: 0.0285,                 loss: 0.2351
agent1:                 episode reward: -0.0285,                 loss: nan
Episode: 6961/101000 (6.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8548s / 520.7962 s
agent0:                 episode reward: -0.3331,                 loss: 0.2356
agent1:                 episode reward: 0.3331,                 loss: nan
Episode: 6981/101000 (6.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7977s / 522.5939 s
agent0:                 episode reward: -0.5079,                 loss: 0.2328
agent1:                 episode reward: 0.5079,                 loss: nan
Episode: 7001/101000 (6.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1169s / 524.7108 s
agent0:                 episode reward: -0.5669,                 loss: 0.2342
agent1:                 episode reward: 0.5669,                 loss: nan
Episode: 7021/101000 (6.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0123s / 526.7231 s
agent0:                 episode reward: -0.3026,                 loss: 0.2343
agent1:                 episode reward: 0.3026,                 loss: nan
Episode: 7041/101000 (6.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8614s / 528.5845 s
agent0:                 episode reward: -0.4351,                 loss: 0.2336
agent1:                 episode reward: 0.4351,                 loss: nan
Episode: 7061/101000 (6.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6464s / 530.2310 s
agent0:                 episode reward: -0.3490,                 loss: 0.2360
agent1:                 episode reward: 0.3490,                 loss: nan
Episode: 7081/101000 (7.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7798s / 532.0107 s
agent0:                 episode reward: -0.0747,                 loss: 0.2356
agent1:                 episode reward: 0.0747,                 loss: nan
Episode: 7101/101000 (7.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9068s / 533.9176 s
agent0:                 episode reward: -0.0068,                 loss: 0.2353
agent1:                 episode reward: 0.0068,                 loss: nan
Episode: 7121/101000 (7.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0071s / 535.9247 s
agent0:                 episode reward: -0.1817,                 loss: 0.2346
agent1:                 episode reward: 0.1817,                 loss: nan
Episode: 7141/101000 (7.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9167s / 537.8414 s
agent0:                 episode reward: -1.0991,                 loss: 0.2347
agent1:                 episode reward: 1.0991,                 loss: nan
Episode: 7161/101000 (7.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8329s / 539.6743 s
agent0:                 episode reward: 0.3383,                 loss: 0.2360
agent1:                 episode reward: -0.3383,                 loss: 0.1678
Score delta: 1.691291483196407, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/6728_0.
Episode: 7181/101000 (7.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8308s / 541.5051 s
agent0:                 episode reward: -0.7338,                 loss: 0.2069
agent1:                 episode reward: 0.7338,                 loss: 0.1678
Score delta: 1.5776714230452389, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/6753_1.
Episode: 7201/101000 (7.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5412s / 543.0463 s
agent0:                 episode reward: 0.0353,                 loss: 0.2057
agent1:                 episode reward: -0.0353,                 loss: nan
Episode: 7221/101000 (7.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0270s / 545.0733 s
agent0:                 episode reward: -0.4152,                 loss: 0.2063
agent1:                 episode reward: 0.4152,                 loss: nan
Episode: 7241/101000 (7.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6698s / 546.7431 s
agent0:                 episode reward: -0.3481,                 loss: 0.2072
agent1:                 episode reward: 0.3481,                 loss: nan
Episode: 7261/101000 (7.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5207s / 548.2637 s
agent0:                 episode reward: -0.4286,                 loss: 0.2097
agent1:                 episode reward: 0.4286,                 loss: nan
Episode: 7281/101000 (7.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6069s / 549.8707 s
agent0:                 episode reward: 0.0901,                 loss: 0.2080
agent1:                 episode reward: -0.0901,                 loss: nan
Episode: 7301/101000 (7.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3433s / 552.2139 s
agent0:                 episode reward: -0.4951,                 loss: 0.2062
agent1:                 episode reward: 0.4951,                 loss: nan
Episode: 7321/101000 (7.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6701s / 553.8841 s
agent0:                 episode reward: -0.1142,                 loss: 0.2098
agent1:                 episode reward: 0.1142,                 loss: nan
Episode: 7341/101000 (7.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0597s / 555.9438 s
agent0:                 episode reward: -0.1662,                 loss: 0.2088
agent1:                 episode reward: 0.1662,                 loss: nan
Episode: 7361/101000 (7.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8251s / 557.7689 s
agent0:                 episode reward: -0.4875,                 loss: 0.2077
agent1:                 episode reward: 0.4875,                 loss: nan
Episode: 7381/101000 (7.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6449s / 559.4138 s
agent0:                 episode reward: -0.4038,                 loss: 0.2076
agent1:                 episode reward: 0.4038,                 loss: nan
Episode: 7401/101000 (7.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2789s / 561.6927 s
agent0:                 episode reward: -0.3366,                 loss: 0.2092
agent1:                 episode reward: 0.3366,                 loss: nan
Episode: 7421/101000 (7.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3184s / 564.0111 s
agent0:                 episode reward: -0.2550,                 loss: 0.2105
agent1:                 episode reward: 0.2550,                 loss: nan
Episode: 7441/101000 (7.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3599s / 566.3710 s
agent0:                 episode reward: -0.0885,                 loss: 0.2094
agent1:                 episode reward: 0.0885,                 loss: nan
Episode: 7461/101000 (7.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9586s / 568.3296 s
agent0:                 episode reward: -0.8525,                 loss: 0.2082
agent1:                 episode reward: 0.8525,                 loss: nan
Episode: 7481/101000 (7.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9884s / 570.3180 s
agent0:                 episode reward: -0.3304,                 loss: 0.2099
agent1:                 episode reward: 0.3304,                 loss: nan
Episode: 7501/101000 (7.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8401s / 572.1581 s
agent0:                 episode reward: -0.0703,                 loss: 0.2118
agent1:                 episode reward: 0.0703,                 loss: nan
Episode: 7521/101000 (7.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5809s / 573.7390 s
agent0:                 episode reward: -0.7405,                 loss: 0.2147
agent1:                 episode reward: 0.7405,                 loss: nan
Episode: 7541/101000 (7.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9380s / 575.6770 s
agent0:                 episode reward: -0.4268,                 loss: 0.2165
agent1:                 episode reward: 0.4268,                 loss: nan
Episode: 7561/101000 (7.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0065s / 577.6835 s
agent0:                 episode reward: -0.4116,                 loss: 0.2154
agent1:                 episode reward: 0.4116,                 loss: nan
Episode: 7581/101000 (7.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9475s / 579.6309 s
agent0:                 episode reward: -0.2700,                 loss: 0.2147
agent1:                 episode reward: 0.2700,                 loss: nan
Episode: 7601/101000 (7.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6308s / 581.2618 s
agent0:                 episode reward: -0.8694,                 loss: 0.2149
agent1:                 episode reward: 0.8694,                 loss: nan
Episode: 7621/101000 (7.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8593s / 583.1210 s
agent0:                 episode reward: -0.2835,                 loss: 0.2131
agent1:                 episode reward: 0.2835,                 loss: nan
Episode: 7641/101000 (7.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6280s / 584.7490 s
agent0:                 episode reward: -0.7469,                 loss: 0.2112
agent1:                 episode reward: 0.7469,                 loss: nan
Episode: 7661/101000 (7.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9486s / 586.6976 s
agent0:                 episode reward: -0.2998,                 loss: 0.2115
agent1:                 episode reward: 0.2998,                 loss: nan
Episode: 7681/101000 (7.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4854s / 589.1831 s
agent0:                 episode reward: -0.5894,                 loss: 0.2121
agent1:                 episode reward: 0.5894,                 loss: nan
Episode: 7701/101000 (7.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9356s / 591.1187 s
agent0:                 episode reward: -0.1792,                 loss: 0.2140
agent1:                 episode reward: 0.1792,                 loss: nan
Episode: 7721/101000 (7.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0952s / 593.2139 s
agent0:                 episode reward: -0.1787,                 loss: 0.2150
agent1:                 episode reward: 0.1787,                 loss: nan
Episode: 7741/101000 (7.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5598s / 594.7738 s
agent0:                 episode reward: -0.0482,                 loss: 0.2153
agent1:                 episode reward: 0.0482,                 loss: nan
Episode: 7761/101000 (7.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8380s / 596.6118 s
agent0:                 episode reward: -0.1891,                 loss: 0.2176
agent1:                 episode reward: 0.1891,                 loss: nan
Episode: 7781/101000 (7.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6383s / 598.2501 s
agent0:                 episode reward: -0.3196,                 loss: 0.2156
agent1:                 episode reward: 0.3196,                 loss: nan
Episode: 7801/101000 (7.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6248s / 599.8749 s
agent0:                 episode reward: -0.3287,                 loss: 0.2151
agent1:                 episode reward: 0.3287,                 loss: nan
Episode: 7821/101000 (7.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7639s / 601.6388 s
agent0:                 episode reward: -0.0795,                 loss: 0.2140
agent1:                 episode reward: 0.0795,                 loss: nan
Episode: 7841/101000 (7.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5695s / 603.2084 s
agent0:                 episode reward: -0.4720,                 loss: 0.2316
agent1:                 episode reward: 0.4720,                 loss: nan
Episode: 7861/101000 (7.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4093s / 604.6177 s
agent0:                 episode reward: 0.1115,                 loss: 0.2345
agent1:                 episode reward: -0.1115,                 loss: nan
Episode: 7881/101000 (7.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2603s / 605.8780 s
agent0:                 episode reward: 0.1854,                 loss: 0.2360
agent1:                 episode reward: -0.1854,                 loss: 0.1771
Score delta: 1.7161881078764185, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/7440_0.
Episode: 7901/101000 (7.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4472s / 607.3252 s
agent0:                 episode reward: -0.7125,                 loss: 0.1925
agent1:                 episode reward: 0.7125,                 loss: 0.1790
Score delta: 1.5634354895470615, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/7473_1.
Episode: 7921/101000 (7.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1091s / 609.4343 s
agent0:                 episode reward: -0.1505,                 loss: 0.1915
agent1:                 episode reward: 0.1505,                 loss: nan
Episode: 7941/101000 (7.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0997s / 610.5341 s
agent0:                 episode reward: -0.4638,                 loss: 0.1892
agent1:                 episode reward: 0.4638,                 loss: nan
Episode: 7961/101000 (7.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7829s / 612.3169 s
agent0:                 episode reward: -0.5523,                 loss: 0.1902
agent1:                 episode reward: 0.5523,                 loss: nan
Episode: 7981/101000 (7.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2672s / 614.5841 s
agent0:                 episode reward: -0.4986,                 loss: 0.1904
agent1:                 episode reward: 0.4986,                 loss: nan
Episode: 8001/101000 (7.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6219s / 616.2060 s
agent0:                 episode reward: 0.1060,                 loss: 0.1919
agent1:                 episode reward: -0.1060,                 loss: nan
Episode: 8021/101000 (7.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0097s / 618.2157 s
agent0:                 episode reward: -0.2139,                 loss: 0.1870
agent1:                 episode reward: 0.2139,                 loss: 0.1791
Score delta: 1.637969407872251, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/7581_0.
Episode: 8041/101000 (7.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9658s / 620.1815 s
agent0:                 episode reward: -0.4881,                 loss: 0.2389
agent1:                 episode reward: 0.4881,                 loss: 0.1796
Score delta: 1.5722483571629065, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/7602_1.
Episode: 8061/101000 (7.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8446s / 622.0261 s
agent0:                 episode reward: -0.5372,                 loss: 0.2395
agent1:                 episode reward: 0.5372,                 loss: nan
Episode: 8081/101000 (8.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6004s / 623.6265 s
agent0:                 episode reward: -0.0848,                 loss: 0.2392
agent1:                 episode reward: 0.0848,                 loss: nan
Episode: 8101/101000 (8.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7732s / 625.3997 s
agent0:                 episode reward: -0.1233,                 loss: 0.2402
agent1:                 episode reward: 0.1233,                 loss: nan
Episode: 8121/101000 (8.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7289s / 627.1285 s
agent0:                 episode reward: -0.2215,                 loss: 0.2411
agent1:                 episode reward: 0.2215,                 loss: nan
Episode: 8141/101000 (8.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 0.8151s / 627.9436 s
agent0:                 episode reward: -0.0128,                 loss: 0.2412
agent1:                 episode reward: 0.0128,                 loss: nan
Episode: 8161/101000 (8.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8424s / 629.7861 s
agent0:                 episode reward: -0.3307,                 loss: 0.2394
agent1:                 episode reward: 0.3307,                 loss: nan
Episode: 8181/101000 (8.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9066s / 631.6927 s
agent0:                 episode reward: -0.2019,                 loss: 0.2415
agent1:                 episode reward: 0.2019,                 loss: nan
Episode: 8201/101000 (8.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9886s / 633.6813 s
agent0:                 episode reward: -0.5714,                 loss: 0.2444
agent1:                 episode reward: 0.5714,                 loss: nan
Episode: 8221/101000 (8.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9111s / 635.5924 s
agent0:                 episode reward: -0.4631,                 loss: 0.2483
agent1:                 episode reward: 0.4631,                 loss: nan
Episode: 8241/101000 (8.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7313s / 637.3237 s
agent0:                 episode reward: -0.3391,                 loss: 0.2492
agent1:                 episode reward: 0.3391,                 loss: nan
Episode: 8261/101000 (8.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8320s / 639.1557 s
agent0:                 episode reward: 0.0392,                 loss: 0.2521
agent1:                 episode reward: -0.0392,                 loss: nan
Episode: 8281/101000 (8.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8406s / 640.9964 s
agent0:                 episode reward: -0.0656,                 loss: 0.2489
agent1:                 episode reward: 0.0656,                 loss: nan
Episode: 8301/101000 (8.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6509s / 642.6473 s
agent0:                 episode reward: -0.2423,                 loss: 0.2494
agent1:                 episode reward: 0.2423,                 loss: nan
Episode: 8321/101000 (8.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9430s / 644.5903 s
agent0:                 episode reward: -0.3758,                 loss: 0.2487
agent1:                 episode reward: 0.3758,                 loss: nan
Episode: 8341/101000 (8.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6999s / 646.2901 s
agent0:                 episode reward: -0.4047,                 loss: 0.2507
agent1:                 episode reward: 0.4047,                 loss: nan
Episode: 8361/101000 (8.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4653s / 647.7554 s
agent0:                 episode reward: -0.5750,                 loss: 0.2503
agent1:                 episode reward: 0.5750,                 loss: nan
Episode: 8381/101000 (8.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0502s / 649.8056 s
agent0:                 episode reward: -0.3825,                 loss: 0.2474
agent1:                 episode reward: 0.3825,                 loss: nan
Episode: 8401/101000 (8.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8585s / 651.6641 s
agent0:                 episode reward: 0.1840,                 loss: 0.2500
agent1:                 episode reward: -0.1840,                 loss: nan
Episode: 8421/101000 (8.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6541s / 653.3182 s
agent0:                 episode reward: -0.1977,                 loss: 0.2472
agent1:                 episode reward: 0.1977,                 loss: nan
Episode: 8441/101000 (8.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0937s / 655.4119 s
agent0:                 episode reward: -0.0046,                 loss: 0.2513
agent1:                 episode reward: 0.0046,                 loss: nan
Episode: 8461/101000 (8.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9943s / 657.4062 s
agent0:                 episode reward: -0.1717,                 loss: 0.2466
agent1:                 episode reward: 0.1717,                 loss: nan
Episode: 8481/101000 (8.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7585s / 659.1646 s
agent0:                 episode reward: -0.5435,                 loss: 0.2495
agent1:                 episode reward: 0.5435,                 loss: nan
Episode: 8501/101000 (8.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7232s / 660.8879 s
agent0:                 episode reward: 0.0547,                 loss: 0.2536
agent1:                 episode reward: -0.0547,                 loss: nan
Episode: 8521/101000 (8.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6339s / 662.5218 s
agent0:                 episode reward: -0.0444,                 loss: 0.2539
agent1:                 episode reward: 0.0444,                 loss: nan
Episode: 8541/101000 (8.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8060s / 664.3278 s
agent0:                 episode reward: -0.0359,                 loss: 0.2481
agent1:                 episode reward: 0.0359,                 loss: nan
Episode: 8561/101000 (8.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9520s / 666.2798 s
agent0:                 episode reward: -0.3206,                 loss: 0.2528
agent1:                 episode reward: 0.3206,                 loss: nan
Episode: 8581/101000 (8.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8574s / 668.1372 s
agent0:                 episode reward: -0.4867,                 loss: 0.2511
agent1:                 episode reward: 0.4867,                 loss: nan
Episode: 8601/101000 (8.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1940s / 670.3312 s
agent0:                 episode reward: -0.1067,                 loss: 0.2561
agent1:                 episode reward: 0.1067,                 loss: nan
Episode: 8621/101000 (8.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7902s / 672.1215 s
agent0:                 episode reward: -0.2917,                 loss: 0.2546
agent1:                 episode reward: 0.2917,                 loss: nan
Episode: 8641/101000 (8.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0785s / 674.2000 s
agent0:                 episode reward: -0.2326,                 loss: 0.2548
agent1:                 episode reward: 0.2326,                 loss: nan
Episode: 8661/101000 (8.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5853s / 675.7852 s
agent0:                 episode reward: -0.3069,                 loss: 0.2550
agent1:                 episode reward: 0.3069,                 loss: nan
Episode: 8681/101000 (8.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8455s / 677.6308 s
agent0:                 episode reward: -0.2772,                 loss: 0.2559
agent1:                 episode reward: 0.2772,                 loss: nan
Episode: 8701/101000 (8.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6435s / 679.2742 s
agent0:                 episode reward: -0.3851,                 loss: 0.2546
agent1:                 episode reward: 0.3851,                 loss: nan
Episode: 8721/101000 (8.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1828s / 681.4571 s
agent0:                 episode reward: -0.3236,                 loss: 0.2547
agent1:                 episode reward: 0.3236,                 loss: nan
Episode: 8741/101000 (8.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1983s / 682.6554 s
agent0:                 episode reward: -0.6061,                 loss: 0.2551
agent1:                 episode reward: 0.6061,                 loss: nan
Episode: 8761/101000 (8.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6529s / 684.3083 s
agent0:                 episode reward: -0.0577,                 loss: 0.2560
agent1:                 episode reward: 0.0577,                 loss: nan
Episode: 8781/101000 (8.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7574s / 686.0657 s
agent0:                 episode reward: -0.1763,                 loss: 0.2567
agent1:                 episode reward: 0.1763,                 loss: nan
Episode: 8801/101000 (8.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7588s / 687.8245 s
agent0:                 episode reward: 0.1315,                 loss: 0.2531
agent1:                 episode reward: -0.1315,                 loss: nan
Episode: 8821/101000 (8.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9218s / 689.7462 s
agent0:                 episode reward: -0.3342,                 loss: 0.2574
agent1:                 episode reward: 0.3342,                 loss: nan
Episode: 8841/101000 (8.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4338s / 691.1800 s
agent0:                 episode reward: -0.3413,                 loss: 0.2545
agent1:                 episode reward: 0.3413,                 loss: nan
Episode: 8861/101000 (8.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8554s / 693.0354 s
agent0:                 episode reward: -0.3611,                 loss: 0.2551
agent1:                 episode reward: 0.3611,                 loss: nan
Episode: 8881/101000 (8.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2114s / 695.2468 s
agent0:                 episode reward: -0.4689,                 loss: 0.2478
agent1:                 episode reward: 0.4689,                 loss: nan
Episode: 8901/101000 (8.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8402s / 697.0871 s
agent0:                 episode reward: -0.1499,                 loss: 0.2256
agent1:                 episode reward: 0.1499,                 loss: nan
Episode: 8921/101000 (8.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1515s / 699.2386 s
agent0:                 episode reward: -0.7594,                 loss: 0.2247
agent1:                 episode reward: 0.7594,                 loss: nan
Episode: 8941/101000 (8.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1236s / 701.3622 s
agent0:                 episode reward: -0.2414,                 loss: 0.2277
agent1:                 episode reward: 0.2414,                 loss: nan
Episode: 8961/101000 (8.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1912s / 703.5534 s
agent0:                 episode reward: -0.3617,                 loss: 0.2241
agent1:                 episode reward: 0.3617,                 loss: nan
Episode: 8981/101000 (8.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8327s / 705.3861 s
agent0:                 episode reward: -0.5566,                 loss: 0.2255
agent1:                 episode reward: 0.5566,                 loss: nan
Episode: 9001/101000 (8.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4807s / 706.8668 s
agent0:                 episode reward: 0.0974,                 loss: 0.2254
agent1:                 episode reward: -0.0974,                 loss: nan
Episode: 9021/101000 (8.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1682s / 708.0350 s
agent0:                 episode reward: 0.0345,                 loss: 0.2270
agent1:                 episode reward: -0.0345,                 loss: 0.1644
Score delta: 1.730438217870877, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/8584_0.
Episode: 9041/101000 (8.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3257s / 709.3606 s
agent0:                 episode reward: -0.3373,                 loss: nan
agent1:                 episode reward: 0.3373,                 loss: 0.1586
Episode: 9061/101000 (8.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6714s / 711.0321 s
agent0:                 episode reward: -0.1023,                 loss: nan
agent1:                 episode reward: 0.1023,                 loss: 0.1591
Episode: 9081/101000 (8.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2687s / 712.3008 s
agent0:                 episode reward: -0.4295,                 loss: nan
agent1:                 episode reward: 0.4295,                 loss: 0.1581
Episode: 9101/101000 (9.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6861s / 713.9869 s
agent0:                 episode reward: -0.3862,                 loss: nan
agent1:                 episode reward: 0.3862,                 loss: 0.1580
Episode: 9121/101000 (9.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9448s / 715.9317 s
agent0:                 episode reward: -0.4531,                 loss: 0.1684
agent1:                 episode reward: 0.4531,                 loss: 0.1588
Score delta: 1.5391958662060177, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/8684_1.
Episode: 9141/101000 (9.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0069s / 717.9387 s
agent0:                 episode reward: -0.2124,                 loss: 0.1655
agent1:                 episode reward: 0.2124,                 loss: nan
Episode: 9161/101000 (9.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5632s / 719.5019 s
agent0:                 episode reward: -0.7007,                 loss: 0.1650
agent1:                 episode reward: 0.7007,                 loss: nan
Episode: 9181/101000 (9.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6373s / 721.1391 s
agent0:                 episode reward: -0.5720,                 loss: 0.1638
agent1:                 episode reward: 0.5720,                 loss: nan
Episode: 9201/101000 (9.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2420s / 723.3811 s
agent0:                 episode reward: -0.1566,                 loss: 0.1617
agent1:                 episode reward: 0.1566,                 loss: nan
Episode: 9221/101000 (9.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7966s / 725.1777 s
agent0:                 episode reward: -0.3029,                 loss: 0.1641
agent1:                 episode reward: 0.3029,                 loss: nan
Episode: 9241/101000 (9.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7353s / 726.9130 s
agent0:                 episode reward: 0.3019,                 loss: 0.1636
agent1:                 episode reward: -0.3019,                 loss: nan
Episode: 9261/101000 (9.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8589s / 728.7718 s
agent0:                 episode reward: -0.0723,                 loss: 0.1608
agent1:                 episode reward: 0.0723,                 loss: nan
Episode: 9281/101000 (9.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8485s / 730.6204 s
agent0:                 episode reward: -0.3501,                 loss: 0.1612
agent1:                 episode reward: 0.3501,                 loss: nan
Episode: 9301/101000 (9.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9298s / 732.5502 s
agent0:                 episode reward: -0.4671,                 loss: 0.1632
agent1:                 episode reward: 0.4671,                 loss: nan
Episode: 9321/101000 (9.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4202s / 734.9704 s
agent0:                 episode reward: -0.3702,                 loss: 0.1892
agent1:                 episode reward: 0.3702,                 loss: nan
Episode: 9341/101000 (9.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8015s / 736.7719 s
agent0:                 episode reward: -0.2771,                 loss: 0.2095
agent1:                 episode reward: 0.2771,                 loss: nan
Episode: 9361/101000 (9.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2389s / 739.0108 s
agent0:                 episode reward: 0.1518,                 loss: 0.2075
agent1:                 episode reward: -0.1518,                 loss: nan
Episode: 9381/101000 (9.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8669s / 740.8777 s
agent0:                 episode reward: -0.4742,                 loss: 0.2110
agent1:                 episode reward: 0.4742,                 loss: nan
Episode: 9401/101000 (9.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0255s / 742.9032 s
agent0:                 episode reward: -0.4987,                 loss: 0.2101
agent1:                 episode reward: 0.4987,                 loss: nan
Episode: 9421/101000 (9.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0036s / 744.9068 s
agent0:                 episode reward: -0.6244,                 loss: 0.2084
agent1:                 episode reward: 0.6244,                 loss: nan
Episode: 9441/101000 (9.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6314s / 746.5382 s
agent0:                 episode reward: -0.5578,                 loss: 0.2063
agent1:                 episode reward: 0.5578,                 loss: nan
Episode: 9461/101000 (9.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0066s / 748.5447 s
agent0:                 episode reward: -0.2984,                 loss: 0.2088
agent1:                 episode reward: 0.2984,                 loss: nan
Episode: 9481/101000 (9.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9693s / 750.5140 s
agent0:                 episode reward: -0.3738,                 loss: 0.2076
agent1:                 episode reward: 0.3738,                 loss: nan
Episode: 9501/101000 (9.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0470s / 752.5610 s
agent0:                 episode reward: -0.6925,                 loss: 0.2100
agent1:                 episode reward: 0.6925,                 loss: nan
Episode: 9521/101000 (9.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1368s / 754.6978 s
agent0:                 episode reward: -0.1703,                 loss: 0.2083
agent1:                 episode reward: 0.1703,                 loss: nan
Episode: 9541/101000 (9.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7910s / 756.4888 s
agent0:                 episode reward: -0.3669,                 loss: 0.2110
agent1:                 episode reward: 0.3669,                 loss: nan
Episode: 9561/101000 (9.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9686s / 758.4573 s
agent0:                 episode reward: -0.6688,                 loss: 0.2076
agent1:                 episode reward: 0.6688,                 loss: nan
Episode: 9581/101000 (9.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7357s / 760.1930 s
agent0:                 episode reward: -0.5005,                 loss: 0.2047
agent1:                 episode reward: 0.5005,                 loss: nan
Episode: 9601/101000 (9.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2052s / 762.3982 s
agent0:                 episode reward: 0.0290,                 loss: 0.2063
agent1:                 episode reward: -0.0290,                 loss: nan
Episode: 9621/101000 (9.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0679s / 764.4662 s
agent0:                 episode reward: -0.5210,                 loss: 0.2069
agent1:                 episode reward: 0.5210,                 loss: nan
Episode: 9641/101000 (9.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2672s / 766.7333 s
agent0:                 episode reward: -0.4113,                 loss: 0.2094
agent1:                 episode reward: 0.4113,                 loss: nan
Episode: 9661/101000 (9.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8384s / 768.5718 s
agent0:                 episode reward: 0.1088,                 loss: 0.2308
agent1:                 episode reward: -0.1088,                 loss: nan
Episode: 9681/101000 (9.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1757s / 770.7475 s
agent0:                 episode reward: -0.4309,                 loss: 0.2310
agent1:                 episode reward: 0.4309,                 loss: nan
Episode: 9701/101000 (9.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0504s / 772.7979 s
agent0:                 episode reward: -0.5155,                 loss: 0.2317
agent1:                 episode reward: 0.5155,                 loss: nan
Episode: 9721/101000 (9.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9827s / 774.7806 s
agent0:                 episode reward: -0.2731,                 loss: 0.2308
agent1:                 episode reward: 0.2731,                 loss: nan
Episode: 9741/101000 (9.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9757s / 776.7563 s
agent0:                 episode reward: -0.9177,                 loss: 0.2315
agent1:                 episode reward: 0.9177,                 loss: nan
Episode: 9761/101000 (9.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0271s / 778.7834 s
agent0:                 episode reward: -0.3928,                 loss: 0.2319
agent1:                 episode reward: 0.3928,                 loss: nan
Episode: 9781/101000 (9.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8560s / 780.6394 s
agent0:                 episode reward: -0.4739,                 loss: 0.2329
agent1:                 episode reward: 0.4739,                 loss: nan
Episode: 9801/101000 (9.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7848s / 782.4242 s
agent0:                 episode reward: -0.2124,                 loss: 0.2327
agent1:                 episode reward: 0.2124,                 loss: nan
Episode: 9821/101000 (9.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3735s / 784.7977 s
agent0:                 episode reward: -0.1190,                 loss: 0.2317
agent1:                 episode reward: 0.1190,                 loss: nan
Episode: 9841/101000 (9.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4673s / 786.2650 s
agent0:                 episode reward: -0.4775,                 loss: 0.2338
agent1:                 episode reward: 0.4775,                 loss: nan
Episode: 9861/101000 (9.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4504s / 787.7153 s
agent0:                 episode reward: -0.3287,                 loss: 0.2318
agent1:                 episode reward: 0.3287,                 loss: nan
Episode: 9881/101000 (9.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6137s / 789.3291 s
agent0:                 episode reward: 0.0166,                 loss: 0.2307
agent1:                 episode reward: -0.0166,                 loss: nan
Episode: 9901/101000 (9.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7681s / 791.0971 s
agent0:                 episode reward: -0.2563,                 loss: 0.2318
agent1:                 episode reward: 0.2563,                 loss: nan
Episode: 9921/101000 (9.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7919s / 792.8890 s
agent0:                 episode reward: 0.0206,                 loss: 0.2321
agent1:                 episode reward: -0.0206,                 loss: nan
Episode: 9941/101000 (9.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6878s / 794.5768 s
agent0:                 episode reward: 0.1566,                 loss: 0.2375
agent1:                 episode reward: -0.1566,                 loss: 0.1741
Score delta: 1.5560609207042702, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/9502_0.
Episode: 9961/101000 (9.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2610s / 795.8379 s
agent0:                 episode reward: -0.4248,                 loss: 0.2467
agent1:                 episode reward: 0.4248,                 loss: 0.1718
Score delta: 1.6934060516243594, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/9531_1.
Episode: 9981/101000 (9.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0406s / 797.8784 s
agent0:                 episode reward: -0.6518,                 loss: 0.2420
agent1:                 episode reward: 0.6518,                 loss: nan
Episode: 10001/101000 (9.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5485s / 799.4270 s
agent0:                 episode reward: -0.1842,                 loss: 0.2448
agent1:                 episode reward: 0.1842,                 loss: nan
Episode: 10021/101000 (9.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5289s / 800.9559 s
agent0:                 episode reward: -0.4159,                 loss: 0.2434
agent1:                 episode reward: 0.4159,                 loss: nan
Episode: 10041/101000 (9.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1517s / 803.1076 s
agent0:                 episode reward: -0.3624,                 loss: 0.2411
agent1:                 episode reward: 0.3624,                 loss: nan
Episode: 10061/101000 (9.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7808s / 804.8884 s
agent0:                 episode reward: -0.6232,                 loss: 0.2449
agent1:                 episode reward: 0.6232,                 loss: nan
Episode: 10081/101000 (9.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5991s / 806.4875 s
agent0:                 episode reward: -0.4865,                 loss: 0.2439
agent1:                 episode reward: 0.4865,                 loss: nan
Episode: 10101/101000 (10.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9598s / 808.4473 s
agent0:                 episode reward: -0.0992,                 loss: 0.2456
agent1:                 episode reward: 0.0992,                 loss: nan
Episode: 10121/101000 (10.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8073s / 810.2546 s
agent0:                 episode reward: -0.8405,                 loss: 0.2480
agent1:                 episode reward: 0.8405,                 loss: nan
Episode: 10141/101000 (10.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0565s / 812.3111 s
agent0:                 episode reward: -0.2227,                 loss: 0.2465
agent1:                 episode reward: 0.2227,                 loss: nan
Episode: 10161/101000 (10.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8029s / 814.1140 s
agent0:                 episode reward: -0.4277,                 loss: 0.2446
agent1:                 episode reward: 0.4277,                 loss: nan
Episode: 10181/101000 (10.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7074s / 815.8214 s
agent0:                 episode reward: -0.4216,                 loss: 0.2449
agent1:                 episode reward: 0.4216,                 loss: nan
Episode: 10201/101000 (10.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6922s / 817.5136 s
agent0:                 episode reward: -0.6956,                 loss: 0.2440
agent1:                 episode reward: 0.6956,                 loss: nan
Episode: 10221/101000 (10.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3698s / 818.8835 s
agent0:                 episode reward: -0.1377,                 loss: 0.2487
agent1:                 episode reward: 0.1377,                 loss: nan
Episode: 10241/101000 (10.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6219s / 820.5054 s
agent0:                 episode reward: -0.0933,                 loss: 0.2434
agent1:                 episode reward: 0.0933,                 loss: nan
Episode: 10261/101000 (10.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8519s / 822.3573 s
agent0:                 episode reward: -0.4683,                 loss: 0.2441
agent1:                 episode reward: 0.4683,                 loss: nan
Episode: 10281/101000 (10.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9516s / 824.3089 s
agent0:                 episode reward: -0.4593,                 loss: 0.2456
agent1:                 episode reward: 0.4593,                 loss: nan
Episode: 10301/101000 (10.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9723s / 826.2812 s
agent0:                 episode reward: -0.2104,                 loss: 0.2460
agent1:                 episode reward: 0.2104,                 loss: nan
Episode: 10321/101000 (10.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8218s / 828.1029 s
agent0:                 episode reward: -0.2920,                 loss: 0.2444
agent1:                 episode reward: 0.2920,                 loss: nan
Episode: 10341/101000 (10.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7952s / 829.8981 s
agent0:                 episode reward: 0.0541,                 loss: 0.2473
agent1:                 episode reward: -0.0541,                 loss: nan
Episode: 10361/101000 (10.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5829s / 831.4810 s
agent0:                 episode reward: 0.0579,                 loss: 0.2543
agent1:                 episode reward: -0.0579,                 loss: nan
Episode: 10381/101000 (10.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1644s / 833.6454 s
agent0:                 episode reward: 0.6408,                 loss: 0.2511
agent1:                 episode reward: -0.6408,                 loss: nan
Episode: 10401/101000 (10.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6557s / 835.3011 s
agent0:                 episode reward: -0.1296,                 loss: 0.2554
agent1:                 episode reward: 0.1296,                 loss: 0.1712
Score delta: 1.5437322254792303, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/9959_0.
Episode: 10421/101000 (10.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0344s / 837.3355 s
agent0:                 episode reward: -0.5262,                 loss: nan
agent1:                 episode reward: 0.5262,                 loss: 0.1686
Episode: 10441/101000 (10.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9080s / 839.2435 s
agent0:                 episode reward: -0.4693,                 loss: 0.2456
agent1:                 episode reward: 0.4693,                 loss: 0.1698
Score delta: 1.5719763520184453, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/9997_1.
Episode: 10461/101000 (10.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5991s / 840.8426 s
agent0:                 episode reward: -0.4711,                 loss: 0.2481
agent1:                 episode reward: 0.4711,                 loss: nan
Episode: 10481/101000 (10.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8642s / 842.7068 s
agent0:                 episode reward: -0.3135,                 loss: 0.2408
agent1:                 episode reward: 0.3135,                 loss: nan
Episode: 10501/101000 (10.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5174s / 844.2242 s
agent0:                 episode reward: -0.2317,                 loss: 0.2477
agent1:                 episode reward: 0.2317,                 loss: nan
Episode: 10521/101000 (10.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1115s / 845.3358 s
agent0:                 episode reward: 0.2382,                 loss: 0.2454
agent1:                 episode reward: -0.2382,                 loss: nan
Episode: 10541/101000 (10.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7285s / 847.0642 s
agent0:                 episode reward: -0.1741,                 loss: 0.2461
agent1:                 episode reward: 0.1741,                 loss: nan
Episode: 10561/101000 (10.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9984s / 849.0626 s
agent0:                 episode reward: -0.3356,                 loss: 0.2458
agent1:                 episode reward: 0.3356,                 loss: nan
Episode: 10581/101000 (10.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6661s / 850.7287 s
agent0:                 episode reward: 0.0554,                 loss: 0.2444
agent1:                 episode reward: -0.0554,                 loss: nan
Episode: 10601/101000 (10.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5474s / 852.2761 s
agent0:                 episode reward: 0.3909,                 loss: 0.2454
agent1:                 episode reward: -0.3909,                 loss: 0.1868
Score delta: 1.867229846188233, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/10172_0.
Episode: 10621/101000 (10.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1678s / 854.4439 s
agent0:                 episode reward: -0.2862,                 loss: 0.2552
agent1:                 episode reward: 0.2862,                 loss: 0.1753
Score delta: 1.6213150826545562, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/10193_1.
Episode: 10641/101000 (10.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7570s / 856.2009 s
agent0:                 episode reward: -0.0226,                 loss: 0.2545
agent1:                 episode reward: 0.0226,                 loss: nan
Episode: 10661/101000 (10.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6333s / 857.8342 s
agent0:                 episode reward: -0.6723,                 loss: 0.2533
agent1:                 episode reward: 0.6723,                 loss: 0.1700
Score delta: 1.8021084195237471, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/10220_0.
Episode: 10681/101000 (10.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2496s / 860.0837 s
agent0:                 episode reward: -0.0992,                 loss: 0.2505
agent1:                 episode reward: 0.0992,                 loss: 0.1695
Score delta: 3.005694420077423, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/10241_1.
Episode: 10701/101000 (10.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7852s / 861.8690 s
agent0:                 episode reward: -0.4764,                 loss: 0.2480
agent1:                 episode reward: 0.4764,                 loss: nan
Episode: 10721/101000 (10.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1717s / 864.0407 s
agent0:                 episode reward: -0.1566,                 loss: 0.2472
agent1:                 episode reward: 0.1566,                 loss: nan
Episode: 10741/101000 (10.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5772s / 865.6179 s
agent0:                 episode reward: -0.4435,                 loss: 0.2469
agent1:                 episode reward: 0.4435,                 loss: nan
Episode: 10761/101000 (10.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7937s / 867.4116 s
agent0:                 episode reward: -0.2966,                 loss: 0.2440
agent1:                 episode reward: 0.2966,                 loss: nan
Episode: 10781/101000 (10.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2293s / 869.6409 s
agent0:                 episode reward: -0.5613,                 loss: 0.2348
agent1:                 episode reward: 0.5613,                 loss: nan
Episode: 10801/101000 (10.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6947s / 871.3355 s
agent0:                 episode reward: -0.4407,                 loss: 0.2374
agent1:                 episode reward: 0.4407,                 loss: nan
Episode: 10821/101000 (10.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9950s / 873.3305 s
agent0:                 episode reward: -0.2262,                 loss: 0.2365
agent1:                 episode reward: 0.2262,                 loss: nan
Episode: 10841/101000 (10.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3269s / 875.6574 s
agent0:                 episode reward: -0.5867,                 loss: 0.2371
agent1:                 episode reward: 0.5867,                 loss: nan
Episode: 10861/101000 (10.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9885s / 877.6459 s
agent0:                 episode reward: -0.2495,                 loss: 0.2361
agent1:                 episode reward: 0.2495,                 loss: nan
Episode: 10881/101000 (10.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2979s / 879.9439 s
agent0:                 episode reward: -0.2590,                 loss: 0.2377
agent1:                 episode reward: 0.2590,                 loss: nan
Episode: 10901/101000 (10.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6761s / 881.6200 s
agent0:                 episode reward: 0.1623,                 loss: 0.2391
agent1:                 episode reward: -0.1623,                 loss: nan
Episode: 10921/101000 (10.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1480s / 883.7680 s
agent0:                 episode reward: -0.4126,                 loss: 0.2352
agent1:                 episode reward: 0.4126,                 loss: nan
Episode: 10941/101000 (10.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8700s / 885.6380 s
agent0:                 episode reward: -0.4455,                 loss: 0.2393
agent1:                 episode reward: 0.4455,                 loss: nan
Episode: 10961/101000 (10.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3556s / 887.9936 s
agent0:                 episode reward: -0.0016,                 loss: 0.2377
agent1:                 episode reward: 0.0016,                 loss: nan
Episode: 10981/101000 (10.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5248s / 889.5184 s
agent0:                 episode reward: -0.7092,                 loss: 0.2374
agent1:                 episode reward: 0.7092,                 loss: nan
Episode: 11001/101000 (10.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8441s / 891.3625 s
agent0:                 episode reward: -0.0696,                 loss: 0.2382
agent1:                 episode reward: 0.0696,                 loss: nan
Episode: 11021/101000 (10.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9134s / 893.2758 s
agent0:                 episode reward: -0.4903,                 loss: 0.2392
agent1:                 episode reward: 0.4903,                 loss: nan
Episode: 11041/101000 (10.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8986s / 895.1745 s
agent0:                 episode reward: -0.4946,                 loss: 0.2380
agent1:                 episode reward: 0.4946,                 loss: nan
Episode: 11061/101000 (10.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2927s / 897.4671 s
agent0:                 episode reward: 0.0968,                 loss: 0.2383
agent1:                 episode reward: -0.0968,                 loss: nan
Episode: 11081/101000 (10.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9881s / 899.4552 s
agent0:                 episode reward: -0.9322,                 loss: 0.2347
agent1:                 episode reward: 0.9322,                 loss: nan
Episode: 11101/101000 (10.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3558s / 901.8111 s
agent0:                 episode reward: 0.1615,                 loss: 0.2259
agent1:                 episode reward: -0.1615,                 loss: nan
Episode: 11121/101000 (11.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1406s / 903.9516 s
agent0:                 episode reward: -0.2289,                 loss: 0.2193
agent1:                 episode reward: 0.2289,                 loss: nan
Episode: 11141/101000 (11.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4831s / 905.4348 s
agent0:                 episode reward: -0.4277,                 loss: 0.2186
agent1:                 episode reward: 0.4277,                 loss: nan
Episode: 11161/101000 (11.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0781s / 907.5129 s
agent0:                 episode reward: -0.3755,                 loss: 0.2213
agent1:                 episode reward: 0.3755,                 loss: nan
Episode: 11181/101000 (11.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6597s / 910.1726 s
agent0:                 episode reward: -0.1677,                 loss: 0.2193
agent1:                 episode reward: 0.1677,                 loss: nan
Episode: 11201/101000 (11.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1629s / 912.3356 s
agent0:                 episode reward: -0.3000,                 loss: 0.2216
agent1:                 episode reward: 0.3000,                 loss: nan
Episode: 11221/101000 (11.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7313s / 915.0668 s
agent0:                 episode reward: 0.0090,                 loss: 0.2173
agent1:                 episode reward: -0.0090,                 loss: nan
Episode: 11241/101000 (11.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5921s / 916.6590 s
agent0:                 episode reward: 0.1305,                 loss: 0.2170
agent1:                 episode reward: -0.1305,                 loss: nan
Episode: 11261/101000 (11.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0768s / 918.7358 s
agent0:                 episode reward: -0.1042,                 loss: 0.2194
agent1:                 episode reward: 0.1042,                 loss: nan
Episode: 11281/101000 (11.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9490s / 920.6848 s
agent0:                 episode reward: -0.0584,                 loss: 0.2209
agent1:                 episode reward: 0.0584,                 loss: nan
Episode: 11301/101000 (11.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9453s / 922.6301 s
agent0:                 episode reward: -0.9204,                 loss: 0.2211
agent1:                 episode reward: 0.9204,                 loss: nan
Episode: 11321/101000 (11.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9415s / 924.5716 s
agent0:                 episode reward: -0.5336,                 loss: 0.2172
agent1:                 episode reward: 0.5336,                 loss: nan
Episode: 11341/101000 (11.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4841s / 927.0558 s
agent0:                 episode reward: -0.1128,                 loss: 0.2196
agent1:                 episode reward: 0.1128,                 loss: nan
Episode: 11361/101000 (11.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5047s / 928.5605 s
agent0:                 episode reward: -0.2177,                 loss: 0.2205
agent1:                 episode reward: 0.2177,                 loss: nan
Episode: 11381/101000 (11.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8565s / 930.4170 s
agent0:                 episode reward: -0.5861,                 loss: 0.2203
agent1:                 episode reward: 0.5861,                 loss: nan
Episode: 11401/101000 (11.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0318s / 932.4488 s
agent0:                 episode reward: -0.2978,                 loss: 0.2170
agent1:                 episode reward: 0.2978,                 loss: nan
Episode: 11421/101000 (11.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6970s / 934.1459 s
agent0:                 episode reward: -0.3635,                 loss: 0.2221
agent1:                 episode reward: 0.3635,                 loss: nan
Episode: 11441/101000 (11.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9736s / 936.1194 s
agent0:                 episode reward: 0.1223,                 loss: 0.2206
agent1:                 episode reward: -0.1223,                 loss: nan
Episode: 11461/101000 (11.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4482s / 938.5676 s
agent0:                 episode reward: -0.0025,                 loss: 0.2193
agent1:                 episode reward: 0.0025,                 loss: nan
Episode: 11481/101000 (11.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9181s / 940.4857 s
agent0:                 episode reward: 0.1977,                 loss: 0.2167
agent1:                 episode reward: -0.1977,                 loss: 0.1733
Score delta: 1.7000309246932594, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/11040_0.
Episode: 11501/101000 (11.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7219s / 942.2076 s
agent0:                 episode reward: -0.4281,                 loss: 0.2540
agent1:                 episode reward: 0.4281,                 loss: 0.1780
Score delta: 1.505006523785497, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/11066_1.
Episode: 11521/101000 (11.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9890s / 944.1966 s
agent0:                 episode reward: -0.1520,                 loss: 0.2521
agent1:                 episode reward: 0.1520,                 loss: nan
Episode: 11541/101000 (11.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1299s / 946.3264 s
agent0:                 episode reward: -0.0989,                 loss: 0.2523
agent1:                 episode reward: 0.0989,                 loss: nan
Episode: 11561/101000 (11.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6038s / 947.9303 s
agent0:                 episode reward: -0.6022,                 loss: 0.2530
agent1:                 episode reward: 0.6022,                 loss: nan
Episode: 11581/101000 (11.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1046s / 950.0349 s
agent0:                 episode reward: -0.0377,                 loss: 0.2503
agent1:                 episode reward: 0.0377,                 loss: nan
Episode: 11601/101000 (11.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0926s / 952.1274 s
agent0:                 episode reward: -0.3789,                 loss: 0.2513
agent1:                 episode reward: 0.3789,                 loss: nan
Episode: 11621/101000 (11.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0271s / 954.1545 s
agent0:                 episode reward: -0.2730,                 loss: 0.2541
agent1:                 episode reward: 0.2730,                 loss: nan
Episode: 11641/101000 (11.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9920s / 956.1465 s
agent0:                 episode reward: -0.5059,                 loss: 0.2493
agent1:                 episode reward: 0.5059,                 loss: 0.1770
Score delta: 1.7544803121109127, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/11201_0.
Episode: 11661/101000 (11.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9757s / 958.1221 s
agent0:                 episode reward: -0.5590,                 loss: 0.2604
agent1:                 episode reward: 0.5590,                 loss: 0.1734
Score delta: 1.944780733558579, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/11222_1.
Episode: 11681/101000 (11.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0932s / 960.2154 s
agent0:                 episode reward: -0.1277,                 loss: 0.2618
agent1:                 episode reward: 0.1277,                 loss: nan
Episode: 11701/101000 (11.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9750s / 962.1903 s
agent0:                 episode reward: -0.8479,                 loss: 0.2605
agent1:                 episode reward: 0.8479,                 loss: nan
Episode: 11721/101000 (11.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4608s / 963.6511 s
agent0:                 episode reward: -0.0025,                 loss: 0.2625
agent1:                 episode reward: 0.0025,                 loss: nan
Episode: 11741/101000 (11.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0063s / 965.6574 s
agent0:                 episode reward: -0.7384,                 loss: 0.2619
agent1:                 episode reward: 0.7384,                 loss: nan
Episode: 11761/101000 (11.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0538s / 967.7112 s
agent0:                 episode reward: 0.1394,                 loss: 0.2597
agent1:                 episode reward: -0.1394,                 loss: nan
Episode: 11781/101000 (11.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9751s / 969.6864 s
agent0:                 episode reward: -0.4617,                 loss: 0.2605
agent1:                 episode reward: 0.4617,                 loss: nan
Episode: 11801/101000 (11.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5597s / 971.2461 s
agent0:                 episode reward: -0.0738,                 loss: 0.2553
agent1:                 episode reward: 0.0738,                 loss: nan
Episode: 11821/101000 (11.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8065s / 973.0526 s
agent0:                 episode reward: -0.7481,                 loss: 0.2495
agent1:                 episode reward: 0.7481,                 loss: nan
Episode: 11841/101000 (11.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0427s / 975.0953 s
agent0:                 episode reward: -0.4831,                 loss: 0.2506
agent1:                 episode reward: 0.4831,                 loss: nan
Episode: 11861/101000 (11.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0532s / 977.1485 s
agent0:                 episode reward: -0.2684,                 loss: 0.2503
agent1:                 episode reward: 0.2684,                 loss: nan
Episode: 11881/101000 (11.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8982s / 979.0467 s
agent0:                 episode reward: -0.2863,                 loss: 0.2518
agent1:                 episode reward: 0.2863,                 loss: nan
Episode: 11901/101000 (11.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8391s / 980.8859 s
agent0:                 episode reward: -0.4521,                 loss: 0.2484
agent1:                 episode reward: 0.4521,                 loss: nan
Episode: 11921/101000 (11.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3967s / 983.2826 s
agent0:                 episode reward: -0.2810,                 loss: 0.2517
agent1:                 episode reward: 0.2810,                 loss: nan
Episode: 11941/101000 (11.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3319s / 985.6145 s
agent0:                 episode reward: -0.2699,                 loss: 0.2517
agent1:                 episode reward: 0.2699,                 loss: nan
Episode: 11961/101000 (11.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6875s / 987.3020 s
agent0:                 episode reward: -0.4927,                 loss: 0.2553
agent1:                 episode reward: 0.4927,                 loss: nan
Episode: 11981/101000 (11.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3192s / 989.6212 s
agent0:                 episode reward: -0.6133,                 loss: 0.2503
agent1:                 episode reward: 0.6133,                 loss: nan
Episode: 12001/101000 (11.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7402s / 991.3613 s
agent0:                 episode reward: -0.6948,                 loss: 0.2479
agent1:                 episode reward: 0.6948,                 loss: nan
Episode: 12021/101000 (11.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9320s / 993.2933 s
agent0:                 episode reward: -0.0339,                 loss: 0.2513
agent1:                 episode reward: 0.0339,                 loss: nan
Episode: 12041/101000 (11.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3672s / 995.6606 s
agent0:                 episode reward: -0.7144,                 loss: 0.2541
agent1:                 episode reward: 0.7144,                 loss: nan
Episode: 12061/101000 (11.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4108s / 998.0713 s
agent0:                 episode reward: -0.5588,                 loss: 0.2484
agent1:                 episode reward: 0.5588,                 loss: nan
Episode: 12081/101000 (11.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0189s / 1000.0902 s
agent0:                 episode reward: -0.4107,                 loss: 0.2548
agent1:                 episode reward: 0.4107,                 loss: nan
Episode: 12101/101000 (11.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9429s / 1002.0331 s
agent0:                 episode reward: -0.5535,                 loss: 0.2541
agent1:                 episode reward: 0.5535,                 loss: nan
Episode: 12121/101000 (12.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2956s / 1004.3287 s
agent0:                 episode reward: -0.4921,                 loss: 0.2536
agent1:                 episode reward: 0.4921,                 loss: nan
Episode: 12141/101000 (12.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7228s / 1006.0515 s
agent0:                 episode reward: -0.1078,                 loss: 0.2493
agent1:                 episode reward: 0.1078,                 loss: nan
Episode: 12161/101000 (12.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2614s / 1008.3129 s
agent0:                 episode reward: -0.0663,                 loss: 0.2461
agent1:                 episode reward: 0.0663,                 loss: nan
Episode: 12181/101000 (12.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8117s / 1010.1246 s
agent0:                 episode reward: -0.7022,                 loss: 0.2457
agent1:                 episode reward: 0.7022,                 loss: nan
Episode: 12201/101000 (12.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7386s / 1011.8631 s
agent0:                 episode reward: -0.3317,                 loss: 0.2477
agent1:                 episode reward: 0.3317,                 loss: nan
Episode: 12221/101000 (12.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5543s / 1013.4174 s
agent0:                 episode reward: -0.3536,                 loss: 0.2442
agent1:                 episode reward: 0.3536,                 loss: nan
Episode: 12241/101000 (12.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2238s / 1015.6412 s
agent0:                 episode reward: -0.4316,                 loss: 0.2469
agent1:                 episode reward: 0.4316,                 loss: nan
Episode: 12261/101000 (12.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9101s / 1017.5513 s
agent0:                 episode reward: 0.2093,                 loss: 0.2461
agent1:                 episode reward: -0.2093,                 loss: nan
Episode: 12281/101000 (12.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0345s / 1019.5858 s
agent0:                 episode reward: 0.0021,                 loss: 0.2471
agent1:                 episode reward: -0.0021,                 loss: nan
Episode: 12301/101000 (12.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5414s / 1021.1272 s
agent0:                 episode reward: 0.1765,                 loss: 0.2467
agent1:                 episode reward: -0.1765,                 loss: nan
Episode: 12321/101000 (12.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8954s / 1023.0226 s
agent0:                 episode reward: -0.5104,                 loss: 0.2462
agent1:                 episode reward: 0.5104,                 loss: nan
Episode: 12341/101000 (12.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9666s / 1024.9892 s
agent0:                 episode reward: -0.5456,                 loss: 0.2452
agent1:                 episode reward: 0.5456,                 loss: nan
Episode: 12361/101000 (12.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9675s / 1026.9567 s
agent0:                 episode reward: -0.0491,                 loss: 0.2468
agent1:                 episode reward: 0.0491,                 loss: nan
Episode: 12381/101000 (12.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9671s / 1028.9239 s
agent0:                 episode reward: -0.2624,                 loss: 0.2462
agent1:                 episode reward: 0.2624,                 loss: nan
Episode: 12401/101000 (12.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1716s / 1031.0954 s
agent0:                 episode reward: 0.0298,                 loss: 0.2477
agent1:                 episode reward: -0.0298,                 loss: nan
Episode: 12421/101000 (12.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2359s / 1033.3313 s
agent0:                 episode reward: -0.3129,                 loss: 0.2469
agent1:                 episode reward: 0.3129,                 loss: nan
Episode: 12441/101000 (12.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2099s / 1035.5412 s
agent0:                 episode reward: -0.2154,                 loss: 0.2443
agent1:                 episode reward: 0.2154,                 loss: nan
Episode: 12461/101000 (12.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5787s / 1037.1198 s
agent0:                 episode reward: 0.0946,                 loss: 0.2496
agent1:                 episode reward: -0.0946,                 loss: nan
Episode: 12481/101000 (12.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7113s / 1039.8312 s
agent0:                 episode reward: -0.2887,                 loss: 0.2471
agent1:                 episode reward: 0.2887,                 loss: nan
Episode: 12501/101000 (12.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1185s / 1041.9496 s
agent0:                 episode reward: -0.4983,                 loss: 0.2456
agent1:                 episode reward: 0.4983,                 loss: nan
Episode: 12521/101000 (12.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0091s / 1043.9587 s
agent0:                 episode reward: -0.7801,                 loss: 0.2473
agent1:                 episode reward: 0.7801,                 loss: nan
Episode: 12541/101000 (12.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6220s / 1045.5807 s
agent0:                 episode reward: -0.6658,                 loss: 0.2436
agent1:                 episode reward: 0.6658,                 loss: nan
Episode: 12561/101000 (12.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9687s / 1047.5493 s
agent0:                 episode reward: -0.1723,                 loss: 0.2460
agent1:                 episode reward: 0.1723,                 loss: nan
Episode: 12581/101000 (12.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0062s / 1049.5556 s
agent0:                 episode reward: -0.3662,                 loss: 0.2462
agent1:                 episode reward: 0.3662,                 loss: nan
Episode: 12601/101000 (12.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1748s / 1051.7303 s
agent0:                 episode reward: -0.4014,                 loss: 0.2455
agent1:                 episode reward: 0.4014,                 loss: nan
Episode: 12621/101000 (12.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8653s / 1053.5956 s
agent0:                 episode reward: -0.2622,                 loss: 0.2443
agent1:                 episode reward: 0.2622,                 loss: nan
Episode: 12641/101000 (12.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1787s / 1054.7743 s
agent0:                 episode reward: -0.2546,                 loss: 0.2462
agent1:                 episode reward: 0.2546,                 loss: nan
Episode: 12661/101000 (12.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8606s / 1056.6349 s
agent0:                 episode reward: -0.6129,                 loss: 0.2462
agent1:                 episode reward: 0.6129,                 loss: nan
Episode: 12681/101000 (12.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7784s / 1058.4133 s
agent0:                 episode reward: -0.3951,                 loss: 0.2456
agent1:                 episode reward: 0.3951,                 loss: nan
Episode: 12701/101000 (12.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2623s / 1060.6756 s
agent0:                 episode reward: -0.3392,                 loss: 0.2456
agent1:                 episode reward: 0.3392,                 loss: nan
Episode: 12721/101000 (12.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3786s / 1063.0542 s
agent0:                 episode reward: 0.0284,                 loss: 0.2459
agent1:                 episode reward: -0.0284,                 loss: nan
Episode: 12741/101000 (12.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1178s / 1065.1720 s
agent0:                 episode reward: -0.6481,                 loss: 0.2457
agent1:                 episode reward: 0.6481,                 loss: nan
Episode: 12761/101000 (12.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9545s / 1067.1265 s
agent0:                 episode reward: -0.0523,                 loss: 0.2453
agent1:                 episode reward: 0.0523,                 loss: nan
Episode: 12781/101000 (12.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9647s / 1069.0912 s
agent0:                 episode reward: -0.4435,                 loss: 0.2460
agent1:                 episode reward: 0.4435,                 loss: nan
Episode: 12801/101000 (12.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0460s / 1071.1372 s
agent0:                 episode reward: -0.4554,                 loss: 0.2460
agent1:                 episode reward: 0.4554,                 loss: nan
Episode: 12821/101000 (12.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2430s / 1073.3802 s
agent0:                 episode reward: -0.2772,                 loss: 0.2367
agent1:                 episode reward: 0.2772,                 loss: nan
Episode: 12841/101000 (12.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7649s / 1075.1451 s
agent0:                 episode reward: -0.4361,                 loss: 0.2403
agent1:                 episode reward: 0.4361,                 loss: nan
Episode: 12861/101000 (12.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3676s / 1077.5127 s
agent0:                 episode reward: -0.4510,                 loss: 0.2373
agent1:                 episode reward: 0.4510,                 loss: nan
Episode: 12881/101000 (12.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1030s / 1079.6157 s
agent0:                 episode reward: -0.1857,                 loss: 0.2374
agent1:                 episode reward: 0.1857,                 loss: nan
Episode: 12901/101000 (12.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9672s / 1081.5830 s
agent0:                 episode reward: -0.2820,                 loss: 0.2375
agent1:                 episode reward: 0.2820,                 loss: nan
Episode: 12921/101000 (12.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0689s / 1083.6519 s
agent0:                 episode reward: -0.4910,                 loss: 0.2368
agent1:                 episode reward: 0.4910,                 loss: nan
Episode: 12941/101000 (12.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5094s / 1086.1613 s
agent0:                 episode reward: -0.2733,                 loss: 0.2380
agent1:                 episode reward: 0.2733,                 loss: nan
Episode: 12961/101000 (12.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9112s / 1088.0725 s
agent0:                 episode reward: -0.3209,                 loss: 0.2377
agent1:                 episode reward: 0.3209,                 loss: nan
Episode: 12981/101000 (12.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6398s / 1089.7124 s
agent0:                 episode reward: -0.2237,                 loss: 0.2353
agent1:                 episode reward: 0.2237,                 loss: nan
Episode: 13001/101000 (12.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3231s / 1092.0354 s
agent0:                 episode reward: -0.7287,                 loss: 0.2376
agent1:                 episode reward: 0.7287,                 loss: nan
Episode: 13021/101000 (12.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3486s / 1094.3840 s
agent0:                 episode reward: -0.6697,                 loss: 0.2372
agent1:                 episode reward: 0.6697,                 loss: nan
Episode: 13041/101000 (12.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9063s / 1096.2903 s
agent0:                 episode reward: -0.2245,                 loss: 0.2392
agent1:                 episode reward: 0.2245,                 loss: nan
Episode: 13061/101000 (12.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7156s / 1098.0059 s
agent0:                 episode reward: -0.2134,                 loss: 0.2371
agent1:                 episode reward: 0.2134,                 loss: nan
Episode: 13081/101000 (12.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0721s / 1100.0781 s
agent0:                 episode reward: 0.0375,                 loss: 0.2382
agent1:                 episode reward: -0.0375,                 loss: 0.1685
Score delta: 1.7427122556196326, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/12653_0.
Episode: 13101/101000 (12.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9116s / 1101.9896 s
agent0:                 episode reward: -0.4159,                 loss: nan
agent1:                 episode reward: 0.4159,                 loss: 0.1726
Episode: 13121/101000 (12.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4517s / 1103.4414 s
agent0:                 episode reward: -0.7147,                 loss: 0.2763
agent1:                 episode reward: 0.7147,                 loss: 0.1704
Score delta: 1.7849574055327282, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/12694_1.
Episode: 13141/101000 (13.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2037s / 1105.6450 s
agent0:                 episode reward: -0.9362,                 loss: 0.2635
agent1:                 episode reward: 0.9362,                 loss: nan
Episode: 13161/101000 (13.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6944s / 1108.3395 s
agent0:                 episode reward: 0.2277,                 loss: 0.2600
agent1:                 episode reward: -0.2277,                 loss: 0.1868
Score delta: 1.7263405795220883, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/12726_0.
Episode: 13181/101000 (13.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9948s / 1110.3343 s
agent0:                 episode reward: -0.6494,                 loss: 0.2477
agent1:                 episode reward: 0.6494,                 loss: 0.1853
Score delta: 1.783539094255702, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/12750_1.
Episode: 13201/101000 (13.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2889s / 1112.6231 s
agent0:                 episode reward: -0.5124,                 loss: 0.2451
agent1:                 episode reward: 0.5124,                 loss: nan
Episode: 13221/101000 (13.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0746s / 1114.6977 s
agent0:                 episode reward: -0.4604,                 loss: 0.2432
agent1:                 episode reward: 0.4604,                 loss: nan
Episode: 13241/101000 (13.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8675s / 1116.5652 s
agent0:                 episode reward: -0.3405,                 loss: 0.2438
agent1:                 episode reward: 0.3405,                 loss: nan
Episode: 13261/101000 (13.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3469s / 1118.9121 s
agent0:                 episode reward: -0.2655,                 loss: 0.2419
agent1:                 episode reward: 0.2655,                 loss: nan
Episode: 13281/101000 (13.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7646s / 1120.6767 s
agent0:                 episode reward: -0.0438,                 loss: 0.2431
agent1:                 episode reward: 0.0438,                 loss: nan
Episode: 13301/101000 (13.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5873s / 1122.2640 s
agent0:                 episode reward: 0.0573,                 loss: 0.2415
agent1:                 episode reward: -0.0573,                 loss: nan
Episode: 13321/101000 (13.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8580s / 1124.1220 s
agent0:                 episode reward: -0.4546,                 loss: 0.2406
agent1:                 episode reward: 0.4546,                 loss: nan
Episode: 13341/101000 (13.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6780s / 1125.8000 s
agent0:                 episode reward: -0.2005,                 loss: 0.2442
agent1:                 episode reward: 0.2005,                 loss: nan
Episode: 13361/101000 (13.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5079s / 1128.3079 s
agent0:                 episode reward: -0.7197,                 loss: 0.2405
agent1:                 episode reward: 0.7197,                 loss: nan
Episode: 13381/101000 (13.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7342s / 1130.0421 s
agent0:                 episode reward: -0.4727,                 loss: 0.2437
agent1:                 episode reward: 0.4727,                 loss: nan
Episode: 13401/101000 (13.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9906s / 1132.0327 s
agent0:                 episode reward: -0.2740,                 loss: 0.2407
agent1:                 episode reward: 0.2740,                 loss: nan
Episode: 13421/101000 (13.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8731s / 1133.9057 s
agent0:                 episode reward: -0.3166,                 loss: 0.2415
agent1:                 episode reward: 0.3166,                 loss: nan
Episode: 13441/101000 (13.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2199s / 1136.1256 s
agent0:                 episode reward: -0.3676,                 loss: 0.2405
agent1:                 episode reward: 0.3676,                 loss: nan
Episode: 13461/101000 (13.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8264s / 1137.9521 s
agent0:                 episode reward: -0.8600,                 loss: 0.2414
agent1:                 episode reward: 0.8600,                 loss: nan
Episode: 13481/101000 (13.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1174s / 1140.0695 s
agent0:                 episode reward: -0.2246,                 loss: 0.2401
agent1:                 episode reward: 0.2246,                 loss: nan
Episode: 13501/101000 (13.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3719s / 1142.4414 s
agent0:                 episode reward: -0.1731,                 loss: 0.2409
agent1:                 episode reward: 0.1731,                 loss: nan
Episode: 13521/101000 (13.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9154s / 1144.3567 s
agent0:                 episode reward: -0.1226,                 loss: 0.2399
agent1:                 episode reward: 0.1226,                 loss: nan
Episode: 13541/101000 (13.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2935s / 1146.6502 s
agent0:                 episode reward: 0.0448,                 loss: 0.2435
agent1:                 episode reward: -0.0448,                 loss: nan
Episode: 13561/101000 (13.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6907s / 1148.3410 s
agent0:                 episode reward: -0.0437,                 loss: 0.2393
agent1:                 episode reward: 0.0437,                 loss: nan
Episode: 13581/101000 (13.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8889s / 1150.2299 s
agent0:                 episode reward: -0.0329,                 loss: 0.2415
agent1:                 episode reward: 0.0329,                 loss: nan
Episode: 13601/101000 (13.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8185s / 1152.0484 s
agent0:                 episode reward: -0.3635,                 loss: 0.2410
agent1:                 episode reward: 0.3635,                 loss: nan
Episode: 13621/101000 (13.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2793s / 1153.3277 s
agent0:                 episode reward: -0.3690,                 loss: 0.2395
agent1:                 episode reward: 0.3690,                 loss: nan
Episode: 13641/101000 (13.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1133s / 1155.4410 s
agent0:                 episode reward: -0.6203,                 loss: 0.2391
agent1:                 episode reward: 0.6203,                 loss: nan
Episode: 13661/101000 (13.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1477s / 1157.5887 s
agent0:                 episode reward: -0.3378,                 loss: 0.2367
agent1:                 episode reward: 0.3378,                 loss: nan
Episode: 13681/101000 (13.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8855s / 1159.4742 s
agent0:                 episode reward: -0.3100,                 loss: 0.2419
agent1:                 episode reward: 0.3100,                 loss: nan
Episode: 13701/101000 (13.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9028s / 1161.3770 s
agent0:                 episode reward: -0.5148,                 loss: 0.2403
agent1:                 episode reward: 0.5148,                 loss: nan
Episode: 13721/101000 (13.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0619s / 1163.4389 s
agent0:                 episode reward: -0.4985,                 loss: 0.2403
agent1:                 episode reward: 0.4985,                 loss: nan
Episode: 13741/101000 (13.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3482s / 1165.7871 s
agent0:                 episode reward: -0.2872,                 loss: 0.2419
agent1:                 episode reward: 0.2872,                 loss: nan
Episode: 13761/101000 (13.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2232s / 1168.0103 s
agent0:                 episode reward: -0.3974,                 loss: 0.2434
agent1:                 episode reward: 0.3974,                 loss: nan
Episode: 13781/101000 (13.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2184s / 1169.2287 s
agent0:                 episode reward: -0.2910,                 loss: 0.2392
agent1:                 episode reward: 0.2910,                 loss: nan
Episode: 13801/101000 (13.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0262s / 1171.2549 s
agent0:                 episode reward: 0.0600,                 loss: 0.2377
agent1:                 episode reward: -0.0600,                 loss: nan
Episode: 13821/101000 (13.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7628s / 1173.0177 s
agent0:                 episode reward: -0.3071,                 loss: 0.2413
agent1:                 episode reward: 0.3071,                 loss: nan
Episode: 13841/101000 (13.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0367s / 1175.0544 s
agent0:                 episode reward: -0.1899,                 loss: 0.2393
agent1:                 episode reward: 0.1899,                 loss: nan
Episode: 13861/101000 (13.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9371s / 1176.9916 s
agent0:                 episode reward: -0.5569,                 loss: 0.2405
agent1:                 episode reward: 0.5569,                 loss: nan
Episode: 13881/101000 (13.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0211s / 1179.0127 s
agent0:                 episode reward: -0.2719,                 loss: 0.2340
agent1:                 episode reward: 0.2719,                 loss: nan
Episode: 13901/101000 (13.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9469s / 1180.9596 s
agent0:                 episode reward: -0.3734,                 loss: 0.2349
agent1:                 episode reward: 0.3734,                 loss: nan
Episode: 13921/101000 (13.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1447s / 1183.1043 s
agent0:                 episode reward: -0.4952,                 loss: 0.2293
agent1:                 episode reward: 0.4952,                 loss: nan
Episode: 13941/101000 (13.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7620s / 1184.8663 s
agent0:                 episode reward: 0.0519,                 loss: 0.2341
agent1:                 episode reward: -0.0519,                 loss: nan
Episode: 13961/101000 (13.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2140s / 1187.0803 s
agent0:                 episode reward: -0.4244,                 loss: 0.2335
agent1:                 episode reward: 0.4244,                 loss: nan
Episode: 13981/101000 (13.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9991s / 1189.0794 s
agent0:                 episode reward: -0.6387,                 loss: 0.2339
agent1:                 episode reward: 0.6387,                 loss: nan
Episode: 14001/101000 (13.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1214s / 1191.2008 s
agent0:                 episode reward: -0.4771,                 loss: 0.2329
agent1:                 episode reward: 0.4771,                 loss: nan
Episode: 14021/101000 (13.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8868s / 1193.0877 s
agent0:                 episode reward: -0.1641,                 loss: 0.2334
agent1:                 episode reward: 0.1641,                 loss: nan
Episode: 14041/101000 (13.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4986s / 1195.5863 s
agent0:                 episode reward: -0.7235,                 loss: 0.2350
agent1:                 episode reward: 0.7235,                 loss: nan
Episode: 14061/101000 (13.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5693s / 1198.1556 s
agent0:                 episode reward: 0.1834,                 loss: 0.2343
agent1:                 episode reward: -0.1834,                 loss: nan
Episode: 14081/101000 (13.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5139s / 1200.6695 s
agent0:                 episode reward: -0.0105,                 loss: 0.2329
agent1:                 episode reward: 0.0105,                 loss: nan
Episode: 14101/101000 (13.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3775s / 1203.0470 s
agent0:                 episode reward: -0.1843,                 loss: 0.2329
agent1:                 episode reward: 0.1843,                 loss: nan
Episode: 14121/101000 (13.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1820s / 1205.2290 s
agent0:                 episode reward: -0.0455,                 loss: 0.2316
agent1:                 episode reward: 0.0455,                 loss: nan
Episode: 14141/101000 (14.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2054s / 1207.4344 s
agent0:                 episode reward: -0.3467,                 loss: 0.2318
agent1:                 episode reward: 0.3467,                 loss: nan
Episode: 14161/101000 (14.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1463s / 1209.5807 s
agent0:                 episode reward: -0.7898,                 loss: 0.2318
agent1:                 episode reward: 0.7898,                 loss: nan
Episode: 14181/101000 (14.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0374s / 1211.6181 s
agent0:                 episode reward: -0.6036,                 loss: 0.2367
agent1:                 episode reward: 0.6036,                 loss: nan
Episode: 14201/101000 (14.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1058s / 1213.7239 s
agent0:                 episode reward: -0.4427,                 loss: 0.2347
agent1:                 episode reward: 0.4427,                 loss: nan
Episode: 14221/101000 (14.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6298s / 1216.3537 s
agent0:                 episode reward: -0.0130,                 loss: 0.2305
agent1:                 episode reward: 0.0130,                 loss: nan
Episode: 14241/101000 (14.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1127s / 1218.4664 s
agent0:                 episode reward: -0.4998,                 loss: 0.2285
agent1:                 episode reward: 0.4998,                 loss: nan
Episode: 14261/101000 (14.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2518s / 1220.7182 s
agent0:                 episode reward: -0.1072,                 loss: 0.2304
agent1:                 episode reward: 0.1072,                 loss: nan
Episode: 14281/101000 (14.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8767s / 1222.5949 s
agent0:                 episode reward: -0.2065,                 loss: 0.2310
agent1:                 episode reward: 0.2065,                 loss: nan
Episode: 14301/101000 (14.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6059s / 1225.2008 s
agent0:                 episode reward: -0.2778,                 loss: 0.2311
agent1:                 episode reward: 0.2778,                 loss: nan
Episode: 14321/101000 (14.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2299s / 1227.4307 s
agent0:                 episode reward: -0.1647,                 loss: 0.2331
agent1:                 episode reward: 0.1647,                 loss: nan
Episode: 14341/101000 (14.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0221s / 1229.4528 s
agent0:                 episode reward: -0.2268,                 loss: 0.2328
agent1:                 episode reward: 0.2268,                 loss: nan
Episode: 14361/101000 (14.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7019s / 1231.1547 s
agent0:                 episode reward: -0.4314,                 loss: 0.2273
agent1:                 episode reward: 0.4314,                 loss: nan
Episode: 14381/101000 (14.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0099s / 1233.1646 s
agent0:                 episode reward: -0.1813,                 loss: 0.2308
agent1:                 episode reward: 0.1813,                 loss: nan
Episode: 14401/101000 (14.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1319s / 1235.2965 s
agent0:                 episode reward: -0.3490,                 loss: 0.2295
agent1:                 episode reward: 0.3490,                 loss: nan
Episode: 14421/101000 (14.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2721s / 1237.5686 s
agent0:                 episode reward: -0.6698,                 loss: 0.2335
agent1:                 episode reward: 0.6698,                 loss: nan
Episode: 14441/101000 (14.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3840s / 1239.9526 s
agent0:                 episode reward: -0.3311,                 loss: 0.2322
agent1:                 episode reward: 0.3311,                 loss: nan
Episode: 14461/101000 (14.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4017s / 1242.3543 s
agent0:                 episode reward: -0.5562,                 loss: 0.2312
agent1:                 episode reward: 0.5562,                 loss: nan
Episode: 14481/101000 (14.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1087s / 1244.4631 s
agent0:                 episode reward: 0.4315,                 loss: 0.2287
agent1:                 episode reward: -0.4315,                 loss: nan
Episode: 14501/101000 (14.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2766s / 1246.7397 s
agent0:                 episode reward: -0.3464,                 loss: 0.2321
agent1:                 episode reward: 0.3464,                 loss: nan
Episode: 14521/101000 (14.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1447s / 1248.8844 s
agent0:                 episode reward: -0.6172,                 loss: 0.2307
agent1:                 episode reward: 0.6172,                 loss: nan
Episode: 14541/101000 (14.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0545s / 1250.9389 s
agent0:                 episode reward: -0.2352,                 loss: 0.2307
agent1:                 episode reward: 0.2352,                 loss: nan
Episode: 14561/101000 (14.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1857s / 1253.1246 s
agent0:                 episode reward: -0.2835,                 loss: 0.2276
agent1:                 episode reward: 0.2835,                 loss: nan
Episode: 14581/101000 (14.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8912s / 1255.0158 s
agent0:                 episode reward: -0.2651,                 loss: 0.2296
agent1:                 episode reward: 0.2651,                 loss: nan
Episode: 14601/101000 (14.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4238s / 1257.4396 s
agent0:                 episode reward: -0.2998,                 loss: 0.2279
agent1:                 episode reward: 0.2998,                 loss: nan
Episode: 14621/101000 (14.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0692s / 1259.5089 s
agent0:                 episode reward: -0.2745,                 loss: 0.2309
agent1:                 episode reward: 0.2745,                 loss: nan
Episode: 14641/101000 (14.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6163s / 1262.1251 s
agent0:                 episode reward: -0.1122,                 loss: 0.2274
agent1:                 episode reward: 0.1122,                 loss: nan
Episode: 14661/101000 (14.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2298s / 1264.3549 s
agent0:                 episode reward: -0.1842,                 loss: 0.2304
agent1:                 episode reward: 0.1842,                 loss: nan
Episode: 14681/101000 (14.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2367s / 1266.5917 s
agent0:                 episode reward: -0.2429,                 loss: 0.2267
agent1:                 episode reward: 0.2429,                 loss: nan
Episode: 14701/101000 (14.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7826s / 1268.3743 s
agent0:                 episode reward: -0.1125,                 loss: 0.2311
agent1:                 episode reward: 0.1125,                 loss: nan
Episode: 14721/101000 (14.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7183s / 1270.0927 s
agent0:                 episode reward: -0.7813,                 loss: 0.2306
agent1:                 episode reward: 0.7813,                 loss: nan
Episode: 14741/101000 (14.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2629s / 1272.3556 s
agent0:                 episode reward: -0.4289,                 loss: 0.2279
agent1:                 episode reward: 0.4289,                 loss: nan
Episode: 14761/101000 (14.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1742s / 1274.5298 s
agent0:                 episode reward: -0.4219,                 loss: 0.2310
agent1:                 episode reward: 0.4219,                 loss: nan
Episode: 14781/101000 (14.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2432s / 1276.7730 s
agent0:                 episode reward: -0.1198,                 loss: 0.2282
agent1:                 episode reward: 0.1198,                 loss: nan
Episode: 14801/101000 (14.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2549s / 1279.0279 s
agent0:                 episode reward: -0.2676,                 loss: 0.2298
agent1:                 episode reward: 0.2676,                 loss: nan
Episode: 14821/101000 (14.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3388s / 1280.3667 s
agent0:                 episode reward: -0.7454,                 loss: 0.2267
agent1:                 episode reward: 0.7454,                 loss: nan
Episode: 14841/101000 (14.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0354s / 1282.4022 s
agent0:                 episode reward: -0.3245,                 loss: 0.2290
agent1:                 episode reward: 0.3245,                 loss: nan
Episode: 14861/101000 (14.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0607s / 1284.4629 s
agent0:                 episode reward: -0.0010,                 loss: 0.2295
agent1:                 episode reward: 0.0010,                 loss: nan
Episode: 14881/101000 (14.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1881s / 1286.6510 s
agent0:                 episode reward: -0.0523,                 loss: 0.2270
agent1:                 episode reward: 0.0523,                 loss: nan
Episode: 14901/101000 (14.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0508s / 1288.7018 s
agent0:                 episode reward: -0.1900,                 loss: 0.2254
agent1:                 episode reward: 0.1900,                 loss: nan
Episode: 14921/101000 (14.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4853s / 1291.1871 s
agent0:                 episode reward: -0.1080,                 loss: 0.2232
agent1:                 episode reward: 0.1080,                 loss: nan
Episode: 14941/101000 (14.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8081s / 1292.9952 s
agent0:                 episode reward: -0.1246,                 loss: 0.2228
agent1:                 episode reward: 0.1246,                 loss: nan
Episode: 14961/101000 (14.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0326s / 1295.0278 s
agent0:                 episode reward: 0.1529,                 loss: 0.2235
agent1:                 episode reward: -0.1529,                 loss: nan
Episode: 14981/101000 (14.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1722s / 1297.2000 s
agent0:                 episode reward: -0.7347,                 loss: 0.2238
agent1:                 episode reward: 0.7347,                 loss: nan
Episode: 15001/101000 (14.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3098s / 1299.5097 s
agent0:                 episode reward: -0.0293,                 loss: 0.2230
agent1:                 episode reward: 0.0293,                 loss: nan
Episode: 15021/101000 (14.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9555s / 1301.4652 s
agent0:                 episode reward: -0.5074,                 loss: 0.2203
agent1:                 episode reward: 0.5074,                 loss: nan
Episode: 15041/101000 (14.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8070s / 1303.2722 s
agent0:                 episode reward: -0.4766,                 loss: 0.2239
agent1:                 episode reward: 0.4766,                 loss: nan
Episode: 15061/101000 (14.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2607s / 1305.5329 s
agent0:                 episode reward: 0.0958,                 loss: 0.2222
agent1:                 episode reward: -0.0958,                 loss: nan
Episode: 15081/101000 (14.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9695s / 1307.5024 s
agent0:                 episode reward: -0.3781,                 loss: 0.2223
agent1:                 episode reward: 0.3781,                 loss: nan
Episode: 15101/101000 (14.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3408s / 1309.8432 s
agent0:                 episode reward: 0.0546,                 loss: 0.2239
agent1:                 episode reward: -0.0546,                 loss: nan
Episode: 15121/101000 (14.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4736s / 1312.3169 s
agent0:                 episode reward: -0.3292,                 loss: 0.2238
agent1:                 episode reward: 0.3292,                 loss: nan
Episode: 15141/101000 (14.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7968s / 1314.1137 s
agent0:                 episode reward: -0.3816,                 loss: 0.2253
agent1:                 episode reward: 0.3816,                 loss: nan
Episode: 15161/101000 (15.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3592s / 1315.4729 s
agent0:                 episode reward: -0.0963,                 loss: 0.2218
agent1:                 episode reward: 0.0963,                 loss: nan
Episode: 15181/101000 (15.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6777s / 1318.1506 s
agent0:                 episode reward: -0.7156,                 loss: 0.2222
agent1:                 episode reward: 0.7156,                 loss: nan
Episode: 15201/101000 (15.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1041s / 1320.2548 s
agent0:                 episode reward: -0.3061,                 loss: 0.2250
agent1:                 episode reward: 0.3061,                 loss: nan
Episode: 15221/101000 (15.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2129s / 1322.4676 s
agent0:                 episode reward: 0.0558,                 loss: 0.2299
agent1:                 episode reward: -0.0558,                 loss: nan
Episode: 15241/101000 (15.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2965s / 1324.7641 s
agent0:                 episode reward: -0.2120,                 loss: 0.2303
agent1:                 episode reward: 0.2120,                 loss: nan
Episode: 15261/101000 (15.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1512s / 1326.9154 s
agent0:                 episode reward: -0.1002,                 loss: 0.2301
agent1:                 episode reward: 0.1002,                 loss: nan
Episode: 15281/101000 (15.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0139s / 1328.9293 s
agent0:                 episode reward: -0.3312,                 loss: 0.2315
agent1:                 episode reward: 0.3312,                 loss: nan
Episode: 15301/101000 (15.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0993s / 1331.0286 s
agent0:                 episode reward: -0.0988,                 loss: 0.2301
agent1:                 episode reward: 0.0988,                 loss: nan
Episode: 15321/101000 (15.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8153s / 1332.8439 s
agent0:                 episode reward: -0.2791,                 loss: 0.2304
agent1:                 episode reward: 0.2791,                 loss: nan
Episode: 15341/101000 (15.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2235s / 1335.0674 s
agent0:                 episode reward: -0.0819,                 loss: 0.2315
agent1:                 episode reward: 0.0819,                 loss: nan
Episode: 15361/101000 (15.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3062s / 1337.3736 s
agent0:                 episode reward: -0.6588,                 loss: 0.2275
agent1:                 episode reward: 0.6588,                 loss: nan
Episode: 15381/101000 (15.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4554s / 1339.8290 s
agent0:                 episode reward: 0.3223,                 loss: 0.2291
agent1:                 episode reward: -0.3223,                 loss: nan
Episode: 15401/101000 (15.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3218s / 1342.1508 s
agent0:                 episode reward: -0.4633,                 loss: 0.2317
agent1:                 episode reward: 0.4633,                 loss: nan
Episode: 15421/101000 (15.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9575s / 1344.1083 s
agent0:                 episode reward: -0.5142,                 loss: 0.2297
agent1:                 episode reward: 0.5142,                 loss: nan
Episode: 15441/101000 (15.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0869s / 1346.1952 s
agent0:                 episode reward: -0.4318,                 loss: 0.2296
agent1:                 episode reward: 0.4318,                 loss: nan
Episode: 15461/101000 (15.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9027s / 1348.0979 s
agent0:                 episode reward: -0.2773,                 loss: 0.2283
agent1:                 episode reward: 0.2773,                 loss: nan
Episode: 15481/101000 (15.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7166s / 1349.8145 s
agent0:                 episode reward: -0.3375,                 loss: 0.2320
agent1:                 episode reward: 0.3375,                 loss: nan
Episode: 15501/101000 (15.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8022s / 1351.6168 s
agent0:                 episode reward: -0.6116,                 loss: 0.2301
agent1:                 episode reward: 0.6116,                 loss: nan
Episode: 15521/101000 (15.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8118s / 1353.4286 s
agent0:                 episode reward: -0.3126,                 loss: 0.2291
agent1:                 episode reward: 0.3126,                 loss: nan
Episode: 15541/101000 (15.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7307s / 1355.1592 s
agent0:                 episode reward: -0.2264,                 loss: 0.2267
agent1:                 episode reward: 0.2264,                 loss: nan
Episode: 15561/101000 (15.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5398s / 1356.6991 s
agent0:                 episode reward: -0.2396,                 loss: 0.2146
agent1:                 episode reward: 0.2396,                 loss: nan
Episode: 15581/101000 (15.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3587s / 1359.0578 s
agent0:                 episode reward: -0.5084,                 loss: 0.2165
agent1:                 episode reward: 0.5084,                 loss: nan
Episode: 15601/101000 (15.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4779s / 1361.5357 s
agent0:                 episode reward: -0.5530,                 loss: 0.2152
agent1:                 episode reward: 0.5530,                 loss: nan
Episode: 15621/101000 (15.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3055s / 1363.8412 s
agent0:                 episode reward: -0.2164,                 loss: 0.2161
agent1:                 episode reward: 0.2164,                 loss: nan
Episode: 15641/101000 (15.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8397s / 1365.6809 s
agent0:                 episode reward: -0.4715,                 loss: 0.2157
agent1:                 episode reward: 0.4715,                 loss: nan
Episode: 15661/101000 (15.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1995s / 1367.8803 s
agent0:                 episode reward: -0.0180,                 loss: 0.2162
agent1:                 episode reward: 0.0180,                 loss: nan
Episode: 15681/101000 (15.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8344s / 1369.7148 s
agent0:                 episode reward: -0.6335,                 loss: 0.2141
agent1:                 episode reward: 0.6335,                 loss: nan
Episode: 15701/101000 (15.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4491s / 1372.1639 s
agent0:                 episode reward: -0.5672,                 loss: 0.2173
agent1:                 episode reward: 0.5672,                 loss: nan
Episode: 15721/101000 (15.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1799s / 1374.3438 s
agent0:                 episode reward: -0.1469,                 loss: 0.2172
agent1:                 episode reward: 0.1469,                 loss: nan
Episode: 15741/101000 (15.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9962s / 1376.3400 s
agent0:                 episode reward: 0.0364,                 loss: 0.2163
agent1:                 episode reward: -0.0364,                 loss: nan
Episode: 15761/101000 (15.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0990s / 1378.4390 s
agent0:                 episode reward: -0.3224,                 loss: 0.2178
agent1:                 episode reward: 0.3224,                 loss: nan
Episode: 15781/101000 (15.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3030s / 1380.7420 s
agent0:                 episode reward: -0.3119,                 loss: 0.2154
agent1:                 episode reward: 0.3119,                 loss: nan
Episode: 15801/101000 (15.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9919s / 1382.7340 s
agent0:                 episode reward: -0.0019,                 loss: 0.2160
agent1:                 episode reward: 0.0019,                 loss: nan
Episode: 15821/101000 (15.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0852s / 1384.8192 s
agent0:                 episode reward: -0.0733,                 loss: 0.2146
agent1:                 episode reward: 0.0733,                 loss: nan
Episode: 15841/101000 (15.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7690s / 1386.5882 s
agent0:                 episode reward: -0.5919,                 loss: 0.2180
agent1:                 episode reward: 0.5919,                 loss: nan
Episode: 15861/101000 (15.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1755s / 1388.7636 s
agent0:                 episode reward: -0.2222,                 loss: 0.2166
agent1:                 episode reward: 0.2222,                 loss: nan
Episode: 15881/101000 (15.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9962s / 1390.7599 s
agent0:                 episode reward: -0.3018,                 loss: 0.2165
agent1:                 episode reward: 0.3018,                 loss: nan
Episode: 15901/101000 (15.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2022s / 1392.9621 s
agent0:                 episode reward: -0.1338,                 loss: 0.2166
agent1:                 episode reward: 0.1338,                 loss: nan
Episode: 15921/101000 (15.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9595s / 1394.9217 s
agent0:                 episode reward: -0.3839,                 loss: 0.2177
agent1:                 episode reward: 0.3839,                 loss: nan
Episode: 15941/101000 (15.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2767s / 1397.1983 s
agent0:                 episode reward: -0.3764,                 loss: 0.2176
agent1:                 episode reward: 0.3764,                 loss: nan
Episode: 15961/101000 (15.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2186s / 1399.4169 s
agent0:                 episode reward: -0.6478,                 loss: 0.2184
agent1:                 episode reward: 0.6478,                 loss: nan
Episode: 15981/101000 (15.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9918s / 1401.4088 s
agent0:                 episode reward: -0.2163,                 loss: 0.2187
agent1:                 episode reward: 0.2163,                 loss: nan
Episode: 16001/101000 (15.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2542s / 1403.6629 s
agent0:                 episode reward: -0.0802,                 loss: 0.2165
agent1:                 episode reward: 0.0802,                 loss: nan
Episode: 16021/101000 (15.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1957s / 1405.8586 s
agent0:                 episode reward: -0.3360,                 loss: 0.2151
agent1:                 episode reward: 0.3360,                 loss: nan
Episode: 16041/101000 (15.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6613s / 1407.5199 s
agent0:                 episode reward: -0.0921,                 loss: 0.2169
agent1:                 episode reward: 0.0921,                 loss: nan
Episode: 16061/101000 (15.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2040s / 1409.7239 s
agent0:                 episode reward: 0.2843,                 loss: 0.2159
agent1:                 episode reward: -0.2843,                 loss: nan
Episode: 16081/101000 (15.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0625s / 1411.7864 s
agent0:                 episode reward: -0.6217,                 loss: 0.2214
agent1:                 episode reward: 0.6217,                 loss: 0.1845
Score delta: 1.688488328836592, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/15636_0.
Episode: 16101/101000 (15.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9346s / 1413.7210 s
agent0:                 episode reward: -0.3269,                 loss: 0.2736
agent1:                 episode reward: 0.3269,                 loss: 0.1874
Score delta: 1.5107532910142982, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/15661_1.
Episode: 16121/101000 (15.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4152s / 1416.1362 s
agent0:                 episode reward: -0.7661,                 loss: 0.2682
agent1:                 episode reward: 0.7661,                 loss: nan
Episode: 16141/101000 (15.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3522s / 1418.4884 s
agent0:                 episode reward: -0.7015,                 loss: 0.2665
agent1:                 episode reward: 0.7015,                 loss: nan
Episode: 16161/101000 (16.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3315s / 1420.8200 s
agent0:                 episode reward: -0.2478,                 loss: 0.2667
agent1:                 episode reward: 0.2478,                 loss: nan
Episode: 16181/101000 (16.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1486s / 1422.9686 s
agent0:                 episode reward: -0.4864,                 loss: 0.2714
agent1:                 episode reward: 0.4864,                 loss: nan
Episode: 16201/101000 (16.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1205s / 1425.0890 s
agent0:                 episode reward: -0.3438,                 loss: 0.2687
agent1:                 episode reward: 0.3438,                 loss: nan
Episode: 16221/101000 (16.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9487s / 1427.0377 s
agent0:                 episode reward: -0.2026,                 loss: 0.2673
agent1:                 episode reward: 0.2026,                 loss: nan
Episode: 16241/101000 (16.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2097s / 1429.2474 s
agent0:                 episode reward: -0.1500,                 loss: 0.2427
agent1:                 episode reward: 0.1500,                 loss: nan
Episode: 16261/101000 (16.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4307s / 1431.6782 s
agent0:                 episode reward: -0.0884,                 loss: 0.2397
agent1:                 episode reward: 0.0884,                 loss: nan
Episode: 16281/101000 (16.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2969s / 1433.9751 s
agent0:                 episode reward: -0.2961,                 loss: 0.2395
agent1:                 episode reward: 0.2961,                 loss: nan
Episode: 16301/101000 (16.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1415s / 1436.1165 s
agent0:                 episode reward: -0.2653,                 loss: 0.2384
agent1:                 episode reward: 0.2653,                 loss: nan
Episode: 16321/101000 (16.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2547s / 1438.3712 s
agent0:                 episode reward: -0.3463,                 loss: 0.2414
agent1:                 episode reward: 0.3463,                 loss: nan
Episode: 16341/101000 (16.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0297s / 1440.4009 s
agent0:                 episode reward: -0.3191,                 loss: 0.2367
agent1:                 episode reward: 0.3191,                 loss: nan
Episode: 16361/101000 (16.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3842s / 1442.7852 s
agent0:                 episode reward: -0.8947,                 loss: 0.2388
agent1:                 episode reward: 0.8947,                 loss: nan
Episode: 16381/101000 (16.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9043s / 1444.6894 s
agent0:                 episode reward: -0.0286,                 loss: 0.2388
agent1:                 episode reward: 0.0286,                 loss: nan
Episode: 16401/101000 (16.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3163s / 1447.0057 s
agent0:                 episode reward: -0.2785,                 loss: 0.2409
agent1:                 episode reward: 0.2785,                 loss: nan
Episode: 16421/101000 (16.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4762s / 1449.4819 s
agent0:                 episode reward: -0.2107,                 loss: 0.2412
agent1:                 episode reward: 0.2107,                 loss: nan
Episode: 16441/101000 (16.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0793s / 1451.5613 s
agent0:                 episode reward: -0.1906,                 loss: 0.2373
agent1:                 episode reward: 0.1906,                 loss: nan
Episode: 16461/101000 (16.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5585s / 1454.1198 s
agent0:                 episode reward: -0.3090,                 loss: 0.2377
agent1:                 episode reward: 0.3090,                 loss: nan
Episode: 16481/101000 (16.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5178s / 1456.6375 s
agent0:                 episode reward: -0.0922,                 loss: 0.2401
agent1:                 episode reward: 0.0922,                 loss: nan
Episode: 16501/101000 (16.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9182s / 1458.5557 s
agent0:                 episode reward: 0.2751,                 loss: 0.2428
agent1:                 episode reward: -0.2751,                 loss: 0.1754
Score delta: 1.7798236534597343, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/16059_0.
Episode: 16521/101000 (16.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0062s / 1460.5619 s
agent0:                 episode reward: -0.3729,                 loss: 0.2709
agent1:                 episode reward: 0.3729,                 loss: 0.1757
Score delta: 1.7494999521585874, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/16081_1.
Episode: 16541/101000 (16.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5284s / 1463.0904 s
agent0:                 episode reward: -0.4428,                 loss: 0.2681
agent1:                 episode reward: 0.4428,                 loss: nan
Episode: 16561/101000 (16.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0833s / 1465.1736 s
agent0:                 episode reward: -0.6662,                 loss: 0.2672
agent1:                 episode reward: 0.6662,                 loss: nan
Episode: 16581/101000 (16.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6804s / 1467.8540 s
agent0:                 episode reward: -0.4736,                 loss: 0.2648
agent1:                 episode reward: 0.4736,                 loss: nan
Episode: 16601/101000 (16.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0233s / 1469.8773 s
agent0:                 episode reward: 0.0721,                 loss: 0.2322
agent1:                 episode reward: -0.0721,                 loss: nan
Episode: 16621/101000 (16.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2814s / 1472.1587 s
agent0:                 episode reward: -0.3345,                 loss: 0.2352
agent1:                 episode reward: 0.3345,                 loss: nan
Episode: 16641/101000 (16.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9243s / 1474.0830 s
agent0:                 episode reward: -0.6395,                 loss: 0.2316
agent1:                 episode reward: 0.6395,                 loss: nan
Episode: 16661/101000 (16.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3750s / 1476.4580 s
agent0:                 episode reward: 0.0083,                 loss: 0.2319
agent1:                 episode reward: -0.0083,                 loss: nan
Episode: 16681/101000 (16.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6568s / 1479.1148 s
agent0:                 episode reward: -0.0853,                 loss: 0.2315
agent1:                 episode reward: 0.0853,                 loss: nan
Episode: 16701/101000 (16.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5641s / 1480.6789 s
agent0:                 episode reward: -0.0530,                 loss: 0.2333
agent1:                 episode reward: 0.0530,                 loss: nan
Episode: 16721/101000 (16.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0687s / 1482.7476 s
agent0:                 episode reward: 0.0387,                 loss: 0.2325
agent1:                 episode reward: -0.0387,                 loss: nan
Episode: 16741/101000 (16.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4781s / 1484.2257 s
agent0:                 episode reward: 0.0308,                 loss: 0.2303
agent1:                 episode reward: -0.0308,                 loss: nan
Episode: 16761/101000 (16.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0426s / 1486.2683 s
agent0:                 episode reward: -0.2595,                 loss: 0.2325
agent1:                 episode reward: 0.2595,                 loss: nan
Episode: 16781/101000 (16.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0754s / 1488.3438 s
agent0:                 episode reward: -0.3831,                 loss: 0.2325
agent1:                 episode reward: 0.3831,                 loss: nan
Episode: 16801/101000 (16.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8269s / 1490.1707 s
agent0:                 episode reward: -0.4445,                 loss: 0.2307
agent1:                 episode reward: 0.4445,                 loss: nan
Episode: 16821/101000 (16.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3242s / 1492.4949 s
agent0:                 episode reward: -0.2638,                 loss: 0.2310
agent1:                 episode reward: 0.2638,                 loss: nan
Episode: 16841/101000 (16.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7584s / 1494.2533 s
agent0:                 episode reward: -0.0374,                 loss: 0.2306
agent1:                 episode reward: 0.0374,                 loss: nan
Episode: 16861/101000 (16.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5427s / 1496.7960 s
agent0:                 episode reward: 0.0243,                 loss: 0.2296
agent1:                 episode reward: -0.0243,                 loss: nan
Episode: 16881/101000 (16.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0688s / 1498.8648 s
agent0:                 episode reward: -0.6081,                 loss: 0.2300
agent1:                 episode reward: 0.6081,                 loss: nan
Episode: 16901/101000 (16.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3117s / 1501.1764 s
agent0:                 episode reward: -0.3553,                 loss: 0.2325
agent1:                 episode reward: 0.3553,                 loss: nan
Episode: 16921/101000 (16.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8768s / 1503.0532 s
agent0:                 episode reward: -0.0697,                 loss: 0.2270
agent1:                 episode reward: 0.0697,                 loss: nan
Episode: 16941/101000 (16.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5791s / 1505.6323 s
agent0:                 episode reward: -0.1583,                 loss: 0.2213
agent1:                 episode reward: 0.1583,                 loss: nan
Episode: 16961/101000 (16.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9307s / 1507.5629 s
agent0:                 episode reward: -0.0555,                 loss: 0.2210
agent1:                 episode reward: 0.0555,                 loss: nan
Episode: 16981/101000 (16.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0639s / 1509.6269 s
agent0:                 episode reward: -0.2671,                 loss: 0.2208
agent1:                 episode reward: 0.2671,                 loss: nan
Episode: 17001/101000 (16.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2408s / 1511.8676 s
agent0:                 episode reward: -0.5711,                 loss: 0.2219
agent1:                 episode reward: 0.5711,                 loss: nan
Episode: 17021/101000 (16.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0279s / 1513.8956 s
agent0:                 episode reward: 0.1574,                 loss: 0.2219
agent1:                 episode reward: -0.1574,                 loss: nan
Episode: 17041/101000 (16.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8108s / 1515.7064 s
agent0:                 episode reward: 0.1765,                 loss: 0.2220
agent1:                 episode reward: -0.1765,                 loss: nan
Episode: 17061/101000 (16.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1724s / 1517.8787 s
agent0:                 episode reward: -0.1847,                 loss: 0.2197
agent1:                 episode reward: 0.1847,                 loss: nan
Episode: 17081/101000 (16.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9627s / 1519.8415 s
agent0:                 episode reward: -0.7505,                 loss: 0.2218
agent1:                 episode reward: 0.7505,                 loss: nan
Episode: 17101/101000 (16.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6938s / 1522.5352 s
agent0:                 episode reward: -0.4651,                 loss: 0.2226
agent1:                 episode reward: 0.4651,                 loss: nan
Episode: 17121/101000 (16.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1206s / 1524.6558 s
agent0:                 episode reward: -0.0965,                 loss: 0.2209
agent1:                 episode reward: 0.0965,                 loss: nan
Episode: 17141/101000 (16.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2560s / 1526.9118 s
agent0:                 episode reward: -0.0938,                 loss: 0.2207
agent1:                 episode reward: 0.0938,                 loss: nan
Episode: 17161/101000 (16.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8722s / 1528.7839 s
agent0:                 episode reward: -0.3803,                 loss: 0.2188
agent1:                 episode reward: 0.3803,                 loss: nan
Episode: 17181/101000 (17.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7099s / 1530.4938 s
agent0:                 episode reward: -0.1278,                 loss: 0.2200
agent1:                 episode reward: 0.1278,                 loss: nan
Episode: 17201/101000 (17.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0521s / 1532.5459 s
agent0:                 episode reward: 0.1004,                 loss: 0.2212
agent1:                 episode reward: -0.1004,                 loss: nan
Episode: 17221/101000 (17.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9233s / 1534.4692 s
agent0:                 episode reward: 0.1497,                 loss: 0.2199
agent1:                 episode reward: -0.1497,                 loss: nan
Episode: 17241/101000 (17.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1697s / 1536.6390 s
agent0:                 episode reward: -0.2125,                 loss: 0.2194
agent1:                 episode reward: 0.2125,                 loss: nan
Episode: 17261/101000 (17.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9468s / 1538.5858 s
agent0:                 episode reward: -0.2059,                 loss: 0.2103
agent1:                 episode reward: 0.2059,                 loss: nan
Episode: 17281/101000 (17.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4060s / 1540.9918 s
agent0:                 episode reward: -0.8372,                 loss: 0.2093
agent1:                 episode reward: 0.8372,                 loss: nan
Episode: 17301/101000 (17.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0206s / 1543.0124 s
agent0:                 episode reward: -0.1046,                 loss: 0.2103
agent1:                 episode reward: 0.1046,                 loss: nan
Episode: 17321/101000 (17.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9017s / 1544.9141 s
agent0:                 episode reward: 0.0143,                 loss: 0.2101
agent1:                 episode reward: -0.0143,                 loss: nan
Episode: 17341/101000 (17.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4931s / 1547.4072 s
agent0:                 episode reward: -0.4701,                 loss: 0.2104
agent1:                 episode reward: 0.4701,                 loss: nan
Episode: 17361/101000 (17.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8856s / 1549.2927 s
agent0:                 episode reward: 0.0273,                 loss: 0.2101
agent1:                 episode reward: -0.0273,                 loss: nan
Episode: 17381/101000 (17.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3489s / 1551.6416 s
agent0:                 episode reward: -0.3752,                 loss: 0.2072
agent1:                 episode reward: 0.3752,                 loss: nan
Episode: 17401/101000 (17.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2715s / 1553.9131 s
agent0:                 episode reward: -0.2547,                 loss: 0.2094
agent1:                 episode reward: 0.2547,                 loss: nan
Episode: 17421/101000 (17.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2376s / 1556.1507 s
agent0:                 episode reward: -0.1721,                 loss: 0.2095
agent1:                 episode reward: 0.1721,                 loss: nan
Episode: 17441/101000 (17.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4547s / 1558.6055 s
agent0:                 episode reward: -0.0383,                 loss: 0.2069
agent1:                 episode reward: 0.0383,                 loss: nan
Episode: 17461/101000 (17.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0828s / 1560.6883 s
agent0:                 episode reward: -0.7505,                 loss: 0.2104
agent1:                 episode reward: 0.7505,                 loss: nan
Episode: 17481/101000 (17.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9527s / 1562.6409 s
agent0:                 episode reward: -0.3364,                 loss: 0.2078
agent1:                 episode reward: 0.3364,                 loss: nan
Episode: 17501/101000 (17.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0959s / 1564.7368 s
agent0:                 episode reward: -0.5133,                 loss: 0.2081
agent1:                 episode reward: 0.5133,                 loss: nan
Episode: 17521/101000 (17.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6588s / 1566.3956 s
agent0:                 episode reward: -0.3849,                 loss: 0.2070
agent1:                 episode reward: 0.3849,                 loss: nan
Episode: 17541/101000 (17.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2398s / 1568.6354 s
agent0:                 episode reward: 0.1386,                 loss: 0.2099
agent1:                 episode reward: -0.1386,                 loss: nan
Episode: 17561/101000 (17.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3350s / 1570.9704 s
agent0:                 episode reward: -0.1714,                 loss: 0.2089
agent1:                 episode reward: 0.1714,                 loss: nan
Episode: 17581/101000 (17.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2855s / 1573.2559 s
agent0:                 episode reward: -0.0915,                 loss: 0.2084
agent1:                 episode reward: 0.0915,                 loss: nan
Episode: 17601/101000 (17.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9950s / 1575.2509 s
agent0:                 episode reward: -0.2462,                 loss: 0.1982
agent1:                 episode reward: 0.2462,                 loss: nan
Episode: 17621/101000 (17.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1360s / 1577.3869 s
agent0:                 episode reward: 0.0769,                 loss: 0.2030
agent1:                 episode reward: -0.0769,                 loss: nan
Episode: 17641/101000 (17.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9041s / 1579.2910 s
agent0:                 episode reward: -0.0180,                 loss: 0.2037
agent1:                 episode reward: 0.0180,                 loss: nan
Episode: 17661/101000 (17.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8278s / 1581.1188 s
agent0:                 episode reward: -0.4622,                 loss: 0.2011
agent1:                 episode reward: 0.4622,                 loss: nan
Episode: 17681/101000 (17.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8740s / 1582.9927 s
agent0:                 episode reward: -0.3429,                 loss: 0.2000
agent1:                 episode reward: 0.3429,                 loss: nan
Episode: 17701/101000 (17.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2784s / 1585.2711 s
agent0:                 episode reward: -0.1076,                 loss: 0.2005
agent1:                 episode reward: 0.1076,                 loss: nan
Episode: 17721/101000 (17.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7219s / 1587.9931 s
agent0:                 episode reward: -0.6345,                 loss: 0.1999
agent1:                 episode reward: 0.6345,                 loss: nan
Episode: 17741/101000 (17.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7879s / 1589.7810 s
agent0:                 episode reward: -0.1729,                 loss: 0.2018
agent1:                 episode reward: 0.1729,                 loss: nan
Episode: 17761/101000 (17.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0982s / 1591.8792 s
agent0:                 episode reward: -0.3885,                 loss: 0.2000
agent1:                 episode reward: 0.3885,                 loss: nan
Episode: 17781/101000 (17.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2872s / 1594.1663 s
agent0:                 episode reward: -0.0352,                 loss: 0.2006
agent1:                 episode reward: 0.0352,                 loss: nan
Episode: 17801/101000 (17.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0929s / 1596.2593 s
agent0:                 episode reward: -0.5415,                 loss: 0.2012
agent1:                 episode reward: 0.5415,                 loss: nan
Episode: 17821/101000 (17.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4249s / 1598.6842 s
agent0:                 episode reward: -0.1964,                 loss: 0.2004
agent1:                 episode reward: 0.1964,                 loss: nan
Episode: 17841/101000 (17.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3595s / 1601.0437 s
agent0:                 episode reward: -0.1298,                 loss: 0.2020
agent1:                 episode reward: 0.1298,                 loss: nan
Episode: 17861/101000 (17.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9507s / 1602.9943 s
agent0:                 episode reward: -0.4327,                 loss: 0.2011
agent1:                 episode reward: 0.4327,                 loss: nan
Episode: 17881/101000 (17.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9249s / 1604.9192 s
agent0:                 episode reward: 0.2191,                 loss: 0.2011
agent1:                 episode reward: -0.2191,                 loss: nan
Episode: 17901/101000 (17.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1950s / 1607.1142 s
agent0:                 episode reward: -0.3831,                 loss: 0.2026
agent1:                 episode reward: 0.3831,                 loss: nan
Episode: 17921/101000 (17.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6013s / 1609.7156 s
agent0:                 episode reward: -0.5451,                 loss: 0.1981
agent1:                 episode reward: 0.5451,                 loss: nan
Episode: 17941/101000 (17.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0643s / 1611.7799 s
agent0:                 episode reward: -0.2537,                 loss: 0.1935
agent1:                 episode reward: 0.2537,                 loss: nan
Episode: 17961/101000 (17.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3440s / 1614.1239 s
agent0:                 episode reward: 0.2900,                 loss: 0.1921
agent1:                 episode reward: -0.2900,                 loss: nan
Episode: 17981/101000 (17.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1145s / 1616.2384 s
agent0:                 episode reward: 0.2352,                 loss: 0.1951
agent1:                 episode reward: -0.2352,                 loss: nan
Episode: 18001/101000 (17.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3919s / 1618.6303 s
agent0:                 episode reward: -0.4433,                 loss: 0.1942
agent1:                 episode reward: 0.4433,                 loss: nan
Episode: 18021/101000 (17.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1941s / 1620.8244 s
agent0:                 episode reward: -0.1314,                 loss: 0.1940
agent1:                 episode reward: 0.1314,                 loss: nan
Episode: 18041/101000 (17.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1954s / 1623.0198 s
agent0:                 episode reward: 0.2736,                 loss: 0.1947
agent1:                 episode reward: -0.2736,                 loss: 0.1702
Score delta: 1.5794678129272661, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/17612_0.
Episode: 18061/101000 (17.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3839s / 1625.4037 s
agent0:                 episode reward: -0.6907,                 loss: nan
agent1:                 episode reward: 0.6907,                 loss: 0.1713
Score delta: 2.0169485620636345, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/17635_1.
Episode: 18081/101000 (17.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2213s / 1627.6250 s
agent0:                 episode reward: -0.6140,                 loss: 0.2291
agent1:                 episode reward: 0.6140,                 loss: nan
Episode: 18101/101000 (17.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0263s / 1629.6512 s
agent0:                 episode reward: -0.1913,                 loss: 0.2254
agent1:                 episode reward: 0.1913,                 loss: 0.1726
Score delta: 1.5826508347993646, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/17668_0.
Episode: 18121/101000 (17.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6737s / 1632.3249 s
agent0:                 episode reward: -0.9238,                 loss: 0.2707
agent1:                 episode reward: 0.9238,                 loss: 0.1723
Score delta: 1.7229228794074725, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/17692_1.
Episode: 18141/101000 (17.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1979s / 1634.5228 s
agent0:                 episode reward: 0.3967,                 loss: 0.2658
agent1:                 episode reward: -0.3967,                 loss: nan
Episode: 18161/101000 (17.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6923s / 1636.2151 s
agent0:                 episode reward: -0.4538,                 loss: 0.2667
agent1:                 episode reward: 0.4538,                 loss: 0.1721
Score delta: 1.7112644364674192, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/17717_0.
Episode: 18181/101000 (18.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2990s / 1638.5141 s
agent0:                 episode reward: -0.4561,                 loss: 0.2383
agent1:                 episode reward: 0.4561,                 loss: 0.1742
Score delta: 1.9213444986524404, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/17742_1.
Episode: 18201/101000 (18.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0828s / 1640.5969 s
agent0:                 episode reward: -0.1558,                 loss: 0.2421
agent1:                 episode reward: 0.1558,                 loss: nan
Episode: 18221/101000 (18.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9324s / 1642.5292 s
agent0:                 episode reward: -0.3571,                 loss: 0.2403
agent1:                 episode reward: 0.3571,                 loss: nan
Episode: 18241/101000 (18.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1072s / 1644.6364 s
agent0:                 episode reward: 0.3123,                 loss: 0.2412
agent1:                 episode reward: -0.3123,                 loss: nan
Episode: 18261/101000 (18.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2644s / 1646.9009 s
agent0:                 episode reward: -0.3825,                 loss: 0.2421
agent1:                 episode reward: 0.3825,                 loss: nan
Episode: 18281/101000 (18.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9088s / 1648.8096 s
agent0:                 episode reward: -1.0292,                 loss: 0.2428
agent1:                 episode reward: 1.0292,                 loss: nan
Episode: 18301/101000 (18.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0616s / 1650.8712 s
agent0:                 episode reward: -0.3119,                 loss: 0.2405
agent1:                 episode reward: 0.3119,                 loss: nan
Episode: 18321/101000 (18.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0934s / 1652.9646 s
agent0:                 episode reward: 0.0577,                 loss: 0.2389
agent1:                 episode reward: -0.0577,                 loss: nan
Episode: 18341/101000 (18.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9465s / 1654.9111 s
agent0:                 episode reward: -0.5501,                 loss: 0.2221
agent1:                 episode reward: 0.5501,                 loss: nan
Episode: 18361/101000 (18.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8961s / 1656.8072 s
agent0:                 episode reward: -0.2392,                 loss: 0.2230
agent1:                 episode reward: 0.2392,                 loss: nan
Episode: 18381/101000 (18.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1380s / 1658.9452 s
agent0:                 episode reward: -0.2584,                 loss: 0.2204
agent1:                 episode reward: 0.2584,                 loss: nan
Episode: 18401/101000 (18.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2733s / 1661.2185 s
agent0:                 episode reward: -0.6058,                 loss: 0.2235
agent1:                 episode reward: 0.6058,                 loss: nan
Episode: 18421/101000 (18.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0354s / 1663.2538 s
agent0:                 episode reward: -0.0800,                 loss: 0.2223
agent1:                 episode reward: 0.0800,                 loss: nan
Episode: 18441/101000 (18.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2633s / 1665.5172 s
agent0:                 episode reward: -0.1849,                 loss: 0.2244
agent1:                 episode reward: 0.1849,                 loss: nan
Episode: 18461/101000 (18.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6356s / 1668.1528 s
agent0:                 episode reward: -0.6462,                 loss: 0.2247
agent1:                 episode reward: 0.6462,                 loss: nan
Episode: 18481/101000 (18.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9652s / 1671.1180 s
agent0:                 episode reward: -0.3976,                 loss: 0.2238
agent1:                 episode reward: 0.3976,                 loss: nan
Episode: 18501/101000 (18.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4380s / 1673.5560 s
agent0:                 episode reward: -0.8227,                 loss: 0.2224
agent1:                 episode reward: 0.8227,                 loss: nan
Episode: 18521/101000 (18.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6514s / 1676.2074 s
agent0:                 episode reward: -0.3690,                 loss: 0.2216
agent1:                 episode reward: 0.3690,                 loss: nan
Episode: 18541/101000 (18.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1319s / 1679.3393 s
agent0:                 episode reward: -0.2383,                 loss: 0.2233
agent1:                 episode reward: 0.2383,                 loss: nan
Episode: 18561/101000 (18.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2574s / 1681.5967 s
agent0:                 episode reward: 0.2362,                 loss: 0.2220
agent1:                 episode reward: -0.2362,                 loss: nan
Episode: 18581/101000 (18.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8656s / 1683.4623 s
agent0:                 episode reward: -0.4604,                 loss: 0.2202
agent1:                 episode reward: 0.4604,                 loss: nan
Episode: 18601/101000 (18.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5158s / 1685.9781 s
agent0:                 episode reward: -0.1633,                 loss: 0.2201
agent1:                 episode reward: 0.1633,                 loss: nan
Episode: 18621/101000 (18.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1330s / 1688.1111 s
agent0:                 episode reward: -0.2399,                 loss: 0.2206
agent1:                 episode reward: 0.2399,                 loss: nan
Episode: 18641/101000 (18.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6376s / 1690.7486 s
agent0:                 episode reward: -0.2983,                 loss: 0.2198
agent1:                 episode reward: 0.2983,                 loss: nan
Episode: 18661/101000 (18.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4571s / 1693.2058 s
agent0:                 episode reward: -0.3033,                 loss: 0.2155
agent1:                 episode reward: 0.3033,                 loss: nan
Episode: 18681/101000 (18.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5139s / 1695.7197 s
agent0:                 episode reward: 0.0186,                 loss: 0.2137
agent1:                 episode reward: -0.0186,                 loss: nan
Episode: 18701/101000 (18.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9277s / 1697.6474 s
agent0:                 episode reward: -0.1477,                 loss: 0.2123
agent1:                 episode reward: 0.1477,                 loss: nan
Episode: 18721/101000 (18.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0106s / 1699.6580 s
agent0:                 episode reward: -0.5687,                 loss: 0.2116
agent1:                 episode reward: 0.5687,                 loss: nan
Episode: 18741/101000 (18.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7330s / 1702.3910 s
agent0:                 episode reward: 0.1283,                 loss: 0.2123
agent1:                 episode reward: -0.1283,                 loss: 0.1733
Score delta: 1.5486654785911331, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/18308_0.
Episode: 18761/101000 (18.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2110s / 1704.6020 s
agent0:                 episode reward: -0.7985,                 loss: 0.2724
agent1:                 episode reward: 0.7985,                 loss: 0.1719
Score delta: 1.674787287960975, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/18334_1.
Episode: 18781/101000 (18.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7051s / 1706.3071 s
agent0:                 episode reward: -0.8341,                 loss: 0.2673
agent1:                 episode reward: 0.8341,                 loss: nan
Episode: 18801/101000 (18.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8869s / 1708.1941 s
agent0:                 episode reward: -0.2392,                 loss: 0.2678
agent1:                 episode reward: 0.2392,                 loss: nan
Episode: 18821/101000 (18.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0622s / 1710.2562 s
agent0:                 episode reward: -0.1459,                 loss: 0.2733
agent1:                 episode reward: 0.1459,                 loss: 0.1763
Score delta: 1.5488030542111753, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/18384_0.
Episode: 18841/101000 (18.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3027s / 1712.5590 s
agent0:                 episode reward: -0.6356,                 loss: nan
agent1:                 episode reward: 0.6356,                 loss: 0.1734
Score delta: 1.7121811687058925, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/18415_1.
Episode: 18861/101000 (18.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0380s / 1714.5970 s
agent0:                 episode reward: -0.7272,                 loss: 0.1883
agent1:                 episode reward: 0.7272,                 loss: nan
Episode: 18881/101000 (18.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2689s / 1716.8659 s
agent0:                 episode reward: -0.8938,                 loss: 0.1786
agent1:                 episode reward: 0.8938,                 loss: nan
Episode: 18901/101000 (18.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6381s / 1719.5040 s
agent0:                 episode reward: -0.7629,                 loss: 0.1754
agent1:                 episode reward: 0.7629,                 loss: nan
Episode: 18921/101000 (18.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2523s / 1721.7563 s
agent0:                 episode reward: -0.6007,                 loss: 0.1715
agent1:                 episode reward: 0.6007,                 loss: nan
Episode: 18941/101000 (18.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1416s / 1723.8979 s
agent0:                 episode reward: -0.1616,                 loss: 0.1699
agent1:                 episode reward: 0.1616,                 loss: nan
Episode: 18961/101000 (18.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6353s / 1726.5331 s
agent0:                 episode reward: -0.8298,                 loss: 0.1679
agent1:                 episode reward: 0.8298,                 loss: nan
Episode: 18981/101000 (18.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6839s / 1729.2170 s
agent0:                 episode reward: -0.6976,                 loss: 0.1677
agent1:                 episode reward: 0.6976,                 loss: nan
Episode: 19001/101000 (18.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8508s / 1732.0678 s
agent0:                 episode reward: -1.1082,                 loss: 0.1682
agent1:                 episode reward: 1.1082,                 loss: nan
Episode: 19021/101000 (18.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4999s / 1734.5676 s
agent0:                 episode reward: -0.4969,                 loss: 0.1678
agent1:                 episode reward: 0.4969,                 loss: nan
Episode: 19041/101000 (18.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7923s / 1737.3599 s
agent0:                 episode reward: -0.6304,                 loss: 0.1650
agent1:                 episode reward: 0.6304,                 loss: nan
Episode: 19061/101000 (18.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9116s / 1740.2715 s
agent0:                 episode reward: -0.3702,                 loss: 0.1748
agent1:                 episode reward: 0.3702,                 loss: nan
Episode: 19081/101000 (18.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8056s / 1743.0770 s
agent0:                 episode reward: -0.4795,                 loss: 0.1734
agent1:                 episode reward: 0.4795,                 loss: nan
Episode: 19101/101000 (18.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5113s / 1745.5883 s
agent0:                 episode reward: -0.2209,                 loss: 0.1725
agent1:                 episode reward: 0.2209,                 loss: nan
Episode: 19121/101000 (18.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6217s / 1748.2100 s
agent0:                 episode reward: 0.0390,                 loss: 0.1702
agent1:                 episode reward: -0.0390,                 loss: nan
Episode: 19141/101000 (18.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6317s / 1750.8418 s
agent0:                 episode reward: -0.4508,                 loss: 0.1701
agent1:                 episode reward: 0.4508,                 loss: nan
Episode: 19161/101000 (18.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3381s / 1753.1798 s
agent0:                 episode reward: 0.1043,                 loss: 0.1689
agent1:                 episode reward: -0.1043,                 loss: nan
Episode: 19181/101000 (18.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8747s / 1756.0545 s
agent0:                 episode reward: -0.2685,                 loss: 0.1680
agent1:                 episode reward: 0.2685,                 loss: nan
Episode: 19201/101000 (19.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9386s / 1758.9931 s
agent0:                 episode reward: -0.5141,                 loss: 0.1661
agent1:                 episode reward: 0.5141,                 loss: nan
Episode: 19221/101000 (19.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1614s / 1762.1545 s
agent0:                 episode reward: -0.5975,                 loss: 0.1639
agent1:                 episode reward: 0.5975,                 loss: nan
Episode: 19241/101000 (19.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4387s / 1765.5932 s
agent0:                 episode reward: -1.1272,                 loss: 0.1643
agent1:                 episode reward: 1.1272,                 loss: nan
Episode: 19261/101000 (19.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5845s / 1770.1776 s
agent0:                 episode reward: -0.0985,                 loss: 0.1635
agent1:                 episode reward: 0.0985,                 loss: nan
Episode: 19281/101000 (19.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5605s / 1774.7382 s
agent0:                 episode reward: 0.0705,                 loss: 0.1611
agent1:                 episode reward: -0.0705,                 loss: nan
Episode: 19301/101000 (19.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8766s / 1777.6148 s
agent0:                 episode reward: -0.3210,                 loss: 0.1619
agent1:                 episode reward: 0.3210,                 loss: nan
Episode: 19321/101000 (19.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9821s / 1780.5969 s
agent0:                 episode reward: -0.8626,                 loss: 0.1609
agent1:                 episode reward: 0.8626,                 loss: nan
Episode: 19341/101000 (19.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3673s / 1784.9642 s
agent0:                 episode reward: -0.4260,                 loss: 0.1581
agent1:                 episode reward: 0.4260,                 loss: nan
Episode: 19361/101000 (19.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4723s / 1788.4365 s
agent0:                 episode reward: -0.4098,                 loss: 0.1607
agent1:                 episode reward: 0.4098,                 loss: nan
Episode: 19381/101000 (19.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0147s / 1792.4512 s
agent0:                 episode reward: -0.9601,                 loss: 0.1677
agent1:                 episode reward: 0.9601,                 loss: nan
Episode: 19401/101000 (19.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6383s / 1796.0895 s
agent0:                 episode reward: -0.8720,                 loss: 0.1887
agent1:                 episode reward: 0.8720,                 loss: nan
Episode: 19421/101000 (19.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1214s / 1800.2109 s
agent0:                 episode reward: -0.4422,                 loss: 0.1903
agent1:                 episode reward: 0.4422,                 loss: nan
Episode: 19441/101000 (19.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1918s / 1804.4026 s
agent0:                 episode reward: -0.3847,                 loss: 0.1886
agent1:                 episode reward: 0.3847,                 loss: nan
Episode: 19461/101000 (19.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6943s / 1809.0969 s
agent0:                 episode reward: -0.4452,                 loss: 0.1859
agent1:                 episode reward: 0.4452,                 loss: nan
Episode: 19481/101000 (19.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1655s / 1813.2624 s
agent0:                 episode reward: -0.9192,                 loss: 0.1852
agent1:                 episode reward: 0.9192,                 loss: nan
Episode: 19501/101000 (19.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0413s / 1817.3037 s
agent0:                 episode reward: -0.9950,                 loss: 0.1854
agent1:                 episode reward: 0.9950,                 loss: nan
Episode: 19521/101000 (19.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2583s / 1822.5620 s
agent0:                 episode reward: -0.0484,                 loss: 0.1826
agent1:                 episode reward: 0.0484,                 loss: nan
Episode: 19541/101000 (19.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4129s / 1826.9749 s
agent0:                 episode reward: -0.6319,                 loss: 0.1834
agent1:                 episode reward: 0.6319,                 loss: nan
Episode: 19561/101000 (19.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1634s / 1832.1383 s
agent0:                 episode reward: -0.5483,                 loss: 0.1862
agent1:                 episode reward: 0.5483,                 loss: nan
Episode: 19581/101000 (19.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8595s / 1835.9978 s
agent0:                 episode reward: -0.3081,                 loss: 0.1834
agent1:                 episode reward: 0.3081,                 loss: nan
Episode: 19601/101000 (19.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9404s / 1841.9383 s
agent0:                 episode reward: -0.5587,                 loss: 0.1852
agent1:                 episode reward: 0.5587,                 loss: nan
Episode: 19621/101000 (19.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8627s / 1848.8010 s
agent0:                 episode reward: -0.4450,                 loss: 0.1863
agent1:                 episode reward: 0.4450,                 loss: nan
Episode: 19641/101000 (19.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7556s / 1854.5566 s
agent0:                 episode reward: -0.0989,                 loss: 0.1859
agent1:                 episode reward: 0.0989,                 loss: nan
Episode: 19661/101000 (19.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6725s / 1861.2292 s
agent0:                 episode reward: -0.3509,                 loss: 0.1842
agent1:                 episode reward: 0.3509,                 loss: nan
Episode: 19681/101000 (19.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6990s / 1868.9282 s
agent0:                 episode reward: -0.8259,                 loss: 0.1830
agent1:                 episode reward: 0.8259,                 loss: nan
Episode: 19701/101000 (19.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9043s / 1875.8325 s
agent0:                 episode reward: -0.7625,                 loss: 0.1843
agent1:                 episode reward: 0.7625,                 loss: nan
Episode: 19721/101000 (19.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0972s / 1881.9297 s
agent0:                 episode reward: -0.3714,                 loss: 0.1971
agent1:                 episode reward: 0.3714,                 loss: nan
Episode: 19741/101000 (19.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8217s / 1889.7514 s
agent0:                 episode reward: -0.2549,                 loss: 0.2046
agent1:                 episode reward: 0.2549,                 loss: nan
Episode: 19761/101000 (19.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6264s / 1896.3778 s
agent0:                 episode reward: -0.1217,                 loss: 0.2046
agent1:                 episode reward: 0.1217,                 loss: nan
Episode: 19781/101000 (19.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5993s / 1904.9771 s
agent0:                 episode reward: -0.6002,                 loss: 0.2052
agent1:                 episode reward: 0.6002,                 loss: nan
Episode: 19801/101000 (19.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3334s / 1911.3105 s
agent0:                 episode reward: -0.1462,                 loss: 0.2014
agent1:                 episode reward: 0.1462,                 loss: nan
Episode: 19821/101000 (19.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9707s / 1918.2812 s
agent0:                 episode reward: -0.3895,                 loss: 0.2039
agent1:                 episode reward: 0.3895,                 loss: nan
Episode: 19841/101000 (19.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2339s / 1924.5151 s
agent0:                 episode reward: -0.2283,                 loss: 0.2038
agent1:                 episode reward: 0.2283,                 loss: nan
Episode: 19861/101000 (19.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6130s / 1932.1280 s
agent0:                 episode reward: -0.4513,                 loss: 0.2033
agent1:                 episode reward: 0.4513,                 loss: nan
Episode: 19881/101000 (19.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9649s / 1939.0929 s
agent0:                 episode reward: -0.6118,                 loss: 0.2028
agent1:                 episode reward: 0.6118,                 loss: nan
Episode: 19901/101000 (19.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1305s / 1944.2234 s
agent0:                 episode reward: -0.4646,                 loss: 0.2031
agent1:                 episode reward: 0.4646,                 loss: nan
Episode: 19921/101000 (19.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7254s / 1952.9488 s
agent0:                 episode reward: -0.5763,                 loss: 0.2033
agent1:                 episode reward: 0.5763,                 loss: nan
Episode: 19941/101000 (19.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3640s / 1959.3127 s
agent0:                 episode reward: -0.3013,                 loss: 0.2041
agent1:                 episode reward: 0.3013,                 loss: nan
Episode: 19961/101000 (19.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5432s / 1965.8559 s
agent0:                 episode reward: 0.1452,                 loss: 0.2037
agent1:                 episode reward: -0.1452,                 loss: nan
Episode: 19981/101000 (19.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0955s / 1970.9514 s
agent0:                 episode reward: -0.8116,                 loss: 0.2029
agent1:                 episode reward: 0.8116,                 loss: nan
Episode: 20001/101000 (19.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6319s / 1977.5833 s
agent0:                 episode reward: -0.5171,                 loss: 0.2044
agent1:                 episode reward: 0.5171,                 loss: nan
Episode: 20021/101000 (19.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8748s / 1984.4582 s
agent0:                 episode reward: -0.5160,                 loss: 0.2031
agent1:                 episode reward: 0.5160,                 loss: nan
Episode: 20041/101000 (19.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1538s / 1990.6120 s
agent0:                 episode reward: -0.2091,                 loss: 0.2034
agent1:                 episode reward: 0.2091,                 loss: nan
Episode: 20061/101000 (19.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2591s / 1997.8711 s
agent0:                 episode reward: -0.5714,                 loss: 0.2122
agent1:                 episode reward: 0.5714,                 loss: nan
Episode: 20081/101000 (19.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6939s / 2005.5649 s
agent0:                 episode reward: 0.0748,                 loss: 0.2123
agent1:                 episode reward: -0.0748,                 loss: nan
Episode: 20101/101000 (19.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0924s / 2011.6574 s
agent0:                 episode reward: -0.2969,                 loss: 0.2083
agent1:                 episode reward: 0.2969,                 loss: nan
Episode: 20121/101000 (19.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3309s / 2018.9882 s
agent0:                 episode reward: -0.4536,                 loss: 0.2105
agent1:                 episode reward: 0.4536,                 loss: nan
Episode: 20141/101000 (19.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1541s / 2027.1423 s
agent0:                 episode reward: -0.0769,                 loss: 0.2100
agent1:                 episode reward: 0.0769,                 loss: nan
Episode: 20161/101000 (19.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6906s / 2033.8329 s
agent0:                 episode reward: -0.1955,                 loss: 0.2101
agent1:                 episode reward: 0.1955,                 loss: nan
Episode: 20181/101000 (19.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0627s / 2040.8957 s
agent0:                 episode reward: -0.0221,                 loss: 0.2106
agent1:                 episode reward: 0.0221,                 loss: nan
Episode: 20201/101000 (20.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3123s / 2048.2079 s
agent0:                 episode reward: -0.1958,                 loss: 0.2103
agent1:                 episode reward: 0.1958,                 loss: nan
Episode: 20221/101000 (20.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5297s / 2054.7376 s
agent0:                 episode reward: -0.5387,                 loss: 0.2122
agent1:                 episode reward: 0.5387,                 loss: nan
Episode: 20241/101000 (20.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6872s / 2062.4248 s
agent0:                 episode reward: -0.2716,                 loss: 0.2094
agent1:                 episode reward: 0.2716,                 loss: nan
Episode: 20261/101000 (20.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8760s / 2069.3008 s
agent0:                 episode reward: -0.3202,                 loss: 0.2078
agent1:                 episode reward: 0.3202,                 loss: nan
Episode: 20281/101000 (20.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4524s / 2075.7532 s
agent0:                 episode reward: -0.3944,                 loss: 0.2101
agent1:                 episode reward: 0.3944,                 loss: nan
Episode: 20301/101000 (20.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6875s / 2083.4407 s
agent0:                 episode reward: -0.2315,                 loss: 0.2134
agent1:                 episode reward: 0.2315,                 loss: nan
Episode: 20321/101000 (20.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6297s / 2087.0704 s
agent0:                 episode reward: -0.0434,                 loss: 0.2090
agent1:                 episode reward: 0.0434,                 loss: nan
Episode: 20341/101000 (20.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9788s / 2094.0492 s
agent0:                 episode reward: -0.1329,                 loss: 0.2082
agent1:                 episode reward: 0.1329,                 loss: nan
Episode: 20361/101000 (20.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4750s / 2101.5242 s
agent0:                 episode reward: -0.2309,                 loss: 0.2109
agent1:                 episode reward: 0.2309,                 loss: nan
Episode: 20381/101000 (20.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0244s / 2108.5486 s
agent0:                 episode reward: -0.0725,                 loss: 0.2095
agent1:                 episode reward: 0.0725,                 loss: nan
Episode: 20401/101000 (20.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0259s / 2114.5745 s
agent0:                 episode reward: -0.1382,                 loss: 0.2075
agent1:                 episode reward: 0.1382,                 loss: nan
Episode: 20421/101000 (20.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7129s / 2120.2874 s
agent0:                 episode reward: -0.4195,                 loss: 0.2065
agent1:                 episode reward: 0.4195,                 loss: nan
Episode: 20441/101000 (20.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0663s / 2125.3536 s
agent0:                 episode reward: -0.3608,                 loss: 0.2058
agent1:                 episode reward: 0.3608,                 loss: nan
Episode: 20461/101000 (20.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1220s / 2131.4756 s
agent0:                 episode reward: -0.0528,                 loss: 0.2042
agent1:                 episode reward: 0.0528,                 loss: nan
Episode: 20481/101000 (20.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9729s / 2138.4485 s
agent0:                 episode reward: -0.4674,                 loss: 0.2044
agent1:                 episode reward: 0.4674,                 loss: nan
Episode: 20501/101000 (20.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2835s / 2145.7320 s
agent0:                 episode reward: -0.5105,                 loss: 0.2056
agent1:                 episode reward: 0.5105,                 loss: nan
Episode: 20521/101000 (20.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9216s / 2153.6536 s
agent0:                 episode reward: -0.1256,                 loss: 0.2052
agent1:                 episode reward: 0.1256,                 loss: nan
Episode: 20541/101000 (20.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7516s / 2161.4052 s
agent0:                 episode reward: -0.3253,                 loss: 0.2041
agent1:                 episode reward: 0.3253,                 loss: nan
Episode: 20561/101000 (20.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9653s / 2168.3705 s
agent0:                 episode reward: -0.2964,                 loss: 0.2041
agent1:                 episode reward: 0.2964,                 loss: nan
Episode: 20581/101000 (20.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0549s / 2174.4253 s
agent0:                 episode reward: -0.4842,                 loss: 0.2052
agent1:                 episode reward: 0.4842,                 loss: nan
Episode: 20601/101000 (20.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9319s / 2181.3573 s
agent0:                 episode reward: -0.3301,                 loss: 0.2064
agent1:                 episode reward: 0.3301,                 loss: nan
Episode: 20621/101000 (20.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6227s / 2188.9799 s
agent0:                 episode reward: -0.1876,                 loss: 0.2041
agent1:                 episode reward: 0.1876,                 loss: nan
Episode: 20641/101000 (20.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0097s / 2195.9897 s
agent0:                 episode reward: -0.3061,                 loss: 0.2046
agent1:                 episode reward: 0.3061,                 loss: nan
Episode: 20661/101000 (20.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4634s / 2201.4531 s
agent0:                 episode reward: -0.3599,                 loss: 0.2027
agent1:                 episode reward: 0.3599,                 loss: nan
Episode: 20681/101000 (20.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1186s / 2207.5717 s
agent0:                 episode reward: -0.0098,                 loss: 0.2071
agent1:                 episode reward: 0.0098,                 loss: nan
Episode: 20701/101000 (20.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8491s / 2213.4208 s
agent0:                 episode reward: -0.1531,                 loss: 0.2059
agent1:                 episode reward: 0.1531,                 loss: nan
Episode: 20721/101000 (20.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2590s / 2220.6797 s
agent0:                 episode reward: -0.3305,                 loss: 0.2117
agent1:                 episode reward: 0.3305,                 loss: nan
Episode: 20741/101000 (20.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1336s / 2226.8133 s
agent0:                 episode reward: -0.2060,                 loss: 0.2140
agent1:                 episode reward: 0.2060,                 loss: nan
Episode: 20761/101000 (20.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4777s / 2234.2910 s
agent0:                 episode reward: -0.2399,                 loss: 0.2144
agent1:                 episode reward: 0.2399,                 loss: nan
Episode: 20781/101000 (20.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3108s / 2242.6018 s
agent0:                 episode reward: -0.5164,                 loss: 0.2124
agent1:                 episode reward: 0.5164,                 loss: nan
Episode: 20801/101000 (20.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5114s / 2250.1132 s
agent0:                 episode reward: -0.1491,                 loss: 0.2151
agent1:                 episode reward: 0.1491,                 loss: nan
Episode: 20821/101000 (20.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0595s / 2258.1727 s
agent0:                 episode reward: -0.3499,                 loss: 0.2139
agent1:                 episode reward: 0.3499,                 loss: nan
Episode: 20841/101000 (20.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1084s / 2265.2811 s
agent0:                 episode reward: -0.2811,                 loss: 0.2131
agent1:                 episode reward: 0.2811,                 loss: nan
Episode: 20861/101000 (20.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4679s / 2272.7490 s
agent0:                 episode reward: -0.2063,                 loss: 0.2133
agent1:                 episode reward: 0.2063,                 loss: nan
Episode: 20881/101000 (20.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8059s / 2278.5549 s
agent0:                 episode reward: -0.5671,                 loss: 0.2123
agent1:                 episode reward: 0.5671,                 loss: nan
Episode: 20901/101000 (20.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4506s / 2283.0055 s
agent0:                 episode reward: -0.4574,                 loss: 0.2128
agent1:                 episode reward: 0.4574,                 loss: nan
Episode: 20921/101000 (20.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8073s / 2291.8128 s
agent0:                 episode reward: -0.1617,                 loss: 0.2129
agent1:                 episode reward: 0.1617,                 loss: nan
Episode: 20941/101000 (20.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1245s / 2299.9373 s
agent0:                 episode reward: -0.0477,                 loss: 0.2120
agent1:                 episode reward: 0.0477,                 loss: nan
Episode: 20961/101000 (20.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8961s / 2306.8333 s
agent0:                 episode reward: -0.1183,                 loss: 0.2130
agent1:                 episode reward: 0.1183,                 loss: nan
Episode: 20981/101000 (20.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9962s / 2312.8295 s
agent0:                 episode reward: 0.4474,                 loss: 0.2142
agent1:                 episode reward: -0.4474,                 loss: nan
Episode: 21001/101000 (20.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8234s / 2319.6529 s
agent0:                 episode reward: -0.2572,                 loss: 0.2109
agent1:                 episode reward: 0.2572,                 loss: nan
Episode: 21021/101000 (20.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5298s / 2327.1828 s
agent0:                 episode reward: -0.5556,                 loss: 0.2132
agent1:                 episode reward: 0.5556,                 loss: nan
Episode: 21041/101000 (20.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0147s / 2333.1975 s
agent0:                 episode reward: -0.2918,                 loss: 0.2132
agent1:                 episode reward: 0.2918,                 loss: nan
Episode: 21061/101000 (20.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4318s / 2340.6293 s
agent0:                 episode reward: -0.5048,                 loss: 0.2041
agent1:                 episode reward: 0.5048,                 loss: nan
Episode: 21081/101000 (20.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1800s / 2347.8092 s
agent0:                 episode reward: -0.1819,                 loss: 0.2056
agent1:                 episode reward: 0.1819,                 loss: nan
Episode: 21101/101000 (20.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5607s / 2355.3699 s
agent0:                 episode reward: -0.4115,                 loss: 0.2040
agent1:                 episode reward: 0.4115,                 loss: nan
Episode: 21121/101000 (20.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0682s / 2360.4381 s
agent0:                 episode reward: -0.2700,                 loss: 0.2053
agent1:                 episode reward: 0.2700,                 loss: nan
Episode: 21141/101000 (20.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9846s / 2367.4226 s
agent0:                 episode reward: -0.6478,                 loss: 0.2051
agent1:                 episode reward: 0.6478,                 loss: nan
Episode: 21161/101000 (20.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0691s / 2372.4918 s
agent0:                 episode reward: -0.4201,                 loss: 0.2049
agent1:                 episode reward: 0.4201,                 loss: nan
Episode: 21181/101000 (20.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5083s / 2379.0001 s
agent0:                 episode reward: -0.2377,                 loss: 0.2040
agent1:                 episode reward: 0.2377,                 loss: nan
Episode: 21201/101000 (20.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5277s / 2384.5278 s
agent0:                 episode reward: -0.0765,                 loss: 0.2035
agent1:                 episode reward: 0.0765,                 loss: nan
Episode: 21221/101000 (21.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6756s / 2393.2034 s
agent0:                 episode reward: -0.4875,                 loss: 0.2028
agent1:                 episode reward: 0.4875,                 loss: nan
Episode: 21241/101000 (21.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6906s / 2400.8940 s
agent0:                 episode reward: 0.1786,                 loss: 0.2054
agent1:                 episode reward: -0.1786,                 loss: nan
Episode: 21261/101000 (21.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3580s / 2406.2519 s
agent0:                 episode reward: -0.2589,                 loss: 0.2042
agent1:                 episode reward: 0.2589,                 loss: nan
Episode: 21281/101000 (21.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9208s / 2413.1727 s
agent0:                 episode reward: -0.6459,                 loss: 0.2032
agent1:                 episode reward: 0.6459,                 loss: nan
Episode: 21301/101000 (21.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3423s / 2421.5150 s
agent0:                 episode reward: -0.0528,                 loss: 0.2023
agent1:                 episode reward: 0.0528,                 loss: nan
Episode: 21321/101000 (21.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6492s / 2426.1642 s
agent0:                 episode reward: 0.0402,                 loss: 0.2054
agent1:                 episode reward: -0.0402,                 loss: nan
Episode: 21341/101000 (21.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7704s / 2433.9346 s
agent0:                 episode reward: -0.2986,                 loss: 0.2022
agent1:                 episode reward: 0.2986,                 loss: nan
Episode: 21361/101000 (21.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3968s / 2440.3314 s
agent0:                 episode reward: -0.3269,                 loss: 0.2040
agent1:                 episode reward: 0.3269,                 loss: nan
Episode: 21381/101000 (21.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4419s / 2448.7733 s
agent0:                 episode reward: -0.7392,                 loss: 0.2058
agent1:                 episode reward: 0.7392,                 loss: nan
Episode: 21401/101000 (21.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3795s / 2455.1528 s
agent0:                 episode reward: -0.5510,                 loss: 0.2058
agent1:                 episode reward: 0.5510,                 loss: nan
Episode: 21421/101000 (21.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9533s / 2463.1061 s
agent0:                 episode reward: -0.2662,                 loss: 0.2056
agent1:                 episode reward: 0.2662,                 loss: nan
Episode: 21441/101000 (21.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2371s / 2469.3432 s
agent0:                 episode reward: 0.2123,                 loss: 0.2025
agent1:                 episode reward: -0.2123,                 loss: nan
Episode: 21461/101000 (21.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1872s / 2475.5304 s
agent0:                 episode reward: -0.0060,                 loss: 0.1989
agent1:                 episode reward: 0.0060,                 loss: 0.1753
Score delta: 1.5507521696378894, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/21016_0.
Episode: 21481/101000 (21.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0624s / 2482.5928 s
agent0:                 episode reward: -0.8729,                 loss: 0.2631
agent1:                 episode reward: 0.8729,                 loss: 0.1730
Score delta: 1.6142878665052212, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/21051_1.
Episode: 21501/101000 (21.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7040s / 2491.2968 s
agent0:                 episode reward: -0.7042,                 loss: 0.2507
agent1:                 episode reward: 0.7042,                 loss: nan
Episode: 21521/101000 (21.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5041s / 2497.8009 s
agent0:                 episode reward: -0.7888,                 loss: 0.2502
agent1:                 episode reward: 0.7888,                 loss: nan
Episode: 21541/101000 (21.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7363s / 2504.5372 s
agent0:                 episode reward: -0.7875,                 loss: 0.2519
agent1:                 episode reward: 0.7875,                 loss: nan
Episode: 21561/101000 (21.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9386s / 2510.4758 s
agent0:                 episode reward: 0.1541,                 loss: 0.2474
agent1:                 episode reward: -0.1541,                 loss: nan
Episode: 21581/101000 (21.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9825s / 2518.4583 s
agent0:                 episode reward: -0.6204,                 loss: 0.2496
agent1:                 episode reward: 0.6204,                 loss: nan
Episode: 21601/101000 (21.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8780s / 2526.3363 s
agent0:                 episode reward: -0.5143,                 loss: 0.2478
agent1:                 episode reward: 0.5143,                 loss: nan
Episode: 21621/101000 (21.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7604s / 2533.0968 s
agent0:                 episode reward: 0.1327,                 loss: 0.2463
agent1:                 episode reward: -0.1327,                 loss: nan
Episode: 21641/101000 (21.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5703s / 2541.6671 s
agent0:                 episode reward: -0.2905,                 loss: 0.2466
agent1:                 episode reward: 0.2905,                 loss: nan
Episode: 21661/101000 (21.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0576s / 2548.7247 s
agent0:                 episode reward: -0.3682,                 loss: 0.2449
agent1:                 episode reward: 0.3682,                 loss: nan
Episode: 21681/101000 (21.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4881s / 2557.2128 s
agent0:                 episode reward: -0.1635,                 loss: 0.2476
agent1:                 episode reward: 0.1635,                 loss: nan
Episode: 21701/101000 (21.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8464s / 2566.0592 s
agent0:                 episode reward: 0.0364,                 loss: 0.2457
agent1:                 episode reward: -0.0364,                 loss: nan
Episode: 21721/101000 (21.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6397s / 2573.6989 s
agent0:                 episode reward: -0.0971,                 loss: 0.2459
agent1:                 episode reward: 0.0971,                 loss: nan
Episode: 21741/101000 (21.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1401s / 2580.8389 s
agent0:                 episode reward: -0.2052,                 loss: 0.2454
agent1:                 episode reward: 0.2052,                 loss: nan
Episode: 21761/101000 (21.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2075s / 2589.0465 s
agent0:                 episode reward: -0.8054,                 loss: 0.2091
agent1:                 episode reward: 0.8054,                 loss: nan
Episode: 21781/101000 (21.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8938s / 2597.9403 s
agent0:                 episode reward: 0.4521,                 loss: 0.2088
agent1:                 episode reward: -0.4521,                 loss: nan
Episode: 21801/101000 (21.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2641s / 2604.2044 s
agent0:                 episode reward: 0.1816,                 loss: 0.2057
agent1:                 episode reward: -0.1816,                 loss: nan
Episode: 21821/101000 (21.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3405s / 2610.5449 s
agent0:                 episode reward: -0.2882,                 loss: 0.2054
agent1:                 episode reward: 0.2882,                 loss: nan
Episode: 21841/101000 (21.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6659s / 2617.2108 s
agent0:                 episode reward: -0.4714,                 loss: 0.2049
agent1:                 episode reward: 0.4714,                 loss: nan
Episode: 21861/101000 (21.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4492s / 2624.6600 s
agent0:                 episode reward: -0.5904,                 loss: 0.2064
agent1:                 episode reward: 0.5904,                 loss: nan
Episode: 21881/101000 (21.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1369s / 2632.7969 s
agent0:                 episode reward: -0.2236,                 loss: 0.2055
agent1:                 episode reward: 0.2236,                 loss: nan
Episode: 21901/101000 (21.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0078s / 2638.8047 s
agent0:                 episode reward: -0.5965,                 loss: 0.2048
agent1:                 episode reward: 0.5965,                 loss: nan
Episode: 21921/101000 (21.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4145s / 2646.2192 s
agent0:                 episode reward: -0.0902,                 loss: 0.2051
agent1:                 episode reward: 0.0902,                 loss: nan
Episode: 21941/101000 (21.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9232s / 2654.1424 s
agent0:                 episode reward: -0.5976,                 loss: 0.2054
agent1:                 episode reward: 0.5976,                 loss: nan
Episode: 21961/101000 (21.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7629s / 2662.9053 s
agent0:                 episode reward: -0.5810,                 loss: 0.2044
agent1:                 episode reward: 0.5810,                 loss: nan
Episode: 21981/101000 (21.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0131s / 2669.9184 s
agent0:                 episode reward: -0.3706,                 loss: 0.2045
agent1:                 episode reward: 0.3706,                 loss: nan
Episode: 22001/101000 (21.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4387s / 2675.3571 s
agent0:                 episode reward: -0.4053,                 loss: 0.2034
agent1:                 episode reward: 0.4053,                 loss: nan
Episode: 22021/101000 (21.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1026s / 2679.4597 s
agent0:                 episode reward: -0.0484,                 loss: 0.2036
agent1:                 episode reward: 0.0484,                 loss: nan
Episode: 22041/101000 (21.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3347s / 2687.7945 s
agent0:                 episode reward: -0.5802,                 loss: 0.2051
agent1:                 episode reward: 0.5802,                 loss: nan
Episode: 22061/101000 (21.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9621s / 2693.7565 s
agent0:                 episode reward: 0.0307,                 loss: 0.2061
agent1:                 episode reward: -0.0307,                 loss: nan
Episode: 22081/101000 (21.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5540s / 2703.3105 s
agent0:                 episode reward: -0.2987,                 loss: 0.2025
agent1:                 episode reward: 0.2987,                 loss: nan
Episode: 22101/101000 (21.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4188s / 2707.7293 s
agent0:                 episode reward: -0.1952,                 loss: 0.1950
agent1:                 episode reward: 0.1952,                 loss: nan
Episode: 22121/101000 (21.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6706s / 2710.3998 s
agent0:                 episode reward: -0.3707,                 loss: 0.1941
agent1:                 episode reward: 0.3707,                 loss: nan
Episode: 22141/101000 (21.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7475s / 2717.1473 s
agent0:                 episode reward: -0.4005,                 loss: 0.1973
agent1:                 episode reward: 0.4005,                 loss: nan
Episode: 22161/101000 (21.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8582s / 2725.0055 s
agent0:                 episode reward: -0.1567,                 loss: 0.1932
agent1:                 episode reward: 0.1567,                 loss: nan
Episode: 22181/101000 (21.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0202s / 2733.0256 s
agent0:                 episode reward: -0.0113,                 loss: 0.1941
agent1:                 episode reward: 0.0113,                 loss: nan
Episode: 22201/101000 (21.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3193s / 2739.3450 s
agent0:                 episode reward: -0.1576,                 loss: 0.1948
agent1:                 episode reward: 0.1576,                 loss: nan
Episode: 22221/101000 (22.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9530s / 2745.2980 s
agent0:                 episode reward: -0.2054,                 loss: 0.1956
agent1:                 episode reward: 0.2054,                 loss: nan
Episode: 22241/101000 (22.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6948s / 2752.9928 s
agent0:                 episode reward: -0.1701,                 loss: 0.1937
agent1:                 episode reward: 0.1701,                 loss: nan
Episode: 22261/101000 (22.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1894s / 2761.1821 s
agent0:                 episode reward: -0.3727,                 loss: 0.1941
agent1:                 episode reward: 0.3727,                 loss: nan
Episode: 22281/101000 (22.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5239s / 2769.7061 s
agent0:                 episode reward: -0.0830,                 loss: 0.1959
agent1:                 episode reward: 0.0830,                 loss: nan
Episode: 22301/101000 (22.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5242s / 2777.2303 s
agent0:                 episode reward: 0.0054,                 loss: 0.1967
agent1:                 episode reward: -0.0054,                 loss: nan
Episode: 22321/101000 (22.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0836s / 2783.3139 s
agent0:                 episode reward: -0.1424,                 loss: 0.1955
agent1:                 episode reward: 0.1424,                 loss: nan
Episode: 22341/101000 (22.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6266s / 2790.9405 s
agent0:                 episode reward: 0.1700,                 loss: 0.1932
agent1:                 episode reward: -0.1700,                 loss: nan
Episode: 22361/101000 (22.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8626s / 2799.8031 s
agent0:                 episode reward: -0.0575,                 loss: 0.1955
agent1:                 episode reward: 0.0575,                 loss: nan
Episode: 22381/101000 (22.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5731s / 2807.3762 s
agent0:                 episode reward: -0.1338,                 loss: 0.1959
agent1:                 episode reward: 0.1338,                 loss: nan
Episode: 22401/101000 (22.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1186s / 2813.4948 s
agent0:                 episode reward: 0.1183,                 loss: 0.1966
agent1:                 episode reward: -0.1183,                 loss: nan
Episode: 22421/101000 (22.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6459s / 2820.1407 s
agent0:                 episode reward: -0.2164,                 loss: 0.1933
agent1:                 episode reward: 0.2164,                 loss: nan
Episode: 22441/101000 (22.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4839s / 2828.6245 s
agent0:                 episode reward: -0.0000,                 loss: 0.1915
agent1:                 episode reward: 0.0000,                 loss: nan
Episode: 22461/101000 (22.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2055s / 2835.8300 s
agent0:                 episode reward: -0.3750,                 loss: 0.1933
agent1:                 episode reward: 0.3750,                 loss: nan
Episode: 22481/101000 (22.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8390s / 2843.6690 s
agent0:                 episode reward: -0.5876,                 loss: 0.1921
agent1:                 episode reward: 0.5876,                 loss: nan
Episode: 22501/101000 (22.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5017s / 2849.1706 s
agent0:                 episode reward: 0.1080,                 loss: 0.1921
agent1:                 episode reward: -0.1080,                 loss: nan
Episode: 22521/101000 (22.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8513s / 2856.0220 s
agent0:                 episode reward: -0.0729,                 loss: 0.1927
agent1:                 episode reward: 0.0729,                 loss: nan
Episode: 22541/101000 (22.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9475s / 2863.9695 s
agent0:                 episode reward: -0.0866,                 loss: 0.1944
agent1:                 episode reward: 0.0866,                 loss: nan
Episode: 22561/101000 (22.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3193s / 2869.2887 s
agent0:                 episode reward: -0.0693,                 loss: 0.1940
agent1:                 episode reward: 0.0693,                 loss: nan
Episode: 22581/101000 (22.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4320s / 2876.7207 s
agent0:                 episode reward: -0.0875,                 loss: 0.1929
agent1:                 episode reward: 0.0875,                 loss: nan
Episode: 22601/101000 (22.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3539s / 2883.0747 s
agent0:                 episode reward: -0.3410,                 loss: 0.1941
agent1:                 episode reward: 0.3410,                 loss: nan
Episode: 22621/101000 (22.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0545s / 2892.1291 s
agent0:                 episode reward: -0.5417,                 loss: 0.1916
agent1:                 episode reward: 0.5417,                 loss: nan
Episode: 22641/101000 (22.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7328s / 2897.8619 s
agent0:                 episode reward: -0.5674,                 loss: 0.1920
agent1:                 episode reward: 0.5674,                 loss: nan
Episode: 22661/101000 (22.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2804s / 2905.1423 s
agent0:                 episode reward: -0.1515,                 loss: 0.1929
agent1:                 episode reward: 0.1515,                 loss: nan
Episode: 22681/101000 (22.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0995s / 2912.2418 s
agent0:                 episode reward: -0.5152,                 loss: 0.1917
agent1:                 episode reward: 0.5152,                 loss: nan
Episode: 22701/101000 (22.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7347s / 2917.9765 s
agent0:                 episode reward: -0.9699,                 loss: 0.1919
agent1:                 episode reward: 0.9699,                 loss: nan
Episode: 22721/101000 (22.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2244s / 2923.2009 s
agent0:                 episode reward: -0.1267,                 loss: 0.1934
agent1:                 episode reward: 0.1267,                 loss: nan
Episode: 22741/101000 (22.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0902s / 2930.2912 s
agent0:                 episode reward: -0.1103,                 loss: 0.1918
agent1:                 episode reward: 0.1103,                 loss: nan
Episode: 22761/101000 (22.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1092s / 2936.4003 s
agent0:                 episode reward: -0.0588,                 loss: 0.1880
agent1:                 episode reward: 0.0588,                 loss: nan
Episode: 22781/101000 (22.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5661s / 2943.9665 s
agent0:                 episode reward: 0.1292,                 loss: 0.1870
agent1:                 episode reward: -0.1292,                 loss: nan
Episode: 22801/101000 (22.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5465s / 2951.5130 s
agent0:                 episode reward: -0.2035,                 loss: 0.1857
agent1:                 episode reward: 0.2035,                 loss: nan
Episode: 22821/101000 (22.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7957s / 2960.3087 s
agent0:                 episode reward: -0.4837,                 loss: 0.1852
agent1:                 episode reward: 0.4837,                 loss: nan
Episode: 22841/101000 (22.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7122s / 2969.0209 s
agent0:                 episode reward: -0.0188,                 loss: 0.1891
agent1:                 episode reward: 0.0188,                 loss: nan
Episode: 22861/101000 (22.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7478s / 2975.7687 s
agent0:                 episode reward: 0.0103,                 loss: 0.1873
agent1:                 episode reward: -0.0103,                 loss: nan
Episode: 22881/101000 (22.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5697s / 2982.3384 s
agent0:                 episode reward: 0.0218,                 loss: 0.1868
agent1:                 episode reward: -0.0218,                 loss: nan
Episode: 22901/101000 (22.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8419s / 2989.1803 s
agent0:                 episode reward: -0.4310,                 loss: 0.1889
agent1:                 episode reward: 0.4310,                 loss: nan
Episode: 22921/101000 (22.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6772s / 2994.8576 s
agent0:                 episode reward: -0.3020,                 loss: 0.1869
agent1:                 episode reward: 0.3020,                 loss: nan
Episode: 22941/101000 (22.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9691s / 3001.8266 s
agent0:                 episode reward: 0.1097,                 loss: 0.1868
agent1:                 episode reward: -0.1097,                 loss: nan
Episode: 22961/101000 (22.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9436s / 3009.7703 s
agent0:                 episode reward: -0.5827,                 loss: 0.1854
agent1:                 episode reward: 0.5827,                 loss: nan
Episode: 22981/101000 (22.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8657s / 3016.6360 s
agent0:                 episode reward: -0.2557,                 loss: 0.1888
agent1:                 episode reward: 0.2557,                 loss: nan
Episode: 23001/101000 (22.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0707s / 3022.7067 s
agent0:                 episode reward: 0.0260,                 loss: 0.1883
agent1:                 episode reward: -0.0260,                 loss: nan
Episode: 23021/101000 (22.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0148s / 3030.7215 s
agent0:                 episode reward: -0.0381,                 loss: 0.1874
agent1:                 episode reward: 0.0381,                 loss: nan
Episode: 23041/101000 (22.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3700s / 3037.0915 s
agent0:                 episode reward: -0.0312,                 loss: 0.1875
agent1:                 episode reward: 0.0312,                 loss: nan
Episode: 23061/101000 (22.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3992s / 3045.4907 s
agent0:                 episode reward: -0.2954,                 loss: 0.1870
agent1:                 episode reward: 0.2954,                 loss: nan
Episode: 23081/101000 (22.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3653s / 3049.8560 s
agent0:                 episode reward: -0.5004,                 loss: 0.1860
agent1:                 episode reward: 0.5004,                 loss: nan
Episode: 23101/101000 (22.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4291s / 3056.2851 s
agent0:                 episode reward: -0.5049,                 loss: 0.1845
agent1:                 episode reward: 0.5049,                 loss: nan
Episode: 23121/101000 (22.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0931s / 3062.3782 s
agent0:                 episode reward: -0.3072,                 loss: 0.1866
agent1:                 episode reward: 0.3072,                 loss: nan
Episode: 23141/101000 (22.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1996s / 3069.5778 s
agent0:                 episode reward: -0.1595,                 loss: 0.1848
agent1:                 episode reward: 0.1595,                 loss: nan
Episode: 23161/101000 (22.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0802s / 3077.6580 s
agent0:                 episode reward: -0.0509,                 loss: 0.1826
agent1:                 episode reward: 0.0509,                 loss: nan
Episode: 23181/101000 (22.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4497s / 3084.1077 s
agent0:                 episode reward: 0.0691,                 loss: 0.1865
agent1:                 episode reward: -0.0691,                 loss: nan
Episode: 23201/101000 (22.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7955s / 3091.9032 s
agent0:                 episode reward: -0.0832,                 loss: 0.1863
agent1:                 episode reward: 0.0832,                 loss: nan
Episode: 23221/101000 (22.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7918s / 3098.6951 s
agent0:                 episode reward: 0.2165,                 loss: 0.1856
agent1:                 episode reward: -0.2165,                 loss: nan
Episode: 23241/101000 (23.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6979s / 3106.3929 s
agent0:                 episode reward: -0.0097,                 loss: 0.1853
agent1:                 episode reward: 0.0097,                 loss: nan
Episode: 23261/101000 (23.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8660s / 3113.2589 s
agent0:                 episode reward: -0.3314,                 loss: 0.1836
agent1:                 episode reward: 0.3314,                 loss: nan
Episode: 23281/101000 (23.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8474s / 3120.1063 s
agent0:                 episode reward: -0.2637,                 loss: 0.1848
agent1:                 episode reward: 0.2637,                 loss: nan
Episode: 23301/101000 (23.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2285s / 3127.3349 s
agent0:                 episode reward: -0.4262,                 loss: 0.1845
agent1:                 episode reward: 0.4262,                 loss: nan
Episode: 23321/101000 (23.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0515s / 3133.3864 s
agent0:                 episode reward: 0.0967,                 loss: 0.1836
agent1:                 episode reward: -0.0967,                 loss: nan
Episode: 23341/101000 (23.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4540s / 3140.8404 s
agent0:                 episode reward: -0.6610,                 loss: 0.1845
agent1:                 episode reward: 0.6610,                 loss: nan
Episode: 23361/101000 (23.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5956s / 3148.4360 s
agent0:                 episode reward: -0.3043,                 loss: 0.1852
agent1:                 episode reward: 0.3043,                 loss: nan
Episode: 23381/101000 (23.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4114s / 3155.8474 s
agent0:                 episode reward: -0.1034,                 loss: 0.1852
agent1:                 episode reward: 0.1034,                 loss: nan
Episode: 23401/101000 (23.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3747s / 3163.2221 s
agent0:                 episode reward: 0.4327,                 loss: 0.1873
agent1:                 episode reward: -0.4327,                 loss: nan
Episode: 23421/101000 (23.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5665s / 3169.7886 s
agent0:                 episode reward: -0.4136,                 loss: 0.1840
agent1:                 episode reward: 0.4136,                 loss: 0.1735
Score delta: 1.5975360893138515, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/22979_0.
Episode: 23441/101000 (23.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6061s / 3176.3947 s
agent0:                 episode reward: -0.6240,                 loss: 0.2194
agent1:                 episode reward: 0.6240,                 loss: 0.1721
Score delta: 1.9108323472806472, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/23012_1.
Episode: 23461/101000 (23.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2865s / 3184.6812 s
agent0:                 episode reward: -0.3402,                 loss: 0.2077
agent1:                 episode reward: 0.3402,                 loss: nan
Episode: 23481/101000 (23.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6136s / 3192.2947 s
agent0:                 episode reward: -0.5895,                 loss: 0.2092
agent1:                 episode reward: 0.5895,                 loss: nan
Episode: 23501/101000 (23.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1684s / 3200.4631 s
agent0:                 episode reward: -0.3524,                 loss: 0.2066
agent1:                 episode reward: 0.3524,                 loss: nan
Episode: 23521/101000 (23.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9980s / 3205.4611 s
agent0:                 episode reward: -0.5898,                 loss: 0.2083
agent1:                 episode reward: 0.5898,                 loss: nan
Episode: 23541/101000 (23.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0493s / 3213.5104 s
agent0:                 episode reward: -0.8772,                 loss: 0.2073
agent1:                 episode reward: 0.8772,                 loss: nan
Episode: 23561/101000 (23.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4397s / 3220.9501 s
agent0:                 episode reward: -0.0638,                 loss: 0.2067
agent1:                 episode reward: 0.0638,                 loss: nan
Episode: 23581/101000 (23.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7066s / 3228.6568 s
agent0:                 episode reward: -0.0873,                 loss: 0.2055
agent1:                 episode reward: 0.0873,                 loss: 0.1861
Score delta: 1.521377052027177, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/23147_0.
Episode: 23601/101000 (23.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9119s / 3235.5687 s
agent0:                 episode reward: -0.4909,                 loss: nan
agent1:                 episode reward: 0.4909,                 loss: 0.1882
Episode: 23621/101000 (23.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9649s / 3243.5336 s
agent0:                 episode reward: 0.0639,                 loss: 0.2404
agent1:                 episode reward: -0.0639,                 loss: 0.1847
Score delta: 1.7284801234741813, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/23177_1.
Episode: 23641/101000 (23.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8558s / 3249.3894 s
agent0:                 episode reward: -0.2270,                 loss: 0.2428
agent1:                 episode reward: 0.2270,                 loss: nan
Episode: 23661/101000 (23.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6343s / 3257.0238 s
agent0:                 episode reward: 0.0157,                 loss: 0.2406
agent1:                 episode reward: -0.0157,                 loss: nan
Episode: 23681/101000 (23.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4287s / 3265.4525 s
agent0:                 episode reward: -0.1854,                 loss: 0.2403
agent1:                 episode reward: 0.1854,                 loss: nan
Episode: 23701/101000 (23.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8551s / 3271.3076 s
agent0:                 episode reward: 0.1595,                 loss: 0.2397
agent1:                 episode reward: -0.1595,                 loss: nan
Score delta: 1.5725144309244796, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/23275_0.
Episode: 23721/101000 (23.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1399s / 3276.4475 s
agent0:                 episode reward: -0.4849,                 loss: nan
agent1:                 episode reward: 0.4849,                 loss: 0.1712
Episode: 23741/101000 (23.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9666s / 3284.4140 s
agent0:                 episode reward: -0.9748,                 loss: 0.2682
agent1:                 episode reward: 0.9748,                 loss: 0.1706
Score delta: 2.0086210971697556, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/23306_1.
Episode: 23761/101000 (23.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6794s / 3292.0934 s
agent0:                 episode reward: 0.3082,                 loss: 0.2612
agent1:                 episode reward: -0.3082,                 loss: nan
Episode: 23781/101000 (23.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9727s / 3298.0662 s
agent0:                 episode reward: -0.4815,                 loss: 0.2623
agent1:                 episode reward: 0.4815,                 loss: nan
Episode: 23801/101000 (23.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1106s / 3304.1767 s
agent0:                 episode reward: -0.1773,                 loss: 0.2587
agent1:                 episode reward: 0.1773,                 loss: nan
Episode: 23821/101000 (23.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3401s / 3313.5168 s
agent0:                 episode reward: -0.1706,                 loss: 0.2606
agent1:                 episode reward: 0.1706,                 loss: nan
Episode: 23841/101000 (23.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2971s / 3322.8139 s
agent0:                 episode reward: -0.0248,                 loss: 0.2510
agent1:                 episode reward: 0.0248,                 loss: nan
Episode: 23861/101000 (23.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5268s / 3329.3407 s
agent0:                 episode reward: -0.0416,                 loss: 0.2033
agent1:                 episode reward: 0.0416,                 loss: nan
Episode: 23881/101000 (23.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9732s / 3335.3139 s
agent0:                 episode reward: -0.1686,                 loss: 0.2022
agent1:                 episode reward: 0.1686,                 loss: nan
Episode: 23901/101000 (23.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1888s / 3342.5027 s
agent0:                 episode reward: 0.0339,                 loss: 0.2044
agent1:                 episode reward: -0.0339,                 loss: nan
Episode: 23921/101000 (23.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4801s / 3351.9828 s
agent0:                 episode reward: -0.2534,                 loss: 0.2011
agent1:                 episode reward: 0.2534,                 loss: nan
Episode: 23941/101000 (23.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7756s / 3356.7584 s
agent0:                 episode reward: -0.2398,                 loss: 0.2016
agent1:                 episode reward: 0.2398,                 loss: nan
Episode: 23961/101000 (23.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7069s / 3364.4653 s
agent0:                 episode reward: -0.2818,                 loss: 0.2036
agent1:                 episode reward: 0.2818,                 loss: nan
Episode: 23981/101000 (23.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5589s / 3371.0242 s
agent0:                 episode reward: -0.2226,                 loss: 0.2020
agent1:                 episode reward: 0.2226,                 loss: nan
Episode: 24001/101000 (23.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9491s / 3377.9733 s
agent0:                 episode reward: -0.5468,                 loss: 0.1991
agent1:                 episode reward: 0.5468,                 loss: nan
Episode: 24021/101000 (23.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1035s / 3385.0768 s
agent0:                 episode reward: -0.0647,                 loss: 0.2020
agent1:                 episode reward: 0.0647,                 loss: nan
Episode: 24041/101000 (23.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9067s / 3393.9835 s
agent0:                 episode reward: -0.5578,                 loss: 0.2020
agent1:                 episode reward: 0.5578,                 loss: nan
Episode: 24061/101000 (23.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8853s / 3402.8688 s
agent0:                 episode reward: -0.4078,                 loss: 0.2018
agent1:                 episode reward: 0.4078,                 loss: nan
Episode: 24081/101000 (23.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7595s / 3409.6283 s
agent0:                 episode reward: 0.2600,                 loss: 0.2017
agent1:                 episode reward: -0.2600,                 loss: nan
Episode: 24101/101000 (23.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5894s / 3418.2177 s
agent0:                 episode reward: -0.3215,                 loss: 0.2006
agent1:                 episode reward: 0.3215,                 loss: nan
Episode: 24121/101000 (23.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3642s / 3424.5820 s
agent0:                 episode reward: 0.2336,                 loss: 0.2012
agent1:                 episode reward: -0.2336,                 loss: nan
Episode: 24141/101000 (23.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2494s / 3432.8314 s
agent0:                 episode reward: 0.1832,                 loss: 0.2006
agent1:                 episode reward: -0.1832,                 loss: 0.1813
Score delta: 1.5083642123582552, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/23710_0.
Episode: 24161/101000 (23.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2587s / 3438.0900 s
agent0:                 episode reward: -0.2853,                 loss: nan
agent1:                 episode reward: 0.2853,                 loss: 0.1755
Episode: 24181/101000 (23.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8743s / 3441.9643 s
agent0:                 episode reward: -0.6502,                 loss: 0.2026
agent1:                 episode reward: 0.6502,                 loss: 0.1761
Score delta: 1.7949011759925686, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/23752_1.
Episode: 24201/101000 (23.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8118s / 3449.7761 s
agent0:                 episode reward: -0.4346,                 loss: 0.1819
agent1:                 episode reward: 0.4346,                 loss: nan
Episode: 24221/101000 (23.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6917s / 3455.4678 s
agent0:                 episode reward: -0.9029,                 loss: 0.1778
agent1:                 episode reward: 0.9029,                 loss: nan
Episode: 24241/101000 (24.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8869s / 3463.3547 s
agent0:                 episode reward: -0.7967,                 loss: 0.1781
agent1:                 episode reward: 0.7967,                 loss: nan
Episode: 24261/101000 (24.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6700s / 3469.0247 s
agent0:                 episode reward: -1.0445,                 loss: 0.1739
agent1:                 episode reward: 1.0445,                 loss: nan
Episode: 24281/101000 (24.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5162s / 3478.5408 s
agent0:                 episode reward: -0.2494,                 loss: 0.1726
agent1:                 episode reward: 0.2494,                 loss: nan
Episode: 24301/101000 (24.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5459s / 3487.0868 s
agent0:                 episode reward: -0.1904,                 loss: 0.1742
agent1:                 episode reward: 0.1904,                 loss: nan
Episode: 24321/101000 (24.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8910s / 3494.9778 s
agent0:                 episode reward: -0.8688,                 loss: 0.1719
agent1:                 episode reward: 0.8688,                 loss: nan
Episode: 24341/101000 (24.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5801s / 3501.5578 s
agent0:                 episode reward: -0.7727,                 loss: 0.1701
agent1:                 episode reward: 0.7727,                 loss: nan
Episode: 24361/101000 (24.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1110s / 3509.6688 s
agent0:                 episode reward: -0.8057,                 loss: 0.1690
agent1:                 episode reward: 0.8057,                 loss: nan
Episode: 24381/101000 (24.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8794s / 3516.5482 s
agent0:                 episode reward: -0.4296,                 loss: 0.1696
agent1:                 episode reward: 0.4296,                 loss: nan
Episode: 24401/101000 (24.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7107s / 3523.2589 s
agent0:                 episode reward: -0.6721,                 loss: 0.1687
agent1:                 episode reward: 0.6721,                 loss: nan
Episode: 24421/101000 (24.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5708s / 3530.8296 s
agent0:                 episode reward: -0.6952,                 loss: 0.1671
agent1:                 episode reward: 0.6952,                 loss: nan
Episode: 24441/101000 (24.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5351s / 3537.3647 s
agent0:                 episode reward: -0.3128,                 loss: 0.1678
agent1:                 episode reward: 0.3128,                 loss: nan
Episode: 24461/101000 (24.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4102s / 3544.7750 s
agent0:                 episode reward: -0.4488,                 loss: 0.1673
agent1:                 episode reward: 0.4488,                 loss: nan
Episode: 24481/101000 (24.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7047s / 3550.4797 s
agent0:                 episode reward: -0.6973,                 loss: 0.1645
agent1:                 episode reward: 0.6973,                 loss: nan
Episode: 24501/101000 (24.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3570s / 3555.8367 s
agent0:                 episode reward: -0.4189,                 loss: 0.1640
agent1:                 episode reward: 0.4189,                 loss: nan
Episode: 24521/101000 (24.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8586s / 3563.6953 s
agent0:                 episode reward: -0.6233,                 loss: 0.1630
agent1:                 episode reward: 0.6233,                 loss: nan
Episode: 24541/101000 (24.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1566s / 3571.8519 s
agent0:                 episode reward: -0.2578,                 loss: 0.1636
agent1:                 episode reward: 0.2578,                 loss: nan
Episode: 24561/101000 (24.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1011s / 3580.9530 s
agent0:                 episode reward: -0.7593,                 loss: 0.1721
agent1:                 episode reward: 0.7593,                 loss: nan
Episode: 24581/101000 (24.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5410s / 3586.4940 s
agent0:                 episode reward: -0.6980,                 loss: 0.1741
agent1:                 episode reward: 0.6980,                 loss: nan
Episode: 24601/101000 (24.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0993s / 3594.5933 s
agent0:                 episode reward: -0.4323,                 loss: 0.1742
agent1:                 episode reward: 0.4323,                 loss: nan
Episode: 24621/101000 (24.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0096s / 3603.6029 s
agent0:                 episode reward: -0.8063,                 loss: 0.1718
agent1:                 episode reward: 0.8063,                 loss: nan
Episode: 24641/101000 (24.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8005s / 3612.4035 s
agent0:                 episode reward: -1.0022,                 loss: 0.1714
agent1:                 episode reward: 1.0022,                 loss: nan
Episode: 24661/101000 (24.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4780s / 3619.8814 s
agent0:                 episode reward: -0.5868,                 loss: 0.1699
agent1:                 episode reward: 0.5868,                 loss: nan
Episode: 24681/101000 (24.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7268s / 3626.6082 s
agent0:                 episode reward: -0.8239,                 loss: 0.1705
agent1:                 episode reward: 0.8239,                 loss: nan
Episode: 24701/101000 (24.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8473s / 3633.4555 s
agent0:                 episode reward: -0.6037,                 loss: 0.1687
agent1:                 episode reward: 0.6037,                 loss: nan
Episode: 24721/101000 (24.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3971s / 3640.8527 s
agent0:                 episode reward: -0.4111,                 loss: 0.1668
agent1:                 episode reward: 0.4111,                 loss: nan
Episode: 24741/101000 (24.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9593s / 3647.8119 s
agent0:                 episode reward: -0.3883,                 loss: 0.1680
agent1:                 episode reward: 0.3883,                 loss: nan
Episode: 24761/101000 (24.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7059s / 3654.5178 s
agent0:                 episode reward: -0.3708,                 loss: 0.1662
agent1:                 episode reward: 0.3708,                 loss: nan
Episode: 24781/101000 (24.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9417s / 3660.4595 s
agent0:                 episode reward: -0.2934,                 loss: 0.1679
agent1:                 episode reward: 0.2934,                 loss: nan
Episode: 24801/101000 (24.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1663s / 3667.6258 s
agent0:                 episode reward: -0.2994,                 loss: 0.1661
agent1:                 episode reward: 0.2994,                 loss: nan
Episode: 24821/101000 (24.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5674s / 3674.1932 s
agent0:                 episode reward: -0.3163,                 loss: 0.1667
agent1:                 episode reward: 0.3163,                 loss: nan
Episode: 24841/101000 (24.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9238s / 3680.1169 s
agent0:                 episode reward: 0.0002,                 loss: 0.1656
agent1:                 episode reward: -0.0002,                 loss: nan
Episode: 24861/101000 (24.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3419s / 3687.4589 s
agent0:                 episode reward: -0.4104,                 loss: 0.1658
agent1:                 episode reward: 0.4104,                 loss: nan
Episode: 24881/101000 (24.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8377s / 3694.2965 s
agent0:                 episode reward: 0.0195,                 loss: 0.1701
agent1:                 episode reward: -0.0195,                 loss: nan
Episode: 24901/101000 (24.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6086s / 3699.9051 s
agent0:                 episode reward: -0.2735,                 loss: 0.1947
agent1:                 episode reward: 0.2735,                 loss: nan
Episode: 24921/101000 (24.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8516s / 3707.7568 s
agent0:                 episode reward: -0.4274,                 loss: 0.1925
agent1:                 episode reward: 0.4274,                 loss: nan
Episode: 24941/101000 (24.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1597s / 3712.9165 s
agent0:                 episode reward: -0.6746,                 loss: 0.1939
agent1:                 episode reward: 0.6746,                 loss: nan
Episode: 24961/101000 (24.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5174s / 3718.4339 s
agent0:                 episode reward: -0.4873,                 loss: 0.1940
agent1:                 episode reward: 0.4873,                 loss: nan
Episode: 24981/101000 (24.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0248s / 3725.4587 s
agent0:                 episode reward: -0.0848,                 loss: 0.1943
agent1:                 episode reward: 0.0848,                 loss: nan
Episode: 25001/101000 (24.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6254s / 3733.0842 s
agent0:                 episode reward: -0.5190,                 loss: 0.1925
agent1:                 episode reward: 0.5190,                 loss: nan
Episode: 25021/101000 (24.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3564s / 3741.4406 s
agent0:                 episode reward: -0.6665,                 loss: 0.1941
agent1:                 episode reward: 0.6665,                 loss: nan
Episode: 25041/101000 (24.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6672s / 3747.1079 s
agent0:                 episode reward: -0.2029,                 loss: 0.1942
agent1:                 episode reward: 0.2029,                 loss: nan
Episode: 25061/101000 (24.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1211s / 3753.2290 s
agent0:                 episode reward: -0.2672,                 loss: 0.1931
agent1:                 episode reward: 0.2672,                 loss: nan
Episode: 25081/101000 (24.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8875s / 3761.1165 s
agent0:                 episode reward: -0.7555,                 loss: 0.1934
agent1:                 episode reward: 0.7555,                 loss: nan
Episode: 25101/101000 (24.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2280s / 3767.3445 s
agent0:                 episode reward: -0.5613,                 loss: 0.1954
agent1:                 episode reward: 0.5613,                 loss: nan
Episode: 25121/101000 (24.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7328s / 3774.0773 s
agent0:                 episode reward: 0.0527,                 loss: 0.1946
agent1:                 episode reward: -0.0527,                 loss: nan
Episode: 25141/101000 (24.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4927s / 3781.5701 s
agent0:                 episode reward: -0.6545,                 loss: 0.1931
agent1:                 episode reward: 0.6545,                 loss: nan
Episode: 25161/101000 (24.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2471s / 3787.8172 s
agent0:                 episode reward: -0.9183,                 loss: 0.1926
agent1:                 episode reward: 0.9183,                 loss: nan
Episode: 25181/101000 (24.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8069s / 3795.6241 s
agent0:                 episode reward: -0.1451,                 loss: 0.1936
agent1:                 episode reward: 0.1451,                 loss: nan
Episode: 25201/101000 (24.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8436s / 3802.4677 s
agent0:                 episode reward: -0.3356,                 loss: 0.1916
agent1:                 episode reward: 0.3356,                 loss: nan
Episode: 25221/101000 (24.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2278s / 3808.6955 s
agent0:                 episode reward: -0.3456,                 loss: 0.1968
agent1:                 episode reward: 0.3456,                 loss: nan
Episode: 25241/101000 (24.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0450s / 3817.7405 s
agent0:                 episode reward: -0.2711,                 loss: 0.1992
agent1:                 episode reward: 0.2711,                 loss: nan
Episode: 25261/101000 (25.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2160s / 3825.9565 s
agent0:                 episode reward: -0.4067,                 loss: 0.1982
agent1:                 episode reward: 0.4067,                 loss: nan
Episode: 25281/101000 (25.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6884s / 3832.6449 s
agent0:                 episode reward: -0.6082,                 loss: 0.1980
agent1:                 episode reward: 0.6082,                 loss: nan
Episode: 25301/101000 (25.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1030s / 3837.7480 s
agent0:                 episode reward: -0.2089,                 loss: 0.1980
agent1:                 episode reward: 0.2089,                 loss: nan
Episode: 25321/101000 (25.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3763s / 3843.1242 s
agent0:                 episode reward: -0.3898,                 loss: 0.1988
agent1:                 episode reward: 0.3898,                 loss: nan
Episode: 25341/101000 (25.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8718s / 3849.9960 s
agent0:                 episode reward: -0.5154,                 loss: 0.1973
agent1:                 episode reward: 0.5154,                 loss: nan
Episode: 25361/101000 (25.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3704s / 3857.3664 s
agent0:                 episode reward: -0.5301,                 loss: 0.2005
agent1:                 episode reward: 0.5301,                 loss: nan
Episode: 25381/101000 (25.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4586s / 3863.8250 s
agent0:                 episode reward: 0.0400,                 loss: 0.1991
agent1:                 episode reward: -0.0400,                 loss: nan
Episode: 25401/101000 (25.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8402s / 3871.6652 s
agent0:                 episode reward: -0.3144,                 loss: 0.1983
agent1:                 episode reward: 0.3144,                 loss: nan
Episode: 25421/101000 (25.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6204s / 3880.2856 s
agent0:                 episode reward: -0.2926,                 loss: 0.1997
agent1:                 episode reward: 0.2926,                 loss: nan
Episode: 25441/101000 (25.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8183s / 3888.1039 s
agent0:                 episode reward: -0.2138,                 loss: 0.1992
agent1:                 episode reward: 0.2138,                 loss: nan
Episode: 25461/101000 (25.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5818s / 3894.6856 s
agent0:                 episode reward: -0.6066,                 loss: 0.1971
agent1:                 episode reward: 0.6066,                 loss: nan
Episode: 25481/101000 (25.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0353s / 3901.7210 s
agent0:                 episode reward: -0.1345,                 loss: 0.1963
agent1:                 episode reward: 0.1345,                 loss: nan
Episode: 25501/101000 (25.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0791s / 3908.8001 s
agent0:                 episode reward: 0.0477,                 loss: 0.1997
agent1:                 episode reward: -0.0477,                 loss: nan
Episode: 25521/101000 (25.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7117s / 3916.5118 s
agent0:                 episode reward: -0.4577,                 loss: 0.1979
agent1:                 episode reward: 0.4577,                 loss: nan
Episode: 25541/101000 (25.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1502s / 3923.6620 s
agent0:                 episode reward: 0.3344,                 loss: 0.1990
agent1:                 episode reward: -0.3344,                 loss: 0.1715
Score delta: 1.502543047057669, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/25113_0.
Episode: 25561/101000 (25.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4728s / 3931.1348 s
agent0:                 episode reward: -0.1887,                 loss: nan
agent1:                 episode reward: 0.1887,                 loss: 0.1702
Episode: 25581/101000 (25.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7053s / 3938.8401 s
agent0:                 episode reward: -0.5155,                 loss: 0.2628
agent1:                 episode reward: 0.5155,                 loss: 0.1713
Score delta: 2.0090578584268006, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/25146_1.
Episode: 25601/101000 (25.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9704s / 3945.8105 s
agent0:                 episode reward: -0.1058,                 loss: 0.2264
agent1:                 episode reward: 0.1058,                 loss: nan
Episode: 25621/101000 (25.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6363s / 3954.4469 s
agent0:                 episode reward: -0.0533,                 loss: 0.2259
agent1:                 episode reward: 0.0533,                 loss: nan
Episode: 25641/101000 (25.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4674s / 3962.9142 s
agent0:                 episode reward: -0.6849,                 loss: 0.2267
agent1:                 episode reward: 0.6849,                 loss: nan
Episode: 25661/101000 (25.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6433s / 3970.5576 s
agent0:                 episode reward: -0.1983,                 loss: 0.2229
agent1:                 episode reward: 0.1983,                 loss: nan
Episode: 25681/101000 (25.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6245s / 3979.1821 s
agent0:                 episode reward: -0.1253,                 loss: 0.2234
agent1:                 episode reward: 0.1253,                 loss: nan
Episode: 25701/101000 (25.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4324s / 3984.6145 s
agent0:                 episode reward: -0.2058,                 loss: 0.2239
agent1:                 episode reward: 0.2058,                 loss: nan
Episode: 25721/101000 (25.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7506s / 3993.3650 s
agent0:                 episode reward: -0.2825,                 loss: 0.2204
agent1:                 episode reward: 0.2825,                 loss: nan
Episode: 25741/101000 (25.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1315s / 3997.4966 s
agent0:                 episode reward: -0.3022,                 loss: 0.2221
agent1:                 episode reward: 0.3022,                 loss: nan
Episode: 25761/101000 (25.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1396s / 4001.6362 s
agent0:                 episode reward: -0.2825,                 loss: 0.2245
agent1:                 episode reward: 0.2825,                 loss: nan
Episode: 25781/101000 (25.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3929s / 4011.0291 s
agent0:                 episode reward: -0.3272,                 loss: 0.2233
agent1:                 episode reward: 0.3272,                 loss: nan
Episode: 25801/101000 (25.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3677s / 4020.3968 s
agent0:                 episode reward: -0.6487,                 loss: 0.2243
agent1:                 episode reward: 0.6487,                 loss: nan
Episode: 25821/101000 (25.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7945s / 4027.1913 s
agent0:                 episode reward: -0.5515,                 loss: 0.2230
agent1:                 episode reward: 0.5515,                 loss: nan
Episode: 25841/101000 (25.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1875s / 4032.3788 s
agent0:                 episode reward: -0.4211,                 loss: 0.2251
agent1:                 episode reward: 0.4211,                 loss: nan
Episode: 25861/101000 (25.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2986s / 4037.6774 s
agent0:                 episode reward: -0.0917,                 loss: 0.2233
agent1:                 episode reward: 0.0917,                 loss: nan
Episode: 25881/101000 (25.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9108s / 4046.5883 s
agent0:                 episode reward: -0.0284,                 loss: 0.2240
agent1:                 episode reward: 0.0284,                 loss: nan
Episode: 25901/101000 (25.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5542s / 4054.1424 s
agent0:                 episode reward: -0.2728,                 loss: 0.2202
agent1:                 episode reward: 0.2728,                 loss: nan
Episode: 25921/101000 (25.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3535s / 4061.4960 s
agent0:                 episode reward: 0.0169,                 loss: 0.2085
agent1:                 episode reward: -0.0169,                 loss: nan
Episode: 25941/101000 (25.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4856s / 4068.9816 s
agent0:                 episode reward: -0.7101,                 loss: 0.1995
agent1:                 episode reward: 0.7101,                 loss: 0.1740
Score delta: 1.5597590249863, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/25499_0.
Episode: 25961/101000 (25.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7197s / 4076.7013 s
agent0:                 episode reward: -0.8935,                 loss: nan
agent1:                 episode reward: 0.8935,                 loss: 0.1736
Episode: 25981/101000 (25.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0616s / 4084.7629 s
agent0:                 episode reward: -0.0288,                 loss: 0.2409
agent1:                 episode reward: 0.0288,                 loss: 0.1750
Score delta: 1.723382827279255, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/25544_1.
Episode: 26001/101000 (25.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9893s / 4088.7522 s
agent0:                 episode reward: 0.1154,                 loss: 0.2377
agent1:                 episode reward: -0.1154,                 loss: nan
Episode: 26021/101000 (25.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3488s / 4097.1010 s
agent0:                 episode reward: 0.0696,                 loss: 0.2373
agent1:                 episode reward: -0.0696,                 loss: nan
Episode: 26041/101000 (25.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1760s / 4103.2771 s
agent0:                 episode reward: -0.1690,                 loss: 0.2404
agent1:                 episode reward: 0.1690,                 loss: nan
Episode: 26061/101000 (25.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1277s / 4110.4048 s
agent0:                 episode reward: -0.1946,                 loss: 0.2409
agent1:                 episode reward: 0.1946,                 loss: nan
Episode: 26081/101000 (25.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0947s / 4117.4995 s
agent0:                 episode reward: -0.6262,                 loss: 0.2390
agent1:                 episode reward: 0.6262,                 loss: nan
Episode: 26101/101000 (25.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5554s / 4125.0549 s
agent0:                 episode reward: -0.4413,                 loss: 0.2404
agent1:                 episode reward: 0.4413,                 loss: nan
Episode: 26121/101000 (25.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2227s / 4132.2776 s
agent0:                 episode reward: 0.1094,                 loss: 0.2415
agent1:                 episode reward: -0.1094,                 loss: 0.1750
Score delta: 1.7518563721616598, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/25692_0.
Episode: 26141/101000 (25.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9252s / 4139.2028 s
agent0:                 episode reward: -0.2769,                 loss: nan
agent1:                 episode reward: 0.2769,                 loss: 0.1707
Episode: 26161/101000 (25.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5589s / 4147.7617 s
agent0:                 episode reward: -0.1392,                 loss: nan
agent1:                 episode reward: 0.1392,                 loss: 0.1707
Episode: 26181/101000 (25.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7081s / 4153.4698 s
agent0:                 episode reward: -0.7415,                 loss: 0.2940
agent1:                 episode reward: 0.7415,                 loss: 0.1725
Score delta: 1.5807071657731024, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/25751_1.
Episode: 26201/101000 (25.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3247s / 4161.7945 s
agent0:                 episode reward: -0.0210,                 loss: 0.2880
agent1:                 episode reward: 0.0210,                 loss: nan
Episode: 26221/101000 (25.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1440s / 4168.9384 s
agent0:                 episode reward: -0.4089,                 loss: 0.2881
agent1:                 episode reward: 0.4089,                 loss: nan
Episode: 26241/101000 (25.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9417s / 4175.8801 s
agent0:                 episode reward: -0.5208,                 loss: 0.2838
agent1:                 episode reward: 0.5208,                 loss: nan
Episode: 26261/101000 (26.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4978s / 4182.3779 s
agent0:                 episode reward: 0.1491,                 loss: 0.2884
agent1:                 episode reward: -0.1491,                 loss: nan
Episode: 26281/101000 (26.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2108s / 4189.5887 s
agent0:                 episode reward: -0.2715,                 loss: 0.2876
agent1:                 episode reward: 0.2715,                 loss: nan
Episode: 26301/101000 (26.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0903s / 4196.6790 s
agent0:                 episode reward: -0.5035,                 loss: 0.2855
agent1:                 episode reward: 0.5035,                 loss: nan
Episode: 26321/101000 (26.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9079s / 4206.5869 s
agent0:                 episode reward: -0.5939,                 loss: 0.2842
agent1:                 episode reward: 0.5939,                 loss: nan
Episode: 26341/101000 (26.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6080s / 4213.1949 s
agent0:                 episode reward: -0.0399,                 loss: 0.2837
agent1:                 episode reward: 0.0399,                 loss: nan
Episode: 26361/101000 (26.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4488s / 4219.6437 s
agent0:                 episode reward: -0.0712,                 loss: 0.2494
agent1:                 episode reward: 0.0712,                 loss: nan
Episode: 26381/101000 (26.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1383s / 4229.7820 s
agent0:                 episode reward: -0.3252,                 loss: 0.2255
agent1:                 episode reward: 0.3252,                 loss: nan
Episode: 26401/101000 (26.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5427s / 4235.3247 s
agent0:                 episode reward: 0.2923,                 loss: 0.2250
agent1:                 episode reward: -0.2923,                 loss: 0.1904
Score delta: 1.8379969526728588, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/25966_0.
Episode: 26421/101000 (26.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0040s / 4241.3287 s
agent0:                 episode reward: -0.8377,                 loss: nan
agent1:                 episode reward: 0.8377,                 loss: 0.1904
Episode: 26441/101000 (26.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1124s / 4248.4411 s
agent0:                 episode reward: -0.7511,                 loss: 0.2768
agent1:                 episode reward: 0.7511,                 loss: 0.1914
Score delta: 1.6571843574758822, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/26007_1.
Episode: 26461/101000 (26.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5164s / 4257.9575 s
agent0:                 episode reward: -0.4975,                 loss: 0.2735
agent1:                 episode reward: 0.4975,                 loss: nan
Episode: 26481/101000 (26.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4721s / 4266.4296 s
agent0:                 episode reward: 0.0074,                 loss: 0.2725
agent1:                 episode reward: -0.0074,                 loss: nan
Episode: 26501/101000 (26.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0934s / 4272.5229 s
agent0:                 episode reward: -0.5441,                 loss: 0.2747
agent1:                 episode reward: 0.5441,                 loss: nan
Episode: 26521/101000 (26.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8266s / 4278.3495 s
agent0:                 episode reward: -0.1806,                 loss: 0.2714
agent1:                 episode reward: 0.1806,                 loss: nan
Episode: 26541/101000 (26.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0727s / 4285.4222 s
agent0:                 episode reward: -0.2733,                 loss: 0.2715
agent1:                 episode reward: 0.2733,                 loss: nan
Episode: 26561/101000 (26.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0952s / 4293.5174 s
agent0:                 episode reward: -0.2086,                 loss: 0.2720
agent1:                 episode reward: 0.2086,                 loss: nan
Episode: 26581/101000 (26.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7988s / 4302.3162 s
agent0:                 episode reward: -0.4003,                 loss: 0.2739
agent1:                 episode reward: 0.4003,                 loss: nan
Episode: 26601/101000 (26.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0047s / 4311.3208 s
agent0:                 episode reward: -0.3193,                 loss: 0.2731
agent1:                 episode reward: 0.3193,                 loss: nan
Episode: 26621/101000 (26.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9102s / 4319.2310 s
agent0:                 episode reward: -0.0681,                 loss: 0.2750
agent1:                 episode reward: 0.0681,                 loss: nan
Episode: 26641/101000 (26.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0417s / 4326.2727 s
agent0:                 episode reward: -0.3791,                 loss: 0.2715
agent1:                 episode reward: 0.3791,                 loss: nan
Episode: 26661/101000 (26.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8851s / 4334.1578 s
agent0:                 episode reward: -0.3863,                 loss: 0.2704
agent1:                 episode reward: 0.3863,                 loss: nan
Episode: 26681/101000 (26.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8391s / 4341.9969 s
agent0:                 episode reward: -0.4045,                 loss: 0.2681
agent1:                 episode reward: 0.4045,                 loss: nan
Episode: 26701/101000 (26.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9081s / 4348.9050 s
agent0:                 episode reward: -0.1386,                 loss: 0.2737
agent1:                 episode reward: 0.1386,                 loss: nan
Episode: 26721/101000 (26.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2227s / 4357.1277 s
agent0:                 episode reward: -0.2491,                 loss: 0.2698
agent1:                 episode reward: 0.2491,                 loss: nan
Episode: 26741/101000 (26.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9013s / 4365.0290 s
agent0:                 episode reward: 0.2454,                 loss: 0.2131
agent1:                 episode reward: -0.2454,                 loss: nan
Episode: 26761/101000 (26.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3090s / 4373.3380 s
agent0:                 episode reward: 0.1529,                 loss: 0.2057
agent1:                 episode reward: -0.1529,                 loss: 0.1771
Score delta: 2.0992254106202473, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/26321_0.
Episode: 26781/101000 (26.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6128s / 4379.9509 s
agent0:                 episode reward: -0.5968,                 loss: nan
agent1:                 episode reward: 0.5968,                 loss: 0.1762
Episode: 26801/101000 (26.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1733s / 4383.1242 s
agent0:                 episode reward: -0.5548,                 loss: 0.2022
agent1:                 episode reward: 0.5548,                 loss: 0.1751
Score delta: 1.6685116256538006, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/26374_1.
Episode: 26821/101000 (26.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0967s / 4388.2209 s
agent0:                 episode reward: -0.1743,                 loss: 0.2077
agent1:                 episode reward: 0.1743,                 loss: nan
Episode: 26841/101000 (26.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1918s / 4394.4127 s
agent0:                 episode reward: -0.4418,                 loss: 0.2090
agent1:                 episode reward: 0.4418,                 loss: nan
Episode: 26861/101000 (26.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0910s / 4402.5038 s
agent0:                 episode reward: -0.4959,                 loss: 0.2050
agent1:                 episode reward: 0.4959,                 loss: nan
Episode: 26881/101000 (26.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7595s / 4410.2633 s
agent0:                 episode reward: -0.1402,                 loss: 0.2069
agent1:                 episode reward: 0.1402,                 loss: nan
Episode: 26901/101000 (26.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7223s / 4417.9856 s
agent0:                 episode reward: -0.1994,                 loss: 0.2078
agent1:                 episode reward: 0.1994,                 loss: nan
Episode: 26921/101000 (26.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0978s / 4424.0834 s
agent0:                 episode reward: -0.0567,                 loss: 0.2041
agent1:                 episode reward: 0.0567,                 loss: nan
Episode: 26941/101000 (26.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5747s / 4430.6582 s
agent0:                 episode reward: -0.3778,                 loss: 0.2074
agent1:                 episode reward: 0.3778,                 loss: nan
Episode: 26961/101000 (26.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6734s / 4436.3316 s
agent0:                 episode reward: 0.1011,                 loss: 0.2060
agent1:                 episode reward: -0.1011,                 loss: nan
Episode: 26981/101000 (26.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3374s / 4443.6690 s
agent0:                 episode reward: -0.0552,                 loss: 0.2043
agent1:                 episode reward: 0.0552,                 loss: nan
Episode: 27001/101000 (26.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4353s / 4451.1043 s
agent0:                 episode reward: -0.3457,                 loss: 0.2060
agent1:                 episode reward: 0.3457,                 loss: nan
Episode: 27021/101000 (26.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8530s / 4456.9573 s
agent0:                 episode reward: -0.1580,                 loss: 0.2063
agent1:                 episode reward: 0.1580,                 loss: nan
Episode: 27041/101000 (26.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5414s / 4465.4987 s
agent0:                 episode reward: -0.4905,                 loss: 0.2068
agent1:                 episode reward: 0.4905,                 loss: nan
Episode: 27061/101000 (26.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3899s / 4471.8886 s
agent0:                 episode reward: -0.1569,                 loss: 0.2025
agent1:                 episode reward: 0.1569,                 loss: 0.1721
Score delta: 1.5663869873017071, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/26623_0.
Episode: 27081/101000 (26.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7810s / 4478.6697 s
agent0:                 episode reward: -0.3114,                 loss: nan
agent1:                 episode reward: 0.3114,                 loss: 0.1716
Episode: 27101/101000 (26.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1548s / 4486.8245 s
agent0:                 episode reward: -0.2775,                 loss: 0.2252
agent1:                 episode reward: 0.2775,                 loss: 0.1700
Score delta: 1.5355279159765343, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/26663_1.
Episode: 27121/101000 (26.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3347s / 4495.1591 s
agent0:                 episode reward: -0.1384,                 loss: 0.2289
agent1:                 episode reward: 0.1384,                 loss: nan
Episode: 27141/101000 (26.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8747s / 4502.0339 s
agent0:                 episode reward: -0.4135,                 loss: 0.2311
agent1:                 episode reward: 0.4135,                 loss: nan
Episode: 27161/101000 (26.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9115s / 4511.9454 s
agent0:                 episode reward: -0.0989,                 loss: 0.2127
agent1:                 episode reward: 0.0989,                 loss: nan
Episode: 27181/101000 (26.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2287s / 4519.1741 s
agent0:                 episode reward: -0.2050,                 loss: 0.1984
agent1:                 episode reward: 0.2050,                 loss: nan
Episode: 27201/101000 (26.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5556s / 4527.7297 s
agent0:                 episode reward: -0.2879,                 loss: 0.1993
agent1:                 episode reward: 0.2879,                 loss: nan
Episode: 27221/101000 (26.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4852s / 4537.2149 s
agent0:                 episode reward: -0.0800,                 loss: 0.1979
agent1:                 episode reward: 0.0800,                 loss: nan
Episode: 27241/101000 (26.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0282s / 4546.2431 s
agent0:                 episode reward: -0.5161,                 loss: 0.1973
agent1:                 episode reward: 0.5161,                 loss: nan
Episode: 27261/101000 (26.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5410s / 4554.7841 s
agent0:                 episode reward: -0.4020,                 loss: 0.1994
agent1:                 episode reward: 0.4020,                 loss: nan
Episode: 27281/101000 (27.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8445s / 4562.6287 s
agent0:                 episode reward: -0.6143,                 loss: 0.1992
agent1:                 episode reward: 0.6143,                 loss: nan
Episode: 27301/101000 (27.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5173s / 4570.1460 s
agent0:                 episode reward: -0.0260,                 loss: 0.1975
agent1:                 episode reward: 0.0260,                 loss: nan
Episode: 27321/101000 (27.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0268s / 4579.1728 s
agent0:                 episode reward: -0.1824,                 loss: 0.1989
agent1:                 episode reward: 0.1824,                 loss: nan
Episode: 27341/101000 (27.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8392s / 4587.0120 s
agent0:                 episode reward: -0.3182,                 loss: 0.1970
agent1:                 episode reward: 0.3182,                 loss: nan
Episode: 27361/101000 (27.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7155s / 4593.7274 s
agent0:                 episode reward: -0.5331,                 loss: 0.1986
agent1:                 episode reward: 0.5331,                 loss: nan
Episode: 27381/101000 (27.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1633s / 4600.8908 s
agent0:                 episode reward: 0.3859,                 loss: 0.1993
agent1:                 episode reward: -0.3859,                 loss: nan
Episode: 27401/101000 (27.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3821s / 4607.2729 s
agent0:                 episode reward: -0.1162,                 loss: 0.1977
agent1:                 episode reward: 0.1162,                 loss: nan
Episode: 27421/101000 (27.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7169s / 4614.9898 s
agent0:                 episode reward: -0.4575,                 loss: 0.1982
agent1:                 episode reward: 0.4575,                 loss: nan
Episode: 27441/101000 (27.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3592s / 4623.3490 s
agent0:                 episode reward: 0.3789,                 loss: 0.1972
agent1:                 episode reward: -0.3789,                 loss: nan
Episode: 27461/101000 (27.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2563s / 4629.6053 s
agent0:                 episode reward: 0.0842,                 loss: 0.1976
agent1:                 episode reward: -0.0842,                 loss: nan
Episode: 27481/101000 (27.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2153s / 4634.8207 s
agent0:                 episode reward: -0.5026,                 loss: 0.2016
agent1:                 episode reward: 0.5026,                 loss: 0.1786
Score delta: 1.5444666477140878, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/27039_0.
Episode: 27501/101000 (27.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6795s / 4637.5002 s
agent0:                 episode reward: -0.6665,                 loss: nan
agent1:                 episode reward: 0.6665,                 loss: 0.1774
Episode: 27521/101000 (27.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7903s / 4643.2905 s
agent0:                 episode reward: -0.7481,                 loss: 0.2020
agent1:                 episode reward: 0.7481,                 loss: 0.1786
Score delta: 1.9803287519342625, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/27078_1.
Episode: 27541/101000 (27.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4868s / 4650.7773 s
agent0:                 episode reward: -0.7645,                 loss: 0.2103
agent1:                 episode reward: 0.7645,                 loss: nan
Episode: 27561/101000 (27.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2699s / 4658.0472 s
agent0:                 episode reward: 0.1358,                 loss: 0.2077
agent1:                 episode reward: -0.1358,                 loss: nan
Episode: 27581/101000 (27.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7661s / 4663.8133 s
agent0:                 episode reward: -0.8914,                 loss: 0.2098
agent1:                 episode reward: 0.8914,                 loss: nan
Episode: 27601/101000 (27.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2185s / 4672.0319 s
agent0:                 episode reward: -0.3971,                 loss: 0.2094
agent1:                 episode reward: 0.3971,                 loss: nan
Episode: 27621/101000 (27.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0609s / 4679.0928 s
agent0:                 episode reward: -0.5035,                 loss: 0.2064
agent1:                 episode reward: 0.5035,                 loss: nan
Episode: 27641/101000 (27.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6461s / 4688.7389 s
agent0:                 episode reward: -0.2721,                 loss: 0.2064
agent1:                 episode reward: 0.2721,                 loss: nan
Episode: 27661/101000 (27.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6205s / 4697.3594 s
agent0:                 episode reward: -0.5986,                 loss: 0.2061
agent1:                 episode reward: 0.5986,                 loss: nan
Episode: 27681/101000 (27.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1480s / 4705.5074 s
agent0:                 episode reward: -0.6019,                 loss: 0.2066
agent1:                 episode reward: 0.6019,                 loss: nan
Episode: 27701/101000 (27.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6760s / 4712.1834 s
agent0:                 episode reward: -0.2011,                 loss: 0.2070
agent1:                 episode reward: 0.2011,                 loss: nan
Episode: 27721/101000 (27.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1694s / 4719.3528 s
agent0:                 episode reward: -0.5546,                 loss: 0.2047
agent1:                 episode reward: 0.5546,                 loss: nan
Episode: 27741/101000 (27.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0198s / 4724.3726 s
agent0:                 episode reward: 0.1476,                 loss: 0.2066
agent1:                 episode reward: -0.1476,                 loss: nan
Episode: 27761/101000 (27.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7016s / 4732.0741 s
agent0:                 episode reward: -0.4570,                 loss: 0.2043
agent1:                 episode reward: 0.4570,                 loss: nan
Episode: 27781/101000 (27.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3160s / 4740.3902 s
agent0:                 episode reward: -0.1616,                 loss: 0.2059
agent1:                 episode reward: 0.1616,                 loss: nan
Episode: 27801/101000 (27.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4166s / 4745.8067 s
agent0:                 episode reward: 0.0432,                 loss: 0.2053
agent1:                 episode reward: -0.0432,                 loss: nan
Episode: 27821/101000 (27.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5633s / 4750.3700 s
agent0:                 episode reward: -0.5837,                 loss: 0.2038
agent1:                 episode reward: 0.5837,                 loss: nan
Episode: 27841/101000 (27.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4725s / 4759.8424 s
agent0:                 episode reward: -0.3550,                 loss: 0.2041
agent1:                 episode reward: 0.3550,                 loss: nan
Episode: 27861/101000 (27.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8143s / 4766.6567 s
agent0:                 episode reward: -0.5043,                 loss: 0.2043
agent1:                 episode reward: 0.5043,                 loss: nan
Episode: 27881/101000 (27.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6845s / 4774.3412 s
agent0:                 episode reward: -0.5588,                 loss: 0.1994
agent1:                 episode reward: 0.5588,                 loss: nan
Episode: 27901/101000 (27.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4456s / 4781.7868 s
agent0:                 episode reward: -0.3290,                 loss: 0.2022
agent1:                 episode reward: 0.3290,                 loss: nan
Episode: 27921/101000 (27.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7644s / 4789.5512 s
agent0:                 episode reward: -0.4665,                 loss: 0.1984
agent1:                 episode reward: 0.4665,                 loss: nan
Episode: 27941/101000 (27.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5735s / 4797.1247 s
agent0:                 episode reward: 0.0785,                 loss: 0.1981
agent1:                 episode reward: -0.0785,                 loss: nan
Episode: 27961/101000 (27.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4204s / 4804.5451 s
agent0:                 episode reward: -0.4228,                 loss: 0.1991
agent1:                 episode reward: 0.4228,                 loss: nan
Episode: 27981/101000 (27.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9263s / 4812.4713 s
agent0:                 episode reward: -0.2773,                 loss: 0.1965
agent1:                 episode reward: 0.2773,                 loss: nan
Episode: 28001/101000 (27.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4028s / 4819.8741 s
agent0:                 episode reward: -0.1343,                 loss: 0.1966
agent1:                 episode reward: 0.1343,                 loss: nan
Episode: 28021/101000 (27.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8672s / 4827.7413 s
agent0:                 episode reward: -0.4633,                 loss: 0.1994
agent1:                 episode reward: 0.4633,                 loss: nan
Episode: 28041/101000 (27.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3355s / 4833.0768 s
agent0:                 episode reward: -0.4816,                 loss: 0.1978
agent1:                 episode reward: 0.4816,                 loss: nan
Episode: 28061/101000 (27.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2459s / 4841.3227 s
agent0:                 episode reward: -0.0602,                 loss: 0.1975
agent1:                 episode reward: 0.0602,                 loss: nan
Episode: 28081/101000 (27.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6795s / 4850.0022 s
agent0:                 episode reward: -0.3199,                 loss: 0.1966
agent1:                 episode reward: 0.3199,                 loss: nan
Episode: 28101/101000 (27.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4954s / 4856.4976 s
agent0:                 episode reward: -0.2399,                 loss: 0.1972
agent1:                 episode reward: 0.2399,                 loss: nan
Episode: 28121/101000 (27.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1291s / 4863.6267 s
agent0:                 episode reward: -0.3363,                 loss: 0.1979
agent1:                 episode reward: 0.3363,                 loss: nan
Episode: 28141/101000 (27.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4612s / 4871.0879 s
agent0:                 episode reward: -0.5485,                 loss: 0.1970
agent1:                 episode reward: 0.5485,                 loss: nan
Episode: 28161/101000 (27.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1193s / 4878.2072 s
agent0:                 episode reward: -0.1195,                 loss: 0.1980
agent1:                 episode reward: 0.1195,                 loss: nan
Episode: 28181/101000 (27.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0796s / 4887.2869 s
agent0:                 episode reward: -0.5439,                 loss: 0.1960
agent1:                 episode reward: 0.5439,                 loss: nan
Episode: 28201/101000 (27.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0497s / 4894.3365 s
agent0:                 episode reward: 0.3788,                 loss: 0.1937
agent1:                 episode reward: -0.3788,                 loss: nan
Episode: 28221/101000 (27.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6412s / 4900.9777 s
agent0:                 episode reward: -0.5674,                 loss: 0.1916
agent1:                 episode reward: 0.5674,                 loss: nan
Episode: 28241/101000 (27.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6983s / 4907.6761 s
agent0:                 episode reward: -0.3281,                 loss: 0.1910
agent1:                 episode reward: 0.3281,                 loss: nan
Episode: 28261/101000 (27.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7791s / 4914.4552 s
agent0:                 episode reward: -0.3722,                 loss: 0.1930
agent1:                 episode reward: 0.3722,                 loss: nan
Episode: 28281/101000 (28.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2616s / 4921.7167 s
agent0:                 episode reward: -0.7343,                 loss: 0.1922
agent1:                 episode reward: 0.7343,                 loss: nan
Episode: 28301/101000 (28.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9032s / 4930.6199 s
agent0:                 episode reward: -0.5328,                 loss: 0.1915
agent1:                 episode reward: 0.5328,                 loss: nan
Episode: 28321/101000 (28.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6279s / 4937.2478 s
agent0:                 episode reward: -0.6226,                 loss: 0.1910
agent1:                 episode reward: 0.6226,                 loss: nan
Episode: 28341/101000 (28.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2492s / 4945.4970 s
agent0:                 episode reward: -0.1668,                 loss: 0.1917
agent1:                 episode reward: 0.1668,                 loss: nan
Episode: 28361/101000 (28.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1108s / 4952.6078 s
agent0:                 episode reward: -0.3597,                 loss: 0.1903
agent1:                 episode reward: 0.3597,                 loss: nan
Episode: 28381/101000 (28.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3590s / 4961.9668 s
agent0:                 episode reward: -0.4420,                 loss: 0.1910
agent1:                 episode reward: 0.4420,                 loss: nan
Episode: 28401/101000 (28.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9210s / 4968.8878 s
agent0:                 episode reward: -0.0370,                 loss: 0.1908
agent1:                 episode reward: 0.0370,                 loss: nan
Episode: 28421/101000 (28.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7453s / 4975.6330 s
agent0:                 episode reward: -0.4149,                 loss: 0.1906
agent1:                 episode reward: 0.4149,                 loss: nan
Episode: 28441/101000 (28.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5648s / 4983.1978 s
agent0:                 episode reward: -0.1419,                 loss: 0.1903
agent1:                 episode reward: 0.1419,                 loss: nan
Episode: 28461/101000 (28.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4090s / 4990.6068 s
agent0:                 episode reward: -0.2457,                 loss: 0.1905
agent1:                 episode reward: 0.2457,                 loss: nan
Episode: 28481/101000 (28.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4904s / 4999.0972 s
agent0:                 episode reward: 0.1762,                 loss: 0.1905
agent1:                 episode reward: -0.1762,                 loss: nan
Episode: 28501/101000 (28.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5955s / 5006.6927 s
agent0:                 episode reward: -0.3415,                 loss: 0.1887
agent1:                 episode reward: 0.3415,                 loss: nan
Episode: 28521/101000 (28.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9696s / 5013.6623 s
agent0:                 episode reward: 0.0267,                 loss: 0.1910
agent1:                 episode reward: -0.0267,                 loss: nan
Episode: 28541/101000 (28.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7772s / 5021.4396 s
agent0:                 episode reward: -0.6966,                 loss: 0.1933
agent1:                 episode reward: 0.6966,                 loss: nan
Episode: 28561/101000 (28.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3722s / 5028.8118 s
agent0:                 episode reward: -0.1276,                 loss: 0.1961
agent1:                 episode reward: 0.1276,                 loss: nan
Episode: 28581/101000 (28.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1680s / 5036.9797 s
agent0:                 episode reward: -0.1795,                 loss: 0.1958
agent1:                 episode reward: 0.1795,                 loss: nan
Episode: 28601/101000 (28.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9870s / 5043.9667 s
agent0:                 episode reward: -0.0029,                 loss: 0.1940
agent1:                 episode reward: 0.0029,                 loss: 0.1766
Score delta: 1.544408457041158, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/28165_0.
Episode: 28621/101000 (28.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0577s / 5050.0244 s
agent0:                 episode reward: -0.8295,                 loss: nan
agent1:                 episode reward: 0.8295,                 loss: 0.1787
Episode: 28641/101000 (28.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6825s / 5055.7068 s
agent0:                 episode reward: -0.3317,                 loss: nan
agent1:                 episode reward: 0.3317,                 loss: 0.1778
Episode: 28661/101000 (28.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4919s / 5063.1987 s
agent0:                 episode reward: -0.7749,                 loss: 0.2440
agent1:                 episode reward: 0.7749,                 loss: 0.1782
Score delta: 1.8853745201285423, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/28223_1.
Episode: 28681/101000 (28.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3216s / 5068.5204 s
agent0:                 episode reward: 0.0799,                 loss: 0.2405
agent1:                 episode reward: -0.0799,                 loss: nan
Episode: 28701/101000 (28.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1615s / 5074.6818 s
agent0:                 episode reward: -0.2500,                 loss: 0.2417
agent1:                 episode reward: 0.2500,                 loss: nan
Episode: 28721/101000 (28.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0943s / 5084.7761 s
agent0:                 episode reward: -0.2592,                 loss: 0.2428
agent1:                 episode reward: 0.2592,                 loss: nan
Episode: 28741/101000 (28.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5006s / 5093.2767 s
agent0:                 episode reward: 0.0274,                 loss: 0.2406
agent1:                 episode reward: -0.0274,                 loss: nan
Episode: 28761/101000 (28.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1429s / 5100.4196 s
agent0:                 episode reward: -0.3671,                 loss: 0.2416
agent1:                 episode reward: 0.3671,                 loss: nan
Episode: 28781/101000 (28.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6156s / 5110.0352 s
agent0:                 episode reward: 0.1200,                 loss: 0.2399
agent1:                 episode reward: -0.1200,                 loss: nan
Episode: 28801/101000 (28.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7390s / 5116.7741 s
agent0:                 episode reward: 0.1630,                 loss: 0.2420
agent1:                 episode reward: -0.1630,                 loss: nan
Episode: 28821/101000 (28.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5996s / 5120.3737 s
agent0:                 episode reward: -0.1320,                 loss: 0.2437
agent1:                 episode reward: 0.1320,                 loss: nan
Episode: 28841/101000 (28.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2792s / 5126.6529 s
agent0:                 episode reward: -0.2406,                 loss: 0.2422
agent1:                 episode reward: 0.2406,                 loss: nan
Episode: 28861/101000 (28.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1989s / 5134.8518 s
agent0:                 episode reward: 0.0493,                 loss: 0.2405
agent1:                 episode reward: -0.0493,                 loss: 0.1933
Score delta: 1.8125495456426257, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/28428_0.
Episode: 28881/101000 (28.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9086s / 5143.7605 s
agent0:                 episode reward: -0.9242,                 loss: nan
agent1:                 episode reward: 0.9242,                 loss: 0.1919
Episode: 28901/101000 (28.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6139s / 5150.3743 s
agent0:                 episode reward: -0.2162,                 loss: nan
agent1:                 episode reward: 0.2162,                 loss: 0.1909
Episode: 28921/101000 (28.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9699s / 5156.3442 s
agent0:                 episode reward: -0.5898,                 loss: 0.3051
agent1:                 episode reward: 0.5898,                 loss: 0.1928
Score delta: 1.656315553749701, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/28493_1.
Episode: 28941/101000 (28.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6210s / 5163.9652 s
agent0:                 episode reward: -0.4526,                 loss: 0.2885
agent1:                 episode reward: 0.4526,                 loss: nan
Episode: 28961/101000 (28.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2197s / 5172.1849 s
agent0:                 episode reward: -0.8047,                 loss: 0.2891
agent1:                 episode reward: 0.8047,                 loss: nan
Episode: 28981/101000 (28.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8766s / 5181.0615 s
agent0:                 episode reward: -0.7488,                 loss: 0.2783
agent1:                 episode reward: 0.7488,                 loss: nan
Episode: 29001/101000 (28.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0035s / 5189.0651 s
agent0:                 episode reward: -0.4512,                 loss: 0.2199
agent1:                 episode reward: 0.4512,                 loss: nan
Episode: 29021/101000 (28.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9786s / 5197.0437 s
agent0:                 episode reward: 0.4453,                 loss: 0.2194
agent1:                 episode reward: -0.4453,                 loss: nan
Episode: 29041/101000 (28.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6841s / 5205.7278 s
agent0:                 episode reward: -0.0966,                 loss: 0.2198
agent1:                 episode reward: 0.0966,                 loss: 0.1754
Score delta: 1.6972055619860527, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/28596_0.
Episode: 29061/101000 (28.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4894s / 5211.2172 s
agent0:                 episode reward: -0.3328,                 loss: nan
agent1:                 episode reward: 0.3328,                 loss: 0.1759
Episode: 29081/101000 (28.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5559s / 5218.7731 s
agent0:                 episode reward: -0.4812,                 loss: nan
agent1:                 episode reward: 0.4812,                 loss: 0.1745
Episode: 29101/101000 (28.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7807s / 5223.5538 s
agent0:                 episode reward: -0.5612,                 loss: 0.2742
agent1:                 episode reward: 0.5612,                 loss: 0.1740
Score delta: 1.7960877669036097, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/28658_1.
Episode: 29121/101000 (28.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9471s / 5231.5009 s
agent0:                 episode reward: -0.2865,                 loss: 0.2747
agent1:                 episode reward: 0.2865,                 loss: nan
Episode: 29141/101000 (28.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4983s / 5238.9992 s
agent0:                 episode reward: -0.0250,                 loss: 0.2755
agent1:                 episode reward: 0.0250,                 loss: nan
Episode: 29161/101000 (28.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7184s / 5246.7175 s
agent0:                 episode reward: -0.1291,                 loss: 0.2760
agent1:                 episode reward: 0.1291,                 loss: nan
Episode: 29181/101000 (28.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5383s / 5252.2559 s
agent0:                 episode reward: -0.1885,                 loss: 0.2752
agent1:                 episode reward: 0.1885,                 loss: nan
Episode: 29201/101000 (28.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3594s / 5259.6153 s
agent0:                 episode reward: -0.2088,                 loss: 0.2707
agent1:                 episode reward: 0.2088,                 loss: nan
Episode: 29221/101000 (28.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4164s / 5267.0317 s
agent0:                 episode reward: -0.0596,                 loss: 0.2726
agent1:                 episode reward: 0.0596,                 loss: nan
Episode: 29241/101000 (28.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1285s / 5270.1602 s
agent0:                 episode reward: -0.2209,                 loss: 0.2746
agent1:                 episode reward: 0.2209,                 loss: nan
Episode: 29261/101000 (28.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0308s / 5279.1910 s
agent0:                 episode reward: 0.0070,                 loss: 0.2746
agent1:                 episode reward: -0.0070,                 loss: nan
Episode: 29281/101000 (28.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0195s / 5286.2105 s
agent0:                 episode reward: 0.0434,                 loss: 0.2731
agent1:                 episode reward: -0.0434,                 loss: nan
Episode: 29301/101000 (29.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7697s / 5294.9803 s
agent0:                 episode reward: 0.1137,                 loss: 0.2736
agent1:                 episode reward: -0.1137,                 loss: nan
Episode: 29321/101000 (29.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9929s / 5302.9732 s
agent0:                 episode reward: -0.0028,                 loss: 0.2733
agent1:                 episode reward: 0.0028,                 loss: nan
Episode: 29341/101000 (29.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0945s / 5310.0676 s
agent0:                 episode reward: -0.0892,                 loss: 0.2767
agent1:                 episode reward: 0.0892,                 loss: nan
Episode: 29361/101000 (29.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7706s / 5319.8382 s
agent0:                 episode reward: 0.2275,                 loss: 0.2756
agent1:                 episode reward: -0.2275,                 loss: 0.1650
Score delta: 1.5458506331175559, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/28924_0.
Episode: 29381/101000 (29.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4796s / 5327.3178 s
agent0:                 episode reward: -0.5028,                 loss: nan
agent1:                 episode reward: 0.5028,                 loss: 0.1598
Episode: 29401/101000 (29.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7737s / 5335.0915 s
agent0:                 episode reward: -0.6140,                 loss: nan
agent1:                 episode reward: 0.6140,                 loss: 0.1605
Episode: 29421/101000 (29.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6516s / 5342.7431 s
agent0:                 episode reward: -0.1224,                 loss: 0.2417
agent1:                 episode reward: 0.1224,                 loss: 0.1606
Score delta: 1.5797036063225538, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/28976_1.
Episode: 29441/101000 (29.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9097s / 5350.6528 s
agent0:                 episode reward: -0.4077,                 loss: 0.2076
agent1:                 episode reward: 0.4077,                 loss: nan
Episode: 29461/101000 (29.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5900s / 5359.2428 s
agent0:                 episode reward: -0.1355,                 loss: 0.1968
agent1:                 episode reward: 0.1355,                 loss: nan
Episode: 29481/101000 (29.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3770s / 5367.6197 s
agent0:                 episode reward: -0.2741,                 loss: 0.1973
agent1:                 episode reward: 0.2741,                 loss: nan
Episode: 29501/101000 (29.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8209s / 5373.4407 s
agent0:                 episode reward: 0.0764,                 loss: 0.1956
agent1:                 episode reward: -0.0764,                 loss: nan
Episode: 29521/101000 (29.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9433s / 5381.3839 s
agent0:                 episode reward: -0.4250,                 loss: 0.1954
agent1:                 episode reward: 0.4250,                 loss: nan
Episode: 29541/101000 (29.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8360s / 5390.2200 s
agent0:                 episode reward: -0.3130,                 loss: 0.1952
agent1:                 episode reward: 0.3130,                 loss: nan
Episode: 29561/101000 (29.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6527s / 5398.8727 s
agent0:                 episode reward: -0.5703,                 loss: 0.1980
agent1:                 episode reward: 0.5703,                 loss: nan
Episode: 29581/101000 (29.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1590s / 5407.0317 s
agent0:                 episode reward: -0.1979,                 loss: 0.1962
agent1:                 episode reward: 0.1979,                 loss: nan
Episode: 29601/101000 (29.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4497s / 5414.4814 s
agent0:                 episode reward: -0.0981,                 loss: 0.1970
agent1:                 episode reward: 0.0981,                 loss: nan
Episode: 29621/101000 (29.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6161s / 5423.0975 s
agent0:                 episode reward: -0.3306,                 loss: 0.1946
agent1:                 episode reward: 0.3306,                 loss: nan
Episode: 29641/101000 (29.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8392s / 5430.9367 s
agent0:                 episode reward: 0.0783,                 loss: 0.1939
agent1:                 episode reward: -0.0783,                 loss: nan
Episode: 29661/101000 (29.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9562s / 5438.8929 s
agent0:                 episode reward: -0.5165,                 loss: 0.1976
agent1:                 episode reward: 0.5165,                 loss: nan
Episode: 29681/101000 (29.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5592s / 5445.4521 s
agent0:                 episode reward: 0.1106,                 loss: 0.1949
agent1:                 episode reward: -0.1106,                 loss: nan
Episode: 29701/101000 (29.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2934s / 5452.7455 s
agent0:                 episode reward: -0.0146,                 loss: 0.1945
agent1:                 episode reward: 0.0146,                 loss: nan
Episode: 29721/101000 (29.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5044s / 5461.2498 s
agent0:                 episode reward: -0.0949,                 loss: 0.1960
agent1:                 episode reward: 0.0949,                 loss: nan
Episode: 29741/101000 (29.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6652s / 5467.9151 s
agent0:                 episode reward: -0.5286,                 loss: 0.1951
agent1:                 episode reward: 0.5286,                 loss: nan
Episode: 29761/101000 (29.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0371s / 5475.9522 s
agent0:                 episode reward: 0.3052,                 loss: 0.1977
agent1:                 episode reward: -0.3052,                 loss: 0.1835
Score delta: 1.6238228035498175, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/29324_0.
Episode: 29781/101000 (29.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0599s / 5485.0122 s
agent0:                 episode reward: -0.8540,                 loss: nan
agent1:                 episode reward: 0.8540,                 loss: 0.1814
Episode: 29801/101000 (29.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8273s / 5492.8395 s
agent0:                 episode reward: -0.3512,                 loss: nan
agent1:                 episode reward: 0.3512,                 loss: 0.1798
Episode: 29821/101000 (29.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3202s / 5499.1597 s
agent0:                 episode reward: -0.5248,                 loss: nan
agent1:                 episode reward: 0.5248,                 loss: 0.1793
Episode: 29841/101000 (29.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5305s / 5507.6902 s
agent0:                 episode reward: -0.2303,                 loss: 0.1974
agent1:                 episode reward: 0.2303,                 loss: 0.1786
Score delta: 1.5047472937681206, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/29400_1.
Episode: 29861/101000 (29.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7128s / 5516.4030 s
agent0:                 episode reward: -0.0845,                 loss: 0.1888
agent1:                 episode reward: 0.0845,                 loss: nan
Episode: 29881/101000 (29.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0461s / 5526.4492 s
agent0:                 episode reward: -0.5670,                 loss: 0.1884
agent1:                 episode reward: 0.5670,                 loss: nan
Episode: 29901/101000 (29.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5096s / 5533.9588 s
agent0:                 episode reward: 0.0355,                 loss: 0.1890
agent1:                 episode reward: -0.0355,                 loss: nan
Episode: 29921/101000 (29.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9228s / 5542.8815 s
agent0:                 episode reward: -0.0966,                 loss: 0.1880
agent1:                 episode reward: 0.0966,                 loss: nan
Episode: 29941/101000 (29.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5030s / 5549.3845 s
agent0:                 episode reward: 0.0921,                 loss: 0.1881
agent1:                 episode reward: -0.0921,                 loss: nan
Episode: 29961/101000 (29.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5465s / 5558.9311 s
agent0:                 episode reward: 0.4800,                 loss: 0.1884
agent1:                 episode reward: -0.4800,                 loss: 0.1915
Score delta: 1.7100361380809754, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/29534_0.
Episode: 29981/101000 (29.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4722s / 5566.4033 s
agent0:                 episode reward: 0.0027,                 loss: nan
agent1:                 episode reward: -0.0027,                 loss: 0.1765
Episode: 30001/101000 (29.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1442s / 5572.5475 s
agent0:                 episode reward: -0.3110,                 loss: nan
agent1:                 episode reward: 0.3110,                 loss: 0.1663
Episode: 30021/101000 (29.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2683s / 5578.8157 s
agent0:                 episode reward: -0.8770,                 loss: 0.2720
agent1:                 episode reward: 0.8770,                 loss: 0.1626
Score delta: 1.9862536802352149, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/29582_1.
Episode: 30041/101000 (29.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6946s / 5586.5103 s
agent0:                 episode reward: -0.0869,                 loss: 0.2717
agent1:                 episode reward: 0.0869,                 loss: nan
Episode: 30061/101000 (29.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8337s / 5592.3441 s
agent0:                 episode reward: 0.1172,                 loss: 0.2689
agent1:                 episode reward: -0.1172,                 loss: nan
Episode: 30081/101000 (29.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0957s / 5599.4398 s
agent0:                 episode reward: -0.1211,                 loss: 0.2728
agent1:                 episode reward: 0.1211,                 loss: nan
Episode: 30101/101000 (29.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5511s / 5605.9909 s
agent0:                 episode reward: -0.0427,                 loss: 0.2711
agent1:                 episode reward: 0.0427,                 loss: nan
Episode: 30121/101000 (29.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7244s / 5612.7153 s
agent0:                 episode reward: -0.4279,                 loss: 0.2726
agent1:                 episode reward: 0.4279,                 loss: nan
Episode: 30141/101000 (29.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6779s / 5620.3932 s
agent0:                 episode reward: -0.0469,                 loss: 0.2734
agent1:                 episode reward: 0.0469,                 loss: nan
Episode: 30161/101000 (29.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7758s / 5628.1689 s
agent0:                 episode reward: -0.3343,                 loss: 0.2746
agent1:                 episode reward: 0.3343,                 loss: nan
Episode: 30181/101000 (29.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1337s / 5635.3027 s
agent0:                 episode reward: -0.1021,                 loss: 0.2729
agent1:                 episode reward: 0.1021,                 loss: nan
Episode: 30201/101000 (29.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1381s / 5643.4408 s
agent0:                 episode reward: 0.0040,                 loss: 0.2725
agent1:                 episode reward: -0.0040,                 loss: 0.1814
Score delta: 1.6898579170095456, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/29769_0.
Episode: 30221/101000 (29.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6985s / 5650.1393 s
agent0:                 episode reward: -0.7229,                 loss: nan
agent1:                 episode reward: 0.7229,                 loss: 0.1821
Episode: 30241/101000 (29.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7922s / 5657.9315 s
agent0:                 episode reward: -0.6239,                 loss: nan
agent1:                 episode reward: 0.6239,                 loss: 0.1828
Episode: 30261/101000 (29.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9167s / 5664.8482 s
agent0:                 episode reward: -0.6416,                 loss: 0.2400
agent1:                 episode reward: 0.6416,                 loss: 0.1834
Score delta: 1.6569122985522562, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/29823_1.
Episode: 30281/101000 (29.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4876s / 5672.3358 s
agent0:                 episode reward: -0.4350,                 loss: 0.2232
agent1:                 episode reward: 0.4350,                 loss: nan
Episode: 30301/101000 (30.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2655s / 5678.6013 s
agent0:                 episode reward: -0.5947,                 loss: 0.2098
agent1:                 episode reward: 0.5947,                 loss: nan
Episode: 30321/101000 (30.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0747s / 5685.6761 s
agent0:                 episode reward: -0.5536,                 loss: 0.2105
agent1:                 episode reward: 0.5536,                 loss: nan
Episode: 30341/101000 (30.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0590s / 5692.7351 s
agent0:                 episode reward: -0.1725,                 loss: 0.2103
agent1:                 episode reward: 0.1725,                 loss: nan
Episode: 30361/101000 (30.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2527s / 5700.9878 s
agent0:                 episode reward: -0.3895,                 loss: 0.2090
agent1:                 episode reward: 0.3895,                 loss: nan
Episode: 30381/101000 (30.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3843s / 5708.3721 s
agent0:                 episode reward: -0.6731,                 loss: 0.2095
agent1:                 episode reward: 0.6731,                 loss: nan
Episode: 30401/101000 (30.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7532s / 5716.1253 s
agent0:                 episode reward: -1.0970,                 loss: 0.2092
agent1:                 episode reward: 1.0970,                 loss: nan
Episode: 30421/101000 (30.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9526s / 5723.0779 s
agent0:                 episode reward: -0.5160,                 loss: 0.2084
agent1:                 episode reward: 0.5160,                 loss: nan
Episode: 30441/101000 (30.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2427s / 5732.3206 s
agent0:                 episode reward: -0.8650,                 loss: 0.2102
agent1:                 episode reward: 0.8650,                 loss: nan
Episode: 30461/101000 (30.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4841s / 5738.8047 s
agent0:                 episode reward: -0.2544,                 loss: 0.2100
agent1:                 episode reward: 0.2544,                 loss: nan
Episode: 30481/101000 (30.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7620s / 5746.5667 s
agent0:                 episode reward: -0.5294,                 loss: 0.2058
agent1:                 episode reward: 0.5294,                 loss: nan
Episode: 30501/101000 (30.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1313s / 5754.6980 s
agent0:                 episode reward: -0.2135,                 loss: 0.2076
agent1:                 episode reward: 0.2135,                 loss: nan
Episode: 30521/101000 (30.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5630s / 5761.2609 s
agent0:                 episode reward: -0.3757,                 loss: 0.2087
agent1:                 episode reward: 0.3757,                 loss: nan
Episode: 30541/101000 (30.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0996s / 5768.3605 s
agent0:                 episode reward: -0.3791,                 loss: 0.2044
agent1:                 episode reward: 0.3791,                 loss: nan
Episode: 30561/101000 (30.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0238s / 5775.3844 s
agent0:                 episode reward: -0.3191,                 loss: 0.2057
agent1:                 episode reward: 0.3191,                 loss: nan
Episode: 30581/101000 (30.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4515s / 5783.8359 s
agent0:                 episode reward: -0.1336,                 loss: 0.2073
agent1:                 episode reward: 0.1336,                 loss: nan
Episode: 30601/101000 (30.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0895s / 5791.9254 s
agent0:                 episode reward: -0.1845,                 loss: 0.2076
agent1:                 episode reward: 0.1845,                 loss: nan
Episode: 30621/101000 (30.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8814s / 5799.8068 s
agent0:                 episode reward: -0.6175,                 loss: 0.2025
agent1:                 episode reward: 0.6175,                 loss: nan
Episode: 30641/101000 (30.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5968s / 5807.4036 s
agent0:                 episode reward: -0.0726,                 loss: 0.2012
agent1:                 episode reward: 0.0726,                 loss: nan
Episode: 30661/101000 (30.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7107s / 5814.1144 s
agent0:                 episode reward: -0.3034,                 loss: 0.2017
agent1:                 episode reward: 0.3034,                 loss: nan
Episode: 30681/101000 (30.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3542s / 5820.4686 s
agent0:                 episode reward: -0.2427,                 loss: 0.2006
agent1:                 episode reward: 0.2427,                 loss: nan
Episode: 30701/101000 (30.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8312s / 5827.2998 s
agent0:                 episode reward: -0.0781,                 loss: 0.2012
agent1:                 episode reward: 0.0781,                 loss: nan
Episode: 30721/101000 (30.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6740s / 5832.9739 s
agent0:                 episode reward: -0.0659,                 loss: 0.2014
agent1:                 episode reward: 0.0659,                 loss: nan
Episode: 30741/101000 (30.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8702s / 5840.8441 s
agent0:                 episode reward: -0.0082,                 loss: 0.1995
agent1:                 episode reward: 0.0082,                 loss: nan
Episode: 30761/101000 (30.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9474s / 5847.7915 s
agent0:                 episode reward: -0.5120,                 loss: 0.2023
agent1:                 episode reward: 0.5120,                 loss: nan
Episode: 30781/101000 (30.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8459s / 5855.6374 s
agent0:                 episode reward: -0.2483,                 loss: 0.1999
agent1:                 episode reward: 0.2483,                 loss: nan
Episode: 30801/101000 (30.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1004s / 5859.7379 s
agent0:                 episode reward: 0.3851,                 loss: 0.2004
agent1:                 episode reward: -0.3851,                 loss: nan
Episode: 30821/101000 (30.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0833s / 5866.8212 s
agent0:                 episode reward: -0.6677,                 loss: 0.1984
agent1:                 episode reward: 0.6677,                 loss: 0.1833
Score delta: 1.5252619188288161, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/30377_0.
Episode: 30841/101000 (30.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7989s / 5873.6201 s
agent0:                 episode reward: -0.8319,                 loss: nan
agent1:                 episode reward: 0.8319,                 loss: 0.1817
Episode: 30861/101000 (30.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9550s / 5879.5751 s
agent0:                 episode reward: -0.3474,                 loss: 0.2178
agent1:                 episode reward: 0.3474,                 loss: 0.1816
Score delta: 1.5533974799014447, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/30428_1.
Episode: 30881/101000 (30.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9908s / 5889.5659 s
agent0:                 episode reward: 0.0659,                 loss: 0.2218
agent1:                 episode reward: -0.0659,                 loss: nan
Episode: 30901/101000 (30.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5357s / 5899.1016 s
agent0:                 episode reward: -0.1639,                 loss: 0.2204
agent1:                 episode reward: 0.1639,                 loss: nan
Episode: 30921/101000 (30.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1756s / 5903.2772 s
agent0:                 episode reward: 0.0277,                 loss: 0.2221
agent1:                 episode reward: -0.0277,                 loss: nan
Episode: 30941/101000 (30.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6025s / 5912.8797 s
agent0:                 episode reward: -0.4024,                 loss: 0.2193
agent1:                 episode reward: 0.4024,                 loss: nan
Episode: 30961/101000 (30.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9983s / 5920.8780 s
agent0:                 episode reward: -0.2277,                 loss: 0.2213
agent1:                 episode reward: 0.2277,                 loss: nan
Episode: 30981/101000 (30.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7986s / 5926.6766 s
agent0:                 episode reward: -0.2706,                 loss: 0.2212
agent1:                 episode reward: 0.2706,                 loss: nan
Episode: 31001/101000 (30.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7604s / 5932.4370 s
agent0:                 episode reward: -0.3643,                 loss: 0.2084
agent1:                 episode reward: 0.3643,                 loss: nan
Episode: 31021/101000 (30.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6513s / 5940.0883 s
agent0:                 episode reward: -0.3640,                 loss: 0.2019
agent1:                 episode reward: 0.3640,                 loss: nan
Episode: 31041/101000 (30.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7030s / 5947.7913 s
agent0:                 episode reward: -0.4917,                 loss: 0.2002
agent1:                 episode reward: 0.4917,                 loss: nan
Episode: 31061/101000 (30.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6071s / 5953.3984 s
agent0:                 episode reward: -0.4058,                 loss: 0.2007
agent1:                 episode reward: 0.4058,                 loss: nan
Episode: 31081/101000 (30.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4679s / 5961.8663 s
agent0:                 episode reward: 0.1794,                 loss: 0.2031
agent1:                 episode reward: -0.1794,                 loss: nan
Episode: 31101/101000 (30.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9945s / 5968.8607 s
agent0:                 episode reward: 0.4912,                 loss: 0.1990
agent1:                 episode reward: -0.4912,                 loss: 0.2206
Score delta: 1.504960754410701, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/30674_0.
Episode: 31121/101000 (30.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9688s / 5975.8295 s
agent0:                 episode reward: -0.2106,                 loss: nan
agent1:                 episode reward: 0.2106,                 loss: 0.1933
Episode: 31141/101000 (30.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2534s / 5983.0829 s
agent0:                 episode reward: -0.3402,                 loss: nan
agent1:                 episode reward: 0.3402,                 loss: 0.1867
Episode: 31161/101000 (30.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4701s / 5991.5530 s
agent0:                 episode reward: -0.5597,                 loss: 0.2773
agent1:                 episode reward: 0.5597,                 loss: 0.1821
Score delta: 1.7582646689805912, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/30722_1.
Episode: 31181/101000 (30.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4072s / 5999.9602 s
agent0:                 episode reward: 0.3010,                 loss: 0.2734
agent1:                 episode reward: -0.3010,                 loss: nan
Episode: 31201/101000 (30.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3456s / 6008.3058 s
agent0:                 episode reward: -0.0867,                 loss: 0.2726
agent1:                 episode reward: 0.0867,                 loss: nan
Episode: 31221/101000 (30.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7762s / 6016.0820 s
agent0:                 episode reward: -0.0190,                 loss: 0.2762
agent1:                 episode reward: 0.0190,                 loss: nan
Episode: 31241/101000 (30.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6736s / 6024.7556 s
agent0:                 episode reward: -0.2906,                 loss: 0.2743
agent1:                 episode reward: 0.2906,                 loss: nan
Episode: 31261/101000 (30.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5236s / 6033.2793 s
agent0:                 episode reward: 0.3038,                 loss: 0.2712
agent1:                 episode reward: -0.3038,                 loss: nan
Episode: 31281/101000 (30.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4640s / 6041.7433 s
agent0:                 episode reward: 0.2024,                 loss: 0.2724
agent1:                 episode reward: -0.2024,                 loss: nan
Episode: 31301/101000 (30.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0686s / 6048.8119 s
agent0:                 episode reward: -0.2875,                 loss: 0.2714
agent1:                 episode reward: 0.2875,                 loss: nan
Episode: 31321/101000 (31.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6837s / 6056.4956 s
agent0:                 episode reward: 0.1341,                 loss: 0.2707
agent1:                 episode reward: -0.1341,                 loss: nan
Episode: 31341/101000 (31.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4654s / 6062.9609 s
agent0:                 episode reward: 0.4177,                 loss: 0.2716
agent1:                 episode reward: -0.4177,                 loss: 0.1992
Score delta: 1.5286799900551178, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/30909_0.
Episode: 31361/101000 (31.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4524s / 6068.4134 s
agent0:                 episode reward: -0.9080,                 loss: nan
agent1:                 episode reward: 0.9080,                 loss: 0.2002
Episode: 31381/101000 (31.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5848s / 6075.9981 s
agent0:                 episode reward: -0.7489,                 loss: nan
agent1:                 episode reward: 0.7489,                 loss: 0.1991
Episode: 31401/101000 (31.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7462s / 6082.7443 s
agent0:                 episode reward: -0.6236,                 loss: 0.2919
agent1:                 episode reward: 0.6236,                 loss: 0.1944
Score delta: 1.566192585634799, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/30969_1.
Episode: 31421/101000 (31.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4927s / 6089.2370 s
agent0:                 episode reward: -0.8675,                 loss: 0.2913
agent1:                 episode reward: 0.8675,                 loss: nan
Episode: 31441/101000 (31.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9094s / 6094.1464 s
agent0:                 episode reward: -0.1491,                 loss: 0.2423
agent1:                 episode reward: 0.1491,                 loss: nan
Episode: 31461/101000 (31.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8552s / 6102.0016 s
agent0:                 episode reward: -0.5507,                 loss: 0.2130
agent1:                 episode reward: 0.5507,                 loss: nan
Episode: 31481/101000 (31.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3817s / 6112.3833 s
agent0:                 episode reward: -0.6794,                 loss: 0.2132
agent1:                 episode reward: 0.6794,                 loss: nan
Episode: 31501/101000 (31.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2268s / 6121.6101 s
agent0:                 episode reward: -0.3394,                 loss: 0.2134
agent1:                 episode reward: 0.3394,                 loss: nan
Episode: 31521/101000 (31.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7528s / 6129.3628 s
agent0:                 episode reward: -0.3578,                 loss: 0.2091
agent1:                 episode reward: 0.3578,                 loss: nan
Episode: 31541/101000 (31.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0639s / 6137.4267 s
agent0:                 episode reward: -0.5345,                 loss: 0.2099
agent1:                 episode reward: 0.5345,                 loss: nan
Episode: 31561/101000 (31.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7842s / 6146.2110 s
agent0:                 episode reward: -0.1997,                 loss: 0.2141
agent1:                 episode reward: 0.1997,                 loss: nan
Episode: 31581/101000 (31.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3252s / 6154.5362 s
agent0:                 episode reward: -0.6323,                 loss: 0.2122
agent1:                 episode reward: 0.6323,                 loss: nan
Episode: 31601/101000 (31.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5508s / 6160.0870 s
agent0:                 episode reward: -0.3966,                 loss: 0.2101
agent1:                 episode reward: 0.3966,                 loss: nan
Episode: 31621/101000 (31.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9497s / 6165.0367 s
agent0:                 episode reward: -0.1809,                 loss: 0.2114
agent1:                 episode reward: 0.1809,                 loss: nan
Episode: 31641/101000 (31.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0812s / 6171.1179 s
agent0:                 episode reward: -0.2887,                 loss: 0.2110
agent1:                 episode reward: 0.2887,                 loss: nan
Episode: 31661/101000 (31.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2585s / 6177.3764 s
agent0:                 episode reward: -0.2174,                 loss: 0.2132
agent1:                 episode reward: 0.2174,                 loss: nan
Episode: 31681/101000 (31.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6100s / 6185.9864 s
agent0:                 episode reward: -0.5446,                 loss: 0.2113
agent1:                 episode reward: 0.5446,                 loss: nan
Episode: 31701/101000 (31.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8426s / 6194.8290 s
agent0:                 episode reward: -0.0478,                 loss: 0.2090
agent1:                 episode reward: 0.0478,                 loss: nan
Episode: 31721/101000 (31.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6460s / 6202.4750 s
agent0:                 episode reward: -0.4531,                 loss: 0.2091
agent1:                 episode reward: 0.4531,                 loss: nan
Episode: 31741/101000 (31.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8884s / 6213.3634 s
agent0:                 episode reward: 0.1266,                 loss: 0.2130
agent1:                 episode reward: -0.1266,                 loss: nan
Episode: 31761/101000 (31.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6330s / 6222.9964 s
agent0:                 episode reward: -0.2823,                 loss: 0.2099
agent1:                 episode reward: 0.2823,                 loss: nan
Episode: 31781/101000 (31.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8982s / 6229.8946 s
agent0:                 episode reward: -0.3879,                 loss: 0.1977
agent1:                 episode reward: 0.3879,                 loss: nan
Episode: 31801/101000 (31.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2101s / 6237.1047 s
agent0:                 episode reward: -0.1312,                 loss: 0.1950
agent1:                 episode reward: 0.1312,                 loss: nan
Episode: 31821/101000 (31.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5836s / 6244.6883 s
agent0:                 episode reward: -0.3298,                 loss: 0.1947
agent1:                 episode reward: 0.3298,                 loss: nan
Episode: 31841/101000 (31.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4705s / 6251.1588 s
agent0:                 episode reward: -0.5475,                 loss: 0.1970
agent1:                 episode reward: 0.5475,                 loss: nan
Episode: 31861/101000 (31.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5380s / 6258.6968 s
agent0:                 episode reward: -0.0082,                 loss: 0.1960
agent1:                 episode reward: 0.0082,                 loss: nan
Episode: 31881/101000 (31.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8374s / 6265.5342 s
agent0:                 episode reward: -0.3917,                 loss: 0.1950
agent1:                 episode reward: 0.3917,                 loss: nan
Episode: 31901/101000 (31.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5470s / 6274.0812 s
agent0:                 episode reward: -0.1406,                 loss: 0.1972
agent1:                 episode reward: 0.1406,                 loss: nan
Episode: 31921/101000 (31.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8021s / 6280.8833 s
agent0:                 episode reward: -0.1740,                 loss: 0.1970
agent1:                 episode reward: 0.1740,                 loss: nan
Episode: 31941/101000 (31.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5194s / 6289.4027 s
agent0:                 episode reward: -0.0641,                 loss: 0.1956
agent1:                 episode reward: 0.0641,                 loss: 0.1673
Score delta: 1.5354208915166077, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/31499_0.
Episode: 31961/101000 (31.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7870s / 6297.1897 s
agent0:                 episode reward: -0.4271,                 loss: nan
agent1:                 episode reward: 0.4271,                 loss: 0.1617
Episode: 31981/101000 (31.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6159s / 6303.8056 s
agent0:                 episode reward: -0.5262,                 loss: nan
agent1:                 episode reward: 0.5262,                 loss: 0.1608
Episode: 32001/101000 (31.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7176s / 6310.5232 s
agent0:                 episode reward: 0.0404,                 loss: 0.1995
agent1:                 episode reward: -0.0404,                 loss: 0.1606
Score delta: 1.624544783091921, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/31562_1.
Episode: 32021/101000 (31.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6421s / 6318.1653 s
agent0:                 episode reward: 0.0415,                 loss: 0.2005
agent1:                 episode reward: -0.0415,                 loss: nan
Episode: 32041/101000 (31.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8964s / 6327.0617 s
agent0:                 episode reward: -0.2814,                 loss: 0.1991
agent1:                 episode reward: 0.2814,                 loss: nan
Episode: 32061/101000 (31.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9052s / 6334.9669 s
agent0:                 episode reward: -0.0326,                 loss: 0.1984
agent1:                 episode reward: 0.0326,                 loss: 0.1819
Score delta: 1.5816780686556107, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/31627_0.
Episode: 32081/101000 (31.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8098s / 6341.7767 s
agent0:                 episode reward: 0.0292,                 loss: nan
agent1:                 episode reward: -0.0292,                 loss: 0.1827
Episode: 32101/101000 (31.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1923s / 6348.9690 s
agent0:                 episode reward: 0.0929,                 loss: nan
agent1:                 episode reward: -0.0929,                 loss: 0.1849
Episode: 32121/101000 (31.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1100s / 6354.0790 s
agent0:                 episode reward: -0.0238,                 loss: nan
agent1:                 episode reward: 0.0238,                 loss: 0.1820
Episode: 32141/101000 (31.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7747s / 6359.8537 s
agent0:                 episode reward: -1.2474,                 loss: 0.2398
agent1:                 episode reward: 1.2474,                 loss: 0.1870
Score delta: 1.7156434072551618, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/31700_1.
Episode: 32161/101000 (31.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2470s / 6369.1007 s
agent0:                 episode reward: -0.1608,                 loss: 0.2339
agent1:                 episode reward: 0.1608,                 loss: nan
Episode: 32181/101000 (31.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8794s / 6376.9801 s
agent0:                 episode reward: -0.7649,                 loss: 0.2325
agent1:                 episode reward: 0.7649,                 loss: nan
Episode: 32201/101000 (31.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1160s / 6384.0961 s
agent0:                 episode reward: -0.8358,                 loss: 0.2302
agent1:                 episode reward: 0.8358,                 loss: nan
Episode: 32221/101000 (31.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4041s / 6391.5002 s
agent0:                 episode reward: -0.0951,                 loss: 0.2327
agent1:                 episode reward: 0.0951,                 loss: nan
Episode: 32241/101000 (31.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4005s / 6397.9007 s
agent0:                 episode reward: -0.4930,                 loss: 0.2182
agent1:                 episode reward: 0.4930,                 loss: nan
Episode: 32261/101000 (31.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8000s / 6405.7007 s
agent0:                 episode reward: -0.4421,                 loss: 0.2019
agent1:                 episode reward: 0.4421,                 loss: nan
Episode: 32281/101000 (31.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0823s / 6414.7830 s
agent0:                 episode reward: -0.2462,                 loss: 0.2025
agent1:                 episode reward: 0.2462,                 loss: nan
Episode: 32301/101000 (31.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7803s / 6422.5633 s
agent0:                 episode reward: -0.3545,                 loss: 0.2005
agent1:                 episode reward: 0.3545,                 loss: nan
Episode: 32321/101000 (32.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0835s / 6430.6467 s
agent0:                 episode reward: 0.0101,                 loss: 0.2013
agent1:                 episode reward: -0.0101,                 loss: nan
Episode: 32341/101000 (32.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4226s / 6439.0693 s
agent0:                 episode reward: -0.1948,                 loss: 0.1980
agent1:                 episode reward: 0.1948,                 loss: nan
Episode: 32361/101000 (32.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9591s / 6447.0284 s
agent0:                 episode reward: -0.1343,                 loss: 0.2000
agent1:                 episode reward: 0.1343,                 loss: nan
Episode: 32381/101000 (32.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9403s / 6452.9687 s
agent0:                 episode reward: 0.0052,                 loss: 0.2010
agent1:                 episode reward: -0.0052,                 loss: nan
Episode: 32401/101000 (32.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0445s / 6462.0132 s
agent0:                 episode reward: -0.2965,                 loss: 0.2008
agent1:                 episode reward: 0.2965,                 loss: nan
Episode: 32421/101000 (32.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7365s / 6466.7497 s
agent0:                 episode reward: -0.4087,                 loss: 0.1987
agent1:                 episode reward: 0.4087,                 loss: nan
Episode: 32441/101000 (32.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8785s / 6473.6283 s
agent0:                 episode reward: -0.2247,                 loss: 0.1999
agent1:                 episode reward: 0.2247,                 loss: nan
Episode: 32461/101000 (32.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8256s / 6480.4539 s
agent0:                 episode reward: -0.4171,                 loss: 0.1991
agent1:                 episode reward: 0.4171,                 loss: nan
Episode: 32481/101000 (32.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4434s / 6489.8973 s
agent0:                 episode reward: 0.0556,                 loss: 0.1979
agent1:                 episode reward: -0.0556,                 loss: nan
Episode: 32501/101000 (32.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3608s / 6499.2580 s
agent0:                 episode reward: -0.5623,                 loss: 0.2008
agent1:                 episode reward: 0.5623,                 loss: nan
Episode: 32521/101000 (32.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7676s / 6507.0256 s
agent0:                 episode reward: -0.4075,                 loss: 0.2013
agent1:                 episode reward: 0.4075,                 loss: nan
Episode: 32541/101000 (32.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4835s / 6515.5091 s
agent0:                 episode reward: -0.3691,                 loss: 0.1980
agent1:                 episode reward: 0.3691,                 loss: nan
Episode: 32561/101000 (32.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6004s / 6523.1095 s
agent0:                 episode reward: -0.3830,                 loss: 0.1980
agent1:                 episode reward: 0.3830,                 loss: nan
Episode: 32581/101000 (32.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6633s / 6530.7728 s
agent0:                 episode reward: -0.1043,                 loss: 0.1999
agent1:                 episode reward: 0.1043,                 loss: nan
Episode: 32601/101000 (32.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9591s / 6538.7319 s
agent0:                 episode reward: 0.2270,                 loss: 0.1997
agent1:                 episode reward: -0.2270,                 loss: nan
Episode: 32621/101000 (32.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5745s / 6546.3064 s
agent0:                 episode reward: -0.4371,                 loss: 0.1986
agent1:                 episode reward: 0.4371,                 loss: nan
Episode: 32641/101000 (32.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2328s / 6554.5392 s
agent0:                 episode reward: 0.0811,                 loss: 0.1985
agent1:                 episode reward: -0.0811,                 loss: nan
Episode: 32661/101000 (32.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5313s / 6562.0705 s
agent0:                 episode reward: -0.0125,                 loss: 0.1977
agent1:                 episode reward: 0.0125,                 loss: nan
Episode: 32681/101000 (32.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5960s / 6571.6665 s
agent0:                 episode reward: -0.0386,                 loss: 0.1964
agent1:                 episode reward: 0.0386,                 loss: nan
Episode: 32701/101000 (32.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3824s / 6578.0488 s
agent0:                 episode reward: -0.5462,                 loss: 0.1976
agent1:                 episode reward: 0.5462,                 loss: nan
Episode: 32721/101000 (32.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5918s / 6585.6406 s
agent0:                 episode reward: -0.2568,                 loss: 0.1976
agent1:                 episode reward: 0.2568,                 loss: nan
Episode: 32741/101000 (32.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9101s / 6592.5507 s
agent0:                 episode reward: -0.1232,                 loss: 0.1972
agent1:                 episode reward: 0.1232,                 loss: nan
Episode: 32761/101000 (32.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4140s / 6600.9647 s
agent0:                 episode reward: -0.0362,                 loss: 0.1989
agent1:                 episode reward: 0.0362,                 loss: nan
Episode: 32781/101000 (32.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2600s / 6609.2247 s
agent0:                 episode reward: -0.5043,                 loss: 0.1958
agent1:                 episode reward: 0.5043,                 loss: nan
Episode: 32801/101000 (32.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4599s / 6617.6845 s
agent0:                 episode reward: -0.4480,                 loss: 0.1949
agent1:                 episode reward: 0.4480,                 loss: nan
Episode: 32821/101000 (32.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3295s / 6626.0141 s
agent0:                 episode reward: 0.1125,                 loss: 0.1987
agent1:                 episode reward: -0.1125,                 loss: nan
Episode: 32841/101000 (32.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0083s / 6634.0223 s
agent0:                 episode reward: -0.3973,                 loss: 0.1972
agent1:                 episode reward: 0.3973,                 loss: nan
Episode: 32861/101000 (32.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5533s / 6640.5756 s
agent0:                 episode reward: 0.4183,                 loss: 0.1956
agent1:                 episode reward: -0.4183,                 loss: 0.1842
Score delta: 1.6871908788242993, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/32426_0.
Episode: 32881/101000 (32.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2257s / 6648.8013 s
agent0:                 episode reward: -0.5854,                 loss: nan
agent1:                 episode reward: 0.5854,                 loss: 0.1839
Episode: 32901/101000 (32.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5520s / 6656.3533 s
agent0:                 episode reward: -0.0318,                 loss: nan
agent1:                 episode reward: 0.0318,                 loss: 0.1833
Episode: 32921/101000 (32.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6440s / 6662.9972 s
agent0:                 episode reward: -0.1838,                 loss: 0.2445
agent1:                 episode reward: 0.1838,                 loss: 0.1825
Score delta: 1.5970944536475742, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/32487_1.
Episode: 32941/101000 (32.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3411s / 6672.3383 s
agent0:                 episode reward: -0.3144,                 loss: 0.2426
agent1:                 episode reward: 0.3144,                 loss: nan
Episode: 32961/101000 (32.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3311s / 6682.6694 s
agent0:                 episode reward: -0.1289,                 loss: 0.2400
agent1:                 episode reward: 0.1289,                 loss: nan
Episode: 32981/101000 (32.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6873s / 6691.3567 s
agent0:                 episode reward: -0.0963,                 loss: 0.2017
agent1:                 episode reward: 0.0963,                 loss: nan
Episode: 33001/101000 (32.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3760s / 6699.7327 s
agent0:                 episode reward: 0.0114,                 loss: 0.1989
agent1:                 episode reward: -0.0114,                 loss: nan
Episode: 33021/101000 (32.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7960s / 6706.5287 s
agent0:                 episode reward: -0.3918,                 loss: 0.2016
agent1:                 episode reward: 0.3918,                 loss: nan
Episode: 33041/101000 (32.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4415s / 6713.9702 s
agent0:                 episode reward: 0.1035,                 loss: 0.1998
agent1:                 episode reward: -0.1035,                 loss: nan
Episode: 33061/101000 (32.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2902s / 6721.2604 s
agent0:                 episode reward: -0.2235,                 loss: 0.1987
agent1:                 episode reward: 0.2235,                 loss: nan
Episode: 33081/101000 (32.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3441s / 6730.6045 s
agent0:                 episode reward: 0.2071,                 loss: 0.2009
agent1:                 episode reward: -0.2071,                 loss: nan
Episode: 33101/101000 (32.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3202s / 6738.9247 s
agent0:                 episode reward: -0.1119,                 loss: 0.1995
agent1:                 episode reward: 0.1119,                 loss: nan
Episode: 33121/101000 (32.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8427s / 6746.7673 s
agent0:                 episode reward: -0.0014,                 loss: 0.2000
agent1:                 episode reward: 0.0014,                 loss: nan
Episode: 33141/101000 (32.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2918s / 6756.0591 s
agent0:                 episode reward: -0.0862,                 loss: 0.2009
agent1:                 episode reward: 0.0862,                 loss: nan
Episode: 33161/101000 (32.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8800s / 6764.9391 s
agent0:                 episode reward: -0.0537,                 loss: 0.2009
agent1:                 episode reward: 0.0537,                 loss: nan
Episode: 33181/101000 (32.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2223s / 6772.1614 s
agent0:                 episode reward: -0.1094,                 loss: 0.1987
agent1:                 episode reward: 0.1094,                 loss: nan
Episode: 33201/101000 (32.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3318s / 6779.4932 s
agent0:                 episode reward: -0.3856,                 loss: 0.2007
agent1:                 episode reward: 0.3856,                 loss: nan
Episode: 33221/101000 (32.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8316s / 6786.3248 s
agent0:                 episode reward: 0.1187,                 loss: 0.2000
agent1:                 episode reward: -0.1187,                 loss: nan
Episode: 33241/101000 (32.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3488s / 6793.6736 s
agent0:                 episode reward: 0.2928,                 loss: 0.2006
agent1:                 episode reward: -0.2928,                 loss: nan
Episode: 33261/101000 (32.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9065s / 6799.5801 s
agent0:                 episode reward: -0.1675,                 loss: 0.2014
agent1:                 episode reward: 0.1675,                 loss: nan
Episode: 33281/101000 (32.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7087s / 6808.2888 s
agent0:                 episode reward: 0.2583,                 loss: 0.2005
agent1:                 episode reward: -0.2583,                 loss: nan
Episode: 33301/101000 (32.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1291s / 6815.4179 s
agent0:                 episode reward: -0.4783,                 loss: 0.1987
agent1:                 episode reward: 0.4783,                 loss: nan
Episode: 33321/101000 (32.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7787s / 6824.1966 s
agent0:                 episode reward: -0.0485,                 loss: 0.1957
agent1:                 episode reward: 0.0485,                 loss: nan
Episode: 33341/101000 (33.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6397s / 6831.8362 s
agent0:                 episode reward: -0.0888,                 loss: 0.1992
agent1:                 episode reward: 0.0888,                 loss: nan
Episode: 33361/101000 (33.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0767s / 6840.9130 s
agent0:                 episode reward: -0.0051,                 loss: 0.1984
agent1:                 episode reward: 0.0051,                 loss: nan
Episode: 33381/101000 (33.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5017s / 6847.4146 s
agent0:                 episode reward: 0.0342,                 loss: 0.1974
agent1:                 episode reward: -0.0342,                 loss: nan
Episode: 33401/101000 (33.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1365s / 6856.5512 s
agent0:                 episode reward: -0.3392,                 loss: 0.1981
agent1:                 episode reward: 0.3392,                 loss: nan
Episode: 33421/101000 (33.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4408s / 6862.9920 s
agent0:                 episode reward: 0.1959,                 loss: 0.1976
agent1:                 episode reward: -0.1959,                 loss: nan
Episode: 33441/101000 (33.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3181s / 6869.3101 s
agent0:                 episode reward: 0.2335,                 loss: 0.1978
agent1:                 episode reward: -0.2335,                 loss: 0.1791
Score delta: 1.7320902981012825, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/33003_0.
Episode: 33461/101000 (33.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8498s / 6878.1599 s
agent0:                 episode reward: -0.6536,                 loss: nan
agent1:                 episode reward: 0.6536,                 loss: 0.1798
Episode: 33481/101000 (33.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8216s / 6886.9815 s
agent0:                 episode reward: -0.2533,                 loss: nan
agent1:                 episode reward: 0.2533,                 loss: 0.1814
Episode: 33501/101000 (33.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8407s / 6893.8223 s
agent0:                 episode reward: -0.5442,                 loss: 0.1893
agent1:                 episode reward: 0.5442,                 loss: 0.1814
Score delta: 1.5515506382364908, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/33067_1.
Episode: 33521/101000 (33.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9480s / 6902.7703 s
agent0:                 episode reward: -0.2314,                 loss: 0.1907
agent1:                 episode reward: 0.2314,                 loss: nan
Episode: 33541/101000 (33.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1321s / 6911.9024 s
agent0:                 episode reward: -0.1456,                 loss: 0.1891
agent1:                 episode reward: 0.1456,                 loss: nan
Episode: 33561/101000 (33.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7845s / 6920.6868 s
agent0:                 episode reward: -0.4543,                 loss: 0.1892
agent1:                 episode reward: 0.4543,                 loss: nan
Episode: 33581/101000 (33.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4537s / 6928.1405 s
agent0:                 episode reward: 0.0699,                 loss: 0.1887
agent1:                 episode reward: -0.0699,                 loss: nan
Episode: 33601/101000 (33.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5989s / 6936.7394 s
agent0:                 episode reward: -0.4332,                 loss: 0.1904
agent1:                 episode reward: 0.4332,                 loss: nan
Episode: 33621/101000 (33.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9339s / 6944.6733 s
agent0:                 episode reward: -0.1498,                 loss: 0.1899
agent1:                 episode reward: 0.1498,                 loss: nan
Episode: 33641/101000 (33.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1227s / 6951.7960 s
agent0:                 episode reward: 0.3368,                 loss: 0.1916
agent1:                 episode reward: -0.3368,                 loss: 0.1883
Score delta: 1.6698574701493576, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/33201_0.
Episode: 33661/101000 (33.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9982s / 6960.7943 s
agent0:                 episode reward: -0.6370,                 loss: nan
agent1:                 episode reward: 0.6370,                 loss: 0.1854
Episode: 33681/101000 (33.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8636s / 6967.6579 s
agent0:                 episode reward: -0.8114,                 loss: nan
agent1:                 episode reward: 0.8114,                 loss: 0.1863
Episode: 33701/101000 (33.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8427s / 6973.5005 s
agent0:                 episode reward: -0.6195,                 loss: nan
agent1:                 episode reward: 0.6195,                 loss: 0.1837
Episode: 33721/101000 (33.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 11.4040s / 6984.9046 s
agent0:                 episode reward: -0.7189,                 loss: 0.2956
agent1:                 episode reward: 0.7189,                 loss: 0.1733
Score delta: 1.8822141258305627, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/33276_1.
Episode: 33741/101000 (33.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3036s / 6995.2082 s
agent0:                 episode reward: -0.5843,                 loss: 0.2871
agent1:                 episode reward: 0.5843,                 loss: nan
Episode: 33761/101000 (33.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2094s / 7001.4175 s
agent0:                 episode reward: -0.5448,                 loss: 0.2888
agent1:                 episode reward: 0.5448,                 loss: nan
Episode: 33781/101000 (33.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5055s / 7009.9230 s
agent0:                 episode reward: -0.1991,                 loss: 0.2327
agent1:                 episode reward: 0.1991,                 loss: nan
Episode: 33801/101000 (33.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5410s / 7018.4640 s
agent0:                 episode reward: -0.5344,                 loss: 0.2150
agent1:                 episode reward: 0.5344,                 loss: nan
Episode: 33821/101000 (33.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1897s / 7025.6537 s
agent0:                 episode reward: -0.6312,                 loss: 0.2168
agent1:                 episode reward: 0.6312,                 loss: nan
Episode: 33841/101000 (33.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4135s / 7036.0672 s
agent0:                 episode reward: -0.2221,                 loss: 0.2188
agent1:                 episode reward: 0.2221,                 loss: nan
Episode: 33861/101000 (33.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0995s / 7044.1667 s
agent0:                 episode reward: -0.1379,                 loss: 0.2166
agent1:                 episode reward: 0.1379,                 loss: nan
Episode: 33881/101000 (33.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7653s / 7050.9319 s
agent0:                 episode reward: -0.7166,                 loss: 0.2158
agent1:                 episode reward: 0.7166,                 loss: nan
Episode: 33901/101000 (33.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4308s / 7058.3628 s
agent0:                 episode reward: -0.5212,                 loss: 0.2165
agent1:                 episode reward: 0.5212,                 loss: nan
Episode: 33921/101000 (33.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3455s / 7067.7083 s
agent0:                 episode reward: 0.1176,                 loss: 0.2153
agent1:                 episode reward: -0.1176,                 loss: nan
Episode: 33941/101000 (33.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4985s / 7076.2067 s
agent0:                 episode reward: -0.1387,                 loss: 0.2164
agent1:                 episode reward: 0.1387,                 loss: nan
Episode: 33961/101000 (33.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5303s / 7083.7370 s
agent0:                 episode reward: -0.0524,                 loss: 0.2159
agent1:                 episode reward: 0.0524,                 loss: nan
Episode: 33981/101000 (33.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7969s / 7093.5340 s
agent0:                 episode reward: -0.2804,                 loss: 0.2167
agent1:                 episode reward: 0.2804,                 loss: nan
Episode: 34001/101000 (33.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2673s / 7101.8013 s
agent0:                 episode reward: -0.3609,                 loss: 0.2144
agent1:                 episode reward: 0.3609,                 loss: nan
Episode: 34021/101000 (33.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4183s / 7110.2196 s
agent0:                 episode reward: 0.1239,                 loss: 0.2139
agent1:                 episode reward: -0.1239,                 loss: 0.1880
Score delta: 1.556885514434833, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/33584_0.
Episode: 34041/101000 (33.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9233s / 7116.1428 s
agent0:                 episode reward: -0.4888,                 loss: nan
agent1:                 episode reward: 0.4888,                 loss: 0.1850
Episode: 34061/101000 (33.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4589s / 7124.6017 s
agent0:                 episode reward: -0.6079,                 loss: nan
agent1:                 episode reward: 0.6079,                 loss: 0.1880
Episode: 34081/101000 (33.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1622s / 7132.7639 s
agent0:                 episode reward: -0.3040,                 loss: 0.2051
agent1:                 episode reward: 0.3040,                 loss: 0.1868
Score delta: 1.5322016575651856, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/33641_1.
Episode: 34101/101000 (33.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0142s / 7140.7782 s
agent0:                 episode reward: -0.1839,                 loss: 0.2041
agent1:                 episode reward: 0.1839,                 loss: nan
Episode: 34121/101000 (33.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1206s / 7148.8988 s
agent0:                 episode reward: -0.6258,                 loss: 0.2035
agent1:                 episode reward: 0.6258,                 loss: nan
Episode: 34141/101000 (33.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6234s / 7155.5221 s
agent0:                 episode reward: 0.1740,                 loss: 0.2041
agent1:                 episode reward: -0.1740,                 loss: nan
Episode: 34161/101000 (33.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7783s / 7165.3005 s
agent0:                 episode reward: 0.2632,                 loss: 0.2019
agent1:                 episode reward: -0.2632,                 loss: nan
Episode: 34181/101000 (33.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0474s / 7173.3479 s
agent0:                 episode reward: -0.2891,                 loss: 0.1916
agent1:                 episode reward: 0.2891,                 loss: nan
Episode: 34201/101000 (33.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6969s / 7184.0448 s
agent0:                 episode reward: 0.1293,                 loss: 0.1941
agent1:                 episode reward: -0.1293,                 loss: nan
Episode: 34221/101000 (33.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1854s / 7192.2301 s
agent0:                 episode reward: 0.0789,                 loss: 0.1905
agent1:                 episode reward: -0.0789,                 loss: nan
Episode: 34241/101000 (33.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7517s / 7202.9819 s
agent0:                 episode reward: -0.5602,                 loss: 0.1925
agent1:                 episode reward: 0.5602,                 loss: nan
Episode: 34261/101000 (33.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7166s / 7210.6985 s
agent0:                 episode reward: -0.4945,                 loss: 0.1918
agent1:                 episode reward: 0.4945,                 loss: nan
Episode: 34281/101000 (33.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2132s / 7218.9116 s
agent0:                 episode reward: -0.9026,                 loss: 0.1907
agent1:                 episode reward: 0.9026,                 loss: nan
Episode: 34301/101000 (33.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5754s / 7227.4870 s
agent0:                 episode reward: -0.5473,                 loss: 0.1900
agent1:                 episode reward: 0.5473,                 loss: nan
Episode: 34321/101000 (33.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8564s / 7235.3434 s
agent0:                 episode reward: -0.1701,                 loss: 0.1895
agent1:                 episode reward: 0.1701,                 loss: nan
Episode: 34341/101000 (34.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5055s / 7240.8488 s
agent0:                 episode reward: -0.2679,                 loss: 0.1911
agent1:                 episode reward: 0.2679,                 loss: 0.1660
Score delta: 1.9731873539738871, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/33900_0.
Episode: 34361/101000 (34.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5799s / 7248.4287 s
agent0:                 episode reward: -0.3295,                 loss: nan
agent1:                 episode reward: 0.3295,                 loss: 0.1642
Episode: 34381/101000 (34.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0976s / 7256.5263 s
agent0:                 episode reward: -0.6107,                 loss: nan
agent1:                 episode reward: 0.6107,                 loss: 0.1638
Episode: 34401/101000 (34.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7456s / 7264.2719 s
agent0:                 episode reward: -0.6668,                 loss: 0.2395
agent1:                 episode reward: 0.6668,                 loss: 0.1633
Score delta: 1.6167400434592623, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/33971_1.
Episode: 34421/101000 (34.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9300s / 7274.2019 s
agent0:                 episode reward: -0.3902,                 loss: 0.2350
agent1:                 episode reward: 0.3902,                 loss: nan
Episode: 34441/101000 (34.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7011s / 7282.9030 s
agent0:                 episode reward: -0.8490,                 loss: 0.2321
agent1:                 episode reward: 0.8490,                 loss: nan
Episode: 34461/101000 (34.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 11.8767s / 7294.7797 s
agent0:                 episode reward: -0.3847,                 loss: 0.2308
agent1:                 episode reward: 0.3847,                 loss: nan
Episode: 34481/101000 (34.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9563s / 7304.7360 s
agent0:                 episode reward: -0.2302,                 loss: 0.2326
agent1:                 episode reward: 0.2302,                 loss: nan
Episode: 34501/101000 (34.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8946s / 7310.6307 s
agent0:                 episode reward: -0.6414,                 loss: 0.2305
agent1:                 episode reward: 0.6414,                 loss: nan
Episode: 34521/101000 (34.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3612s / 7317.9919 s
agent0:                 episode reward: 0.0135,                 loss: 0.2304
agent1:                 episode reward: -0.0135,                 loss: nan
Episode: 34541/101000 (34.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8338s / 7325.8256 s
agent0:                 episode reward: -0.2079,                 loss: 0.2306
agent1:                 episode reward: 0.2079,                 loss: nan
Episode: 34561/101000 (34.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7242s / 7334.5498 s
agent0:                 episode reward: 0.3444,                 loss: 0.2298
agent1:                 episode reward: -0.3444,                 loss: nan
Episode: 34581/101000 (34.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2996s / 7342.8494 s
agent0:                 episode reward: 0.0863,                 loss: 0.1993
agent1:                 episode reward: -0.0863,                 loss: nan
Episode: 34601/101000 (34.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3330s / 7351.1824 s
agent0:                 episode reward: -0.0552,                 loss: 0.1964
agent1:                 episode reward: 0.0552,                 loss: nan
Episode: 34621/101000 (34.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6224s / 7359.8048 s
agent0:                 episode reward: -0.0753,                 loss: 0.1999
agent1:                 episode reward: 0.0753,                 loss: nan
Episode: 34641/101000 (34.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8547s / 7367.6594 s
agent0:                 episode reward: 0.3460,                 loss: 0.1986
agent1:                 episode reward: -0.3460,                 loss: nan
Episode: 34661/101000 (34.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8862s / 7376.5457 s
agent0:                 episode reward: 0.1329,                 loss: 0.1981
agent1:                 episode reward: -0.1329,                 loss: nan
Episode: 34681/101000 (34.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2440s / 7385.7897 s
agent0:                 episode reward: -0.5976,                 loss: 0.1945
agent1:                 episode reward: 0.5976,                 loss: 0.1871
Score delta: 1.5802915865104068, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/34237_0.
Episode: 34701/101000 (34.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4475s / 7394.2372 s
agent0:                 episode reward: -0.6482,                 loss: nan
agent1:                 episode reward: 0.6482,                 loss: 0.1853
Episode: 34721/101000 (34.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1475s / 7401.3848 s
agent0:                 episode reward: -0.2763,                 loss: nan
agent1:                 episode reward: 0.2763,                 loss: 0.1850
Episode: 34741/101000 (34.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4799s / 7406.8647 s
agent0:                 episode reward: -0.4232,                 loss: 0.2674
agent1:                 episode reward: 0.4232,                 loss: 0.1852
Score delta: 1.7630724688213326, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/34302_1.
Episode: 34761/101000 (34.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3639s / 7415.2286 s
agent0:                 episode reward: 0.1779,                 loss: 0.2599
agent1:                 episode reward: -0.1779,                 loss: nan
Episode: 34781/101000 (34.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5482s / 7423.7768 s
agent0:                 episode reward: -0.0391,                 loss: 0.2599
agent1:                 episode reward: 0.0391,                 loss: nan
Episode: 34801/101000 (34.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2047s / 7431.9815 s
agent0:                 episode reward: -0.3430,                 loss: 0.2610
agent1:                 episode reward: 0.3430,                 loss: nan
Episode: 34821/101000 (34.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8559s / 7440.8374 s
agent0:                 episode reward: -0.3851,                 loss: 0.2594
agent1:                 episode reward: 0.3851,                 loss: nan
Episode: 34841/101000 (34.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8688s / 7448.7062 s
agent0:                 episode reward: 0.0181,                 loss: 0.2613
agent1:                 episode reward: -0.0181,                 loss: nan
Episode: 34861/101000 (34.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5746s / 7458.2808 s
agent0:                 episode reward: -0.1294,                 loss: 0.2603
agent1:                 episode reward: 0.1294,                 loss: nan
Episode: 34881/101000 (34.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9996s / 7466.2804 s
agent0:                 episode reward: -0.1794,                 loss: 0.2581
agent1:                 episode reward: 0.1794,                 loss: nan
Episode: 34901/101000 (34.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6546s / 7476.9350 s
agent0:                 episode reward: 0.2536,                 loss: 0.2596
agent1:                 episode reward: -0.2536,                 loss: nan
Episode: 34921/101000 (34.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1974s / 7484.1324 s
agent0:                 episode reward: -0.0295,                 loss: 0.2586
agent1:                 episode reward: 0.0295,                 loss: nan
Episode: 34941/101000 (34.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5024s / 7492.6348 s
agent0:                 episode reward: 0.2301,                 loss: 0.2612
agent1:                 episode reward: -0.2301,                 loss: nan
Episode: 34961/101000 (34.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4347s / 7500.0695 s
agent0:                 episode reward: -0.0841,                 loss: 0.2553
agent1:                 episode reward: 0.0841,                 loss: nan
Episode: 34981/101000 (34.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3231s / 7507.3925 s
agent0:                 episode reward: 0.0687,                 loss: 0.2143
agent1:                 episode reward: -0.0687,                 loss: nan
Episode: 35001/101000 (34.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7851s / 7514.1777 s
agent0:                 episode reward: -0.0515,                 loss: 0.2139
agent1:                 episode reward: 0.0515,                 loss: nan
Episode: 35021/101000 (34.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0517s / 7521.2294 s
agent0:                 episode reward: 0.0555,                 loss: 0.2122
agent1:                 episode reward: -0.0555,                 loss: nan
Episode: 35041/101000 (34.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9004s / 7529.1299 s
agent0:                 episode reward: -0.4905,                 loss: 0.2166
agent1:                 episode reward: 0.4905,                 loss: nan
Episode: 35061/101000 (34.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7280s / 7538.8578 s
agent0:                 episode reward: -0.2496,                 loss: 0.2153
agent1:                 episode reward: 0.2496,                 loss: nan
Episode: 35081/101000 (34.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7828s / 7547.6406 s
agent0:                 episode reward: -0.3572,                 loss: 0.2132
agent1:                 episode reward: 0.3572,                 loss: nan
Episode: 35101/101000 (34.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1729s / 7554.8135 s
agent0:                 episode reward: 0.1375,                 loss: 0.2137
agent1:                 episode reward: -0.1375,                 loss: nan
Episode: 35121/101000 (34.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2685s / 7562.0820 s
agent0:                 episode reward: 0.1822,                 loss: 0.2135
agent1:                 episode reward: -0.1822,                 loss: nan
Episode: 35141/101000 (34.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3295s / 7569.4115 s
agent0:                 episode reward: 0.3522,                 loss: 0.2136
agent1:                 episode reward: -0.3522,                 loss: nan
Episode: 35161/101000 (34.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5663s / 7575.9779 s
agent0:                 episode reward: -0.4225,                 loss: 0.2137
agent1:                 episode reward: 0.4225,                 loss: nan
Episode: 35181/101000 (34.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0438s / 7585.0217 s
agent0:                 episode reward: 0.2583,                 loss: 0.2155
agent1:                 episode reward: -0.2583,                 loss: nan
Episode: 35201/101000 (34.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0116s / 7591.0333 s
agent0:                 episode reward: -0.1083,                 loss: 0.2134
agent1:                 episode reward: 0.1083,                 loss: nan
Episode: 35221/101000 (34.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7338s / 7600.7671 s
agent0:                 episode reward: -0.4542,                 loss: 0.2143
agent1:                 episode reward: 0.4542,                 loss: nan
Episode: 35241/101000 (34.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3448s / 7610.1119 s
agent0:                 episode reward: -0.1225,                 loss: 0.2125
agent1:                 episode reward: 0.1225,                 loss: nan
Episode: 35261/101000 (34.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1033s / 7618.2152 s
agent0:                 episode reward: -0.3045,                 loss: 0.2140
agent1:                 episode reward: 0.3045,                 loss: nan
Episode: 35281/101000 (34.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9027s / 7625.1180 s
agent0:                 episode reward: -0.6446,                 loss: 0.2143
agent1:                 episode reward: 0.6446,                 loss: nan
Episode: 35301/101000 (34.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1708s / 7634.2888 s
agent0:                 episode reward: -0.3629,                 loss: 0.2081
agent1:                 episode reward: 0.3629,                 loss: nan
Episode: 35321/101000 (34.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5482s / 7643.8369 s
agent0:                 episode reward: -0.7158,                 loss: 0.2015
agent1:                 episode reward: 0.7158,                 loss: 0.1865
Score delta: 1.6647797106282483, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/34878_0.
Episode: 35341/101000 (34.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4209s / 7650.2578 s
agent0:                 episode reward: -0.4363,                 loss: nan
agent1:                 episode reward: 0.4363,                 loss: 0.1862
Episode: 35361/101000 (35.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8937s / 7657.1516 s
agent0:                 episode reward: -0.8006,                 loss: nan
agent1:                 episode reward: 0.8006,                 loss: 0.1862
Episode: 35381/101000 (35.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6450s / 7663.7966 s
agent0:                 episode reward: -0.2334,                 loss: 0.2425
agent1:                 episode reward: 0.2334,                 loss: 0.1845
Score delta: 2.530623859199629, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/34936_1.
Episode: 35401/101000 (35.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9281s / 7671.7247 s
agent0:                 episode reward: 0.1702,                 loss: 0.2416
agent1:                 episode reward: -0.1702,                 loss: nan
Episode: 35421/101000 (35.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1301s / 7679.8547 s
agent0:                 episode reward: -0.4633,                 loss: 0.2446
agent1:                 episode reward: 0.4633,                 loss: nan
Episode: 35441/101000 (35.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3975s / 7690.2522 s
agent0:                 episode reward: 0.4413,                 loss: 0.2443
agent1:                 episode reward: -0.4413,                 loss: 0.1833
Score delta: 1.515415026691581, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/35005_0.
Episode: 35461/101000 (35.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8884s / 7697.1406 s
agent0:                 episode reward: -0.4071,                 loss: nan
agent1:                 episode reward: 0.4071,                 loss: 0.1691
Episode: 35481/101000 (35.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1943s / 7706.3348 s
agent0:                 episode reward: -0.3875,                 loss: nan
agent1:                 episode reward: 0.3875,                 loss: 0.1644
Episode: 35501/101000 (35.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2648s / 7715.5996 s
agent0:                 episode reward: -0.6282,                 loss: 0.2825
agent1:                 episode reward: 0.6282,                 loss: 0.1641
Score delta: 1.640070582716529, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/35068_1.
Episode: 35521/101000 (35.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6675s / 7724.2671 s
agent0:                 episode reward: -0.5553,                 loss: 0.2755
agent1:                 episode reward: 0.5553,                 loss: nan
Episode: 35541/101000 (35.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3326s / 7730.5997 s
agent0:                 episode reward: -0.2451,                 loss: 0.2754
agent1:                 episode reward: 0.2451,                 loss: nan
Episode: 35561/101000 (35.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3463s / 7736.9460 s
agent0:                 episode reward: 0.0906,                 loss: 0.2721
agent1:                 episode reward: -0.0906,                 loss: nan
Episode: 35581/101000 (35.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1878s / 7744.1338 s
agent0:                 episode reward: -0.2391,                 loss: 0.2757
agent1:                 episode reward: 0.2391,                 loss: nan
Episode: 35601/101000 (35.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1206s / 7750.2544 s
agent0:                 episode reward: -0.0311,                 loss: 0.2732
agent1:                 episode reward: 0.0311,                 loss: nan
Episode: 35621/101000 (35.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3223s / 7757.5766 s
agent0:                 episode reward: 0.0127,                 loss: 0.2748
agent1:                 episode reward: -0.0127,                 loss: nan
Episode: 35641/101000 (35.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2446s / 7764.8213 s
agent0:                 episode reward: -0.2057,                 loss: 0.2754
agent1:                 episode reward: 0.2057,                 loss: nan
Episode: 35661/101000 (35.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9848s / 7772.8061 s
agent0:                 episode reward: 0.1854,                 loss: 0.2739
agent1:                 episode reward: -0.1854,                 loss: nan
Episode: 35681/101000 (35.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6885s / 7779.4946 s
agent0:                 episode reward: -0.2434,                 loss: 0.2747
agent1:                 episode reward: 0.2434,                 loss: nan
Episode: 35701/101000 (35.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7307s / 7786.2253 s
agent0:                 episode reward: -0.1751,                 loss: 0.2728
agent1:                 episode reward: 0.1751,                 loss: nan
Episode: 35721/101000 (35.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0182s / 7793.2435 s
agent0:                 episode reward: -0.1396,                 loss: 0.2716
agent1:                 episode reward: 0.1396,                 loss: nan
Episode: 35741/101000 (35.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5267s / 7799.7703 s
agent0:                 episode reward: 0.0677,                 loss: 0.2739
agent1:                 episode reward: -0.0677,                 loss: nan
Episode: 35761/101000 (35.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3104s / 7809.0807 s
agent0:                 episode reward: -0.3408,                 loss: 0.2268
agent1:                 episode reward: 0.3408,                 loss: nan
Episode: 35781/101000 (35.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2709s / 7817.3516 s
agent0:                 episode reward: -0.5287,                 loss: 0.2098
agent1:                 episode reward: 0.5287,                 loss: nan
Episode: 35801/101000 (35.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5783s / 7824.9299 s
agent0:                 episode reward: -0.0379,                 loss: 0.2083
agent1:                 episode reward: 0.0379,                 loss: nan
Episode: 35821/101000 (35.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9687s / 7831.8985 s
agent0:                 episode reward: -0.4073,                 loss: 0.2094
agent1:                 episode reward: 0.4073,                 loss: 0.1881
Score delta: 1.5282941872891336, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/35383_0.
Episode: 35841/101000 (35.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6456s / 7838.5441 s
agent0:                 episode reward: -0.4574,                 loss: nan
agent1:                 episode reward: 0.4574,                 loss: 0.1825
Episode: 35861/101000 (35.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9108s / 7846.4550 s
agent0:                 episode reward: -0.6419,                 loss: nan
agent1:                 episode reward: 0.6419,                 loss: 0.1812
Episode: 35881/101000 (35.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6212s / 7854.0762 s
agent0:                 episode reward: -0.8310,                 loss: 0.2915
agent1:                 episode reward: 0.8310,                 loss: 0.1824
Score delta: 2.3044949988348775, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/35443_1.
Episode: 35901/101000 (35.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8269s / 7860.9031 s
agent0:                 episode reward: -0.6127,                 loss: 0.2833
agent1:                 episode reward: 0.6127,                 loss: nan
Episode: 35921/101000 (35.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8416s / 7867.7447 s
agent0:                 episode reward: -0.4662,                 loss: 0.2795
agent1:                 episode reward: 0.4662,                 loss: nan
Episode: 35941/101000 (35.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6266s / 7877.3713 s
agent0:                 episode reward: -0.4890,                 loss: 0.2791
agent1:                 episode reward: 0.4890,                 loss: nan
Episode: 35961/101000 (35.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9434s / 7884.3147 s
agent0:                 episode reward: -0.3101,                 loss: 0.2810
agent1:                 episode reward: 0.3101,                 loss: nan
Episode: 35981/101000 (35.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5717s / 7893.8864 s
agent0:                 episode reward: 0.4891,                 loss: 0.2817
agent1:                 episode reward: -0.4891,                 loss: nan
Score delta: 1.903656850286395, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/35555_0.
Episode: 36001/101000 (35.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6063s / 7901.4927 s
agent0:                 episode reward: -0.6939,                 loss: nan
agent1:                 episode reward: 0.6939,                 loss: 0.1846
Episode: 36021/101000 (35.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3222s / 7909.8149 s
agent0:                 episode reward: -0.4832,                 loss: nan
agent1:                 episode reward: 0.4832,                 loss: 0.1860
Episode: 36041/101000 (35.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0089s / 7917.8239 s
agent0:                 episode reward: -0.3149,                 loss: nan
agent1:                 episode reward: 0.3149,                 loss: 0.1847
Episode: 36061/101000 (35.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7165s / 7924.5403 s
agent0:                 episode reward: -0.5031,                 loss: 0.2006
agent1:                 episode reward: 0.5031,                 loss: 0.1814
Score delta: 1.5274985260134606, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/35619_1.
Episode: 36081/101000 (35.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7358s / 7932.2761 s
agent0:                 episode reward: -0.1338,                 loss: 0.2012
agent1:                 episode reward: 0.1338,                 loss: nan
Episode: 36101/101000 (35.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9955s / 7943.2716 s
agent0:                 episode reward: 0.1260,                 loss: 0.2005
agent1:                 episode reward: -0.1260,                 loss: nan
Episode: 36121/101000 (35.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9180s / 7951.1897 s
agent0:                 episode reward: -0.1770,                 loss: 0.2046
agent1:                 episode reward: 0.1770,                 loss: 0.2087
Score delta: 1.8986130321461137, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/35680_0.
Episode: 36141/101000 (35.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1401s / 7957.3297 s
agent0:                 episode reward: -0.3346,                 loss: nan
agent1:                 episode reward: 0.3346,                 loss: 0.2031
Episode: 36161/101000 (35.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6030s / 7962.9327 s
agent0:                 episode reward: -0.4493,                 loss: nan
agent1:                 episode reward: 0.4493,                 loss: 0.2057
Episode: 36181/101000 (35.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7594s / 7968.6922 s
agent0:                 episode reward: -0.3975,                 loss: nan
agent1:                 episode reward: 0.3975,                 loss: 0.2041
Episode: 36201/101000 (35.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4801s / 7976.1723 s
agent0:                 episode reward: -0.6870,                 loss: 0.2624
agent1:                 episode reward: 0.6870,                 loss: 0.2035
Score delta: 1.5229299136758958, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/35764_1.
Episode: 36221/101000 (35.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1402s / 7983.3124 s
agent0:                 episode reward: -0.6971,                 loss: 0.2550
agent1:                 episode reward: 0.6971,                 loss: nan
Episode: 36241/101000 (35.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6094s / 7990.9218 s
agent0:                 episode reward: -0.5831,                 loss: 0.2521
agent1:                 episode reward: 0.5831,                 loss: nan
Episode: 36261/101000 (35.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4034s / 7998.3252 s
agent0:                 episode reward: -0.6442,                 loss: 0.2535
agent1:                 episode reward: 0.6442,                 loss: nan
Episode: 36281/101000 (35.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6998s / 8006.0250 s
agent0:                 episode reward: -0.5717,                 loss: 0.2512
agent1:                 episode reward: 0.5717,                 loss: nan
Episode: 36301/101000 (35.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2578s / 8014.2828 s
agent0:                 episode reward: -0.3273,                 loss: 0.2220
agent1:                 episode reward: 0.3273,                 loss: nan
Episode: 36321/101000 (35.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2342s / 8021.5170 s
agent0:                 episode reward: -0.0615,                 loss: 0.2070
agent1:                 episode reward: 0.0615,                 loss: nan
Episode: 36341/101000 (35.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0768s / 8031.5938 s
agent0:                 episode reward: -0.2863,                 loss: 0.2064
agent1:                 episode reward: 0.2863,                 loss: nan
Episode: 36361/101000 (36.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5751s / 8042.1689 s
agent0:                 episode reward: 0.0871,                 loss: 0.2066
agent1:                 episode reward: -0.0871,                 loss: nan
Episode: 36381/101000 (36.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 11.0664s / 8053.2354 s
agent0:                 episode reward: -0.4201,                 loss: 0.2063
agent1:                 episode reward: 0.4201,                 loss: nan
Episode: 36401/101000 (36.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7462s / 8060.9815 s
agent0:                 episode reward: -0.5113,                 loss: 0.2076
agent1:                 episode reward: 0.5113,                 loss: nan
Episode: 36421/101000 (36.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0556s / 8070.0371 s
agent0:                 episode reward: -0.0762,                 loss: 0.2039
agent1:                 episode reward: 0.0762,                 loss: nan
Episode: 36441/101000 (36.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7612s / 8076.7983 s
agent0:                 episode reward: 0.3148,                 loss: 0.2063
agent1:                 episode reward: -0.3148,                 loss: nan
Episode: 36461/101000 (36.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1052s / 8081.9035 s
agent0:                 episode reward: 0.3768,                 loss: 0.2061
agent1:                 episode reward: -0.3768,                 loss: 0.1640
Score delta: 1.5358400892232158, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/36026_0.
Episode: 36481/101000 (36.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3154s / 8089.2189 s
agent0:                 episode reward: -0.4786,                 loss: nan
agent1:                 episode reward: 0.4786,                 loss: 0.1644
Episode: 36501/101000 (36.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4821s / 8097.7010 s
agent0:                 episode reward: -0.1014,                 loss: nan
agent1:                 episode reward: 0.1014,                 loss: 0.1634
Episode: 36521/101000 (36.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6567s / 8105.3577 s
agent0:                 episode reward: -0.6159,                 loss: nan
agent1:                 episode reward: 0.6159,                 loss: 0.1621
Score delta: 1.5417245925328538, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/36095_1.
Episode: 36541/101000 (36.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 11.6990s / 8117.0567 s
agent0:                 episode reward: -0.5759,                 loss: 0.2372
agent1:                 episode reward: 0.5759,                 loss: nan
Episode: 36561/101000 (36.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1498s / 8125.2065 s
agent0:                 episode reward: -0.1930,                 loss: 0.2323
agent1:                 episode reward: 0.1930,                 loss: nan
Episode: 36581/101000 (36.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9865s / 8133.1930 s
agent0:                 episode reward: -0.4686,                 loss: 0.2345
agent1:                 episode reward: 0.4686,                 loss: nan
Episode: 36601/101000 (36.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3164s / 8140.5094 s
agent0:                 episode reward: 0.2636,                 loss: 0.2333
agent1:                 episode reward: -0.2636,                 loss: nan
Episode: 36621/101000 (36.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4172s / 8147.9265 s
agent0:                 episode reward: -0.6845,                 loss: 0.2328
agent1:                 episode reward: 0.6845,                 loss: nan
Episode: 36641/101000 (36.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9711s / 8154.8976 s
agent0:                 episode reward: -0.2246,                 loss: 0.2344
agent1:                 episode reward: 0.2246,                 loss: nan
Episode: 36661/101000 (36.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1731s / 8164.0707 s
agent0:                 episode reward: -0.1435,                 loss: 0.2324
agent1:                 episode reward: 0.1435,                 loss: nan
Episode: 36681/101000 (36.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9009s / 8174.9716 s
agent0:                 episode reward: -0.0115,                 loss: 0.2336
agent1:                 episode reward: 0.0115,                 loss: nan
Episode: 36701/101000 (36.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6767s / 8184.6483 s
agent0:                 episode reward: -0.0663,                 loss: 0.2168
agent1:                 episode reward: 0.0663,                 loss: nan
Episode: 36721/101000 (36.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4539s / 8193.1022 s
agent0:                 episode reward: 0.0769,                 loss: 0.2037
agent1:                 episode reward: -0.0769,                 loss: nan
Episode: 36741/101000 (36.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2264s / 8202.3286 s
agent0:                 episode reward: -0.4346,                 loss: 0.2025
agent1:                 episode reward: 0.4346,                 loss: nan
Episode: 36761/101000 (36.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0630s / 8209.3917 s
agent0:                 episode reward: -0.3471,                 loss: 0.2036
agent1:                 episode reward: 0.3471,                 loss: nan
Episode: 36781/101000 (36.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6648s / 8215.0564 s
agent0:                 episode reward: -0.0296,                 loss: 0.2015
agent1:                 episode reward: 0.0296,                 loss: nan
Episode: 36801/101000 (36.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3545s / 8222.4110 s
agent0:                 episode reward: -0.0576,                 loss: 0.2016
agent1:                 episode reward: 0.0576,                 loss: nan
Episode: 36821/101000 (36.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2430s / 8229.6539 s
agent0:                 episode reward: -0.2666,                 loss: 0.2013
agent1:                 episode reward: 0.2666,                 loss: nan
Episode: 36841/101000 (36.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5362s / 8238.1901 s
agent0:                 episode reward: -0.1627,                 loss: 0.2034
agent1:                 episode reward: 0.1627,                 loss: nan
Episode: 36861/101000 (36.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7196s / 8246.9097 s
agent0:                 episode reward: -0.4308,                 loss: 0.2013
agent1:                 episode reward: 0.4308,                 loss: nan
Episode: 36881/101000 (36.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8323s / 8254.7420 s
agent0:                 episode reward: -0.4364,                 loss: 0.2007
agent1:                 episode reward: 0.4364,                 loss: nan
Episode: 36901/101000 (36.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1157s / 8262.8578 s
agent0:                 episode reward: -0.6213,                 loss: 0.2013
agent1:                 episode reward: 0.6213,                 loss: nan
Episode: 36921/101000 (36.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3680s / 8272.2258 s
agent0:                 episode reward: -0.1469,                 loss: 0.2018
agent1:                 episode reward: 0.1469,                 loss: nan
Episode: 36941/101000 (36.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0502s / 8281.2760 s
agent0:                 episode reward: -0.0426,                 loss: 0.2014
agent1:                 episode reward: 0.0426,                 loss: nan
Episode: 36961/101000 (36.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3348s / 8291.6108 s
agent0:                 episode reward: -0.0198,                 loss: 0.1997
agent1:                 episode reward: 0.0198,                 loss: nan
Episode: 36981/101000 (36.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8184s / 8299.4292 s
agent0:                 episode reward: 0.0446,                 loss: 0.2009
agent1:                 episode reward: -0.0446,                 loss: nan
Episode: 37001/101000 (36.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7737s / 8310.2029 s
agent0:                 episode reward: -0.3890,                 loss: 0.2022
agent1:                 episode reward: 0.3890,                 loss: nan
Episode: 37021/101000 (36.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6093s / 8317.8122 s
agent0:                 episode reward: -0.4653,                 loss: 0.1982
agent1:                 episode reward: 0.4653,                 loss: nan
Episode: 37041/101000 (36.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9210s / 8326.7332 s
agent0:                 episode reward: -0.1891,                 loss: 0.1977
agent1:                 episode reward: 0.1891,                 loss: nan
Episode: 37061/101000 (36.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1141s / 8332.8473 s
agent0:                 episode reward: -0.2342,                 loss: 0.1958
agent1:                 episode reward: 0.2342,                 loss: nan
Episode: 37081/101000 (36.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6024s / 8341.4497 s
agent0:                 episode reward: -0.2482,                 loss: 0.1985
agent1:                 episode reward: 0.2482,                 loss: nan
Episode: 37101/101000 (36.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9731s / 8351.4228 s
agent0:                 episode reward: -0.6112,                 loss: 0.1967
agent1:                 episode reward: 0.6112,                 loss: nan
Episode: 37121/101000 (36.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8272s / 8359.2500 s
agent0:                 episode reward: -0.5226,                 loss: 0.1955
agent1:                 episode reward: 0.5226,                 loss: nan
Episode: 37141/101000 (36.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9479s / 8366.1979 s
agent0:                 episode reward: -0.3443,                 loss: 0.1951
agent1:                 episode reward: 0.3443,                 loss: nan
Episode: 37161/101000 (36.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0413s / 8374.2393 s
agent0:                 episode reward: 0.0776,                 loss: 0.1946
agent1:                 episode reward: -0.0776,                 loss: nan
Episode: 37181/101000 (36.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8593s / 8383.0986 s
agent0:                 episode reward: -0.2241,                 loss: 0.1971
agent1:                 episode reward: 0.2241,                 loss: nan
Episode: 37201/101000 (36.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1090s / 8390.2076 s
agent0:                 episode reward: -0.6073,                 loss: 0.1937
agent1:                 episode reward: 0.6073,                 loss: nan
Episode: 37221/101000 (36.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9239s / 8398.1315 s
agent0:                 episode reward: -0.0903,                 loss: 0.1967
agent1:                 episode reward: 0.0903,                 loss: nan
Episode: 37241/101000 (36.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6078s / 8406.7392 s
agent0:                 episode reward: -0.3743,                 loss: 0.1965
agent1:                 episode reward: 0.3743,                 loss: nan
Episode: 37261/101000 (36.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6765s / 8412.4157 s
agent0:                 episode reward: -0.2223,                 loss: 0.1939
agent1:                 episode reward: 0.2223,                 loss: nan
Episode: 37281/101000 (36.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5263s / 8421.9421 s
agent0:                 episode reward: -0.2172,                 loss: 0.1946
agent1:                 episode reward: 0.2172,                 loss: nan
Episode: 37301/101000 (36.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9237s / 8430.8658 s
agent0:                 episode reward: -0.6859,                 loss: 0.1958
agent1:                 episode reward: 0.6859,                 loss: nan
Episode: 37321/101000 (36.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9362s / 8439.8020 s
agent0:                 episode reward: 0.2456,                 loss: 0.1939
agent1:                 episode reward: -0.2456,                 loss: nan
Episode: 37341/101000 (36.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7736s / 8446.5755 s
agent0:                 episode reward: -0.3358,                 loss: 0.1946
agent1:                 episode reward: 0.3358,                 loss: nan
Episode: 37361/101000 (36.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5837s / 8455.1592 s
agent0:                 episode reward: -0.2059,                 loss: 0.1976
agent1:                 episode reward: 0.2059,                 loss: nan
Episode: 37381/101000 (37.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5031s / 8463.6623 s
agent0:                 episode reward: -0.0950,                 loss: 0.2059
agent1:                 episode reward: 0.0950,                 loss: nan
Episode: 37401/101000 (37.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6079s / 8471.2701 s
agent0:                 episode reward: -0.4361,                 loss: 0.2061
agent1:                 episode reward: 0.4361,                 loss: nan
Episode: 37421/101000 (37.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7949s / 8478.0650 s
agent0:                 episode reward: -0.6027,                 loss: 0.2054
agent1:                 episode reward: 0.6027,                 loss: nan
Episode: 37441/101000 (37.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1198s / 8486.1848 s
agent0:                 episode reward: -0.5195,                 loss: 0.2049
agent1:                 episode reward: 0.5195,                 loss: nan
Episode: 37461/101000 (37.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9665s / 8496.1513 s
agent0:                 episode reward: 0.3152,                 loss: 0.2040
agent1:                 episode reward: -0.3152,                 loss: nan
Episode: 37481/101000 (37.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7033s / 8506.8546 s
agent0:                 episode reward: -0.4626,                 loss: 0.2061
agent1:                 episode reward: 0.4626,                 loss: nan
Episode: 37501/101000 (37.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0063s / 8515.8609 s
agent0:                 episode reward: 0.6060,                 loss: 0.2071
agent1:                 episode reward: -0.6060,                 loss: 0.1923
Score delta: 1.8332692548346892, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/37067_0.
Episode: 37521/101000 (37.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2596s / 8523.1205 s
agent0:                 episode reward: -0.5403,                 loss: nan
agent1:                 episode reward: 0.5403,                 loss: 0.1893
Episode: 37541/101000 (37.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1641s / 8531.2846 s
agent0:                 episode reward: -0.3539,                 loss: nan
agent1:                 episode reward: 0.3539,                 loss: 0.1879
Episode: 37561/101000 (37.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2515s / 8536.5361 s
agent0:                 episode reward: -0.2830,                 loss: nan
agent1:                 episode reward: 0.2830,                 loss: 0.1872
Episode: 37581/101000 (37.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5236s / 8545.0598 s
agent0:                 episode reward: -0.3782,                 loss: 0.2869
agent1:                 episode reward: 0.3782,                 loss: 0.1839
Score delta: 1.5313482963513252, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/37142_1.
Episode: 37601/101000 (37.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5098s / 8552.5696 s
agent0:                 episode reward: -0.0115,                 loss: 0.2804
agent1:                 episode reward: 0.0115,                 loss: nan
Episode: 37621/101000 (37.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8351s / 8560.4046 s
agent0:                 episode reward: -0.3081,                 loss: 0.2787
agent1:                 episode reward: 0.3081,                 loss: nan
Episode: 37641/101000 (37.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0721s / 8570.4767 s
agent0:                 episode reward: -0.0722,                 loss: 0.2785
agent1:                 episode reward: 0.0722,                 loss: nan
Episode: 37661/101000 (37.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6150s / 8579.0917 s
agent0:                 episode reward: -0.3005,                 loss: 0.2801
agent1:                 episode reward: 0.3005,                 loss: nan
Episode: 37681/101000 (37.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3185s / 8587.4103 s
agent0:                 episode reward: -0.2781,                 loss: 0.2790
agent1:                 episode reward: 0.2781,                 loss: nan
Episode: 37701/101000 (37.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8929s / 8595.3032 s
agent0:                 episode reward: -0.1711,                 loss: 0.2792
agent1:                 episode reward: 0.1711,                 loss: nan
Episode: 37721/101000 (37.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0843s / 8604.3875 s
agent0:                 episode reward: 0.0128,                 loss: 0.2775
agent1:                 episode reward: -0.0128,                 loss: nan
Episode: 37741/101000 (37.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8292s / 8612.2167 s
agent0:                 episode reward: -0.2222,                 loss: 0.2768
agent1:                 episode reward: 0.2222,                 loss: nan
Episode: 37761/101000 (37.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4078s / 8622.6245 s
agent0:                 episode reward: 0.1919,                 loss: 0.2728
agent1:                 episode reward: -0.1919,                 loss: nan
Episode: 37781/101000 (37.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1212s / 8629.7457 s
agent0:                 episode reward: 0.1953,                 loss: 0.2350
agent1:                 episode reward: -0.1953,                 loss: 0.2070
Score delta: 1.5040163659253765, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/37345_0.
Episode: 37801/101000 (37.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3787s / 8637.1244 s
agent0:                 episode reward: -0.6317,                 loss: nan
agent1:                 episode reward: 0.6317,                 loss: 0.2072
Episode: 37821/101000 (37.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2875s / 8645.4119 s
agent0:                 episode reward: -0.5735,                 loss: nan
agent1:                 episode reward: 0.5735,                 loss: 0.2052
Episode: 37841/101000 (37.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7491s / 8651.1610 s
agent0:                 episode reward: 0.1508,                 loss: nan
agent1:                 episode reward: -0.1508,                 loss: 0.2062
Episode: 37861/101000 (37.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2772s / 8658.4382 s
agent0:                 episode reward: -0.6455,                 loss: 0.2057
agent1:                 episode reward: 0.6455,                 loss: 0.2070
Score delta: 1.5235051962090442, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/37433_1.
Episode: 37881/101000 (37.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2100s / 8667.6483 s
agent0:                 episode reward: 0.0648,                 loss: 0.2013
agent1:                 episode reward: -0.0648,                 loss: nan
Episode: 37901/101000 (37.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9176s / 8675.5659 s
agent0:                 episode reward: -0.1501,                 loss: 0.2030
agent1:                 episode reward: 0.1501,                 loss: nan
Episode: 37921/101000 (37.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0522s / 8683.6181 s
agent0:                 episode reward: -0.6674,                 loss: 0.2026
agent1:                 episode reward: 0.6674,                 loss: nan
Episode: 37941/101000 (37.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9226s / 8692.5407 s
agent0:                 episode reward: 0.2095,                 loss: 0.2017
agent1:                 episode reward: -0.2095,                 loss: nan
Episode: 37961/101000 (37.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3979s / 8700.9386 s
agent0:                 episode reward: -0.2893,                 loss: 0.2034
agent1:                 episode reward: 0.2893,                 loss: nan
Episode: 37981/101000 (37.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3419s / 8708.2805 s
agent0:                 episode reward: -0.0212,                 loss: 0.2010
agent1:                 episode reward: 0.0212,                 loss: nan
Episode: 38001/101000 (37.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7267s / 8718.0072 s
agent0:                 episode reward: -0.4465,                 loss: 0.2023
agent1:                 episode reward: 0.4465,                 loss: nan
Episode: 38021/101000 (37.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6065s / 8726.6137 s
agent0:                 episode reward: -0.0026,                 loss: 0.2048
agent1:                 episode reward: 0.0026,                 loss: nan
Episode: 38041/101000 (37.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9685s / 8734.5822 s
agent0:                 episode reward: -0.1970,                 loss: 0.2049
agent1:                 episode reward: 0.1970,                 loss: nan
Episode: 38061/101000 (37.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4198s / 8744.0020 s
agent0:                 episode reward: 0.2697,                 loss: 0.2033
agent1:                 episode reward: -0.2697,                 loss: nan
Episode: 38081/101000 (37.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1431s / 8752.1451 s
agent0:                 episode reward: 0.0890,                 loss: 0.2041
agent1:                 episode reward: -0.0890,                 loss: nan
Episode: 38101/101000 (37.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9545s / 8760.0996 s
agent0:                 episode reward: -0.4579,                 loss: 0.2020
agent1:                 episode reward: 0.4579,                 loss: nan
Episode: 38121/101000 (37.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7936s / 8766.8932 s
agent0:                 episode reward: -0.1163,                 loss: 0.2006
agent1:                 episode reward: 0.1163,                 loss: nan
Episode: 38141/101000 (37.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5037s / 8774.3969 s
agent0:                 episode reward: -0.0392,                 loss: 0.2026
agent1:                 episode reward: 0.0392,                 loss: nan
Episode: 38161/101000 (37.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7391s / 8782.1360 s
agent0:                 episode reward: 0.1417,                 loss: 0.2008
agent1:                 episode reward: -0.1417,                 loss: nan
Episode: 38181/101000 (37.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7436s / 8791.8796 s
agent0:                 episode reward: -0.0909,                 loss: 0.1996
agent1:                 episode reward: 0.0909,                 loss: nan
Episode: 38201/101000 (37.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3771s / 8800.2567 s
agent0:                 episode reward: 0.0057,                 loss: 0.2041
agent1:                 episode reward: -0.0057,                 loss: nan
Episode: 38221/101000 (37.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7724s / 8810.0291 s
agent0:                 episode reward: -0.0838,                 loss: 0.2010
agent1:                 episode reward: 0.0838,                 loss: nan
Episode: 38241/101000 (37.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8661s / 8817.8952 s
agent0:                 episode reward: -0.0746,                 loss: 0.2005
agent1:                 episode reward: 0.0746,                 loss: nan
Episode: 38261/101000 (37.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3857s / 8827.2810 s
agent0:                 episode reward: 0.1630,                 loss: 0.2003
agent1:                 episode reward: -0.1630,                 loss: nan
Episode: 38281/101000 (37.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6505s / 8835.9314 s
agent0:                 episode reward: 0.0734,                 loss: 0.1985
agent1:                 episode reward: -0.0734,                 loss: nan
Episode: 38301/101000 (37.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0279s / 8844.9593 s
agent0:                 episode reward: -0.3931,                 loss: 0.2037
agent1:                 episode reward: 0.3931,                 loss: nan
Episode: 38321/101000 (37.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5144s / 8853.4738 s
agent0:                 episode reward: -0.5616,                 loss: 0.2022
agent1:                 episode reward: 0.5616,                 loss: nan
Episode: 38341/101000 (37.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4294s / 8861.9032 s
agent0:                 episode reward: -0.4053,                 loss: 0.2020
agent1:                 episode reward: 0.4053,                 loss: nan
Episode: 38361/101000 (37.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9206s / 8869.8238 s
agent0:                 episode reward: 0.2158,                 loss: 0.2024
agent1:                 episode reward: -0.2158,                 loss: nan
Episode: 38381/101000 (38.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4548s / 8879.2786 s
agent0:                 episode reward: -0.0703,                 loss: 0.2029
agent1:                 episode reward: 0.0703,                 loss: nan
Episode: 38401/101000 (38.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4824s / 8886.7611 s
agent0:                 episode reward: -0.1287,                 loss: 0.2028
agent1:                 episode reward: 0.1287,                 loss: nan
Episode: 38421/101000 (38.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6099s / 8892.3710 s
agent0:                 episode reward: -0.1608,                 loss: 0.2031
agent1:                 episode reward: 0.1608,                 loss: nan
Episode: 38441/101000 (38.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8167s / 8896.1877 s
agent0:                 episode reward: 0.1974,                 loss: 0.2039
agent1:                 episode reward: -0.1974,                 loss: nan
Episode: 38461/101000 (38.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2147s / 8901.4025 s
agent0:                 episode reward: 0.2414,                 loss: 0.2051
agent1:                 episode reward: -0.2414,                 loss: 0.2102
Score delta: 1.8171804809474423, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/38017_0.
Episode: 38481/101000 (38.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9287s / 8911.3312 s
agent0:                 episode reward: -0.1535,                 loss: nan
agent1:                 episode reward: 0.1535,                 loss: 0.2074
Episode: 38501/101000 (38.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1502s / 8921.4814 s
agent0:                 episode reward: -0.2695,                 loss: nan
agent1:                 episode reward: 0.2695,                 loss: 0.2081
Episode: 38521/101000 (38.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3539s / 8927.8353 s
agent0:                 episode reward: -0.3937,                 loss: nan
agent1:                 episode reward: 0.3937,                 loss: 0.2056
Episode: 38541/101000 (38.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6608s / 8935.4960 s
agent0:                 episode reward: -0.2164,                 loss: 0.2077
agent1:                 episode reward: 0.2164,                 loss: 0.2065
Score delta: 1.582757711421162, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/38109_1.
Episode: 38561/101000 (38.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3266s / 8943.8226 s
agent0:                 episode reward: -0.4328,                 loss: 0.2048
agent1:                 episode reward: 0.4328,                 loss: nan
Episode: 38581/101000 (38.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0460s / 8952.8687 s
agent0:                 episode reward: 0.1807,                 loss: 0.2015
agent1:                 episode reward: -0.1807,                 loss: nan
Episode: 38601/101000 (38.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0843s / 8962.9530 s
agent0:                 episode reward: -0.1127,                 loss: 0.2011
agent1:                 episode reward: 0.1127,                 loss: nan
Episode: 38621/101000 (38.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1826s / 8971.1356 s
agent0:                 episode reward: -0.1746,                 loss: 0.2072
agent1:                 episode reward: 0.1746,                 loss: nan
Episode: 38641/101000 (38.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2791s / 8978.4147 s
agent0:                 episode reward: -0.0142,                 loss: 0.2063
agent1:                 episode reward: 0.0142,                 loss: nan
Episode: 38661/101000 (38.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 11.0900s / 8989.5048 s
agent0:                 episode reward: 0.1493,                 loss: 0.2051
agent1:                 episode reward: -0.1493,                 loss: nan
Episode: 38681/101000 (38.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8191s / 8998.3238 s
agent0:                 episode reward: 0.0001,                 loss: 0.2050
agent1:                 episode reward: -0.0001,                 loss: nan
Episode: 38701/101000 (38.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4016s / 9008.7255 s
agent0:                 episode reward: -0.1488,                 loss: 0.2081
agent1:                 episode reward: 0.1488,                 loss: nan
Episode: 38721/101000 (38.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8647s / 9015.5902 s
agent0:                 episode reward: -0.5061,                 loss: 0.2051
agent1:                 episode reward: 0.5061,                 loss: nan
Episode: 38741/101000 (38.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7497s / 9023.3398 s
agent0:                 episode reward: -0.1966,                 loss: 0.2063
agent1:                 episode reward: 0.1966,                 loss: nan
Episode: 38761/101000 (38.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2838s / 9030.6236 s
agent0:                 episode reward: 0.0496,                 loss: 0.2065
agent1:                 episode reward: -0.0496,                 loss: nan
Episode: 38781/101000 (38.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9484s / 9038.5720 s
agent0:                 episode reward: 0.1002,                 loss: 0.2058
agent1:                 episode reward: -0.1002,                 loss: nan
Episode: 38801/101000 (38.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1299s / 9047.7019 s
agent0:                 episode reward: 0.2885,                 loss: 0.2054
agent1:                 episode reward: -0.2885,                 loss: nan
Episode: 38821/101000 (38.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9183s / 9054.6202 s
agent0:                 episode reward: 0.0935,                 loss: 0.2032
agent1:                 episode reward: -0.0935,                 loss: nan
Episode: 38841/101000 (38.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5465s / 9061.1667 s
agent0:                 episode reward: -0.0371,                 loss: 0.2035
agent1:                 episode reward: 0.0371,                 loss: nan
Episode: 38861/101000 (38.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0101s / 9070.1768 s
agent0:                 episode reward: -0.4759,                 loss: 0.2058
agent1:                 episode reward: 0.4759,                 loss: nan
Episode: 38881/101000 (38.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5806s / 9077.7574 s
agent0:                 episode reward: 0.0079,                 loss: 0.2048
agent1:                 episode reward: -0.0079,                 loss: nan
Episode: 38901/101000 (38.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3040s / 9087.0615 s
agent0:                 episode reward: 0.1709,                 loss: 0.2075
agent1:                 episode reward: -0.1709,                 loss: nan
Episode: 38921/101000 (38.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0565s / 9095.1180 s
agent0:                 episode reward: -0.0798,                 loss: 0.2046
agent1:                 episode reward: 0.0798,                 loss: nan
Episode: 38941/101000 (38.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4293s / 9104.5473 s
agent0:                 episode reward: -0.5377,                 loss: 0.2064
agent1:                 episode reward: 0.5377,                 loss: nan
Episode: 38961/101000 (38.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0654s / 9114.6127 s
agent0:                 episode reward: -0.1938,                 loss: 0.2032
agent1:                 episode reward: 0.1938,                 loss: nan
Episode: 38981/101000 (38.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1853s / 9119.7980 s
agent0:                 episode reward: -0.4101,                 loss: 0.2014
agent1:                 episode reward: 0.4101,                 loss: nan
Episode: 39001/101000 (38.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4914s / 9129.2895 s
agent0:                 episode reward: -0.2669,                 loss: 0.2030
agent1:                 episode reward: 0.2669,                 loss: nan
Episode: 39021/101000 (38.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3629s / 9137.6523 s
agent0:                 episode reward: -0.2297,                 loss: 0.2032
agent1:                 episode reward: 0.2297,                 loss: nan
Episode: 39041/101000 (38.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1956s / 9144.8479 s
agent0:                 episode reward: 0.3704,                 loss: 0.2031
agent1:                 episode reward: -0.3704,                 loss: nan
Episode: 39061/101000 (38.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1317s / 9152.9796 s
agent0:                 episode reward: -0.1107,                 loss: 0.2026
agent1:                 episode reward: 0.1107,                 loss: nan
Episode: 39081/101000 (38.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0163s / 9159.9959 s
agent0:                 episode reward: 0.2077,                 loss: 0.2035
agent1:                 episode reward: -0.2077,                 loss: 0.1802
Score delta: 1.836727444503565, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/38654_0.
Episode: 39101/101000 (38.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7203s / 9167.7162 s
agent0:                 episode reward: -0.3498,                 loss: nan
agent1:                 episode reward: 0.3498,                 loss: 0.1877
Episode: 39121/101000 (38.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3699s / 9176.0861 s
agent0:                 episode reward: -0.0155,                 loss: nan
agent1:                 episode reward: 0.0155,                 loss: 0.1878
Episode: 39141/101000 (38.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6786s / 9185.7647 s
agent0:                 episode reward: -0.4007,                 loss: nan
agent1:                 episode reward: 0.4007,                 loss: 0.1890
Episode: 39161/101000 (38.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7279s / 9194.4926 s
agent0:                 episode reward: -0.2574,                 loss: nan
agent1:                 episode reward: 0.2574,                 loss: 0.1879
Episode: 39181/101000 (38.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5820s / 9201.0747 s
agent0:                 episode reward: 0.0361,                 loss: 0.2424
agent1:                 episode reward: -0.0361,                 loss: 0.1876
Score delta: 1.5840478660659207, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/38753_1.
Episode: 39201/101000 (38.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9502s / 9211.0248 s
agent0:                 episode reward: -0.1909,                 loss: 0.2361
agent1:                 episode reward: 0.1909,                 loss: nan
Episode: 39221/101000 (38.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5484s / 9221.5732 s
agent0:                 episode reward: 0.1343,                 loss: 0.2340
agent1:                 episode reward: -0.1343,                 loss: nan
Episode: 39241/101000 (38.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4502s / 9229.0234 s
agent0:                 episode reward: 0.2370,                 loss: 0.2317
agent1:                 episode reward: -0.2370,                 loss: nan
Episode: 39261/101000 (38.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8715s / 9237.8949 s
agent0:                 episode reward: 0.2406,                 loss: 0.2305
agent1:                 episode reward: -0.2406,                 loss: nan
Episode: 39281/101000 (38.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9087s / 9248.8036 s
agent0:                 episode reward: 0.2249,                 loss: 0.2306
agent1:                 episode reward: -0.2249,                 loss: nan
Episode: 39301/101000 (38.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9979s / 9257.8015 s
agent0:                 episode reward: -0.2856,                 loss: 0.2287
agent1:                 episode reward: 0.2856,                 loss: 0.1624
Score delta: 1.5170338451671712, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/38856_0.
Episode: 39321/101000 (38.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0043s / 9264.8058 s
agent0:                 episode reward: -0.9150,                 loss: nan
agent1:                 episode reward: 0.9150,                 loss: 0.1644
Episode: 39341/101000 (38.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6185s / 9273.4244 s
agent0:                 episode reward: -0.3500,                 loss: nan
agent1:                 episode reward: 0.3500,                 loss: 0.1746
Episode: 39361/101000 (38.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3854s / 9280.8097 s
agent0:                 episode reward: -0.7590,                 loss: 0.2881
agent1:                 episode reward: 0.7590,                 loss: 0.1848
Score delta: 2.0031267987223345, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/38924_1.
Episode: 39381/101000 (38.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4353s / 9289.2450 s
agent0:                 episode reward: -0.0365,                 loss: 0.2753
agent1:                 episode reward: 0.0365,                 loss: nan
Episode: 39401/101000 (39.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8613s / 9297.1064 s
agent0:                 episode reward: 0.1717,                 loss: 0.2761
agent1:                 episode reward: -0.1717,                 loss: nan
Episode: 39421/101000 (39.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1549s / 9306.2612 s
agent0:                 episode reward: 0.3863,                 loss: 0.2781
agent1:                 episode reward: -0.3863,                 loss: 0.1833
Score delta: 1.8485885325186566, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/38992_0.
Episode: 39441/101000 (39.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3096s / 9313.5708 s
agent0:                 episode reward: -0.7047,                 loss: nan
agent1:                 episode reward: 0.7047,                 loss: 0.1822
Episode: 39461/101000 (39.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1247s / 9322.6955 s
agent0:                 episode reward: -0.2781,                 loss: nan
agent1:                 episode reward: 0.2781,                 loss: 0.1808
Episode: 39481/101000 (39.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9904s / 9331.6858 s
agent0:                 episode reward: -0.2018,                 loss: nan
agent1:                 episode reward: 0.2018,                 loss: 0.1831
Episode: 39501/101000 (39.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5380s / 9339.2238 s
agent0:                 episode reward: -0.2502,                 loss: 0.2102
agent1:                 episode reward: 0.2502,                 loss: 0.1817
Score delta: 1.5909607869659426, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/39061_1.
Episode: 39521/101000 (39.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8807s / 9349.1045 s
agent0:                 episode reward: 0.2352,                 loss: 0.2033
agent1:                 episode reward: -0.2352,                 loss: nan
Episode: 39541/101000 (39.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7998s / 9359.9043 s
agent0:                 episode reward: 0.0566,                 loss: 0.1953
agent1:                 episode reward: -0.0566,                 loss: nan
Episode: 39561/101000 (39.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3376s / 9370.2419 s
agent0:                 episode reward: -0.1803,                 loss: 0.1944
agent1:                 episode reward: 0.1803,                 loss: nan
Episode: 39581/101000 (39.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1828s / 9379.4247 s
agent0:                 episode reward: -0.0437,                 loss: 0.1948
agent1:                 episode reward: 0.0437,                 loss: nan
Episode: 39601/101000 (39.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4490s / 9387.8737 s
agent0:                 episode reward: 0.2388,                 loss: 0.1951
agent1:                 episode reward: -0.2388,                 loss: nan
Episode: 39621/101000 (39.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9312s / 9394.8048 s
agent0:                 episode reward: 0.1331,                 loss: 0.1918
agent1:                 episode reward: -0.1331,                 loss: nan
Episode: 39641/101000 (39.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3750s / 9405.1799 s
agent0:                 episode reward: -0.0627,                 loss: 0.1936
agent1:                 episode reward: 0.0627,                 loss: nan
Episode: 39661/101000 (39.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0345s / 9414.2143 s
agent0:                 episode reward: -0.3685,                 loss: 0.1944
agent1:                 episode reward: 0.3685,                 loss: nan
Episode: 39681/101000 (39.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5989s / 9423.8132 s
agent0:                 episode reward: -0.5054,                 loss: 0.1941
agent1:                 episode reward: 0.5054,                 loss: nan
Episode: 39701/101000 (39.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9873s / 9432.8005 s
agent0:                 episode reward: 0.1360,                 loss: 0.1929
agent1:                 episode reward: -0.1360,                 loss: nan
Episode: 39721/101000 (39.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6682s / 9441.4687 s
agent0:                 episode reward: 0.3307,                 loss: 0.1933
agent1:                 episode reward: -0.3307,                 loss: nan
Episode: 39741/101000 (39.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 11.4326s / 9452.9013 s
agent0:                 episode reward: 0.2376,                 loss: 0.1941
agent1:                 episode reward: -0.2376,                 loss: 0.1970
Score delta: 1.7978652471903174, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/39301_0.
Episode: 39761/101000 (39.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1478s / 9460.0492 s
agent0:                 episode reward: -0.4002,                 loss: nan
agent1:                 episode reward: 0.4002,                 loss: 0.1934
Episode: 39781/101000 (39.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4427s / 9467.4919 s
agent0:                 episode reward: -0.0113,                 loss: nan
agent1:                 episode reward: 0.0113,                 loss: 0.1928
Episode: 39801/101000 (39.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0339s / 9473.5258 s
agent0:                 episode reward: -0.2977,                 loss: nan
agent1:                 episode reward: 0.2977,                 loss: 0.1916
Episode: 39821/101000 (39.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9550s / 9482.4808 s
agent0:                 episode reward: -0.7333,                 loss: 0.2088
agent1:                 episode reward: 0.7333,                 loss: 0.1919
Score delta: 1.8545193612762532, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/39391_1.
Episode: 39841/101000 (39.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7103s / 9487.1911 s
agent0:                 episode reward: -0.8208,                 loss: 0.2050
agent1:                 episode reward: 0.8208,                 loss: nan
Episode: 39861/101000 (39.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0318s / 9494.2228 s
agent0:                 episode reward: -0.2825,                 loss: 0.2056
agent1:                 episode reward: 0.2825,                 loss: nan
Episode: 39881/101000 (39.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8166s / 9505.0394 s
agent0:                 episode reward: -0.7055,                 loss: 0.2056
agent1:                 episode reward: 0.7055,                 loss: nan
Episode: 39901/101000 (39.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5453s / 9513.5847 s
agent0:                 episode reward: -0.7009,                 loss: 0.2036
agent1:                 episode reward: 0.7009,                 loss: nan
Episode: 39921/101000 (39.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6080s / 9523.1927 s
agent0:                 episode reward: -0.1291,                 loss: 0.2044
agent1:                 episode reward: 0.1291,                 loss: nan
Episode: 39941/101000 (39.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7283s / 9531.9210 s
agent0:                 episode reward: -0.3071,                 loss: 0.2053
agent1:                 episode reward: 0.3071,                 loss: nan
Episode: 39961/101000 (39.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0329s / 9540.9540 s
agent0:                 episode reward: -0.1983,                 loss: 0.2104
agent1:                 episode reward: 0.1983,                 loss: nan
Episode: 39981/101000 (39.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5024s / 9551.4564 s
agent0:                 episode reward: -0.3853,                 loss: 0.2089
agent1:                 episode reward: 0.3853,                 loss: nan
Episode: 40001/101000 (39.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9597s / 9560.4161 s
agent0:                 episode reward: -0.4162,                 loss: 0.2075
agent1:                 episode reward: 0.4162,                 loss: nan
Episode: 40021/101000 (39.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7337s / 9571.1498 s
agent0:                 episode reward: 0.0233,                 loss: 0.2096
agent1:                 episode reward: -0.0233,                 loss: nan
Episode: 40041/101000 (39.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3749s / 9581.5247 s
agent0:                 episode reward: -0.6600,                 loss: 0.2089
agent1:                 episode reward: 0.6600,                 loss: nan
Episode: 40061/101000 (39.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7135s / 9592.2382 s
agent0:                 episode reward: -0.0432,                 loss: 0.2085
agent1:                 episode reward: 0.0432,                 loss: 0.2136
Score delta: 1.632498487571861, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/39626_0.
Episode: 40081/101000 (39.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9891s / 9598.2273 s
agent0:                 episode reward: -0.7496,                 loss: nan
agent1:                 episode reward: 0.7496,                 loss: 0.2083
Episode: 40101/101000 (39.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4481s / 9604.6754 s
agent0:                 episode reward: -0.4318,                 loss: nan
agent1:                 episode reward: 0.4318,                 loss: 0.2076
Episode: 40121/101000 (39.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0244s / 9613.6998 s
agent0:                 episode reward: -0.5296,                 loss: nan
agent1:                 episode reward: 0.5296,                 loss: 0.2070
Episode: 40141/101000 (39.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1801s / 9621.8799 s
agent0:                 episode reward: 0.3009,                 loss: 0.2056
agent1:                 episode reward: -0.3009,                 loss: 0.2137
Score delta: 1.7804393664901834, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/39697_1.
Episode: 40161/101000 (39.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6408s / 9629.5207 s
agent0:                 episode reward: 0.0072,                 loss: 0.2050
agent1:                 episode reward: -0.0072,                 loss: nan
Episode: 40181/101000 (39.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9993s / 9638.5199 s
agent0:                 episode reward: -0.1914,                 loss: 0.2056
agent1:                 episode reward: 0.1914,                 loss: nan
Episode: 40201/101000 (39.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1692s / 9648.6892 s
agent0:                 episode reward: -0.0360,                 loss: 0.2073
agent1:                 episode reward: 0.0360,                 loss: nan
Episode: 40221/101000 (39.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4455s / 9657.1347 s
agent0:                 episode reward: 0.1069,                 loss: 0.2049
agent1:                 episode reward: -0.1069,                 loss: 0.1923
Score delta: 1.6239039964106463, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/39789_0.
Episode: 40241/101000 (39.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0457s / 9666.1803 s
agent0:                 episode reward: -0.7229,                 loss: nan
agent1:                 episode reward: 0.7229,                 loss: 0.1887
Episode: 40261/101000 (39.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0820s / 9674.2624 s
agent0:                 episode reward: -0.3130,                 loss: nan
agent1:                 episode reward: 0.3130,                 loss: 0.1889
Episode: 40281/101000 (39.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3232s / 9681.5856 s
agent0:                 episode reward: -0.6546,                 loss: nan
agent1:                 episode reward: 0.6546,                 loss: 0.1884
Episode: 40301/101000 (39.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5791s / 9690.1648 s
agent0:                 episode reward: -0.5519,                 loss: nan
agent1:                 episode reward: 0.5519,                 loss: 0.1870
Episode: 40321/101000 (39.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9797s / 9697.1444 s
agent0:                 episode reward: -0.3721,                 loss: nan
agent1:                 episode reward: 0.3721,                 loss: 0.1811
Episode: 40341/101000 (39.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1614s / 9705.3059 s
agent0:                 episode reward: -0.3529,                 loss: 0.2115
agent1:                 episode reward: 0.3529,                 loss: 0.1827
Score delta: 1.5344996936111688, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/39899_1.
Episode: 40361/101000 (39.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1733s / 9713.4792 s
agent0:                 episode reward: -0.1395,                 loss: 0.2068
agent1:                 episode reward: 0.1395,                 loss: nan
Episode: 40381/101000 (39.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1135s / 9721.5927 s
agent0:                 episode reward: -0.1205,                 loss: 0.2078
agent1:                 episode reward: 0.1205,                 loss: nan
Episode: 40401/101000 (40.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2729s / 9730.8655 s
agent0:                 episode reward: 0.2584,                 loss: 0.2080
agent1:                 episode reward: -0.2584,                 loss: nan
Episode: 40421/101000 (40.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1558s / 9741.0213 s
agent0:                 episode reward: 0.1228,                 loss: 0.2050
agent1:                 episode reward: -0.1228,                 loss: nan
Episode: 40441/101000 (40.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8969s / 9748.9182 s
agent0:                 episode reward: -0.1103,                 loss: 0.2052
agent1:                 episode reward: 0.1103,                 loss: nan
Episode: 40461/101000 (40.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1614s / 9757.0796 s
agent0:                 episode reward: 0.1741,                 loss: 0.2103
agent1:                 episode reward: -0.1741,                 loss: nan
Episode: 40481/101000 (40.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1403s / 9767.2199 s
agent0:                 episode reward: 0.0146,                 loss: 0.2115
agent1:                 episode reward: -0.0146,                 loss: nan
Episode: 40501/101000 (40.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9380s / 9778.1579 s
agent0:                 episode reward: -0.3021,                 loss: 0.2127
agent1:                 episode reward: 0.3021,                 loss: nan
Episode: 40521/101000 (40.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9573s / 9787.1151 s
agent0:                 episode reward: 0.1903,                 loss: 0.2140
agent1:                 episode reward: -0.1903,                 loss: nan
Episode: 40541/101000 (40.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3673s / 9796.4825 s
agent0:                 episode reward: -0.1398,                 loss: 0.2128
agent1:                 episode reward: 0.1398,                 loss: nan
Episode: 40561/101000 (40.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2616s / 9805.7440 s
agent0:                 episode reward: -0.3569,                 loss: 0.2145
agent1:                 episode reward: 0.3569,                 loss: nan
Episode: 40581/101000 (40.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0548s / 9815.7988 s
agent0:                 episode reward: -0.1217,                 loss: 0.2134
agent1:                 episode reward: 0.1217,                 loss: nan
Episode: 40601/101000 (40.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3385s / 9824.1373 s
agent0:                 episode reward: 0.1351,                 loss: 0.2101
agent1:                 episode reward: -0.1351,                 loss: nan
Episode: 40621/101000 (40.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1157s / 9833.2530 s
agent0:                 episode reward: -0.3949,                 loss: 0.2122
agent1:                 episode reward: 0.3949,                 loss: nan
Episode: 40641/101000 (40.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3357s / 9841.5887 s
agent0:                 episode reward: -0.4078,                 loss: 0.2137
agent1:                 episode reward: 0.4078,                 loss: nan
Episode: 40661/101000 (40.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1146s / 9850.7034 s
agent0:                 episode reward: 0.2643,                 loss: 0.2112
agent1:                 episode reward: -0.2643,                 loss: nan
Episode: 40681/101000 (40.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8367s / 9859.5400 s
agent0:                 episode reward: -0.0289,                 loss: 0.2123
agent1:                 episode reward: 0.0289,                 loss: nan
Episode: 40701/101000 (40.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7713s / 9869.3114 s
agent0:                 episode reward: 0.0355,                 loss: 0.2104
agent1:                 episode reward: -0.0355,                 loss: nan
Episode: 40721/101000 (40.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0071s / 9878.3185 s
agent0:                 episode reward: -0.0784,                 loss: 0.2122
agent1:                 episode reward: 0.0784,                 loss: nan
Episode: 40741/101000 (40.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2142s / 9887.5327 s
agent0:                 episode reward: -0.1448,                 loss: 0.2133
agent1:                 episode reward: 0.1448,                 loss: nan
Episode: 40761/101000 (40.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0295s / 9897.5622 s
agent0:                 episode reward: 0.1830,                 loss: 0.2124
agent1:                 episode reward: -0.1830,                 loss: nan
Episode: 40781/101000 (40.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4271s / 9907.9892 s
agent0:                 episode reward: -0.1511,                 loss: 0.2106
agent1:                 episode reward: 0.1511,                 loss: nan
Episode: 40801/101000 (40.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5976s / 9917.5869 s
agent0:                 episode reward: -0.4384,                 loss: 0.2120
agent1:                 episode reward: 0.4384,                 loss: nan
Episode: 40821/101000 (40.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1834s / 9924.7703 s
agent0:                 episode reward: 0.1801,                 loss: 0.2129
agent1:                 episode reward: -0.1801,                 loss: nan
Episode: 40841/101000 (40.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9007s / 9933.6710 s
agent0:                 episode reward: 0.3953,                 loss: 0.2115
agent1:                 episode reward: -0.3953,                 loss: nan
Episode: 40861/101000 (40.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9224s / 9944.5934 s
agent0:                 episode reward: 0.2396,                 loss: 0.2128
agent1:                 episode reward: -0.2396,                 loss: nan
Episode: 40881/101000 (40.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8177s / 9953.4111 s
agent0:                 episode reward: -0.0303,                 loss: 0.2098
agent1:                 episode reward: 0.0303,                 loss: 0.1942
Score delta: 1.6724897521866033, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/40454_0.
Episode: 40901/101000 (40.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5098s / 9959.9209 s
agent0:                 episode reward: -0.2568,                 loss: nan
agent1:                 episode reward: 0.2568,                 loss: 0.1918
Episode: 40921/101000 (40.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1797s / 9967.1007 s
agent0:                 episode reward: -0.4468,                 loss: nan
agent1:                 episode reward: 0.4468,                 loss: 0.1922
Episode: 40941/101000 (40.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1796s / 9973.2802 s
agent0:                 episode reward: -0.2753,                 loss: nan
agent1:                 episode reward: 0.2753,                 loss: 0.1901
Episode: 40961/101000 (40.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0310s / 9979.3112 s
agent0:                 episode reward: -0.7836,                 loss: 0.3069
agent1:                 episode reward: 0.7836,                 loss: 0.1900
Score delta: 2.618806024964446, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/40527_1.
Episode: 40981/101000 (40.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8820s / 9986.1932 s
agent0:                 episode reward: -0.2978,                 loss: 0.2980
agent1:                 episode reward: 0.2978,                 loss: nan
Episode: 41001/101000 (40.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7853s / 9991.9785 s
agent0:                 episode reward: -0.4134,                 loss: 0.2895
agent1:                 episode reward: 0.4134,                 loss: nan
Episode: 41021/101000 (40.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5709s / 10001.5494 s
agent0:                 episode reward: -0.1135,                 loss: 0.2927
agent1:                 episode reward: 0.1135,                 loss: nan
Episode: 41041/101000 (40.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8538s / 10010.4032 s
agent0:                 episode reward: -0.4820,                 loss: 0.2908
agent1:                 episode reward: 0.4820,                 loss: nan
Episode: 41061/101000 (40.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1692s / 10019.5724 s
agent0:                 episode reward: -0.0316,                 loss: 0.2929
agent1:                 episode reward: 0.0316,                 loss: nan
Episode: 41081/101000 (40.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5648s / 10028.1371 s
agent0:                 episode reward: 0.0453,                 loss: 0.2868
agent1:                 episode reward: -0.0453,                 loss: nan
Episode: 41101/101000 (40.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4529s / 10036.5901 s
agent0:                 episode reward: -0.0183,                 loss: 0.2866
agent1:                 episode reward: 0.0183,                 loss: nan
Episode: 41121/101000 (40.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8537s / 10044.4437 s
agent0:                 episode reward: -0.4840,                 loss: 0.2876
agent1:                 episode reward: 0.4840,                 loss: nan
Episode: 41141/101000 (40.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6611s / 10051.1048 s
agent0:                 episode reward: 0.1344,                 loss: 0.2879
agent1:                 episode reward: -0.1344,                 loss: nan
Episode: 41161/101000 (40.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5319s / 10060.6367 s
agent0:                 episode reward: -0.2803,                 loss: 0.2849
agent1:                 episode reward: 0.2803,                 loss: nan
Episode: 41181/101000 (40.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5174s / 10070.1542 s
agent0:                 episode reward: -0.2703,                 loss: 0.2872
agent1:                 episode reward: 0.2703,                 loss: nan
Episode: 41201/101000 (40.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8441s / 10073.9983 s
agent0:                 episode reward: -0.4672,                 loss: 0.2522
agent1:                 episode reward: 0.4672,                 loss: nan
Episode: 41221/101000 (40.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7521s / 10083.7505 s
agent0:                 episode reward: -0.2650,                 loss: 0.2137
agent1:                 episode reward: 0.2650,                 loss: nan
Episode: 41241/101000 (40.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9101s / 10093.6606 s
agent0:                 episode reward: 0.0512,                 loss: 0.2105
agent1:                 episode reward: -0.0512,                 loss: nan
Episode: 41261/101000 (40.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2114s / 10101.8719 s
agent0:                 episode reward: -0.1009,                 loss: 0.2105
agent1:                 episode reward: 0.1009,                 loss: nan
Episode: 41281/101000 (40.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0149s / 10108.8869 s
agent0:                 episode reward: -0.4158,                 loss: 0.2122
agent1:                 episode reward: 0.4158,                 loss: nan
Episode: 41301/101000 (40.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3320s / 10117.2189 s
agent0:                 episode reward: 0.2276,                 loss: 0.2105
agent1:                 episode reward: -0.2276,                 loss: nan
Episode: 41321/101000 (40.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2667s / 10125.4856 s
agent0:                 episode reward: 0.0522,                 loss: 0.2121
agent1:                 episode reward: -0.0522,                 loss: nan
Episode: 41341/101000 (40.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8516s / 10135.3371 s
agent0:                 episode reward: 0.3124,                 loss: 0.2122
agent1:                 episode reward: -0.3124,                 loss: nan
Episode: 41361/101000 (40.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9303s / 10144.2674 s
agent0:                 episode reward: -0.2441,                 loss: 0.2123
agent1:                 episode reward: 0.2441,                 loss: nan
Episode: 41381/101000 (40.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1079s / 10152.3753 s
agent0:                 episode reward: 0.3054,                 loss: 0.2130
agent1:                 episode reward: -0.3054,                 loss: 0.2139
Score delta: 1.5597085018651884, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/40944_0.
Episode: 41401/101000 (40.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6675s / 10161.0428 s
agent0:                 episode reward: -0.2446,                 loss: nan
agent1:                 episode reward: 0.2446,                 loss: 0.2092
Episode: 41421/101000 (41.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7000s / 10169.7428 s
agent0:                 episode reward: -0.6449,                 loss: nan
agent1:                 episode reward: 0.6449,                 loss: 0.2074
Episode: 41441/101000 (41.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3750s / 10177.1179 s
agent0:                 episode reward: -0.2469,                 loss: nan
agent1:                 episode reward: 0.2469,                 loss: 0.2065
Episode: 41461/101000 (41.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0349s / 10184.1527 s
agent0:                 episode reward: -0.4548,                 loss: 0.3018
agent1:                 episode reward: 0.4548,                 loss: 0.1997
Score delta: 1.56840786103677, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/41018_1.
Episode: 41481/101000 (41.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3021s / 10191.4549 s
agent0:                 episode reward: -0.3635,                 loss: 0.2969
agent1:                 episode reward: 0.3635,                 loss: nan
Episode: 41501/101000 (41.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7862s / 10201.2411 s
agent0:                 episode reward: -0.1774,                 loss: 0.2948
agent1:                 episode reward: 0.1774,                 loss: nan
Episode: 41521/101000 (41.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8851s / 10210.1262 s
agent0:                 episode reward: -0.3189,                 loss: 0.2922
agent1:                 episode reward: 0.3189,                 loss: nan
Episode: 41541/101000 (41.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4101s / 10220.5364 s
agent0:                 episode reward: -0.1759,                 loss: 0.2930
agent1:                 episode reward: 0.1759,                 loss: nan
Episode: 41561/101000 (41.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5873s / 10230.1237 s
agent0:                 episode reward: -0.0051,                 loss: 0.2938
agent1:                 episode reward: 0.0051,                 loss: nan
Episode: 41581/101000 (41.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6052s / 10240.7289 s
agent0:                 episode reward: -0.2994,                 loss: 0.2921
agent1:                 episode reward: 0.2994,                 loss: nan
Episode: 41601/101000 (41.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4333s / 10248.1622 s
agent0:                 episode reward: -0.4610,                 loss: 0.2817
agent1:                 episode reward: 0.4610,                 loss: nan
Episode: 41621/101000 (41.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1826s / 10257.3448 s
agent0:                 episode reward: -0.4028,                 loss: 0.2007
agent1:                 episode reward: 0.4028,                 loss: nan
Episode: 41641/101000 (41.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7631s / 10263.1080 s
agent0:                 episode reward: -0.0246,                 loss: 0.1984
agent1:                 episode reward: 0.0246,                 loss: nan
Episode: 41661/101000 (41.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9530s / 10274.0609 s
agent0:                 episode reward: 0.4439,                 loss: 0.1964
agent1:                 episode reward: -0.4439,                 loss: nan
Episode: 41681/101000 (41.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1780s / 10282.2389 s
agent0:                 episode reward: -0.4236,                 loss: 0.1947
agent1:                 episode reward: 0.4236,                 loss: nan
Episode: 41701/101000 (41.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3613s / 10291.6002 s
agent0:                 episode reward: 0.0073,                 loss: 0.1946
agent1:                 episode reward: -0.0073,                 loss: nan
Episode: 41721/101000 (41.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4208s / 10301.0210 s
agent0:                 episode reward: 0.1596,                 loss: 0.1964
agent1:                 episode reward: -0.1596,                 loss: nan
Episode: 41741/101000 (41.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0387s / 10309.0597 s
agent0:                 episode reward: -0.1938,                 loss: 0.1970
agent1:                 episode reward: 0.1938,                 loss: nan
Episode: 41761/101000 (41.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1692s / 10317.2289 s
agent0:                 episode reward: -0.2416,                 loss: 0.1969
agent1:                 episode reward: 0.2416,                 loss: nan
Episode: 41781/101000 (41.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9825s / 10327.2113 s
agent0:                 episode reward: 0.1091,                 loss: 0.1966
agent1:                 episode reward: -0.1091,                 loss: 0.1891
Score delta: 1.5706694050754364, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/41349_0.
Episode: 41801/101000 (41.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0334s / 10335.2447 s
agent0:                 episode reward: -0.4444,                 loss: nan
agent1:                 episode reward: 0.4444,                 loss: 0.1880
Episode: 41821/101000 (41.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8123s / 10344.0570 s
agent0:                 episode reward: -0.6210,                 loss: nan
agent1:                 episode reward: 0.6210,                 loss: 0.1893
Episode: 41841/101000 (41.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2683s / 10350.3253 s
agent0:                 episode reward: -0.7114,                 loss: nan
agent1:                 episode reward: 0.7114,                 loss: 0.1852
Episode: 41861/101000 (41.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9993s / 10357.3246 s
agent0:                 episode reward: -0.2137,                 loss: nan
agent1:                 episode reward: 0.2137,                 loss: 0.1882
Episode: 41881/101000 (41.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1423s / 10365.4669 s
agent0:                 episode reward: -0.4140,                 loss: nan
agent1:                 episode reward: 0.4140,                 loss: 0.1876
Episode: 41901/101000 (41.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6025s / 10373.0694 s
agent0:                 episode reward: -0.2092,                 loss: 0.2144
agent1:                 episode reward: 0.2092,                 loss: 0.1863
Score delta: 1.7131620660105575, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/41460_1.
Episode: 41921/101000 (41.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0685s / 10381.1380 s
agent0:                 episode reward: -0.0403,                 loss: 0.2115
agent1:                 episode reward: 0.0403,                 loss: nan
Episode: 41941/101000 (41.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4579s / 10389.5959 s
agent0:                 episode reward: 0.0117,                 loss: 0.2109
agent1:                 episode reward: -0.0117,                 loss: nan
Episode: 41961/101000 (41.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0719s / 10398.6677 s
agent0:                 episode reward: 0.4056,                 loss: 0.2128
agent1:                 episode reward: -0.4056,                 loss: nan
Episode: 41981/101000 (41.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2552s / 10406.9230 s
agent0:                 episode reward: -0.2319,                 loss: 0.2121
agent1:                 episode reward: 0.2319,                 loss: 0.2109
Score delta: 1.5014599503770492, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/41537_0.
Episode: 42001/101000 (41.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5140s / 10416.4370 s
agent0:                 episode reward: -0.1520,                 loss: nan
agent1:                 episode reward: 0.1520,                 loss: 0.2095
Episode: 42021/101000 (41.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6221s / 10425.0591 s
agent0:                 episode reward: -0.3936,                 loss: nan
agent1:                 episode reward: 0.3936,                 loss: 0.1998
Episode: 42041/101000 (41.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7107s / 10432.7698 s
agent0:                 episode reward: -0.5303,                 loss: nan
agent1:                 episode reward: 0.5303,                 loss: 0.1865
Score delta: 1.5877138610212334, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/41615_1.
Episode: 42061/101000 (41.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2258s / 10442.9957 s
agent0:                 episode reward: -0.4037,                 loss: 0.2001
agent1:                 episode reward: 0.4037,                 loss: nan
Episode: 42081/101000 (41.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2754s / 10451.2710 s
agent0:                 episode reward: -0.5179,                 loss: 0.2003
agent1:                 episode reward: 0.5179,                 loss: nan
Episode: 42101/101000 (41.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9497s / 10460.2208 s
agent0:                 episode reward: -0.1685,                 loss: 0.2005
agent1:                 episode reward: 0.1685,                 loss: nan
Episode: 42121/101000 (41.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2221s / 10469.4429 s
agent0:                 episode reward: -0.2381,                 loss: 0.1976
agent1:                 episode reward: 0.2381,                 loss: nan
Episode: 42141/101000 (41.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3103s / 10478.7532 s
agent0:                 episode reward: -0.1534,                 loss: 0.2085
agent1:                 episode reward: 0.1534,                 loss: nan
Episode: 42161/101000 (41.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0909s / 10487.8441 s
agent0:                 episode reward: 0.3086,                 loss: 0.2070
agent1:                 episode reward: -0.3086,                 loss: nan
Episode: 42181/101000 (41.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 12.0074s / 10499.8515 s
agent0:                 episode reward: -0.2003,                 loss: 0.2077
agent1:                 episode reward: 0.2003,                 loss: nan
Episode: 42201/101000 (41.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5335s / 10507.3850 s
agent0:                 episode reward: -0.4011,                 loss: 0.2087
agent1:                 episode reward: 0.4011,                 loss: nan
Episode: 42221/101000 (41.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4756s / 10514.8606 s
agent0:                 episode reward: -0.1701,                 loss: 0.2055
agent1:                 episode reward: 0.1701,                 loss: nan
Episode: 42241/101000 (41.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5901s / 10524.4507 s
agent0:                 episode reward: 0.0114,                 loss: 0.2094
agent1:                 episode reward: -0.0114,                 loss: 0.2084
Score delta: 1.5427286317611502, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/41808_0.
Episode: 42261/101000 (41.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3060s / 10530.7567 s
agent0:                 episode reward: -0.4161,                 loss: nan
agent1:                 episode reward: 0.4161,                 loss: 0.2081
Episode: 42281/101000 (41.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5747s / 10538.3314 s
agent0:                 episode reward: -0.6870,                 loss: nan
agent1:                 episode reward: 0.6870,                 loss: 0.2064
Episode: 42301/101000 (41.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2617s / 10544.5931 s
agent0:                 episode reward: -0.3760,                 loss: nan
agent1:                 episode reward: 0.3760,                 loss: 0.2068
Episode: 42321/101000 (41.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2489s / 10552.8421 s
agent0:                 episode reward: -0.3389,                 loss: 0.2138
agent1:                 episode reward: 0.3389,                 loss: 0.2076
Score delta: 1.5819778897038677, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/41885_1.
Episode: 42341/101000 (41.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5513s / 10560.3933 s
agent0:                 episode reward: 0.1157,                 loss: 0.2084
agent1:                 episode reward: -0.1157,                 loss: nan
Episode: 42361/101000 (41.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1336s / 10569.5270 s
agent0:                 episode reward: 0.2085,                 loss: 0.2078
agent1:                 episode reward: -0.2085,                 loss: nan
Episode: 42381/101000 (41.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2822s / 10579.8092 s
agent0:                 episode reward: 0.0796,                 loss: 0.2078
agent1:                 episode reward: -0.0796,                 loss: nan
Episode: 42401/101000 (41.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9419s / 10589.7511 s
agent0:                 episode reward: -0.0170,                 loss: 0.2088
agent1:                 episode reward: 0.0170,                 loss: nan
Episode: 42421/101000 (42.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6576s / 10594.4087 s
agent0:                 episode reward: -0.0748,                 loss: 0.2085
agent1:                 episode reward: 0.0748,                 loss: nan
Episode: 42441/101000 (42.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1852s / 10601.5939 s
agent0:                 episode reward: 0.2205,                 loss: 0.2087
agent1:                 episode reward: -0.2205,                 loss: 0.1836
Score delta: 1.661525175111364, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/42000_0.
Episode: 42461/101000 (42.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2358s / 10608.8297 s
agent0:                 episode reward: -0.2634,                 loss: nan
agent1:                 episode reward: 0.2634,                 loss: 0.1810
Episode: 42481/101000 (42.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7231s / 10617.5529 s
agent0:                 episode reward: -0.6154,                 loss: nan
agent1:                 episode reward: 0.6154,                 loss: 0.1817
Episode: 42501/101000 (42.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7641s / 10626.3170 s
agent0:                 episode reward: -0.9166,                 loss: nan
agent1:                 episode reward: 0.9166,                 loss: 0.1828
Episode: 42521/101000 (42.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8358s / 10635.1527 s
agent0:                 episode reward: -0.7186,                 loss: 0.2181
agent1:                 episode reward: 0.7186,                 loss: 0.1819
Score delta: 1.6469550028470796, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/42087_1.
Episode: 42541/101000 (42.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5927s / 10644.7454 s
agent0:                 episode reward: -0.8318,                 loss: 0.2087
agent1:                 episode reward: 0.8318,                 loss: nan
Episode: 42561/101000 (42.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2787s / 10653.0241 s
agent0:                 episode reward: -0.7561,                 loss: 0.2027
agent1:                 episode reward: 0.7561,                 loss: nan
Episode: 42581/101000 (42.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 11.6790s / 10664.7031 s
agent0:                 episode reward: -0.6925,                 loss: 0.2025
agent1:                 episode reward: 0.6925,                 loss: nan
Episode: 42601/101000 (42.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2155s / 10672.9187 s
agent0:                 episode reward: -0.3594,                 loss: 0.2004
agent1:                 episode reward: 0.3594,                 loss: nan
Episode: 42621/101000 (42.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9654s / 10681.8840 s
agent0:                 episode reward: -0.1575,                 loss: 0.1990
agent1:                 episode reward: 0.1575,                 loss: nan
Episode: 42641/101000 (42.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1329s / 10691.0169 s
agent0:                 episode reward: -0.0843,                 loss: 0.1949
agent1:                 episode reward: 0.0843,                 loss: nan
Episode: 42661/101000 (42.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9413s / 10696.9582 s
agent0:                 episode reward: -0.1130,                 loss: 0.1937
agent1:                 episode reward: 0.1130,                 loss: nan
Episode: 42681/101000 (42.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5611s / 10706.5193 s
agent0:                 episode reward: 0.2661,                 loss: 0.1936
agent1:                 episode reward: -0.2661,                 loss: nan
Episode: 42701/101000 (42.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3414s / 10716.8607 s
agent0:                 episode reward: -0.3023,                 loss: 0.1978
agent1:                 episode reward: 0.3023,                 loss: 0.1870
Score delta: 1.5990230590368228, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/42258_0.
Episode: 42721/101000 (42.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3233s / 10723.1840 s
agent0:                 episode reward: -0.5775,                 loss: nan
agent1:                 episode reward: 0.5775,                 loss: 0.1863
Episode: 42741/101000 (42.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7808s / 10730.9647 s
agent0:                 episode reward: -0.1775,                 loss: nan
agent1:                 episode reward: 0.1775,                 loss: 0.1872
Episode: 42761/101000 (42.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0141s / 10738.9788 s
agent0:                 episode reward: -0.2926,                 loss: nan
agent1:                 episode reward: 0.2926,                 loss: 0.1878
Episode: 42781/101000 (42.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5624s / 10745.5412 s
agent0:                 episode reward: -0.2651,                 loss: 0.2063
agent1:                 episode reward: 0.2651,                 loss: 0.1849
Score delta: 1.6387524437721737, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/42350_1.
Episode: 42801/101000 (42.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9246s / 10754.4658 s
agent0:                 episode reward: 0.6412,                 loss: 0.2146
agent1:                 episode reward: -0.6412,                 loss: nan
Episode: 42821/101000 (42.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4425s / 10763.9083 s
agent0:                 episode reward: 0.0558,                 loss: 0.2133
agent1:                 episode reward: -0.0558,                 loss: nan
Episode: 42841/101000 (42.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9068s / 10771.8151 s
agent0:                 episode reward: -0.3382,                 loss: 0.2104
agent1:                 episode reward: 0.3382,                 loss: nan
Episode: 42861/101000 (42.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7183s / 10781.5334 s
agent0:                 episode reward: 0.1930,                 loss: 0.2120
agent1:                 episode reward: -0.1930,                 loss: nan
Episode: 42881/101000 (42.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0809s / 10790.6143 s
agent0:                 episode reward: 0.5005,                 loss: 0.2115
agent1:                 episode reward: -0.5005,                 loss: 0.1783
Score delta: 1.565194159835644, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/42452_0.
Episode: 42901/101000 (42.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3813s / 10798.9956 s
agent0:                 episode reward: -0.1458,                 loss: nan
agent1:                 episode reward: 0.1458,                 loss: 0.1723
Episode: 42921/101000 (42.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5067s / 10807.5022 s
agent0:                 episode reward: -0.7441,                 loss: nan
agent1:                 episode reward: 0.7441,                 loss: 0.1666
Episode: 42941/101000 (42.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3556s / 10815.8579 s
agent0:                 episode reward: -0.5249,                 loss: nan
agent1:                 episode reward: 0.5249,                 loss: 0.1855
Episode: 42961/101000 (42.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9514s / 10824.8093 s
agent0:                 episode reward: -0.6260,                 loss: 0.2179
agent1:                 episode reward: 0.6260,                 loss: 0.1951
Score delta: 2.08144048127061, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/42532_1.
Episode: 42981/101000 (42.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 12.7880s / 10837.5972 s
agent0:                 episode reward: 0.2188,                 loss: 0.2143
agent1:                 episode reward: -0.2188,                 loss: nan
Episode: 43001/101000 (42.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7200s / 10846.3172 s
agent0:                 episode reward: 0.0694,                 loss: 0.2138
agent1:                 episode reward: -0.0694,                 loss: nan
Episode: 43021/101000 (42.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9142s / 10851.2314 s
agent0:                 episode reward: 0.4744,                 loss: 0.2140
agent1:                 episode reward: -0.4744,                 loss: nan
Episode: 43041/101000 (42.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0100s / 10857.2413 s
agent0:                 episode reward: -0.1071,                 loss: 0.2105
agent1:                 episode reward: 0.1071,                 loss: nan
Episode: 43061/101000 (42.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5769s / 10864.8183 s
agent0:                 episode reward: 0.1068,                 loss: 0.2137
agent1:                 episode reward: -0.1068,                 loss: nan
Episode: 43081/101000 (42.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7310s / 10874.5493 s
agent0:                 episode reward: 0.2560,                 loss: 0.2139
agent1:                 episode reward: -0.2560,                 loss: nan
Episode: 43101/101000 (42.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3139s / 10882.8632 s
agent0:                 episode reward: -0.5336,                 loss: 0.2134
agent1:                 episode reward: 0.5336,                 loss: nan
Episode: 43121/101000 (42.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5033s / 10891.3666 s
agent0:                 episode reward: 0.1029,                 loss: 0.2138
agent1:                 episode reward: -0.1029,                 loss: 0.1882
Score delta: 1.683188326703588, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/42683_0.
Episode: 43141/101000 (42.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8234s / 10899.1900 s
agent0:                 episode reward: -0.5159,                 loss: nan
agent1:                 episode reward: 0.5159,                 loss: 0.1887
Episode: 43161/101000 (42.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9007s / 10908.0907 s
agent0:                 episode reward: -0.2007,                 loss: nan
agent1:                 episode reward: 0.2007,                 loss: 0.1901
Episode: 43181/101000 (42.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5819s / 10916.6725 s
agent0:                 episode reward: -0.3835,                 loss: nan
agent1:                 episode reward: 0.3835,                 loss: 0.1907
Episode: 43201/101000 (42.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7382s / 10924.4108 s
agent0:                 episode reward: -0.4998,                 loss: nan
agent1:                 episode reward: 0.4998,                 loss: 0.1910
Episode: 43221/101000 (42.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7286s / 10933.1394 s
agent0:                 episode reward: -0.7604,                 loss: 0.2774
agent1:                 episode reward: 0.7604,                 loss: 0.1895
Score delta: 1.6339005016284538, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/42787_1.
Episode: 43241/101000 (42.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 11.5455s / 10944.6849 s
agent0:                 episode reward: 0.0372,                 loss: 0.2315
agent1:                 episode reward: -0.0372,                 loss: nan
Episode: 43261/101000 (42.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9118s / 10953.5968 s
agent0:                 episode reward: -0.5020,                 loss: 0.2073
agent1:                 episode reward: 0.5020,                 loss: nan
Episode: 43281/101000 (42.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4699s / 10962.0666 s
agent0:                 episode reward: 0.4180,                 loss: 0.2083
agent1:                 episode reward: -0.4180,                 loss: nan
Episode: 43301/101000 (42.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9469s / 10973.0136 s
agent0:                 episode reward: 0.0491,                 loss: 0.2074
agent1:                 episode reward: -0.0491,                 loss: nan
Episode: 43321/101000 (42.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8461s / 10981.8597 s
agent0:                 episode reward: -0.2319,                 loss: 0.2068
agent1:                 episode reward: 0.2319,                 loss: nan
Episode: 43341/101000 (42.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5888s / 10991.4485 s
agent0:                 episode reward: 0.1004,                 loss: 0.2067
agent1:                 episode reward: -0.1004,                 loss: nan
Episode: 43361/101000 (42.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5625s / 11000.0110 s
agent0:                 episode reward: 0.0205,                 loss: 0.2056
agent1:                 episode reward: -0.0205,                 loss: nan
Episode: 43381/101000 (42.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 11.4257s / 11011.4367 s
agent0:                 episode reward: -0.3555,                 loss: 0.2060
agent1:                 episode reward: 0.3555,                 loss: nan
Episode: 43401/101000 (42.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3074s / 11020.7441 s
agent0:                 episode reward: -0.0582,                 loss: 0.2079
agent1:                 episode reward: 0.0582,                 loss: nan
Episode: 43421/101000 (42.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2570s / 11030.0011 s
agent0:                 episode reward: -0.1513,                 loss: 0.2056
agent1:                 episode reward: 0.1513,                 loss: nan
Episode: 43441/101000 (43.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6678s / 11038.6689 s
agent0:                 episode reward: -0.3598,                 loss: 0.2066
agent1:                 episode reward: 0.3598,                 loss: nan
Episode: 43461/101000 (43.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1925s / 11047.8614 s
agent0:                 episode reward: 0.3060,                 loss: 0.2064
agent1:                 episode reward: -0.3060,                 loss: nan
Episode: 43481/101000 (43.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7810s / 11057.6424 s
agent0:                 episode reward: 0.1034,                 loss: 0.1993
agent1:                 episode reward: -0.1034,                 loss: 0.1931
Score delta: 1.525372851084422, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/43036_0.
Episode: 43501/101000 (43.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1245s / 11065.7668 s
agent0:                 episode reward: -0.4676,                 loss: nan
agent1:                 episode reward: 0.4676,                 loss: 0.1939
Episode: 43521/101000 (43.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0059s / 11073.7727 s
agent0:                 episode reward: -0.1906,                 loss: nan
agent1:                 episode reward: 0.1906,                 loss: 0.1904
Episode: 43541/101000 (43.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3007s / 11080.0734 s
agent0:                 episode reward: -0.5855,                 loss: nan
agent1:                 episode reward: 0.5855,                 loss: 0.1900
Episode: 43561/101000 (43.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4045s / 11088.4779 s
agent0:                 episode reward: 0.0501,                 loss: nan
agent1:                 episode reward: -0.0501,                 loss: 0.1898
Episode: 43581/101000 (43.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7548s / 11097.2327 s
agent0:                 episode reward: -0.0457,                 loss: nan
agent1:                 episode reward: 0.0457,                 loss: 0.1886
Episode: 43601/101000 (43.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0869s / 11107.3196 s
agent0:                 episode reward: -0.3314,                 loss: 0.1988
agent1:                 episode reward: 0.3314,                 loss: 0.1896
Score delta: 1.7281885585358414, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/43163_1.
Episode: 43621/101000 (43.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 11.2102s / 11118.5299 s
agent0:                 episode reward: 0.0486,                 loss: 0.1959
agent1:                 episode reward: -0.0486,                 loss: nan
Episode: 43641/101000 (43.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2187s / 11127.7485 s
agent0:                 episode reward: 0.0238,                 loss: 0.1958
agent1:                 episode reward: -0.0238,                 loss: nan
Episode: 43661/101000 (43.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8749s / 11137.6235 s
agent0:                 episode reward: -0.2725,                 loss: 0.1959
agent1:                 episode reward: 0.2725,                 loss: nan
Episode: 43681/101000 (43.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8338s / 11148.4572 s
agent0:                 episode reward: 0.1517,                 loss: 0.1963
agent1:                 episode reward: -0.1517,                 loss: nan
Episode: 43701/101000 (43.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0738s / 11157.5311 s
agent0:                 episode reward: 0.0782,                 loss: 0.1995
agent1:                 episode reward: -0.0782,                 loss: 0.1690
Score delta: 1.549953796434892, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/43264_0.
Episode: 43721/101000 (43.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5857s / 11165.1168 s
agent0:                 episode reward: 0.1130,                 loss: nan
agent1:                 episode reward: -0.1130,                 loss: 0.1682
Episode: 43741/101000 (43.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6594s / 11174.7762 s
agent0:                 episode reward: -0.3295,                 loss: nan
agent1:                 episode reward: 0.3295,                 loss: 0.1681
Episode: 43761/101000 (43.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7010s / 11183.4772 s
agent0:                 episode reward: -0.6412,                 loss: nan
agent1:                 episode reward: 0.6412,                 loss: 0.1662
Episode: 43781/101000 (43.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0233s / 11191.5005 s
agent0:                 episode reward: -0.4181,                 loss: nan
agent1:                 episode reward: 0.4181,                 loss: 0.1894
Episode: 43801/101000 (43.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6242s / 11199.1246 s
agent0:                 episode reward: 0.2071,                 loss: 0.2068
agent1:                 episode reward: -0.2071,                 loss: 0.1896
Score delta: 1.7574739700733957, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/43359_1.
Episode: 43821/101000 (43.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4370s / 11209.5616 s
agent0:                 episode reward: 0.0821,                 loss: 0.2063
agent1:                 episode reward: -0.0821,                 loss: nan
Episode: 43841/101000 (43.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2149s / 11217.7765 s
agent0:                 episode reward: 0.0205,                 loss: 0.2059
agent1:                 episode reward: -0.0205,                 loss: nan
Episode: 43861/101000 (43.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7797s / 11223.5562 s
agent0:                 episode reward: 0.1929,                 loss: 0.2060
agent1:                 episode reward: -0.1929,                 loss: nan
Episode: 43881/101000 (43.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3528s / 11229.9091 s
agent0:                 episode reward: 0.3955,                 loss: 0.2044
agent1:                 episode reward: -0.3955,                 loss: 0.1877
Score delta: 1.5521611532335138, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/43454_0.
Episode: 43901/101000 (43.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9697s / 11237.8788 s
agent0:                 episode reward: -0.6672,                 loss: nan
agent1:                 episode reward: 0.6672,                 loss: 0.1912
Episode: 43921/101000 (43.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7474s / 11246.6262 s
agent0:                 episode reward: -0.7566,                 loss: nan
agent1:                 episode reward: 0.7566,                 loss: 0.1910
Episode: 43941/101000 (43.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1266s / 11249.7528 s
agent0:                 episode reward: -0.4409,                 loss: nan
agent1:                 episode reward: 0.4409,                 loss: 0.1902
Episode: 43961/101000 (43.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2657s / 11253.0185 s
agent0:                 episode reward: -0.4139,                 loss: nan
agent1:                 episode reward: 0.4139,                 loss: 0.1918
Episode: 43981/101000 (43.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4683s / 11257.4867 s
agent0:                 episode reward: 0.2441,                 loss: nan
agent1:                 episode reward: -0.2441,                 loss: 0.1912
Episode: 44001/101000 (43.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7832s / 11261.2699 s
agent0:                 episode reward: -0.3417,                 loss: nan
agent1:                 episode reward: 0.3417,                 loss: 0.1906
Score delta: 1.6834420721886179, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/43575_1.
Episode: 44021/101000 (43.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1323s / 11265.4022 s
agent0:                 episode reward: -0.2370,                 loss: 0.2069
agent1:                 episode reward: 0.2370,                 loss: nan
Episode: 44041/101000 (43.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6968s / 11269.0991 s
agent0:                 episode reward: 0.2286,                 loss: 0.2049
agent1:                 episode reward: -0.2286,                 loss: nan
Episode: 44061/101000 (43.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4347s / 11272.5337 s
agent0:                 episode reward: -0.0494,                 loss: 0.2064
agent1:                 episode reward: 0.0494,                 loss: nan
Episode: 44081/101000 (43.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1693s / 11276.7030 s
agent0:                 episode reward: 0.1409,                 loss: 0.2035
agent1:                 episode reward: -0.1409,                 loss: nan
Episode: 44101/101000 (43.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6220s / 11280.3251 s
agent0:                 episode reward: 0.0839,                 loss: 0.2037
agent1:                 episode reward: -0.0839,                 loss: nan
Episode: 44121/101000 (43.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4714s / 11283.7965 s
agent0:                 episode reward: -0.3192,                 loss: 0.2035
agent1:                 episode reward: 0.3192,                 loss: nan
Episode: 44141/101000 (43.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5471s / 11287.3436 s
agent0:                 episode reward: 0.1045,                 loss: 0.2045
agent1:                 episode reward: -0.1045,                 loss: nan
Episode: 44161/101000 (43.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7655s / 11291.1091 s
agent0:                 episode reward: 0.0833,                 loss: 0.2030
agent1:                 episode reward: -0.0833,                 loss: 0.1887
Score delta: 1.623403532797208, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/43730_0.
Episode: 44181/101000 (43.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7961s / 11294.9051 s
agent0:                 episode reward: -0.3202,                 loss: nan
agent1:                 episode reward: 0.3202,                 loss: 0.1874
Episode: 44201/101000 (43.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6435s / 11298.5487 s
agent0:                 episode reward: -1.2230,                 loss: nan
agent1:                 episode reward: 1.2230,                 loss: 0.1891
Episode: 44221/101000 (43.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6614s / 11302.2101 s
agent0:                 episode reward: -0.3481,                 loss: nan
agent1:                 episode reward: 0.3481,                 loss: 0.1893
Episode: 44241/101000 (43.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7002s / 11304.9104 s
agent0:                 episode reward: -0.2651,                 loss: nan
agent1:                 episode reward: 0.2651,                 loss: 0.1894
Episode: 44261/101000 (43.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8610s / 11307.7714 s
agent0:                 episode reward: -0.6382,                 loss: nan
agent1:                 episode reward: 0.6382,                 loss: 0.1869
Score delta: 1.841122678131013, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/43835_1.
Episode: 44281/101000 (43.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0104s / 11311.7817 s
agent0:                 episode reward: -0.9207,                 loss: 0.2675
agent1:                 episode reward: 0.9207,                 loss: nan
Episode: 44301/101000 (43.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6432s / 11314.4250 s
agent0:                 episode reward: -0.1523,                 loss: 0.2588
agent1:                 episode reward: 0.1523,                 loss: nan
Episode: 44321/101000 (43.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4024s / 11317.8274 s
agent0:                 episode reward: 0.0911,                 loss: 0.2545
agent1:                 episode reward: -0.0911,                 loss: nan
Episode: 44341/101000 (43.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5212s / 11321.3486 s
agent0:                 episode reward: -0.1884,                 loss: 0.2551
agent1:                 episode reward: 0.1884,                 loss: nan
Episode: 44361/101000 (43.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4625s / 11324.8111 s
agent0:                 episode reward: -0.6357,                 loss: 0.2133
agent1:                 episode reward: 0.6357,                 loss: nan
Episode: 44381/101000 (43.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8888s / 11328.6998 s
agent0:                 episode reward: 0.1575,                 loss: 0.2041
agent1:                 episode reward: -0.1575,                 loss: nan
Episode: 44401/101000 (43.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7853s / 11332.4851 s
agent0:                 episode reward: 0.1000,                 loss: 0.2031
agent1:                 episode reward: -0.1000,                 loss: nan
Episode: 44421/101000 (43.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3753s / 11335.8604 s
agent0:                 episode reward: 0.3159,                 loss: 0.1999
agent1:                 episode reward: -0.3159,                 loss: nan
Episode: 44441/101000 (44.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0568s / 11338.9172 s
agent0:                 episode reward: -0.3706,                 loss: 0.2025
agent1:                 episode reward: 0.3706,                 loss: nan
Episode: 44461/101000 (44.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7791s / 11342.6963 s
agent0:                 episode reward: 0.2511,                 loss: 0.2011
agent1:                 episode reward: -0.2511,                 loss: nan
Episode: 44481/101000 (44.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2067s / 11346.9030 s
agent0:                 episode reward: -0.1526,                 loss: 0.2018
agent1:                 episode reward: 0.1526,                 loss: nan
Episode: 44501/101000 (44.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2772s / 11351.1802 s
agent0:                 episode reward: -0.0701,                 loss: 0.2005
agent1:                 episode reward: 0.0701,                 loss: nan
Episode: 44521/101000 (44.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4140s / 11355.5942 s
agent0:                 episode reward: 0.1521,                 loss: 0.1995
agent1:                 episode reward: -0.1521,                 loss: nan
Score delta: 1.8392104302754817, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/44095_0.
Episode: 44541/101000 (44.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1132s / 11358.7074 s
agent0:                 episode reward: -0.4603,                 loss: nan
agent1:                 episode reward: 0.4603,                 loss: 0.1914
Episode: 44561/101000 (44.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0661s / 11362.7735 s
agent0:                 episode reward: -0.3868,                 loss: nan
agent1:                 episode reward: 0.3868,                 loss: 0.1898
Episode: 44581/101000 (44.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7073s / 11366.4808 s
agent0:                 episode reward: -0.6130,                 loss: nan
agent1:                 episode reward: 0.6130,                 loss: 0.1893
Episode: 44601/101000 (44.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5566s / 11370.0374 s
agent0:                 episode reward: -0.3626,                 loss: nan
agent1:                 episode reward: 0.3626,                 loss: 0.1898
Episode: 44621/101000 (44.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8577s / 11373.8951 s
agent0:                 episode reward: -0.2611,                 loss: nan
agent1:                 episode reward: 0.2611,                 loss: 0.1868
Episode: 44641/101000 (44.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4222s / 11377.3173 s
agent0:                 episode reward: -1.0306,                 loss: 0.2413
agent1:                 episode reward: 1.0306,                 loss: 0.1833
Score delta: 1.8828518990757614, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/44201_1.
Episode: 44661/101000 (44.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2430s / 11380.5603 s
agent0:                 episode reward: -0.0131,                 loss: 0.2381
agent1:                 episode reward: 0.0131,                 loss: nan
Episode: 44681/101000 (44.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4812s / 11384.0415 s
agent0:                 episode reward: -0.1321,                 loss: 0.2341
agent1:                 episode reward: 0.1321,                 loss: nan
Episode: 44701/101000 (44.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4874s / 11387.5290 s
agent0:                 episode reward: 0.2378,                 loss: 0.2347
agent1:                 episode reward: -0.2378,                 loss: nan
Episode: 44721/101000 (44.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5398s / 11391.0688 s
agent0:                 episode reward: -0.3066,                 loss: 0.2322
agent1:                 episode reward: 0.3066,                 loss: nan
Episode: 44741/101000 (44.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4781s / 11395.5469 s
agent0:                 episode reward: 0.1496,                 loss: 0.2298
agent1:                 episode reward: -0.1496,                 loss: nan
Episode: 44761/101000 (44.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2677s / 11398.8146 s
agent0:                 episode reward: -0.6612,                 loss: 0.2302
agent1:                 episode reward: 0.6612,                 loss: nan
Episode: 44781/101000 (44.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4655s / 11403.2800 s
agent0:                 episode reward: -0.0815,                 loss: 0.2294
agent1:                 episode reward: 0.0815,                 loss: nan
Episode: 44801/101000 (44.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7937s / 11407.0738 s
agent0:                 episode reward: -0.5904,                 loss: 0.2049
agent1:                 episode reward: 0.5904,                 loss: nan
Episode: 44821/101000 (44.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1577s / 11410.2315 s
agent0:                 episode reward: 0.3271,                 loss: 0.2051
agent1:                 episode reward: -0.3271,                 loss: nan
Episode: 44841/101000 (44.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4693s / 11413.7007 s
agent0:                 episode reward: -0.1780,                 loss: 0.2024
agent1:                 episode reward: 0.1780,                 loss: nan
Episode: 44861/101000 (44.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1925s / 11417.8933 s
agent0:                 episode reward: 0.1999,                 loss: 0.2033
agent1:                 episode reward: -0.1999,                 loss: nan
Episode: 44881/101000 (44.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2906s / 11421.1839 s
agent0:                 episode reward: -0.0748,                 loss: 0.2029
agent1:                 episode reward: 0.0748,                 loss: nan
Episode: 44901/101000 (44.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8906s / 11424.0745 s
agent0:                 episode reward: -0.3413,                 loss: 0.2034
agent1:                 episode reward: 0.3413,                 loss: nan
Episode: 44921/101000 (44.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9080s / 11426.9825 s
agent0:                 episode reward: 0.1351,                 loss: 0.2010
agent1:                 episode reward: -0.1351,                 loss: nan
Episode: 44941/101000 (44.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4935s / 11430.4760 s
agent0:                 episode reward: -0.0904,                 loss: 0.2008
agent1:                 episode reward: 0.0904,                 loss: nan
Episode: 44961/101000 (44.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6975s / 11434.1735 s
agent0:                 episode reward: -0.1383,                 loss: 0.2045
agent1:                 episode reward: 0.1383,                 loss: nan
Episode: 44981/101000 (44.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4041s / 11438.5776 s
agent0:                 episode reward: -0.1960,                 loss: 0.2015
agent1:                 episode reward: 0.1960,                 loss: nan
Episode: 45001/101000 (44.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3798s / 11441.9575 s
agent0:                 episode reward: -0.0277,                 loss: 0.2016
agent1:                 episode reward: 0.0277,                 loss: nan
Episode: 45021/101000 (44.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1171s / 11445.0745 s
agent0:                 episode reward: 0.0192,                 loss: 0.2026
agent1:                 episode reward: -0.0192,                 loss: 0.1954
Score delta: 1.5398898439130844, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/44588_0.
Episode: 45041/101000 (44.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1342s / 11448.2087 s
agent0:                 episode reward: 0.1953,                 loss: nan
agent1:                 episode reward: -0.1953,                 loss: 0.1930
Episode: 45061/101000 (44.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1542s / 11452.3629 s
agent0:                 episode reward: -0.0551,                 loss: nan
agent1:                 episode reward: 0.0551,                 loss: 0.1899
Episode: 45081/101000 (44.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5035s / 11455.8664 s
agent0:                 episode reward: -0.5825,                 loss: nan
agent1:                 episode reward: 0.5825,                 loss: 0.1926
Episode: 45101/101000 (44.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1070s / 11458.9734 s
agent0:                 episode reward: -0.6055,                 loss: nan
agent1:                 episode reward: 0.6055,                 loss: 0.1913
Episode: 45121/101000 (44.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7185s / 11461.6919 s
agent0:                 episode reward: -0.3468,                 loss: nan
agent1:                 episode reward: 0.3468,                 loss: 0.1904
Score delta: 1.7422664190840187, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/44695_1.
Episode: 45141/101000 (44.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4237s / 11465.1156 s
agent0:                 episode reward: -0.3718,                 loss: 0.2465
agent1:                 episode reward: 0.3718,                 loss: nan
Episode: 45161/101000 (44.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0551s / 11469.1707 s
agent0:                 episode reward: -0.2833,                 loss: 0.2379
agent1:                 episode reward: 0.2833,                 loss: nan
Episode: 45181/101000 (44.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6714s / 11472.8421 s
agent0:                 episode reward: 0.1622,                 loss: 0.2346
agent1:                 episode reward: -0.1622,                 loss: nan
Episode: 45201/101000 (44.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8337s / 11477.6758 s
agent0:                 episode reward: -0.4766,                 loss: 0.2341
agent1:                 episode reward: 0.4766,                 loss: nan
Episode: 45221/101000 (44.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7252s / 11481.4010 s
agent0:                 episode reward: 0.1909,                 loss: 0.2332
agent1:                 episode reward: -0.1909,                 loss: nan
Episode: 45241/101000 (44.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8667s / 11484.2678 s
agent0:                 episode reward: -0.0817,                 loss: 0.2016
agent1:                 episode reward: 0.0817,                 loss: nan
Episode: 45261/101000 (44.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5701s / 11487.8378 s
agent0:                 episode reward: -0.6189,                 loss: 0.1994
agent1:                 episode reward: 0.6189,                 loss: nan
Episode: 45281/101000 (44.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2770s / 11491.1148 s
agent0:                 episode reward: -0.2612,                 loss: 0.1997
agent1:                 episode reward: 0.2612,                 loss: nan
Episode: 45301/101000 (44.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2354s / 11495.3502 s
agent0:                 episode reward: -0.2895,                 loss: 0.1975
agent1:                 episode reward: 0.2895,                 loss: nan
Episode: 45321/101000 (44.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4493s / 11498.7995 s
agent0:                 episode reward: 0.3657,                 loss: 0.1971
agent1:                 episode reward: -0.3657,                 loss: nan
Episode: 45341/101000 (44.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8064s / 11502.6059 s
agent0:                 episode reward: -0.3261,                 loss: 0.1958
agent1:                 episode reward: 0.3261,                 loss: nan
Episode: 45361/101000 (44.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8465s / 11506.4524 s
agent0:                 episode reward: 0.0105,                 loss: 0.1973
agent1:                 episode reward: -0.0105,                 loss: nan
Episode: 45381/101000 (44.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0591s / 11510.5115 s
agent0:                 episode reward: -0.2341,                 loss: 0.1958
agent1:                 episode reward: 0.2341,                 loss: nan
Episode: 45401/101000 (44.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8894s / 11514.4009 s
agent0:                 episode reward: 0.0890,                 loss: 0.1953
agent1:                 episode reward: -0.0890,                 loss: nan
Episode: 45421/101000 (44.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7596s / 11518.1605 s
agent0:                 episode reward: -0.1427,                 loss: 0.1959
agent1:                 episode reward: 0.1427,                 loss: nan
Episode: 45441/101000 (44.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7229s / 11521.8834 s
agent0:                 episode reward: -0.0518,                 loss: 0.1951
agent1:                 episode reward: 0.0518,                 loss: nan
Episode: 45461/101000 (45.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7378s / 11525.6212 s
agent0:                 episode reward: -0.0342,                 loss: 0.1960
agent1:                 episode reward: 0.0342,                 loss: nan
Episode: 45481/101000 (45.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2842s / 11528.9054 s
agent0:                 episode reward: -0.3186,                 loss: 0.1970
agent1:                 episode reward: 0.3186,                 loss: nan
Episode: 45501/101000 (45.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8189s / 11532.7243 s
agent0:                 episode reward: 0.0022,                 loss: 0.1958
agent1:                 episode reward: -0.0022,                 loss: nan
Episode: 45521/101000 (45.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6927s / 11536.4170 s
agent0:                 episode reward: 0.2141,                 loss: 0.1964
agent1:                 episode reward: -0.2141,                 loss: nan
Episode: 45541/101000 (45.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3977s / 11539.8147 s
agent0:                 episode reward: -0.0870,                 loss: 0.1949
agent1:                 episode reward: 0.0870,                 loss: nan
Episode: 45561/101000 (45.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8405s / 11543.6552 s
agent0:                 episode reward: 0.1649,                 loss: 0.1986
agent1:                 episode reward: -0.1649,                 loss: nan
Episode: 45581/101000 (45.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9194s / 11546.5746 s
agent0:                 episode reward: -0.4245,                 loss: 0.2062
agent1:                 episode reward: 0.4245,                 loss: nan
Episode: 45601/101000 (45.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1669s / 11549.7415 s
agent0:                 episode reward: -0.4885,                 loss: 0.2070
agent1:                 episode reward: 0.4885,                 loss: nan
Episode: 45621/101000 (45.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2601s / 11554.0016 s
agent0:                 episode reward: 0.3600,                 loss: 0.2067
agent1:                 episode reward: -0.3600,                 loss: nan
Score delta: 1.606475828407973, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/45195_0.
Episode: 45641/101000 (45.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4620s / 11557.4636 s
agent0:                 episode reward: -0.6478,                 loss: nan
agent1:                 episode reward: 0.6478,                 loss: 0.1959
Episode: 45661/101000 (45.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6144s / 11561.0780 s
agent0:                 episode reward: -0.4736,                 loss: nan
agent1:                 episode reward: 0.4736,                 loss: 0.1959
Episode: 45681/101000 (45.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5701s / 11564.6481 s
agent0:                 episode reward: -0.2725,                 loss: nan
agent1:                 episode reward: 0.2725,                 loss: 0.1938
Episode: 45701/101000 (45.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1260s / 11567.7741 s
agent0:                 episode reward: -0.2627,                 loss: nan
agent1:                 episode reward: 0.2627,                 loss: 0.1919
Episode: 45721/101000 (45.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3743s / 11571.1484 s
agent0:                 episode reward: -0.3606,                 loss: nan
agent1:                 episode reward: 0.3606,                 loss: 0.1924
Episode: 45741/101000 (45.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5850s / 11573.7334 s
agent0:                 episode reward: 0.1006,                 loss: nan
agent1:                 episode reward: -0.1006,                 loss: 0.1914
Episode: 45761/101000 (45.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1334s / 11576.8668 s
agent0:                 episode reward: -0.4477,                 loss: nan
agent1:                 episode reward: 0.4477,                 loss: 0.1914
Episode: 45781/101000 (45.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0198s / 11579.8866 s
agent0:                 episode reward: 0.4904,                 loss: 0.2160
agent1:                 episode reward: -0.4904,                 loss: 0.1888
Score delta: 1.6753020422100993, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/45336_1.
Episode: 45801/101000 (45.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1846s / 11583.0712 s
agent0:                 episode reward: 0.0125,                 loss: 0.2132
agent1:                 episode reward: -0.0125,                 loss: nan
Episode: 45821/101000 (45.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0262s / 11587.0974 s
agent0:                 episode reward: -0.3213,                 loss: 0.2121
agent1:                 episode reward: 0.3213,                 loss: nan
Episode: 45841/101000 (45.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4974s / 11590.5949 s
agent0:                 episode reward: -0.0252,                 loss: 0.2109
agent1:                 episode reward: 0.0252,                 loss: nan
Episode: 45861/101000 (45.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2494s / 11593.8443 s
agent0:                 episode reward: -0.2214,                 loss: 0.2102
agent1:                 episode reward: 0.2214,                 loss: nan
Episode: 45881/101000 (45.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9062s / 11597.7505 s
agent0:                 episode reward: -0.0159,                 loss: 0.2106
agent1:                 episode reward: 0.0159,                 loss: nan
Episode: 45901/101000 (45.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3607s / 11602.1112 s
agent0:                 episode reward: -0.1842,                 loss: 0.2104
agent1:                 episode reward: 0.1842,                 loss: nan
Episode: 45921/101000 (45.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7898s / 11604.9010 s
agent0:                 episode reward: 0.0425,                 loss: 0.2108
agent1:                 episode reward: -0.0425,                 loss: nan
Episode: 45941/101000 (45.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4795s / 11608.3806 s
agent0:                 episode reward: -0.5472,                 loss: 0.2102
agent1:                 episode reward: 0.5472,                 loss: nan
Episode: 45961/101000 (45.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2715s / 11611.6520 s
agent0:                 episode reward: -0.2950,                 loss: 0.2120
agent1:                 episode reward: 0.2950,                 loss: nan
Episode: 45981/101000 (45.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4533s / 11615.1053 s
agent0:                 episode reward: 0.3683,                 loss: 0.2091
agent1:                 episode reward: -0.3683,                 loss: nan
Episode: 46001/101000 (45.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2427s / 11618.3480 s
agent0:                 episode reward: -0.0419,                 loss: 0.2108
agent1:                 episode reward: 0.0419,                 loss: 0.2169
Score delta: 1.554180143463423, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/45561_0.
Episode: 46021/101000 (45.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7414s / 11622.0895 s
agent0:                 episode reward: -0.2532,                 loss: nan
agent1:                 episode reward: 0.2532,                 loss: 0.2127
Episode: 46041/101000 (45.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5045s / 11625.5939 s
agent0:                 episode reward: -0.7464,                 loss: nan
agent1:                 episode reward: 0.7464,                 loss: 0.2119
Episode: 46061/101000 (45.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3488s / 11629.9427 s
agent0:                 episode reward: -0.0005,                 loss: nan
agent1:                 episode reward: 0.0005,                 loss: 0.2017
Episode: 46081/101000 (45.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9699s / 11633.9127 s
agent0:                 episode reward: -0.4054,                 loss: nan
agent1:                 episode reward: 0.4054,                 loss: 0.1901
Episode: 46101/101000 (45.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3500s / 11637.2626 s
agent0:                 episode reward: -0.0725,                 loss: nan
agent1:                 episode reward: 0.0725,                 loss: 0.1910
Episode: 46121/101000 (45.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0023s / 11640.2649 s
agent0:                 episode reward: 0.1105,                 loss: nan
agent1:                 episode reward: -0.1105,                 loss: 0.1909
Episode: 46141/101000 (45.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1021s / 11644.3670 s
agent0:                 episode reward: -0.3817,                 loss: nan
agent1:                 episode reward: 0.3817,                 loss: 0.1921
Episode: 46161/101000 (45.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4875s / 11647.8545 s
agent0:                 episode reward: 0.2601,                 loss: 0.2095
agent1:                 episode reward: -0.2601,                 loss: 0.1900
Score delta: 1.5872962934581065, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/45719_1.
Episode: 46181/101000 (45.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1667s / 11652.0212 s
agent0:                 episode reward: 0.0173,                 loss: 0.2059
agent1:                 episode reward: -0.0173,                 loss: nan
Episode: 46201/101000 (45.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5029s / 11655.5241 s
agent0:                 episode reward: 0.3114,                 loss: 0.2035
agent1:                 episode reward: -0.3114,                 loss: nan
Episode: 46221/101000 (45.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0607s / 11660.5849 s
agent0:                 episode reward: 0.3170,                 loss: 0.2019
agent1:                 episode reward: -0.3170,                 loss: nan
Episode: 46241/101000 (45.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1956s / 11664.7805 s
agent0:                 episode reward: 0.2727,                 loss: 0.2036
agent1:                 episode reward: -0.2727,                 loss: nan
Episode: 46261/101000 (45.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6444s / 11668.4248 s
agent0:                 episode reward: -0.2798,                 loss: 0.2019
agent1:                 episode reward: 0.2798,                 loss: 0.1889
Score delta: 1.7970669591163648, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/45822_0.
Episode: 46281/101000 (45.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7047s / 11671.1296 s
agent0:                 episode reward: -0.2420,                 loss: nan
agent1:                 episode reward: 0.2420,                 loss: 0.1863
Episode: 46301/101000 (45.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4465s / 11674.5761 s
agent0:                 episode reward: -0.2952,                 loss: nan
agent1:                 episode reward: 0.2952,                 loss: 0.1895
Episode: 46321/101000 (45.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2667s / 11679.8428 s
agent0:                 episode reward: -0.3060,                 loss: nan
agent1:                 episode reward: 0.3060,                 loss: 0.1883
Episode: 46341/101000 (45.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4455s / 11683.2882 s
agent0:                 episode reward: -0.7369,                 loss: nan
agent1:                 episode reward: 0.7369,                 loss: 0.1892
Episode: 46361/101000 (45.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2930s / 11686.5813 s
agent0:                 episode reward: -0.6863,                 loss: 0.2190
agent1:                 episode reward: 0.6863,                 loss: 0.1884
Score delta: 1.5441705129096626, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/45922_1.
Episode: 46381/101000 (45.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9308s / 11690.5121 s
agent0:                 episode reward: -1.2143,                 loss: 0.2084
agent1:                 episode reward: 1.2143,                 loss: nan
Episode: 46401/101000 (45.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8507s / 11693.3628 s
agent0:                 episode reward: -0.7946,                 loss: 0.2045
agent1:                 episode reward: 0.7946,                 loss: nan
Episode: 46421/101000 (45.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3161s / 11696.6789 s
agent0:                 episode reward: -0.4661,                 loss: 0.2003
agent1:                 episode reward: 0.4661,                 loss: nan
Episode: 46441/101000 (45.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2986s / 11700.9775 s
agent0:                 episode reward: -0.1410,                 loss: 0.2007
agent1:                 episode reward: 0.1410,                 loss: nan
Episode: 46461/101000 (46.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3368s / 11704.3143 s
agent0:                 episode reward: -0.0247,                 loss: 0.1973
agent1:                 episode reward: 0.0247,                 loss: nan
Episode: 46481/101000 (46.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0468s / 11708.3610 s
agent0:                 episode reward: -0.2188,                 loss: 0.1975
agent1:                 episode reward: 0.2188,                 loss: nan
Episode: 46501/101000 (46.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4012s / 11711.7622 s
agent0:                 episode reward: -0.2040,                 loss: 0.1964
agent1:                 episode reward: 0.2040,                 loss: nan
Episode: 46521/101000 (46.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0027s / 11715.7649 s
agent0:                 episode reward: 0.0411,                 loss: 0.1961
agent1:                 episode reward: -0.0411,                 loss: nan
Episode: 46541/101000 (46.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1601s / 11718.9250 s
agent0:                 episode reward: 0.2982,                 loss: 0.1947
agent1:                 episode reward: -0.2982,                 loss: 0.1926
Score delta: 1.8297367517672067, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/46107_0.
Episode: 46561/101000 (46.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7737s / 11721.6987 s
agent0:                 episode reward: -0.3902,                 loss: nan
agent1:                 episode reward: 0.3902,                 loss: 0.1895
Episode: 46581/101000 (46.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4951s / 11725.1938 s
agent0:                 episode reward: -0.6401,                 loss: nan
agent1:                 episode reward: 0.6401,                 loss: 0.1883
Episode: 46601/101000 (46.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9941s / 11729.1878 s
agent0:                 episode reward: -0.3287,                 loss: nan
agent1:                 episode reward: 0.3287,                 loss: 0.1854
Episode: 46621/101000 (46.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5975s / 11732.7853 s
agent0:                 episode reward: -0.3077,                 loss: nan
agent1:                 episode reward: 0.3077,                 loss: 0.1874
Episode: 46641/101000 (46.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5376s / 11736.3230 s
agent0:                 episode reward: -0.3623,                 loss: nan
agent1:                 episode reward: 0.3623,                 loss: 0.1880
Episode: 46661/101000 (46.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1714s / 11739.4944 s
agent0:                 episode reward: -0.0441,                 loss: nan
agent1:                 episode reward: 0.0441,                 loss: 0.1861
Episode: 46681/101000 (46.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2514s / 11742.7458 s
agent0:                 episode reward: -0.1227,                 loss: nan
agent1:                 episode reward: 0.1227,                 loss: 0.1876
Episode: 46701/101000 (46.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6473s / 11746.3930 s
agent0:                 episode reward: -0.4029,                 loss: nan
agent1:                 episode reward: 0.4029,                 loss: 0.1897
Episode: 46721/101000 (46.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2078s / 11749.6008 s
agent0:                 episode reward: -0.2921,                 loss: 0.1974
agent1:                 episode reward: 0.2921,                 loss: 0.1867
Score delta: 1.539410518800365, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/46289_1.
Episode: 46741/101000 (46.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5571s / 11753.1579 s
agent0:                 episode reward: -0.1643,                 loss: 0.1949
agent1:                 episode reward: 0.1643,                 loss: nan
Episode: 46761/101000 (46.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3956s / 11756.5536 s
agent0:                 episode reward: -0.2214,                 loss: 0.1936
agent1:                 episode reward: 0.2214,                 loss: nan
Episode: 46781/101000 (46.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1123s / 11760.6659 s
agent0:                 episode reward: -0.0128,                 loss: 0.1920
agent1:                 episode reward: 0.0128,                 loss: nan
Episode: 46801/101000 (46.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2790s / 11763.9448 s
agent0:                 episode reward: 0.1372,                 loss: 0.1934
agent1:                 episode reward: -0.1372,                 loss: nan
Episode: 46821/101000 (46.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2769s / 11767.2217 s
agent0:                 episode reward: 0.3489,                 loss: 0.2019
agent1:                 episode reward: -0.3489,                 loss: 0.1880
Score delta: 1.6091726413013085, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/46393_0.
Episode: 46841/101000 (46.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0293s / 11770.2510 s
agent0:                 episode reward: -0.3509,                 loss: nan
agent1:                 episode reward: 0.3509,                 loss: 0.1884
Episode: 46861/101000 (46.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1263s / 11773.3773 s
agent0:                 episode reward: -0.3383,                 loss: nan
agent1:                 episode reward: 0.3383,                 loss: 0.1872
Episode: 46881/101000 (46.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8871s / 11777.2644 s
agent0:                 episode reward: -0.4765,                 loss: nan
agent1:                 episode reward: 0.4765,                 loss: 0.1867
Episode: 46901/101000 (46.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7004s / 11780.9648 s
agent0:                 episode reward: -0.1392,                 loss: nan
agent1:                 episode reward: 0.1392,                 loss: 0.1893
Episode: 46921/101000 (46.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8844s / 11784.8492 s
agent0:                 episode reward: -0.3071,                 loss: 0.2175
agent1:                 episode reward: 0.3071,                 loss: 0.1881
Score delta: 1.576480663854348, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/46490_1.
Episode: 46941/101000 (46.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9108s / 11788.7600 s
agent0:                 episode reward: 0.0344,                 loss: 0.2117
agent1:                 episode reward: -0.0344,                 loss: nan
Episode: 46961/101000 (46.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5107s / 11792.2707 s
agent0:                 episode reward: -0.0799,                 loss: 0.2100
agent1:                 episode reward: 0.0799,                 loss: nan
Episode: 46981/101000 (46.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8299s / 11796.1006 s
agent0:                 episode reward: 0.2021,                 loss: 0.2111
agent1:                 episode reward: -0.2021,                 loss: nan
Episode: 47001/101000 (46.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4917s / 11799.5923 s
agent0:                 episode reward: 0.0587,                 loss: 0.2118
agent1:                 episode reward: -0.0587,                 loss: nan
Episode: 47021/101000 (46.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0311s / 11802.6234 s
agent0:                 episode reward: -0.2045,                 loss: 0.2118
agent1:                 episode reward: 0.2045,                 loss: nan
Episode: 47041/101000 (46.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5985s / 11806.2219 s
agent0:                 episode reward: -0.0111,                 loss: 0.2090
agent1:                 episode reward: 0.0111,                 loss: nan
Episode: 47061/101000 (46.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9185s / 11810.1404 s
agent0:                 episode reward: -0.1751,                 loss: 0.2112
agent1:                 episode reward: 0.1751,                 loss: nan
Episode: 47081/101000 (46.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2352s / 11813.3756 s
agent0:                 episode reward: 0.1663,                 loss: 0.2121
agent1:                 episode reward: -0.1663,                 loss: nan
Episode: 47101/101000 (46.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3618s / 11816.7374 s
agent0:                 episode reward: 0.2002,                 loss: 0.2104
agent1:                 episode reward: -0.2002,                 loss: nan
Episode: 47121/101000 (46.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0030s / 11820.7403 s
agent0:                 episode reward: 0.0868,                 loss: 0.2081
agent1:                 episode reward: -0.0868,                 loss: nan
Episode: 47141/101000 (46.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0487s / 11823.7890 s
agent0:                 episode reward: -0.1726,                 loss: 0.2101
agent1:                 episode reward: 0.1726,                 loss: nan
Episode: 47161/101000 (46.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2099s / 11826.9989 s
agent0:                 episode reward: 0.0269,                 loss: 0.2092
agent1:                 episode reward: -0.0269,                 loss: nan
Episode: 47181/101000 (46.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3854s / 11831.3843 s
agent0:                 episode reward: -0.2021,                 loss: 0.2081
agent1:                 episode reward: 0.2021,                 loss: nan
Episode: 47201/101000 (46.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4121s / 11835.7964 s
agent0:                 episode reward: -0.3500,                 loss: 0.2090
agent1:                 episode reward: 0.3500,                 loss: nan
Episode: 47221/101000 (46.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9765s / 11839.7729 s
agent0:                 episode reward: 0.2937,                 loss: 0.2087
agent1:                 episode reward: -0.2937,                 loss: nan
Episode: 47241/101000 (46.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6215s / 11843.3944 s
agent0:                 episode reward: -0.1466,                 loss: 0.2062
agent1:                 episode reward: 0.1466,                 loss: nan
Episode: 47261/101000 (46.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3291s / 11846.7235 s
agent0:                 episode reward: 0.5234,                 loss: 0.2000
agent1:                 episode reward: -0.5234,                 loss: 0.1923
Score delta: 1.5751271952703696, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/46833_0.
Episode: 47281/101000 (46.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9468s / 11849.6703 s
agent0:                 episode reward: -0.2878,                 loss: nan
agent1:                 episode reward: 0.2878,                 loss: 0.1926
Episode: 47301/101000 (46.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8930s / 11852.5633 s
agent0:                 episode reward: -0.2935,                 loss: nan
agent1:                 episode reward: 0.2935,                 loss: 0.1927
Episode: 47321/101000 (46.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3135s / 11855.8768 s
agent0:                 episode reward: -0.1816,                 loss: nan
agent1:                 episode reward: 0.1816,                 loss: 0.1920
Episode: 47341/101000 (46.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9225s / 11858.7994 s
agent0:                 episode reward: -0.3972,                 loss: nan
agent1:                 episode reward: 0.3972,                 loss: 0.1905
Episode: 47361/101000 (46.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1012s / 11861.9006 s
agent0:                 episode reward: -0.2924,                 loss: nan
agent1:                 episode reward: 0.2924,                 loss: 0.1909
Episode: 47381/101000 (46.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0465s / 11865.9471 s
agent0:                 episode reward: -0.3055,                 loss: nan
agent1:                 episode reward: 0.3055,                 loss: 0.1911
Episode: 47401/101000 (46.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9558s / 11868.9029 s
agent0:                 episode reward: -0.7876,                 loss: 0.2155
agent1:                 episode reward: 0.7876,                 loss: 0.1870
Score delta: 1.5946378159708392, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/46958_1.
Episode: 47421/101000 (46.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7626s / 11872.6655 s
agent0:                 episode reward: -0.0298,                 loss: 0.2121
agent1:                 episode reward: 0.0298,                 loss: nan
Episode: 47441/101000 (46.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2176s / 11875.8831 s
agent0:                 episode reward: -0.6385,                 loss: 0.2120
agent1:                 episode reward: 0.6385,                 loss: nan
Episode: 47461/101000 (46.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4155s / 11879.2986 s
agent0:                 episode reward: 0.0769,                 loss: 0.2117
agent1:                 episode reward: -0.0769,                 loss: nan
Episode: 47481/101000 (47.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0406s / 11883.3392 s
agent0:                 episode reward: -0.2408,                 loss: 0.2088
agent1:                 episode reward: 0.2408,                 loss: nan
Episode: 47501/101000 (47.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5832s / 11886.9224 s
agent0:                 episode reward: 0.2756,                 loss: 0.2108
agent1:                 episode reward: -0.2756,                 loss: 0.2135
Score delta: 1.7272062434046958, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/47071_0.
Episode: 47521/101000 (47.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4637s / 11890.3861 s
agent0:                 episode reward: -0.2107,                 loss: nan
agent1:                 episode reward: 0.2107,                 loss: 0.2122
Episode: 47541/101000 (47.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3732s / 11892.7593 s
agent0:                 episode reward: 0.0805,                 loss: nan
agent1:                 episode reward: -0.0805,                 loss: 0.2112
Episode: 47561/101000 (47.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4139s / 11896.1732 s
agent0:                 episode reward: -0.0415,                 loss: nan
agent1:                 episode reward: 0.0415,                 loss: 0.2131
Episode: 47581/101000 (47.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3718s / 11899.5449 s
agent0:                 episode reward: 0.2341,                 loss: nan
agent1:                 episode reward: -0.2341,                 loss: 0.1956
Episode: 47601/101000 (47.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9566s / 11903.5016 s
agent0:                 episode reward: -0.1732,                 loss: 0.2005
agent1:                 episode reward: 0.1732,                 loss: 0.1867
Score delta: 1.5126420350587808, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/47168_1.
Episode: 47621/101000 (47.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6523s / 11907.1539 s
agent0:                 episode reward: 0.0047,                 loss: 0.1994
agent1:                 episode reward: -0.0047,                 loss: nan
Episode: 47641/101000 (47.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4035s / 11910.5573 s
agent0:                 episode reward: -0.3750,                 loss: 0.1996
agent1:                 episode reward: 0.3750,                 loss: nan
Episode: 47661/101000 (47.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4507s / 11914.0080 s
agent0:                 episode reward: 0.4616,                 loss: 0.1990
agent1:                 episode reward: -0.4616,                 loss: nan
Episode: 47681/101000 (47.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5304s / 11918.5384 s
agent0:                 episode reward: -0.5566,                 loss: 0.2008
agent1:                 episode reward: 0.5566,                 loss: nan
Episode: 47701/101000 (47.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1775s / 11922.7159 s
agent0:                 episode reward: -0.1605,                 loss: 0.1996
agent1:                 episode reward: 0.1605,                 loss: nan
Episode: 47721/101000 (47.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6259s / 11927.3418 s
agent0:                 episode reward: -0.5968,                 loss: 0.1991
agent1:                 episode reward: 0.5968,                 loss: nan
Episode: 47741/101000 (47.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6712s / 11931.0130 s
agent0:                 episode reward: -0.2961,                 loss: 0.1986
agent1:                 episode reward: 0.2961,                 loss: nan
Episode: 47761/101000 (47.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3377s / 11934.3507 s
agent0:                 episode reward: -0.0780,                 loss: 0.2005
agent1:                 episode reward: 0.0780,                 loss: nan
Episode: 47781/101000 (47.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6671s / 11938.0178 s
agent0:                 episode reward: 0.3637,                 loss: 0.2002
agent1:                 episode reward: -0.3637,                 loss: nan
Episode: 47801/101000 (47.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6186s / 11941.6364 s
agent0:                 episode reward: -0.5594,                 loss: 0.1981
agent1:                 episode reward: 0.5594,                 loss: 0.1872
Score delta: 1.5076317201645348, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/47357_0.
Episode: 47821/101000 (47.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1964s / 11944.8328 s
agent0:                 episode reward: -0.2416,                 loss: nan
agent1:                 episode reward: 0.2416,                 loss: 0.1891
Episode: 47841/101000 (47.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5401s / 11948.3729 s
agent0:                 episode reward: 0.0424,                 loss: nan
agent1:                 episode reward: -0.0424,                 loss: 0.1890
Episode: 47861/101000 (47.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5265s / 11951.8994 s
agent0:                 episode reward: -0.2548,                 loss: nan
agent1:                 episode reward: 0.2548,                 loss: 0.1876
Episode: 47881/101000 (47.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1585s / 11955.0579 s
agent0:                 episode reward: -0.6965,                 loss: nan
agent1:                 episode reward: 0.6965,                 loss: 0.1871
Episode: 47901/101000 (47.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8017s / 11958.8596 s
agent0:                 episode reward: -0.0792,                 loss: 0.2049
agent1:                 episode reward: 0.0792,                 loss: 0.1873
Score delta: 1.879023292740051, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/47458_1.
Episode: 47921/101000 (47.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1347s / 11961.9943 s
agent0:                 episode reward: -0.2697,                 loss: 0.2025
agent1:                 episode reward: 0.2697,                 loss: nan
Episode: 47941/101000 (47.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0515s / 11965.0458 s
agent0:                 episode reward: 0.2582,                 loss: 0.2014
agent1:                 episode reward: -0.2582,                 loss: nan
Episode: 47961/101000 (47.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9207s / 11968.9665 s
agent0:                 episode reward: -0.0888,                 loss: 0.2016
agent1:                 episode reward: 0.0888,                 loss: nan
Episode: 47981/101000 (47.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8654s / 11971.8319 s
agent0:                 episode reward: -0.0469,                 loss: 0.2016
agent1:                 episode reward: 0.0469,                 loss: nan
Episode: 48001/101000 (47.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9664s / 11975.7983 s
agent0:                 episode reward: 0.2335,                 loss: 0.2010
agent1:                 episode reward: -0.2335,                 loss: nan
Episode: 48021/101000 (47.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5516s / 11980.3499 s
agent0:                 episode reward: 0.0153,                 loss: 0.2026
agent1:                 episode reward: -0.0153,                 loss: nan
Episode: 48041/101000 (47.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7078s / 11984.0577 s
agent0:                 episode reward: 0.3096,                 loss: 0.2012
agent1:                 episode reward: -0.3096,                 loss: nan
Episode: 48061/101000 (47.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2561s / 11987.3138 s
agent0:                 episode reward: 0.4761,                 loss: 0.2003
agent1:                 episode reward: -0.4761,                 loss: 0.1946
Score delta: 1.6322085810746554, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/47631_0.
Episode: 48081/101000 (47.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3481s / 11990.6619 s
agent0:                 episode reward: -0.2982,                 loss: nan
agent1:                 episode reward: 0.2982,                 loss: 0.1913
Episode: 48101/101000 (47.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5883s / 11994.2502 s
agent0:                 episode reward: 0.2472,                 loss: nan
agent1:                 episode reward: -0.2472,                 loss: 0.1915
Episode: 48121/101000 (47.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9675s / 11998.2177 s
agent0:                 episode reward: -0.8320,                 loss: nan
agent1:                 episode reward: 0.8320,                 loss: 0.1925
Episode: 48141/101000 (47.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5257s / 12001.7434 s
agent0:                 episode reward: -0.3421,                 loss: nan
agent1:                 episode reward: 0.3421,                 loss: 0.1907
Episode: 48161/101000 (47.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2903s / 12005.0337 s
agent0:                 episode reward: -0.3695,                 loss: nan
agent1:                 episode reward: 0.3695,                 loss: 0.1904
Episode: 48181/101000 (47.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8995s / 12008.9332 s
agent0:                 episode reward: -0.0516,                 loss: 0.1955
agent1:                 episode reward: 0.0516,                 loss: 0.1936
Score delta: 1.7153392634337021, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/47739_1.
Episode: 48201/101000 (47.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0957s / 12013.0289 s
agent0:                 episode reward: 0.4134,                 loss: 0.1964
agent1:                 episode reward: -0.4134,                 loss: nan
Episode: 48221/101000 (47.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1348s / 12017.1637 s
agent0:                 episode reward: -0.2195,                 loss: 0.1947
agent1:                 episode reward: 0.2195,                 loss: nan
Episode: 48241/101000 (47.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0089s / 12021.1726 s
agent0:                 episode reward: 0.1027,                 loss: 0.1944
agent1:                 episode reward: -0.1027,                 loss: nan
Episode: 48261/101000 (47.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8921s / 12024.0648 s
agent0:                 episode reward: 0.0095,                 loss: 0.1951
agent1:                 episode reward: -0.0095,                 loss: nan
Episode: 48281/101000 (47.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8446s / 12028.9094 s
agent0:                 episode reward: 0.1841,                 loss: 0.1934
agent1:                 episode reward: -0.1841,                 loss: nan
Episode: 48301/101000 (47.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1964s / 12032.1058 s
agent0:                 episode reward: 0.2388,                 loss: 0.1917
agent1:                 episode reward: -0.2388,                 loss: nan
Episode: 48321/101000 (47.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4584s / 12035.5642 s
agent0:                 episode reward: 0.4059,                 loss: 0.1919
agent1:                 episode reward: -0.4059,                 loss: nan
Score delta: 1.8656126425864197, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/47895_0.
Episode: 48341/101000 (47.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8018s / 12039.3660 s
agent0:                 episode reward: -0.3977,                 loss: nan
agent1:                 episode reward: 0.3977,                 loss: 0.1921
Episode: 48361/101000 (47.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2115s / 12042.5774 s
agent0:                 episode reward: -0.2944,                 loss: nan
agent1:                 episode reward: 0.2944,                 loss: 0.1916
Episode: 48381/101000 (47.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1209s / 12045.6983 s
agent0:                 episode reward: -0.1268,                 loss: nan
agent1:                 episode reward: 0.1268,                 loss: 0.1904
Episode: 48401/101000 (47.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9274s / 12048.6258 s
agent0:                 episode reward: -0.3263,                 loss: nan
agent1:                 episode reward: 0.3263,                 loss: 0.1919
Episode: 48421/101000 (47.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3036s / 12051.9294 s
agent0:                 episode reward: -0.6659,                 loss: nan
agent1:                 episode reward: 0.6659,                 loss: 0.1896
Episode: 48441/101000 (47.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0703s / 12054.9997 s
agent0:                 episode reward: 0.0701,                 loss: nan
agent1:                 episode reward: -0.0701,                 loss: 0.1910
Episode: 48461/101000 (47.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4119s / 12058.4116 s
agent0:                 episode reward: -0.4545,                 loss: 0.2117
agent1:                 episode reward: 0.4545,                 loss: 0.1912
Score delta: 1.6431318688152245, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/48023_1.
Episode: 48481/101000 (48.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5603s / 12061.9719 s
agent0:                 episode reward: -0.0273,                 loss: 0.2126
agent1:                 episode reward: 0.0273,                 loss: nan
Episode: 48501/101000 (48.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2709s / 12066.2427 s
agent0:                 episode reward: 0.4128,                 loss: 0.2095
agent1:                 episode reward: -0.4128,                 loss: nan
Episode: 48521/101000 (48.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9218s / 12070.1646 s
agent0:                 episode reward: -0.2934,                 loss: 0.2101
agent1:                 episode reward: 0.2934,                 loss: nan
Episode: 48541/101000 (48.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4842s / 12073.6488 s
agent0:                 episode reward: 0.0188,                 loss: 0.2118
agent1:                 episode reward: -0.0188,                 loss: nan
Episode: 48561/101000 (48.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9513s / 12077.6001 s
agent0:                 episode reward: 0.3123,                 loss: 0.2097
agent1:                 episode reward: -0.3123,                 loss: nan
Episode: 48581/101000 (48.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0051s / 12080.6052 s
agent0:                 episode reward: -0.1787,                 loss: 0.2096
agent1:                 episode reward: 0.1787,                 loss: nan
Episode: 48601/101000 (48.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1350s / 12084.7402 s
agent0:                 episode reward: 0.3255,                 loss: 0.2117
agent1:                 episode reward: -0.3255,                 loss: nan
Episode: 48621/101000 (48.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8138s / 12088.5541 s
agent0:                 episode reward: 0.1062,                 loss: 0.2109
agent1:                 episode reward: -0.1062,                 loss: nan
Episode: 48641/101000 (48.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8010s / 12092.3551 s
agent0:                 episode reward: -0.2756,                 loss: 0.2099
agent1:                 episode reward: 0.2756,                 loss: nan
Episode: 48661/101000 (48.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2634s / 12095.6185 s
agent0:                 episode reward: 0.2365,                 loss: 0.2079
agent1:                 episode reward: -0.2365,                 loss: nan
Episode: 48681/101000 (48.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5424s / 12100.1609 s
agent0:                 episode reward: 0.0565,                 loss: 0.2100
agent1:                 episode reward: -0.0565,                 loss: nan
Episode: 48701/101000 (48.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6178s / 12103.7787 s
agent0:                 episode reward: 0.2024,                 loss: 0.2099
agent1:                 episode reward: -0.2024,                 loss: nan
Episode: 48721/101000 (48.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5517s / 12107.3303 s
agent0:                 episode reward: 0.3095,                 loss: 0.2108
agent1:                 episode reward: -0.3095,                 loss: nan
Episode: 48741/101000 (48.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2533s / 12111.5836 s
agent0:                 episode reward: 0.3266,                 loss: 0.2083
agent1:                 episode reward: -0.3266,                 loss: 0.2178
Score delta: 1.582761738237699, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/48307_0.
Episode: 48761/101000 (48.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9660s / 12114.5496 s
agent0:                 episode reward: -0.3020,                 loss: nan
agent1:                 episode reward: 0.3020,                 loss: 0.2134
Episode: 48781/101000 (48.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8165s / 12118.3661 s
agent0:                 episode reward: -0.5890,                 loss: nan
agent1:                 episode reward: 0.5890,                 loss: 0.2136
Episode: 48801/101000 (48.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0388s / 12121.4049 s
agent0:                 episode reward: -0.3806,                 loss: nan
agent1:                 episode reward: 0.3806,                 loss: 0.2130
Episode: 48821/101000 (48.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5970s / 12125.0019 s
agent0:                 episode reward: -0.1215,                 loss: nan
agent1:                 episode reward: 0.1215,                 loss: 0.2127
Episode: 48841/101000 (48.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9029s / 12127.9047 s
agent0:                 episode reward: -0.7551,                 loss: 0.2573
agent1:                 episode reward: 0.7551,                 loss: 0.2145
Score delta: 1.8565455938651163, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/48405_1.
Episode: 48861/101000 (48.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6607s / 12132.5654 s
agent0:                 episode reward: -0.1332,                 loss: 0.2402
agent1:                 episode reward: 0.1332,                 loss: nan
Episode: 48881/101000 (48.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9713s / 12136.5367 s
agent0:                 episode reward: -0.2464,                 loss: 0.2344
agent1:                 episode reward: 0.2464,                 loss: nan
Episode: 48901/101000 (48.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3387s / 12139.8754 s
agent0:                 episode reward: -0.1074,                 loss: 0.2135
agent1:                 episode reward: 0.1074,                 loss: nan
Episode: 48921/101000 (48.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6539s / 12142.5293 s
agent0:                 episode reward: -0.3507,                 loss: 0.1922
agent1:                 episode reward: 0.3507,                 loss: nan
Episode: 48941/101000 (48.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0368s / 12146.5661 s
agent0:                 episode reward: 0.1785,                 loss: 0.1897
agent1:                 episode reward: -0.1785,                 loss: 0.1721
Score delta: 1.7280407741014259, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/48506_0.
Episode: 48961/101000 (48.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1626s / 12149.7287 s
agent0:                 episode reward: -0.3690,                 loss: nan
agent1:                 episode reward: 0.3690,                 loss: 0.1698
Episode: 48981/101000 (48.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4318s / 12153.1605 s
agent0:                 episode reward: -0.4396,                 loss: nan
agent1:                 episode reward: 0.4396,                 loss: 0.1672
Episode: 49001/101000 (48.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8729s / 12157.0335 s
agent0:                 episode reward: -0.7098,                 loss: nan
agent1:                 episode reward: 0.7098,                 loss: 0.1666
Episode: 49021/101000 (48.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7803s / 12160.8138 s
agent0:                 episode reward: -0.6020,                 loss: nan
agent1:                 episode reward: 0.6020,                 loss: 0.1674
Episode: 49041/101000 (48.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1834s / 12164.9971 s
agent0:                 episode reward: -0.6018,                 loss: 0.2452
agent1:                 episode reward: 0.6018,                 loss: 0.1664
Score delta: 1.6912853425148018, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/48606_1.
Episode: 49061/101000 (48.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0341s / 12168.0312 s
agent0:                 episode reward: 0.2952,                 loss: 0.2445
agent1:                 episode reward: -0.2952,                 loss: nan
Episode: 49081/101000 (48.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3534s / 12171.3846 s
agent0:                 episode reward: 0.3550,                 loss: 0.2433
agent1:                 episode reward: -0.3550,                 loss: nan
Episode: 49101/101000 (48.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1644s / 12175.5490 s
agent0:                 episode reward: 0.1756,                 loss: 0.2443
agent1:                 episode reward: -0.1756,                 loss: nan
Episode: 49121/101000 (48.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1103s / 12179.6593 s
agent0:                 episode reward: 0.0903,                 loss: 0.2440
agent1:                 episode reward: -0.0903,                 loss: nan
Episode: 49141/101000 (48.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0364s / 12182.6957 s
agent0:                 episode reward: 0.5470,                 loss: 0.2442
agent1:                 episode reward: -0.5470,                 loss: nan
Episode: 49161/101000 (48.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2800s / 12186.9757 s
agent0:                 episode reward: -0.1769,                 loss: 0.2410
agent1:                 episode reward: 0.1769,                 loss: nan
Episode: 49181/101000 (48.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7256s / 12191.7013 s
agent0:                 episode reward: -0.1609,                 loss: 0.2418
agent1:                 episode reward: 0.1609,                 loss: nan
Episode: 49201/101000 (48.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2789s / 12195.9802 s
agent0:                 episode reward: 0.3020,                 loss: 0.2426
agent1:                 episode reward: -0.3020,                 loss: 0.2142
Score delta: 1.5675039675328555, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/48765_0.
Episode: 49221/101000 (48.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6930s / 12199.6732 s
agent0:                 episode reward: -0.7051,                 loss: nan
agent1:                 episode reward: 0.7051,                 loss: 0.2148
Episode: 49241/101000 (48.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6654s / 12202.3386 s
agent0:                 episode reward: -0.1770,                 loss: nan
agent1:                 episode reward: 0.1770,                 loss: 0.2131
Episode: 49261/101000 (48.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4496s / 12205.7882 s
agent0:                 episode reward: -0.6000,                 loss: nan
agent1:                 episode reward: 0.6000,                 loss: 0.2129
Episode: 49281/101000 (48.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3977s / 12209.1858 s
agent0:                 episode reward: -0.0697,                 loss: nan
agent1:                 episode reward: 0.0697,                 loss: 0.2114
Episode: 49301/101000 (48.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5764s / 12213.7623 s
agent0:                 episode reward: -0.1148,                 loss: nan
agent1:                 episode reward: 0.1148,                 loss: 0.2075
Episode: 49321/101000 (48.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7544s / 12217.5167 s
agent0:                 episode reward: -0.0240,                 loss: nan
agent1:                 episode reward: 0.0240,                 loss: 0.1942
Episode: 49341/101000 (48.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7448s / 12220.2615 s
agent0:                 episode reward: -0.5463,                 loss: nan
agent1:                 episode reward: 0.5463,                 loss: 0.1921
Episode: 49361/101000 (48.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6049s / 12223.8664 s
agent0:                 episode reward: 0.3442,                 loss: 0.1931
agent1:                 episode reward: -0.3442,                 loss: 0.1931
Score delta: 1.7072836342692657, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/48916_1.
Episode: 49381/101000 (48.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0799s / 12227.9463 s
agent0:                 episode reward: 0.4420,                 loss: 0.1938
agent1:                 episode reward: -0.4420,                 loss: nan
Episode: 49401/101000 (48.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9387s / 12230.8851 s
agent0:                 episode reward: 0.5510,                 loss: 0.1935
agent1:                 episode reward: -0.5510,                 loss: nan
Episode: 49421/101000 (48.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3407s / 12235.2258 s
agent0:                 episode reward: 0.0047,                 loss: 0.1927
agent1:                 episode reward: -0.0047,                 loss: nan
Episode: 49441/101000 (48.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9159s / 12239.1417 s
agent0:                 episode reward: 0.3127,                 loss: 0.1921
agent1:                 episode reward: -0.3127,                 loss: nan
Episode: 49461/101000 (48.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1188s / 12243.2604 s
agent0:                 episode reward: -0.2768,                 loss: 0.1926
agent1:                 episode reward: 0.2768,                 loss: nan
Episode: 49481/101000 (48.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3323s / 12246.5927 s
agent0:                 episode reward: 0.5515,                 loss: 0.1998
agent1:                 episode reward: -0.5515,                 loss: nan
Episode: 49501/101000 (49.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4559s / 12250.0486 s
agent0:                 episode reward: -0.0779,                 loss: 0.2114
agent1:                 episode reward: 0.0779,                 loss: 0.1892
Score delta: 1.5223507221368666, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/49059_0.
Episode: 49521/101000 (49.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1189s / 12253.1675 s
agent0:                 episode reward: -0.4608,                 loss: nan
agent1:                 episode reward: 0.4608,                 loss: 0.1903
Episode: 49541/101000 (49.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1176s / 12256.2852 s
agent0:                 episode reward: -0.7248,                 loss: nan
agent1:                 episode reward: 0.7248,                 loss: 0.1917
Episode: 49561/101000 (49.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2030s / 12259.4882 s
agent0:                 episode reward: -0.3153,                 loss: nan
agent1:                 episode reward: 0.3153,                 loss: 0.1894
Episode: 49581/101000 (49.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9730s / 12263.4611 s
agent0:                 episode reward: 0.0667,                 loss: nan
agent1:                 episode reward: -0.0667,                 loss: 0.1874
Episode: 49601/101000 (49.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1241s / 12267.5853 s
agent0:                 episode reward: -0.0944,                 loss: nan
agent1:                 episode reward: 0.0944,                 loss: 0.1887
Episode: 49621/101000 (49.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2070s / 12270.7923 s
agent0:                 episode reward: -0.4891,                 loss: nan
agent1:                 episode reward: 0.4891,                 loss: 0.1901
Episode: 49641/101000 (49.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4472s / 12274.2395 s
agent0:                 episode reward: -0.0807,                 loss: nan
agent1:                 episode reward: 0.0807,                 loss: 0.1880
Episode: 49661/101000 (49.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3513s / 12277.5908 s
agent0:                 episode reward: 0.0439,                 loss: 0.2128
agent1:                 episode reward: -0.0439,                 loss: 0.1896
Score delta: 1.6648814684376947, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/49224_1.
Episode: 49681/101000 (49.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1490s / 12281.7398 s
agent0:                 episode reward: 0.2026,                 loss: 0.2101
agent1:                 episode reward: -0.2026,                 loss: nan
Episode: 49701/101000 (49.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1407s / 12285.8805 s
agent0:                 episode reward: 0.0298,                 loss: 0.2121
agent1:                 episode reward: -0.0298,                 loss: nan
Episode: 49721/101000 (49.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8069s / 12289.6873 s
agent0:                 episode reward: 0.2414,                 loss: 0.2101
agent1:                 episode reward: -0.2414,                 loss: nan
Episode: 49741/101000 (49.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1781s / 12293.8654 s
agent0:                 episode reward: -0.1716,                 loss: 0.2092
agent1:                 episode reward: 0.1716,                 loss: nan
Episode: 49761/101000 (49.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0810s / 12297.9464 s
agent0:                 episode reward: 0.4121,                 loss: 0.2088
agent1:                 episode reward: -0.4121,                 loss: nan
Episode: 49781/101000 (49.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4339s / 12302.3803 s
agent0:                 episode reward: -0.6852,                 loss: 0.2089
agent1:                 episode reward: 0.6852,                 loss: nan
Episode: 49801/101000 (49.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6209s / 12307.0011 s
agent0:                 episode reward: -0.0539,                 loss: 0.2094
agent1:                 episode reward: 0.0539,                 loss: nan
Episode: 49821/101000 (49.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6812s / 12310.6823 s
agent0:                 episode reward: 0.0627,                 loss: 0.2084
agent1:                 episode reward: -0.0627,                 loss: nan
Episode: 49841/101000 (49.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5738s / 12314.2561 s
agent0:                 episode reward: -0.1617,                 loss: 0.2085
agent1:                 episode reward: 0.1617,                 loss: nan
Episode: 49861/101000 (49.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8280s / 12318.0841 s
agent0:                 episode reward: 0.0658,                 loss: 0.2078
agent1:                 episode reward: -0.0658,                 loss: nan
Episode: 49881/101000 (49.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7918s / 12321.8759 s
agent0:                 episode reward: -0.0133,                 loss: 0.2087
agent1:                 episode reward: 0.0133,                 loss: nan
Episode: 49901/101000 (49.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2690s / 12325.1449 s
agent0:                 episode reward: 0.1399,                 loss: 0.2084
agent1:                 episode reward: -0.1399,                 loss: 0.1930
Score delta: 1.646227188147942, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/49468_0.
Episode: 49921/101000 (49.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1472s / 12328.2921 s
agent0:                 episode reward: -0.3827,                 loss: nan
agent1:                 episode reward: 0.3827,                 loss: 0.1925
Episode: 49941/101000 (49.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6890s / 12331.9811 s
agent0:                 episode reward: 0.0811,                 loss: nan
agent1:                 episode reward: -0.0811,                 loss: 0.1917
Episode: 49961/101000 (49.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7674s / 12334.7485 s
agent0:                 episode reward: -0.2878,                 loss: nan
agent1:                 episode reward: 0.2878,                 loss: 0.1911
Episode: 49981/101000 (49.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4976s / 12338.2461 s
agent0:                 episode reward: -0.3363,                 loss: nan
agent1:                 episode reward: 0.3363,                 loss: 0.1908
Episode: 50001/101000 (49.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0554s / 12341.3015 s
agent0:                 episode reward: -0.6361,                 loss: nan
agent1:                 episode reward: 0.6361,                 loss: 0.1902
Episode: 50021/101000 (49.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5851s / 12344.8866 s
agent0:                 episode reward: -0.1421,                 loss: nan
agent1:                 episode reward: 0.1421,                 loss: 0.1912
Episode: 50041/101000 (49.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9860s / 12347.8726 s
agent0:                 episode reward: -0.8595,                 loss: 0.2989
agent1:                 episode reward: 0.8595,                 loss: 0.1916
Score delta: 1.6687020656356133, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/49610_1.
Episode: 50061/101000 (49.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3454s / 12352.2181 s
agent0:                 episode reward: -0.3591,                 loss: 0.2613
agent1:                 episode reward: 0.3591,                 loss: nan
Episode: 50081/101000 (49.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8796s / 12356.0976 s
agent0:                 episode reward: -0.0846,                 loss: 0.2587
agent1:                 episode reward: 0.0846,                 loss: nan
Episode: 50101/101000 (49.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9168s / 12359.0144 s
agent0:                 episode reward: -0.1158,                 loss: 0.2554
agent1:                 episode reward: 0.1158,                 loss: nan
Episode: 50121/101000 (49.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0312s / 12362.0456 s
agent0:                 episode reward: -0.2250,                 loss: 0.2373
agent1:                 episode reward: 0.2250,                 loss: nan
Episode: 50141/101000 (49.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6471s / 12365.6927 s
agent0:                 episode reward: -0.1710,                 loss: 0.1970
agent1:                 episode reward: 0.1710,                 loss: nan
Episode: 50161/101000 (49.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7893s / 12369.4821 s
agent0:                 episode reward: 0.2101,                 loss: 0.1949
agent1:                 episode reward: -0.2101,                 loss: nan
Episode: 50181/101000 (49.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4318s / 12373.9138 s
agent0:                 episode reward: -0.2719,                 loss: 0.1950
agent1:                 episode reward: 0.2719,                 loss: nan
Episode: 50201/101000 (49.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8124s / 12377.7263 s
agent0:                 episode reward: 0.1121,                 loss: 0.1943
agent1:                 episode reward: -0.1121,                 loss: nan
Episode: 50221/101000 (49.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2296s / 12380.9558 s
agent0:                 episode reward: -0.2019,                 loss: 0.1928
agent1:                 episode reward: 0.2019,                 loss: nan
Episode: 50241/101000 (49.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0512s / 12385.0070 s
agent0:                 episode reward: -0.2237,                 loss: 0.1959
agent1:                 episode reward: 0.2237,                 loss: nan
Episode: 50261/101000 (49.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1359s / 12389.1429 s
agent0:                 episode reward: -0.2128,                 loss: 0.1911
agent1:                 episode reward: 0.2128,                 loss: nan
Episode: 50281/101000 (49.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7635s / 12391.9064 s
agent0:                 episode reward: 0.1884,                 loss: 0.1927
agent1:                 episode reward: -0.1884,                 loss: nan
Episode: 50301/101000 (49.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8734s / 12395.7799 s
agent0:                 episode reward: 0.0201,                 loss: 0.1913
agent1:                 episode reward: -0.0201,                 loss: nan
Episode: 50321/101000 (49.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4161s / 12399.1960 s
agent0:                 episode reward: -0.0296,                 loss: 0.1898
agent1:                 episode reward: 0.0296,                 loss: nan
Episode: 50341/101000 (49.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3076s / 12403.5036 s
agent0:                 episode reward: 0.2321,                 loss: 0.1924
agent1:                 episode reward: -0.2321,                 loss: nan
Episode: 50361/101000 (49.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7410s / 12407.2446 s
agent0:                 episode reward: 0.1529,                 loss: 0.1915
agent1:                 episode reward: -0.1529,                 loss: nan
Episode: 50381/101000 (49.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6879s / 12410.9325 s
agent0:                 episode reward: 0.0599,                 loss: 0.1933
agent1:                 episode reward: -0.0599,                 loss: nan
Episode: 50401/101000 (49.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1348s / 12415.0673 s
agent0:                 episode reward: -0.0622,                 loss: 0.1890
agent1:                 episode reward: 0.0622,                 loss: nan
Episode: 50421/101000 (49.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0482s / 12419.1155 s
agent0:                 episode reward: 0.1537,                 loss: 0.1917
agent1:                 episode reward: -0.1537,                 loss: nan
Episode: 50441/101000 (49.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6616s / 12423.7771 s
agent0:                 episode reward: -0.6789,                 loss: 0.1931
agent1:                 episode reward: 0.6789,                 loss: nan
Episode: 50461/101000 (49.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0991s / 12427.8762 s
agent0:                 episode reward: 0.3307,                 loss: 0.2001
agent1:                 episode reward: -0.3307,                 loss: nan
Episode: 50481/101000 (49.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6098s / 12431.4861 s
agent0:                 episode reward: -0.0099,                 loss: 0.2034
agent1:                 episode reward: 0.0099,                 loss: nan
Episode: 50501/101000 (50.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3407s / 12434.8268 s
agent0:                 episode reward: -0.3963,                 loss: 0.2029
agent1:                 episode reward: 0.3963,                 loss: nan
Episode: 50521/101000 (50.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0987s / 12438.9255 s
agent0:                 episode reward: 0.0358,                 loss: 0.2042
agent1:                 episode reward: -0.0358,                 loss: nan
Episode: 50541/101000 (50.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9243s / 12442.8497 s
agent0:                 episode reward: -0.7982,                 loss: 0.2052
agent1:                 episode reward: 0.7982,                 loss: nan
Episode: 50561/101000 (50.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1559s / 12446.0056 s
agent0:                 episode reward: 0.1178,                 loss: 0.2031
agent1:                 episode reward: -0.1178,                 loss: nan
Episode: 50581/101000 (50.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7409s / 12449.7465 s
agent0:                 episode reward: -0.3846,                 loss: 0.2024
agent1:                 episode reward: 0.3846,                 loss: nan
Episode: 50601/101000 (50.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2990s / 12454.0454 s
agent0:                 episode reward: 0.3564,                 loss: 0.2038
agent1:                 episode reward: -0.3564,                 loss: nan
Episode: 50621/101000 (50.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8184s / 12456.8638 s
agent0:                 episode reward: 0.2241,                 loss: 0.2027
agent1:                 episode reward: -0.2241,                 loss: nan
Episode: 50641/101000 (50.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6914s / 12460.5552 s
agent0:                 episode reward: -0.1980,                 loss: 0.2037
agent1:                 episode reward: 0.1980,                 loss: nan
Episode: 50661/101000 (50.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5887s / 12464.1440 s
agent0:                 episode reward: 0.4796,                 loss: 0.2024
agent1:                 episode reward: -0.4796,                 loss: 0.1955
Score delta: 1.611434080829714, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/50228_0.
Episode: 50681/101000 (50.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7757s / 12466.9197 s
agent0:                 episode reward: -0.4591,                 loss: nan
agent1:                 episode reward: 0.4591,                 loss: 0.1934
Episode: 50701/101000 (50.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9166s / 12469.8363 s
agent0:                 episode reward: -0.6639,                 loss: nan
agent1:                 episode reward: 0.6639,                 loss: 0.1934
Episode: 50721/101000 (50.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2121s / 12474.0483 s
agent0:                 episode reward: 0.0650,                 loss: nan
agent1:                 episode reward: -0.0650,                 loss: 0.1949
Episode: 50741/101000 (50.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8711s / 12477.9195 s
agent0:                 episode reward: -0.6353,                 loss: nan
agent1:                 episode reward: 0.6353,                 loss: 0.1942
Episode: 50761/101000 (50.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0678s / 12481.9873 s
agent0:                 episode reward: -0.4720,                 loss: nan
agent1:                 episode reward: 0.4720,                 loss: 0.1929
Episode: 50781/101000 (50.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4102s / 12485.3975 s
agent0:                 episode reward: -0.2744,                 loss: nan
agent1:                 episode reward: 0.2744,                 loss: 0.1947
Episode: 50801/101000 (50.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4618s / 12488.8592 s
agent0:                 episode reward: -0.3132,                 loss: nan
agent1:                 episode reward: 0.3132,                 loss: 0.1933
Score delta: 1.5033439637332573, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/50375_1.
Episode: 50821/101000 (50.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0516s / 12491.9109 s
agent0:                 episode reward: -0.3588,                 loss: 0.2046
agent1:                 episode reward: 0.3588,                 loss: nan
Episode: 50841/101000 (50.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4116s / 12495.3224 s
agent0:                 episode reward: 0.4375,                 loss: 0.2006
agent1:                 episode reward: -0.4375,                 loss: nan
Episode: 50861/101000 (50.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2902s / 12498.6127 s
agent0:                 episode reward: 0.0626,                 loss: 0.1991
agent1:                 episode reward: -0.0626,                 loss: nan
Episode: 50881/101000 (50.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7013s / 12502.3139 s
agent0:                 episode reward: -0.0059,                 loss: 0.2000
agent1:                 episode reward: 0.0059,                 loss: nan
Episode: 50901/101000 (50.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0530s / 12505.3669 s
agent0:                 episode reward: 0.3003,                 loss: 0.2004
agent1:                 episode reward: -0.3003,                 loss: nan
Episode: 50921/101000 (50.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8542s / 12508.2211 s
agent0:                 episode reward: -0.3523,                 loss: 0.2002
agent1:                 episode reward: 0.3523,                 loss: nan
Episode: 50941/101000 (50.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7334s / 12511.9545 s
agent0:                 episode reward: -0.1972,                 loss: 0.2031
agent1:                 episode reward: 0.1972,                 loss: nan
Episode: 50961/101000 (50.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2009s / 12515.1554 s
agent0:                 episode reward: -0.4547,                 loss: 0.2043
agent1:                 episode reward: 0.4547,                 loss: nan
Episode: 50981/101000 (50.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3881s / 12519.5435 s
agent0:                 episode reward: 0.2101,                 loss: 0.2024
agent1:                 episode reward: -0.2101,                 loss: nan
Episode: 51001/101000 (50.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0586s / 12523.6021 s
agent0:                 episode reward: 0.0051,                 loss: 0.2046
agent1:                 episode reward: -0.0051,                 loss: 0.1909
Score delta: 1.6515477816607085, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/50564_0.
Episode: 51021/101000 (50.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1290s / 12526.7311 s
agent0:                 episode reward: -0.1796,                 loss: nan
agent1:                 episode reward: 0.1796,                 loss: 0.1878
Episode: 51041/101000 (50.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8602s / 12529.5912 s
agent0:                 episode reward: 0.0167,                 loss: nan
agent1:                 episode reward: -0.0167,                 loss: 0.1917
Episode: 51061/101000 (50.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1122s / 12532.7034 s
agent0:                 episode reward: -0.5147,                 loss: nan
agent1:                 episode reward: 0.5147,                 loss: 0.1894
Episode: 51081/101000 (50.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5804s / 12536.2838 s
agent0:                 episode reward: -0.3106,                 loss: nan
agent1:                 episode reward: 0.3106,                 loss: 0.1881
Episode: 51101/101000 (50.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2896s / 12539.5734 s
agent0:                 episode reward: -0.1983,                 loss: nan
agent1:                 episode reward: 0.1983,                 loss: 0.1886
Episode: 51121/101000 (50.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6139s / 12544.1873 s
agent0:                 episode reward: -0.6440,                 loss: 0.2962
agent1:                 episode reward: 0.6440,                 loss: 0.1877
Score delta: 1.5775500288318451, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/50693_1.
Episode: 51141/101000 (50.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4406s / 12547.6279 s
agent0:                 episode reward: -0.2116,                 loss: 0.2800
agent1:                 episode reward: 0.2116,                 loss: nan
Episode: 51161/101000 (50.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3872s / 12552.0151 s
agent0:                 episode reward: 0.3586,                 loss: 0.2779
agent1:                 episode reward: -0.3586,                 loss: nan
Episode: 51181/101000 (50.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5869s / 12555.6019 s
agent0:                 episode reward: -0.1450,                 loss: 0.2768
agent1:                 episode reward: 0.1450,                 loss: nan
Episode: 51201/101000 (50.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7588s / 12559.3608 s
agent0:                 episode reward: 0.3723,                 loss: 0.2775
agent1:                 episode reward: -0.3723,                 loss: nan
Episode: 51221/101000 (50.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4385s / 12562.7993 s
agent0:                 episode reward: -0.2901,                 loss: 0.2763
agent1:                 episode reward: 0.2901,                 loss: nan
Episode: 51241/101000 (50.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1585s / 12566.9578 s
agent0:                 episode reward: -0.0399,                 loss: 0.2774
agent1:                 episode reward: 0.0399,                 loss: nan
Episode: 51261/101000 (50.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6661s / 12569.6239 s
agent0:                 episode reward: -0.0074,                 loss: 0.2753
agent1:                 episode reward: 0.0074,                 loss: nan
Episode: 51281/101000 (50.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8587s / 12573.4826 s
agent0:                 episode reward: -0.0734,                 loss: 0.2757
agent1:                 episode reward: 0.0734,                 loss: nan
Episode: 51301/101000 (50.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4424s / 12576.9250 s
agent0:                 episode reward: 0.4372,                 loss: 0.2745
agent1:                 episode reward: -0.4372,                 loss: 0.1987
Score delta: 1.9042618707920547, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/50868_0.
Episode: 51321/101000 (50.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6638s / 12580.5888 s
agent0:                 episode reward: 0.0743,                 loss: nan
agent1:                 episode reward: -0.0743,                 loss: 0.1958
Episode: 51341/101000 (50.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7385s / 12584.3273 s
agent0:                 episode reward: -0.4174,                 loss: nan
agent1:                 episode reward: 0.4174,                 loss: 0.1930
Episode: 51361/101000 (50.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9987s / 12587.3261 s
agent0:                 episode reward: -0.5796,                 loss: nan
agent1:                 episode reward: 0.5796,                 loss: 0.1905
Episode: 51381/101000 (50.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5280s / 12590.8541 s
agent0:                 episode reward: -0.2941,                 loss: nan
agent1:                 episode reward: 0.2941,                 loss: 0.1912
Episode: 51401/101000 (50.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8146s / 12594.6687 s
agent0:                 episode reward: -0.6035,                 loss: nan
agent1:                 episode reward: 0.6035,                 loss: 0.1916
Episode: 51421/101000 (50.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5313s / 12598.1999 s
agent0:                 episode reward: -0.2924,                 loss: 0.2100
agent1:                 episode reward: 0.2924,                 loss: 0.1932
Score delta: 1.9856042382592776, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/50982_1.
Episode: 51441/101000 (50.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3858s / 12601.5857 s
agent0:                 episode reward: 0.1562,                 loss: 0.2079
agent1:                 episode reward: -0.1562,                 loss: nan
Episode: 51461/101000 (50.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0988s / 12604.6845 s
agent0:                 episode reward: 0.1902,                 loss: 0.2070
agent1:                 episode reward: -0.1902,                 loss: nan
Episode: 51481/101000 (50.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8520s / 12608.5365 s
agent0:                 episode reward: -0.3955,                 loss: 0.2090
agent1:                 episode reward: 0.3955,                 loss: nan
Episode: 51501/101000 (50.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8540s / 12612.3905 s
agent0:                 episode reward: -0.2179,                 loss: 0.2076
agent1:                 episode reward: 0.2179,                 loss: nan
Episode: 51521/101000 (51.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4546s / 12615.8452 s
agent0:                 episode reward: 0.0709,                 loss: 0.2061
agent1:                 episode reward: -0.0709,                 loss: nan
Episode: 51541/101000 (51.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9389s / 12618.7840 s
agent0:                 episode reward: -0.1209,                 loss: 0.2084
agent1:                 episode reward: 0.1209,                 loss: nan
Episode: 51561/101000 (51.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6317s / 12622.4158 s
agent0:                 episode reward: 0.3080,                 loss: 0.2056
agent1:                 episode reward: -0.3080,                 loss: 0.1967
Score delta: 1.553002325824227, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/51128_0.
Episode: 51581/101000 (51.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0464s / 12625.4622 s
agent0:                 episode reward: -0.3154,                 loss: nan
agent1:                 episode reward: 0.3154,                 loss: 0.1950
Episode: 51601/101000 (51.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9028s / 12629.3650 s
agent0:                 episode reward: -0.3640,                 loss: nan
agent1:                 episode reward: 0.3640,                 loss: 0.1940
Episode: 51621/101000 (51.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8101s / 12633.1751 s
agent0:                 episode reward: 0.5068,                 loss: nan
agent1:                 episode reward: -0.5068,                 loss: 0.1961
Episode: 51641/101000 (51.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8195s / 12636.9946 s
agent0:                 episode reward: -0.0391,                 loss: nan
agent1:                 episode reward: 0.0391,                 loss: 0.1961
Episode: 51661/101000 (51.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9226s / 12640.9172 s
agent0:                 episode reward: -0.4833,                 loss: 0.1972
agent1:                 episode reward: 0.4833,                 loss: 0.1946
Score delta: 1.5593328079967064, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/51234_1.
Episode: 51681/101000 (51.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2616s / 12644.1788 s
agent0:                 episode reward: 0.1274,                 loss: 0.1948
agent1:                 episode reward: -0.1274,                 loss: nan
Episode: 51701/101000 (51.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8168s / 12647.9956 s
agent0:                 episode reward: -0.3981,                 loss: 0.1917
agent1:                 episode reward: 0.3981,                 loss: nan
Episode: 51721/101000 (51.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8557s / 12651.8513 s
agent0:                 episode reward: 0.2267,                 loss: 0.1942
agent1:                 episode reward: -0.2267,                 loss: nan
Episode: 51741/101000 (51.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6145s / 12655.4658 s
agent0:                 episode reward: -0.0029,                 loss: 0.1922
agent1:                 episode reward: 0.0029,                 loss: nan
Episode: 51761/101000 (51.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2354s / 12659.7012 s
agent0:                 episode reward: 0.4101,                 loss: 0.1925
agent1:                 episode reward: -0.4101,                 loss: nan
Episode: 51781/101000 (51.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8989s / 12663.6001 s
agent0:                 episode reward: -0.3292,                 loss: 0.1921
agent1:                 episode reward: 0.3292,                 loss: nan
Episode: 51801/101000 (51.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5207s / 12667.1208 s
agent0:                 episode reward: -0.2027,                 loss: 0.1927
agent1:                 episode reward: 0.2027,                 loss: nan
Episode: 51821/101000 (51.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0890s / 12671.2098 s
agent0:                 episode reward: -0.2285,                 loss: 0.1915
agent1:                 episode reward: 0.2285,                 loss: nan
Episode: 51841/101000 (51.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0212s / 12675.2310 s
agent0:                 episode reward: 0.2426,                 loss: 0.1897
agent1:                 episode reward: -0.2426,                 loss: nan
Episode: 51861/101000 (51.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0738s / 12678.3048 s
agent0:                 episode reward: -0.0381,                 loss: 0.1923
agent1:                 episode reward: 0.0381,                 loss: 0.1922
Score delta: 1.674512854539239, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/51428_0.
Episode: 51881/101000 (51.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1397s / 12682.4445 s
agent0:                 episode reward: -0.0545,                 loss: nan
agent1:                 episode reward: 0.0545,                 loss: 0.1875
Episode: 51901/101000 (51.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4265s / 12685.8710 s
agent0:                 episode reward: -0.3524,                 loss: nan
agent1:                 episode reward: 0.3524,                 loss: 0.1891
Episode: 51921/101000 (51.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5243s / 12689.3953 s
agent0:                 episode reward: -0.4263,                 loss: nan
agent1:                 episode reward: 0.4263,                 loss: 0.1885
Episode: 51941/101000 (51.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4496s / 12692.8449 s
agent0:                 episode reward: -0.1767,                 loss: nan
agent1:                 episode reward: 0.1767,                 loss: 0.1879
Episode: 51961/101000 (51.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5429s / 12696.3878 s
agent0:                 episode reward: -0.6581,                 loss: nan
agent1:                 episode reward: 0.6581,                 loss: 0.1888
Score delta: 1.968136142217911, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/51535_1.
Episode: 51981/101000 (51.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6496s / 12701.0374 s
agent0:                 episode reward: -0.2362,                 loss: 0.3057
agent1:                 episode reward: 0.2362,                 loss: nan
Episode: 52001/101000 (51.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8549s / 12704.8923 s
agent0:                 episode reward: -0.3422,                 loss: 0.2980
agent1:                 episode reward: 0.3422,                 loss: nan
Episode: 52021/101000 (51.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1383s / 12709.0306 s
agent0:                 episode reward: -0.4993,                 loss: 0.2972
agent1:                 episode reward: 0.4993,                 loss: nan
Episode: 52041/101000 (51.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9875s / 12712.0181 s
agent0:                 episode reward: -0.5935,                 loss: 0.2939
agent1:                 episode reward: 0.5935,                 loss: nan
Episode: 52061/101000 (51.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4905s / 12715.5085 s
agent0:                 episode reward: -0.1638,                 loss: 0.2499
agent1:                 episode reward: 0.1638,                 loss: nan
Episode: 52081/101000 (51.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6545s / 12719.1631 s
agent0:                 episode reward: 0.0698,                 loss: 0.2043
agent1:                 episode reward: -0.0698,                 loss: nan
Episode: 52101/101000 (51.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3120s / 12722.4751 s
agent0:                 episode reward: 0.0748,                 loss: 0.2031
agent1:                 episode reward: -0.0748,                 loss: nan
Episode: 52121/101000 (51.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9092s / 12725.3843 s
agent0:                 episode reward: -0.0946,                 loss: 0.2058
agent1:                 episode reward: 0.0946,                 loss: nan
Episode: 52141/101000 (51.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3478s / 12727.7320 s
agent0:                 episode reward: -0.1790,                 loss: 0.2035
agent1:                 episode reward: 0.1790,                 loss: nan
Episode: 52161/101000 (51.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9567s / 12732.6887 s
agent0:                 episode reward: -0.5788,                 loss: 0.2035
agent1:                 episode reward: 0.5788,                 loss: nan
Episode: 52181/101000 (51.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6588s / 12736.3475 s
agent0:                 episode reward: -0.4852,                 loss: 0.2031
agent1:                 episode reward: 0.4852,                 loss: nan
Episode: 52201/101000 (51.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5684s / 12740.9159 s
agent0:                 episode reward: -0.0232,                 loss: 0.2007
agent1:                 episode reward: 0.0232,                 loss: nan
Episode: 52221/101000 (51.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3326s / 12744.2485 s
agent0:                 episode reward: -0.4849,                 loss: 0.2026
agent1:                 episode reward: 0.4849,                 loss: nan
Episode: 52241/101000 (51.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3237s / 12749.5722 s
agent0:                 episode reward: -0.7320,                 loss: 0.2009
agent1:                 episode reward: 0.7320,                 loss: nan
Episode: 52261/101000 (51.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9356s / 12753.5079 s
agent0:                 episode reward: -0.0847,                 loss: 0.2020
agent1:                 episode reward: 0.0847,                 loss: nan
Episode: 52281/101000 (51.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3115s / 12756.8194 s
agent0:                 episode reward: 0.0486,                 loss: 0.2012
agent1:                 episode reward: -0.0486,                 loss: nan
Episode: 52301/101000 (51.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0446s / 12760.8640 s
agent0:                 episode reward: 0.0049,                 loss: 0.2019
agent1:                 episode reward: -0.0049,                 loss: nan
Episode: 52321/101000 (51.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1921s / 12765.0561 s
agent0:                 episode reward: -0.3713,                 loss: 0.2012
agent1:                 episode reward: 0.3713,                 loss: nan
Episode: 52341/101000 (51.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4107s / 12769.4668 s
agent0:                 episode reward: 0.0830,                 loss: 0.2028
agent1:                 episode reward: -0.0830,                 loss: nan
Episode: 52361/101000 (51.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4329s / 12772.8997 s
agent0:                 episode reward: -0.0258,                 loss: 0.2021
agent1:                 episode reward: 0.0258,                 loss: nan
Episode: 52381/101000 (51.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9495s / 12776.8492 s
agent0:                 episode reward: -0.5928,                 loss: 0.2010
agent1:                 episode reward: 0.5928,                 loss: nan
Episode: 52401/101000 (51.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8287s / 12780.6779 s
agent0:                 episode reward: 0.2645,                 loss: 0.2019
agent1:                 episode reward: -0.2645,                 loss: 0.1940
Score delta: 1.773323511323208, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/51966_0.
Episode: 52421/101000 (51.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0403s / 12784.7183 s
agent0:                 episode reward: 0.0979,                 loss: nan
agent1:                 episode reward: -0.0979,                 loss: 0.1911
Episode: 52441/101000 (51.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1464s / 12788.8647 s
agent0:                 episode reward: -0.3552,                 loss: nan
agent1:                 episode reward: 0.3552,                 loss: 0.1909
Episode: 52461/101000 (51.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2204s / 12792.0851 s
agent0:                 episode reward: 0.1220,                 loss: nan
agent1:                 episode reward: -0.1220,                 loss: 0.1889
Episode: 52481/101000 (51.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0522s / 12796.1373 s
agent0:                 episode reward: -0.4316,                 loss: nan
agent1:                 episode reward: 0.4316,                 loss: 0.1904
Episode: 52501/101000 (51.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4630s / 12799.6004 s
agent0:                 episode reward: 0.0123,                 loss: nan
agent1:                 episode reward: -0.0123,                 loss: 0.1902
Episode: 52521/101000 (52.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7525s / 12802.3528 s
agent0:                 episode reward: -0.3954,                 loss: nan
agent1:                 episode reward: 0.3954,                 loss: 0.1900
Episode: 52541/101000 (52.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5443s / 12805.8971 s
agent0:                 episode reward: -0.4509,                 loss: nan
agent1:                 episode reward: 0.4509,                 loss: 0.1895
Episode: 52561/101000 (52.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7658s / 12809.6629 s
agent0:                 episode reward: 0.0715,                 loss: nan
agent1:                 episode reward: -0.0715,                 loss: 0.1884
Episode: 52581/101000 (52.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3291s / 12812.9920 s
agent0:                 episode reward: -0.2945,                 loss: nan
agent1:                 episode reward: 0.2945,                 loss: 0.1910
Episode: 52601/101000 (52.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2100s / 12816.2021 s
agent0:                 episode reward: 0.2395,                 loss: nan
agent1:                 episode reward: -0.2395,                 loss: 0.1883
Episode: 52621/101000 (52.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4800s / 12819.6821 s
agent0:                 episode reward: -0.0093,                 loss: nan
agent1:                 episode reward: 0.0093,                 loss: 0.1887
Episode: 52641/101000 (52.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0652s / 12822.7473 s
agent0:                 episode reward: 0.0322,                 loss: nan
agent1:                 episode reward: -0.0322,                 loss: 0.1910
Episode: 52661/101000 (52.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8930s / 12827.6403 s
agent0:                 episode reward: -0.3635,                 loss: 0.2010
agent1:                 episode reward: 0.3635,                 loss: 0.1896
Score delta: 1.9260609272630724, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/52225_1.
Episode: 52681/101000 (52.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6771s / 12831.3174 s
agent0:                 episode reward: 0.0091,                 loss: 0.2018
agent1:                 episode reward: -0.0091,                 loss: nan
Episode: 52701/101000 (52.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0516s / 12835.3690 s
agent0:                 episode reward: -0.2651,                 loss: 0.2025
agent1:                 episode reward: 0.2651,                 loss: nan
Episode: 52721/101000 (52.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9446s / 12838.3136 s
agent0:                 episode reward: 0.2253,                 loss: 0.2017
agent1:                 episode reward: -0.2253,                 loss: nan
Episode: 52741/101000 (52.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0826s / 12841.3962 s
agent0:                 episode reward: 0.2618,                 loss: 0.2002
agent1:                 episode reward: -0.2618,                 loss: nan
Episode: 52761/101000 (52.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0284s / 12845.4246 s
agent0:                 episode reward: 0.0461,                 loss: 0.1996
agent1:                 episode reward: -0.0461,                 loss: nan
Episode: 52781/101000 (52.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9470s / 12850.3716 s
agent0:                 episode reward: 0.3026,                 loss: 0.2009
agent1:                 episode reward: -0.3026,                 loss: nan
Episode: 52801/101000 (52.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4916s / 12853.8632 s
agent0:                 episode reward: -0.1538,                 loss: 0.1997
agent1:                 episode reward: 0.1538,                 loss: nan
Episode: 52821/101000 (52.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9181s / 12857.7813 s
agent0:                 episode reward: 0.0770,                 loss: 0.1997
agent1:                 episode reward: -0.0770,                 loss: 0.1916
Score delta: 1.5635487328477893, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/52389_0.
Episode: 52841/101000 (52.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8027s / 12861.5840 s
agent0:                 episode reward: -0.2008,                 loss: nan
agent1:                 episode reward: 0.2008,                 loss: 0.1932
Episode: 52861/101000 (52.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7451s / 12865.3291 s
agent0:                 episode reward: -0.5218,                 loss: nan
agent1:                 episode reward: 0.5218,                 loss: 0.1932
Episode: 52881/101000 (52.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0306s / 12868.3597 s
agent0:                 episode reward: -0.4486,                 loss: nan
agent1:                 episode reward: 0.4486,                 loss: 0.1924
Episode: 52901/101000 (52.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2091s / 12871.5689 s
agent0:                 episode reward: -0.3701,                 loss: nan
agent1:                 episode reward: 0.3701,                 loss: 0.1935
Episode: 52921/101000 (52.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4120s / 12874.9809 s
agent0:                 episode reward: -0.1280,                 loss: nan
agent1:                 episode reward: 0.1280,                 loss: 0.1920
Episode: 52941/101000 (52.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6752s / 12878.6560 s
agent0:                 episode reward: -0.6823,                 loss: 0.2012
agent1:                 episode reward: 0.6823,                 loss: 0.1884
Score delta: 1.859846704498839, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/52502_1.
Episode: 52961/101000 (52.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7266s / 12882.3827 s
agent0:                 episode reward: 0.1170,                 loss: 0.2000
agent1:                 episode reward: -0.1170,                 loss: nan
Episode: 52981/101000 (52.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2069s / 12886.5896 s
agent0:                 episode reward: -0.3162,                 loss: 0.1984
agent1:                 episode reward: 0.3162,                 loss: nan
Episode: 53001/101000 (52.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0639s / 12889.6535 s
agent0:                 episode reward: -0.0427,                 loss: 0.1993
agent1:                 episode reward: 0.0427,                 loss: nan
Episode: 53021/101000 (52.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3341s / 12893.9876 s
agent0:                 episode reward: 0.4046,                 loss: 0.1985
agent1:                 episode reward: -0.4046,                 loss: nan
Episode: 53041/101000 (52.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2566s / 12897.2442 s
agent0:                 episode reward: 0.4548,                 loss: 0.1991
agent1:                 episode reward: -0.4548,                 loss: nan
Episode: 53061/101000 (52.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9750s / 12900.2192 s
agent0:                 episode reward: -0.0841,                 loss: 0.2013
agent1:                 episode reward: 0.0841,                 loss: nan
Episode: 53081/101000 (52.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0922s / 12903.3114 s
agent0:                 episode reward: 0.0493,                 loss: 0.1975
agent1:                 episode reward: -0.0493,                 loss: 0.1988
Score delta: 1.51205862008416, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/52645_0.
Episode: 53101/101000 (52.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4365s / 12906.7479 s
agent0:                 episode reward: -0.1051,                 loss: nan
agent1:                 episode reward: 0.1051,                 loss: 0.1933
Episode: 53121/101000 (52.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1859s / 12909.9337 s
agent0:                 episode reward: -0.7718,                 loss: nan
agent1:                 episode reward: 0.7718,                 loss: 0.1935
Episode: 53141/101000 (52.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3589s / 12913.2926 s
agent0:                 episode reward: -0.5052,                 loss: nan
agent1:                 episode reward: 0.5052,                 loss: 0.1909
Episode: 53161/101000 (52.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6899s / 12916.9825 s
agent0:                 episode reward: -0.3826,                 loss: nan
agent1:                 episode reward: 0.3826,                 loss: 0.1916
Episode: 53181/101000 (52.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5898s / 12919.5723 s
agent0:                 episode reward: -0.3457,                 loss: nan
agent1:                 episode reward: 0.3457,                 loss: 0.1914
Episode: 53201/101000 (52.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6765s / 12923.2488 s
agent0:                 episode reward: -0.5001,                 loss: 0.2001
agent1:                 episode reward: 0.5001,                 loss: 0.1921
Score delta: 1.5370783303733533, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/52774_1.
Episode: 53221/101000 (52.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4274s / 12926.6762 s
agent0:                 episode reward: -0.1219,                 loss: 0.1946
agent1:                 episode reward: 0.1219,                 loss: nan
Episode: 53241/101000 (52.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6628s / 12930.3390 s
agent0:                 episode reward: -0.2745,                 loss: 0.2019
agent1:                 episode reward: 0.2745,                 loss: nan
Episode: 53261/101000 (52.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5892s / 12933.9282 s
agent0:                 episode reward: -0.2420,                 loss: 0.2010
agent1:                 episode reward: 0.2420,                 loss: nan
Episode: 53281/101000 (52.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6548s / 12937.5830 s
agent0:                 episode reward: 0.5934,                 loss: 0.1979
agent1:                 episode reward: -0.5934,                 loss: nan
Episode: 53301/101000 (52.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1368s / 12941.7197 s
agent0:                 episode reward: -0.0561,                 loss: 0.2007
agent1:                 episode reward: 0.0561,                 loss: nan
Episode: 53321/101000 (52.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7946s / 12945.5144 s
agent0:                 episode reward: 0.0571,                 loss: 0.1978
agent1:                 episode reward: -0.0571,                 loss: nan
Episode: 53341/101000 (52.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1530s / 12948.6674 s
agent0:                 episode reward: 0.3079,                 loss: 0.2012
agent1:                 episode reward: -0.3079,                 loss: nan
Episode: 53361/101000 (52.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1871s / 12951.8546 s
agent0:                 episode reward: 0.1263,                 loss: 0.1975
agent1:                 episode reward: -0.1263,                 loss: nan
Episode: 53381/101000 (52.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9936s / 12955.8482 s
agent0:                 episode reward: 0.1883,                 loss: 0.1995
agent1:                 episode reward: -0.1883,                 loss: nan
Episode: 53401/101000 (52.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9331s / 12959.7812 s
agent0:                 episode reward: 0.1263,                 loss: 0.1963
agent1:                 episode reward: -0.1263,                 loss: nan
Episode: 53421/101000 (52.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6392s / 12963.4205 s
agent0:                 episode reward: 0.0029,                 loss: 0.1992
agent1:                 episode reward: -0.0029,                 loss: nan
Episode: 53441/101000 (52.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2830s / 12966.7035 s
agent0:                 episode reward: -0.7927,                 loss: 0.1976
agent1:                 episode reward: 0.7927,                 loss: nan
Episode: 53461/101000 (52.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8147s / 12970.5182 s
agent0:                 episode reward: -0.1688,                 loss: 0.1971
agent1:                 episode reward: 0.1688,                 loss: nan
Episode: 53481/101000 (52.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4864s / 12975.0046 s
agent0:                 episode reward: 0.2397,                 loss: 0.1992
agent1:                 episode reward: -0.2397,                 loss: nan
Episode: 53501/101000 (52.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2192s / 12978.2239 s
agent0:                 episode reward: 0.3171,                 loss: 0.1996
agent1:                 episode reward: -0.3171,                 loss: nan
Episode: 53521/101000 (52.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7183s / 12981.9422 s
agent0:                 episode reward: -0.3919,                 loss: 0.1975
agent1:                 episode reward: 0.3919,                 loss: nan
Episode: 53541/101000 (53.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6496s / 12985.5917 s
agent0:                 episode reward: 0.1397,                 loss: 0.1993
agent1:                 episode reward: -0.1397,                 loss: 0.2015
Score delta: 1.5814438216341808, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/53109_0.
Episode: 53561/101000 (53.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6507s / 12989.2425 s
agent0:                 episode reward: -0.3578,                 loss: nan
agent1:                 episode reward: 0.3578,                 loss: 0.1964
Episode: 53581/101000 (53.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1987s / 12992.4412 s
agent0:                 episode reward: -0.6625,                 loss: nan
agent1:                 episode reward: 0.6625,                 loss: 0.1942
Episode: 53601/101000 (53.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3808s / 12995.8219 s
agent0:                 episode reward: 0.0986,                 loss: nan
agent1:                 episode reward: -0.0986,                 loss: 0.1965
Episode: 53621/101000 (53.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5926s / 12999.4145 s
agent0:                 episode reward: -0.0576,                 loss: nan
agent1:                 episode reward: 0.0576,                 loss: 0.1952
Episode: 53641/101000 (53.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9498s / 13002.3643 s
agent0:                 episode reward: -0.0207,                 loss: nan
agent1:                 episode reward: 0.0207,                 loss: 0.1944
Episode: 53661/101000 (53.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1117s / 13005.4760 s
agent0:                 episode reward: -0.3896,                 loss: 0.2424
agent1:                 episode reward: 0.3896,                 loss: 0.1935
Score delta: 1.7751327805979578, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/53228_1.
Episode: 53681/101000 (53.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3358s / 13008.8117 s
agent0:                 episode reward: -0.0049,                 loss: 0.2232
agent1:                 episode reward: 0.0049,                 loss: nan
Episode: 53701/101000 (53.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3249s / 13013.1367 s
agent0:                 episode reward: 0.3491,                 loss: 0.2015
agent1:                 episode reward: -0.3491,                 loss: nan
Episode: 53721/101000 (53.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3656s / 13017.5023 s
agent0:                 episode reward: -0.2133,                 loss: 0.2004
agent1:                 episode reward: 0.2133,                 loss: nan
Episode: 53741/101000 (53.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4748s / 13021.9771 s
agent0:                 episode reward: 0.1384,                 loss: 0.2007
agent1:                 episode reward: -0.1384,                 loss: nan
Episode: 53761/101000 (53.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4748s / 13026.4518 s
agent0:                 episode reward: 0.0700,                 loss: 0.2019
agent1:                 episode reward: -0.0700,                 loss: nan
Episode: 53781/101000 (53.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0542s / 13029.5061 s
agent0:                 episode reward: 0.2887,                 loss: 0.2023
agent1:                 episode reward: -0.2887,                 loss: nan
Episode: 53801/101000 (53.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5294s / 13033.0354 s
agent0:                 episode reward: 0.1983,                 loss: 0.2015
agent1:                 episode reward: -0.1983,                 loss: 0.1917
Score delta: 1.6350948134189682, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/53371_0.
Episode: 53821/101000 (53.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9661s / 13037.0015 s
agent0:                 episode reward: -0.4696,                 loss: nan
agent1:                 episode reward: 0.4696,                 loss: 0.1878
Episode: 53841/101000 (53.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8032s / 13040.8047 s
agent0:                 episode reward: -0.3521,                 loss: nan
agent1:                 episode reward: 0.3521,                 loss: 0.1895
Episode: 53861/101000 (53.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6896s / 13044.4943 s
agent0:                 episode reward: -0.3518,                 loss: nan
agent1:                 episode reward: 0.3518,                 loss: 0.1878
Episode: 53881/101000 (53.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1313s / 13047.6255 s
agent0:                 episode reward: -0.0791,                 loss: nan
agent1:                 episode reward: 0.0791,                 loss: 0.1876
Episode: 53901/101000 (53.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1185s / 13051.7440 s
agent0:                 episode reward: -0.3915,                 loss: nan
agent1:                 episode reward: 0.3915,                 loss: 0.1850
Episode: 53921/101000 (53.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3129s / 13055.0569 s
agent0:                 episode reward: -0.4912,                 loss: nan
agent1:                 episode reward: 0.4912,                 loss: 0.1853
Episode: 53941/101000 (53.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6479s / 13058.7048 s
agent0:                 episode reward: 0.1642,                 loss: nan
agent1:                 episode reward: -0.1642,                 loss: 0.1824
Episode: 53961/101000 (53.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3855s / 13063.0902 s
agent0:                 episode reward: -0.5314,                 loss: 0.2043
agent1:                 episode reward: 0.5314,                 loss: 0.1838
Score delta: 1.5322505855373971, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/53527_1.
Episode: 53981/101000 (53.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7823s / 13067.8725 s
agent0:                 episode reward: 0.0549,                 loss: 0.2017
agent1:                 episode reward: -0.0549,                 loss: nan
Episode: 54001/101000 (53.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0611s / 13071.9337 s
agent0:                 episode reward: 0.0659,                 loss: 0.2002
agent1:                 episode reward: -0.0659,                 loss: nan
Episode: 54021/101000 (53.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6861s / 13076.6198 s
agent0:                 episode reward: 0.0261,                 loss: 0.2005
agent1:                 episode reward: -0.0261,                 loss: nan
Episode: 54041/101000 (53.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1558s / 13080.7756 s
agent0:                 episode reward: -0.5385,                 loss: 0.1985
agent1:                 episode reward: 0.5385,                 loss: nan
Episode: 54061/101000 (53.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2373s / 13085.0129 s
agent0:                 episode reward: -0.2291,                 loss: 0.2010
agent1:                 episode reward: 0.2291,                 loss: nan
Episode: 54081/101000 (53.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0450s / 13090.0579 s
agent0:                 episode reward: -0.0813,                 loss: 0.1995
agent1:                 episode reward: 0.0813,                 loss: nan
Episode: 54101/101000 (53.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8247s / 13093.8826 s
agent0:                 episode reward: 0.0721,                 loss: 0.1982
agent1:                 episode reward: -0.0721,                 loss: nan
Episode: 54121/101000 (53.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7339s / 13097.6165 s
agent0:                 episode reward: -0.4022,                 loss: 0.1995
agent1:                 episode reward: 0.4022,                 loss: nan
Episode: 54141/101000 (53.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3669s / 13100.9833 s
agent0:                 episode reward: -0.3660,                 loss: 0.1998
agent1:                 episode reward: 0.3660,                 loss: nan
Episode: 54161/101000 (53.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9885s / 13103.9719 s
agent0:                 episode reward: 0.0400,                 loss: 0.1989
agent1:                 episode reward: -0.0400,                 loss: nan
Episode: 54181/101000 (53.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4460s / 13107.4179 s
agent0:                 episode reward: 0.1551,                 loss: 0.2091
agent1:                 episode reward: -0.1551,                 loss: nan
Episode: 54201/101000 (53.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2062s / 13111.6240 s
agent0:                 episode reward: 0.2509,                 loss: 0.2108
agent1:                 episode reward: -0.2509,                 loss: nan
Episode: 54221/101000 (53.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0653s / 13115.6893 s
agent0:                 episode reward: 0.1729,                 loss: 0.2089
agent1:                 episode reward: -0.1729,                 loss: nan
Episode: 54241/101000 (53.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8155s / 13119.5048 s
agent0:                 episode reward: 0.2596,                 loss: 0.2119
agent1:                 episode reward: -0.2596,                 loss: 0.1925
Score delta: 1.5405947485865432, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/53811_0.
Episode: 54261/101000 (53.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9349s / 13122.4396 s
agent0:                 episode reward: -0.0655,                 loss: nan
agent1:                 episode reward: 0.0655,                 loss: 0.1886
Episode: 54281/101000 (53.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9079s / 13126.3475 s
agent0:                 episode reward: 0.0468,                 loss: nan
agent1:                 episode reward: -0.0468,                 loss: 0.1900
Episode: 54301/101000 (53.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9273s / 13130.2748 s
agent0:                 episode reward: -0.8110,                 loss: nan
agent1:                 episode reward: 0.8110,                 loss: 0.1917
Episode: 54321/101000 (53.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2389s / 13134.5137 s
agent0:                 episode reward: -0.2036,                 loss: nan
agent1:                 episode reward: 0.2036,                 loss: 0.1912
Episode: 54341/101000 (53.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2174s / 13137.7311 s
agent0:                 episode reward: -0.2574,                 loss: nan
agent1:                 episode reward: 0.2574,                 loss: 0.1912
Episode: 54361/101000 (53.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0745s / 13140.8056 s
agent0:                 episode reward: -0.3698,                 loss: nan
agent1:                 episode reward: 0.3698,                 loss: 0.1918
Episode: 54381/101000 (53.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6775s / 13144.4831 s
agent0:                 episode reward: -0.1634,                 loss: nan
agent1:                 episode reward: 0.1634,                 loss: 0.1925
Episode: 54401/101000 (53.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5673s / 13148.0504 s
agent0:                 episode reward: -0.0622,                 loss: nan
agent1:                 episode reward: 0.0622,                 loss: 0.1917
Episode: 54421/101000 (53.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7289s / 13151.7793 s
agent0:                 episode reward: -0.2088,                 loss: nan
agent1:                 episode reward: 0.2088,                 loss: 0.1899
Episode: 54441/101000 (53.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4596s / 13155.2389 s
agent0:                 episode reward: -0.0753,                 loss: 0.1995
agent1:                 episode reward: 0.0753,                 loss: 0.1853
Score delta: 1.5479230852132506, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/53997_1.
Episode: 54461/101000 (53.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4854s / 13158.7243 s
agent0:                 episode reward: -0.2436,                 loss: 0.1994
agent1:                 episode reward: 0.2436,                 loss: nan
Episode: 54481/101000 (53.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2316s / 13162.9558 s
agent0:                 episode reward: -0.1385,                 loss: 0.1992
agent1:                 episode reward: 0.1385,                 loss: nan
Episode: 54501/101000 (53.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9969s / 13166.9527 s
agent0:                 episode reward: -0.1823,                 loss: 0.1958
agent1:                 episode reward: 0.1823,                 loss: nan
Episode: 54521/101000 (53.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7180s / 13169.6708 s
agent0:                 episode reward: 0.2403,                 loss: 0.1970
agent1:                 episode reward: -0.2403,                 loss: nan
Episode: 54541/101000 (54.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0647s / 13173.7355 s
agent0:                 episode reward: -0.6505,                 loss: 0.1969
agent1:                 episode reward: 0.6505,                 loss: nan
Episode: 54561/101000 (54.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5864s / 13178.3219 s
agent0:                 episode reward: -0.2677,                 loss: 0.1943
agent1:                 episode reward: 0.2677,                 loss: nan
Episode: 54581/101000 (54.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2629s / 13182.5848 s
agent0:                 episode reward: -0.0405,                 loss: 0.1948
agent1:                 episode reward: 0.0405,                 loss: nan
Episode: 54601/101000 (54.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0900s / 13186.6748 s
agent0:                 episode reward: 0.0241,                 loss: 0.1951
agent1:                 episode reward: -0.0241,                 loss: nan
Episode: 54621/101000 (54.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8890s / 13189.5638 s
agent0:                 episode reward: -0.4317,                 loss: 0.1920
agent1:                 episode reward: 0.4317,                 loss: nan
Episode: 54641/101000 (54.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9994s / 13192.5632 s
agent0:                 episode reward: -0.4134,                 loss: 0.1942
agent1:                 episode reward: 0.4134,                 loss: nan
Episode: 54661/101000 (54.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4622s / 13197.0254 s
agent0:                 episode reward: -0.1115,                 loss: 0.1932
agent1:                 episode reward: 0.1115,                 loss: nan
Episode: 54681/101000 (54.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2199s / 13201.2453 s
agent0:                 episode reward: 0.1026,                 loss: 0.1945
agent1:                 episode reward: -0.1026,                 loss: nan
Episode: 54701/101000 (54.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4053s / 13204.6505 s
agent0:                 episode reward: -0.3372,                 loss: 0.2095
agent1:                 episode reward: 0.3372,                 loss: nan
Episode: 54721/101000 (54.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3315s / 13208.9820 s
agent0:                 episode reward: -0.2617,                 loss: 0.2102
agent1:                 episode reward: 0.2617,                 loss: nan
Episode: 54741/101000 (54.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2522s / 13212.2342 s
agent0:                 episode reward: -0.0936,                 loss: 0.2105
agent1:                 episode reward: 0.0936,                 loss: nan
Episode: 54761/101000 (54.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6458s / 13218.8799 s
agent0:                 episode reward: -0.0092,                 loss: 0.2115
agent1:                 episode reward: 0.0092,                 loss: nan
Episode: 54781/101000 (54.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6393s / 13222.5192 s
agent0:                 episode reward: 0.0772,                 loss: 0.2097
agent1:                 episode reward: -0.0772,                 loss: nan
Episode: 54801/101000 (54.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1856s / 13226.7048 s
agent0:                 episode reward: 0.2890,                 loss: 0.2092
agent1:                 episode reward: -0.2890,                 loss: nan
Episode: 54821/101000 (54.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7330s / 13230.4378 s
agent0:                 episode reward: -0.1867,                 loss: 0.2083
agent1:                 episode reward: 0.1867,                 loss: nan
Episode: 54841/101000 (54.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3790s / 13233.8168 s
agent0:                 episode reward: 0.1835,                 loss: 0.2107
agent1:                 episode reward: -0.1835,                 loss: nan
Episode: 54861/101000 (54.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1640s / 13237.9809 s
agent0:                 episode reward: -0.2592,                 loss: 0.2126
agent1:                 episode reward: 0.2592,                 loss: 0.1970
Score delta: 1.666396522444792, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/54417_0.
Episode: 54881/101000 (54.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8334s / 13241.8143 s
agent0:                 episode reward: -0.4075,                 loss: nan
agent1:                 episode reward: 0.4075,                 loss: 0.1920
Episode: 54901/101000 (54.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2778s / 13245.0921 s
agent0:                 episode reward: -0.4405,                 loss: nan
agent1:                 episode reward: 0.4405,                 loss: 0.1917
Episode: 54921/101000 (54.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0783s / 13249.1704 s
agent0:                 episode reward: -0.7178,                 loss: nan
agent1:                 episode reward: 0.7178,                 loss: 0.1939
Episode: 54941/101000 (54.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1678s / 13253.3382 s
agent0:                 episode reward: -0.0045,                 loss: nan
agent1:                 episode reward: 0.0045,                 loss: 0.1946
Episode: 54961/101000 (54.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6613s / 13256.9994 s
agent0:                 episode reward: -0.4504,                 loss: nan
agent1:                 episode reward: 0.4504,                 loss: 0.1964
Episode: 54981/101000 (54.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7758s / 13260.7753 s
agent0:                 episode reward: -0.7420,                 loss: 0.2235
agent1:                 episode reward: 0.7420,                 loss: 0.1970
Score delta: 1.6477317429378466, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/54544_1.
Episode: 55001/101000 (54.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4789s / 13264.2541 s
agent0:                 episode reward: -0.0261,                 loss: 0.2193
agent1:                 episode reward: 0.0261,                 loss: nan
Episode: 55021/101000 (54.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1791s / 13267.4332 s
agent0:                 episode reward: -0.1310,                 loss: 0.2156
agent1:                 episode reward: 0.1310,                 loss: nan
Episode: 55041/101000 (54.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7052s / 13271.1384 s
agent0:                 episode reward: -0.4469,                 loss: 0.2192
agent1:                 episode reward: 0.4469,                 loss: nan
Episode: 55061/101000 (54.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2157s / 13275.3542 s
agent0:                 episode reward: -0.3712,                 loss: 0.2170
agent1:                 episode reward: 0.3712,                 loss: nan
Episode: 55081/101000 (54.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1850s / 13278.5392 s
agent0:                 episode reward: -0.4140,                 loss: 0.2195
agent1:                 episode reward: 0.4140,                 loss: nan
Episode: 55101/101000 (54.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9463s / 13282.4856 s
agent0:                 episode reward: 0.2923,                 loss: 0.2147
agent1:                 episode reward: -0.2923,                 loss: nan
Episode: 55121/101000 (54.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9841s / 13285.4696 s
agent0:                 episode reward: -0.3336,                 loss: 0.2178
agent1:                 episode reward: 0.3336,                 loss: nan
Episode: 55141/101000 (54.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2993s / 13288.7689 s
agent0:                 episode reward: -0.0426,                 loss: 0.2148
agent1:                 episode reward: 0.0426,                 loss: nan
Episode: 55161/101000 (54.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3365s / 13292.1054 s
agent0:                 episode reward: 0.1284,                 loss: 0.2006
agent1:                 episode reward: -0.1284,                 loss: 0.1975
Score delta: 1.5085768395251957, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/54728_0.
Episode: 55181/101000 (54.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3584s / 13295.4638 s
agent0:                 episode reward: -0.4317,                 loss: nan
agent1:                 episode reward: 0.4317,                 loss: 0.1952
Episode: 55201/101000 (54.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6753s / 13299.1391 s
agent0:                 episode reward: -0.2364,                 loss: nan
agent1:                 episode reward: 0.2364,                 loss: 0.1955
Episode: 55221/101000 (54.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5803s / 13302.7194 s
agent0:                 episode reward: -0.3329,                 loss: nan
agent1:                 episode reward: 0.3329,                 loss: 0.1966
Episode: 55241/101000 (54.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8450s / 13306.5644 s
agent0:                 episode reward: -0.7135,                 loss: nan
agent1:                 episode reward: 0.7135,                 loss: 0.1955
Episode: 55261/101000 (54.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4242s / 13310.9885 s
agent0:                 episode reward: -0.1908,                 loss: nan
agent1:                 episode reward: 0.1908,                 loss: 0.1959
Episode: 55281/101000 (54.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5350s / 13314.5235 s
agent0:                 episode reward: -0.2052,                 loss: nan
agent1:                 episode reward: 0.2052,                 loss: 0.1951
Episode: 55301/101000 (54.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9283s / 13317.4518 s
agent0:                 episode reward: -0.0503,                 loss: 0.2012
agent1:                 episode reward: 0.0503,                 loss: 0.1949
Score delta: 1.5847915591655097, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/54864_1.
Episode: 55321/101000 (54.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8026s / 13321.2544 s
agent0:                 episode reward: 0.3115,                 loss: 0.1987
agent1:                 episode reward: -0.3115,                 loss: nan
Episode: 55341/101000 (54.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1435s / 13325.3979 s
agent0:                 episode reward: -0.0262,                 loss: 0.1983
agent1:                 episode reward: 0.0262,                 loss: nan
Episode: 55361/101000 (54.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8959s / 13329.2938 s
agent0:                 episode reward: 0.3496,                 loss: 0.2002
agent1:                 episode reward: -0.3496,                 loss: nan
Episode: 55381/101000 (54.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6823s / 13332.9761 s
agent0:                 episode reward: 0.0671,                 loss: 0.1996
agent1:                 episode reward: -0.0671,                 loss: nan
Episode: 55401/101000 (54.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0714s / 13337.0475 s
agent0:                 episode reward: 0.2219,                 loss: 0.2000
agent1:                 episode reward: -0.2219,                 loss: nan
Episode: 55421/101000 (54.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9118s / 13340.9593 s
agent0:                 episode reward: -0.1792,                 loss: 0.2006
agent1:                 episode reward: 0.1792,                 loss: nan
Episode: 55441/101000 (54.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4499s / 13344.4092 s
agent0:                 episode reward: 0.0305,                 loss: 0.2013
agent1:                 episode reward: -0.0305,                 loss: nan
Episode: 55461/101000 (54.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8335s / 13348.2426 s
agent0:                 episode reward: 0.0826,                 loss: 0.2012
agent1:                 episode reward: -0.0826,                 loss: 0.1911
Score delta: 1.5833806155687937, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/55023_0.
Episode: 55481/101000 (54.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3592s / 13351.6018 s
agent0:                 episode reward: -0.3918,                 loss: nan
agent1:                 episode reward: 0.3918,                 loss: 0.1900
Episode: 55501/101000 (54.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2452s / 13354.8470 s
agent0:                 episode reward: -0.0094,                 loss: nan
agent1:                 episode reward: 0.0094,                 loss: 0.1902
Episode: 55521/101000 (54.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3277s / 13359.1747 s
agent0:                 episode reward: 0.2856,                 loss: nan
agent1:                 episode reward: -0.2856,                 loss: 0.1894
Episode: 55541/101000 (54.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7122s / 13362.8869 s
agent0:                 episode reward: -0.2278,                 loss: nan
agent1:                 episode reward: 0.2278,                 loss: 0.1927
Episode: 55561/101000 (55.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8257s / 13366.7126 s
agent0:                 episode reward: -0.1442,                 loss: nan
agent1:                 episode reward: 0.1442,                 loss: 0.1917
Episode: 55581/101000 (55.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2469s / 13369.9595 s
agent0:                 episode reward: -0.0453,                 loss: nan
agent1:                 episode reward: 0.0453,                 loss: 0.1903
Episode: 55601/101000 (55.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9202s / 13372.8797 s
agent0:                 episode reward: -0.0347,                 loss: nan
agent1:                 episode reward: 0.0347,                 loss: 0.1917
Episode: 55621/101000 (55.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9176s / 13375.7973 s
agent0:                 episode reward: -0.2806,                 loss: nan
agent1:                 episode reward: 0.2806,                 loss: 0.1959
Episode: 55641/101000 (55.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7929s / 13379.5902 s
agent0:                 episode reward: -0.5977,                 loss: 0.2604
agent1:                 episode reward: 0.5977,                 loss: 0.1970
Score delta: 1.7419459196028435, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/55202_1.
Episode: 55661/101000 (55.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0077s / 13383.5979 s
agent0:                 episode reward: -0.2953,                 loss: 0.2505
agent1:                 episode reward: 0.2953,                 loss: nan
Episode: 55681/101000 (55.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6603s / 13387.2582 s
agent0:                 episode reward: -0.6467,                 loss: 0.2504
agent1:                 episode reward: 0.6467,                 loss: nan
Episode: 55701/101000 (55.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8428s / 13391.1010 s
agent0:                 episode reward: -0.5968,                 loss: 0.2520
agent1:                 episode reward: 0.5968,                 loss: nan
Episode: 55721/101000 (55.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1940s / 13395.2950 s
agent0:                 episode reward: -0.0194,                 loss: 0.2488
agent1:                 episode reward: 0.0194,                 loss: nan
Episode: 55741/101000 (55.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6058s / 13398.9008 s
agent0:                 episode reward: 0.3184,                 loss: 0.2458
agent1:                 episode reward: -0.3184,                 loss: nan
Episode: 55761/101000 (55.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7910s / 13402.6919 s
agent0:                 episode reward: -0.8283,                 loss: 0.2489
agent1:                 episode reward: 0.8283,                 loss: nan
Episode: 55781/101000 (55.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1139s / 13405.8057 s
agent0:                 episode reward: -0.0859,                 loss: 0.2504
agent1:                 episode reward: 0.0859,                 loss: nan
Episode: 55801/101000 (55.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8319s / 13409.6376 s
agent0:                 episode reward: -0.4604,                 loss: 0.2262
agent1:                 episode reward: 0.4604,                 loss: nan
Episode: 55821/101000 (55.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4751s / 13413.1128 s
agent0:                 episode reward: -0.2185,                 loss: 0.2075
agent1:                 episode reward: 0.2185,                 loss: nan
Episode: 55841/101000 (55.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7181s / 13417.8308 s
agent0:                 episode reward: -0.3341,                 loss: 0.2067
agent1:                 episode reward: 0.3341,                 loss: nan
Episode: 55861/101000 (55.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3357s / 13421.1665 s
agent0:                 episode reward: 0.1588,                 loss: 0.2060
agent1:                 episode reward: -0.1588,                 loss: nan
Episode: 55881/101000 (55.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3402s / 13424.5068 s
agent0:                 episode reward: -0.1179,                 loss: 0.2083
agent1:                 episode reward: 0.1179,                 loss: nan
Episode: 55901/101000 (55.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1021s / 13427.6088 s
agent0:                 episode reward: 0.1159,                 loss: 0.2087
agent1:                 episode reward: -0.1159,                 loss: nan
Episode: 55921/101000 (55.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9818s / 13431.5906 s
agent0:                 episode reward: 0.5256,                 loss: 0.2071
agent1:                 episode reward: -0.5256,                 loss: 0.1714
Score delta: 1.5942168933179695, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/55493_0.
Episode: 55941/101000 (55.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4113s / 13436.0020 s
agent0:                 episode reward: -0.8125,                 loss: nan
agent1:                 episode reward: 0.8125,                 loss: 0.1694
Episode: 55961/101000 (55.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7057s / 13439.7077 s
agent0:                 episode reward: -0.7638,                 loss: nan
agent1:                 episode reward: 0.7638,                 loss: 0.1653
Episode: 55981/101000 (55.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1461s / 13443.8538 s
agent0:                 episode reward: 0.0055,                 loss: nan
agent1:                 episode reward: -0.0055,                 loss: 0.1651
Episode: 56001/101000 (55.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5696s / 13447.4234 s
agent0:                 episode reward: -0.4582,                 loss: nan
agent1:                 episode reward: 0.4582,                 loss: 0.1647
Episode: 56021/101000 (55.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8363s / 13451.2597 s
agent0:                 episode reward: -0.1379,                 loss: nan
agent1:                 episode reward: 0.1379,                 loss: 0.1649
Episode: 56041/101000 (55.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0361s / 13454.2958 s
agent0:                 episode reward: -0.6351,                 loss: nan
agent1:                 episode reward: 0.6351,                 loss: 0.1636
Episode: 56061/101000 (55.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6978s / 13457.9936 s
agent0:                 episode reward: 0.1035,                 loss: 0.2017
agent1:                 episode reward: -0.1035,                 loss: 0.1670
Score delta: 2.022745609136619, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/55619_1.
Episode: 56081/101000 (55.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3907s / 13461.3844 s
agent0:                 episode reward: -0.0464,                 loss: 0.2021
agent1:                 episode reward: 0.0464,                 loss: nan
Episode: 56101/101000 (55.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6131s / 13465.9975 s
agent0:                 episode reward: -0.5403,                 loss: 0.2010
agent1:                 episode reward: 0.5403,                 loss: nan
Episode: 56121/101000 (55.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2470s / 13471.2445 s
agent0:                 episode reward: 0.4292,                 loss: 0.2021
agent1:                 episode reward: -0.4292,                 loss: nan
Episode: 56141/101000 (55.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9362s / 13476.1807 s
agent0:                 episode reward: 0.1372,                 loss: 0.2001
agent1:                 episode reward: -0.1372,                 loss: nan
Episode: 56161/101000 (55.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4125s / 13479.5932 s
agent0:                 episode reward: 0.1295,                 loss: 0.2007
agent1:                 episode reward: -0.1295,                 loss: nan
Episode: 56181/101000 (55.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1621s / 13483.7553 s
agent0:                 episode reward: 0.4545,                 loss: 0.2022
agent1:                 episode reward: -0.4545,                 loss: nan
Episode: 56201/101000 (55.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8756s / 13487.6309 s
agent0:                 episode reward: 0.4530,                 loss: 0.1987
agent1:                 episode reward: -0.4530,                 loss: 0.1995
Score delta: 1.611433473308407, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/55760_0.
Episode: 56221/101000 (55.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3047s / 13490.9356 s
agent0:                 episode reward: -0.1379,                 loss: nan
agent1:                 episode reward: 0.1379,                 loss: 0.1965
Episode: 56241/101000 (55.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3437s / 13494.2793 s
agent0:                 episode reward: -0.0553,                 loss: nan
agent1:                 episode reward: 0.0553,                 loss: 0.1946
Episode: 56261/101000 (55.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1728s / 13497.4522 s
agent0:                 episode reward: -0.3623,                 loss: nan
agent1:                 episode reward: 0.3623,                 loss: 0.1953
Episode: 56281/101000 (55.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6997s / 13501.1519 s
agent0:                 episode reward: -0.2557,                 loss: nan
agent1:                 episode reward: 0.2557,                 loss: 0.1961
Episode: 56301/101000 (55.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2677s / 13504.4196 s
agent0:                 episode reward: -0.3774,                 loss: nan
agent1:                 episode reward: 0.3774,                 loss: 0.1960
Episode: 56321/101000 (55.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5741s / 13507.9937 s
agent0:                 episode reward: 0.1255,                 loss: nan
agent1:                 episode reward: -0.1255,                 loss: 0.1946
Episode: 56341/101000 (55.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4245s / 13511.4182 s
agent0:                 episode reward: -0.3439,                 loss: nan
agent1:                 episode reward: 0.3439,                 loss: 0.1932
Episode: 56361/101000 (55.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2662s / 13515.6844 s
agent0:                 episode reward: -0.4425,                 loss: 0.3015
agent1:                 episode reward: 0.4425,                 loss: 0.1927
Score delta: 1.5365006678590671, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/55928_1.
Episode: 56381/101000 (55.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0531s / 13519.7375 s
agent0:                 episode reward: 0.0318,                 loss: 0.2827
agent1:                 episode reward: -0.0318,                 loss: nan
Episode: 56401/101000 (55.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6759s / 13523.4134 s
agent0:                 episode reward: 0.1029,                 loss: 0.2799
agent1:                 episode reward: -0.1029,                 loss: nan
Episode: 56421/101000 (55.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7649s / 13527.1783 s
agent0:                 episode reward: -0.0119,                 loss: 0.2645
agent1:                 episode reward: 0.0119,                 loss: nan
Episode: 56441/101000 (55.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1454s / 13531.3237 s
agent0:                 episode reward: -0.0789,                 loss: 0.2015
agent1:                 episode reward: 0.0789,                 loss: nan
Episode: 56461/101000 (55.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4808s / 13534.8045 s
agent0:                 episode reward: -0.0205,                 loss: 0.1996
agent1:                 episode reward: 0.0205,                 loss: nan
Episode: 56481/101000 (55.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8621s / 13538.6667 s
agent0:                 episode reward: 0.0099,                 loss: 0.2016
agent1:                 episode reward: -0.0099,                 loss: nan
Episode: 56501/101000 (55.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3751s / 13542.0417 s
agent0:                 episode reward: 0.0640,                 loss: 0.2008
agent1:                 episode reward: -0.0640,                 loss: nan
Episode: 56521/101000 (55.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4204s / 13546.4621 s
agent0:                 episode reward: 0.0549,                 loss: 0.2010
agent1:                 episode reward: -0.0549,                 loss: nan
Episode: 56541/101000 (55.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0421s / 13550.5042 s
agent0:                 episode reward: -0.1499,                 loss: 0.2023
agent1:                 episode reward: 0.1499,                 loss: nan
Episode: 56561/101000 (56.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8006s / 13554.3048 s
agent0:                 episode reward: -0.0696,                 loss: 0.2012
agent1:                 episode reward: 0.0696,                 loss: nan
Episode: 56581/101000 (56.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3409s / 13557.6457 s
agent0:                 episode reward: 0.2025,                 loss: 0.1995
agent1:                 episode reward: -0.2025,                 loss: nan
Episode: 56601/101000 (56.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8444s / 13561.4902 s
agent0:                 episode reward: 0.0019,                 loss: 0.2020
agent1:                 episode reward: -0.0019,                 loss: 0.1955
Score delta: 1.7190348888227962, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/56163_0.
Episode: 56621/101000 (56.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2958s / 13565.7859 s
agent0:                 episode reward: -0.1574,                 loss: nan
agent1:                 episode reward: 0.1574,                 loss: 0.1895
Episode: 56641/101000 (56.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5420s / 13569.3279 s
agent0:                 episode reward: -0.4531,                 loss: nan
agent1:                 episode reward: 0.4531,                 loss: 0.1877
Episode: 56661/101000 (56.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9855s / 13573.3134 s
agent0:                 episode reward: -0.1597,                 loss: nan
agent1:                 episode reward: 0.1597,                 loss: 0.1901
Episode: 56681/101000 (56.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3733s / 13576.6867 s
agent0:                 episode reward: 0.0350,                 loss: nan
agent1:                 episode reward: -0.0350,                 loss: 0.1897
Episode: 56701/101000 (56.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8090s / 13580.4957 s
agent0:                 episode reward: -0.8151,                 loss: nan
agent1:                 episode reward: 0.8151,                 loss: 0.1889
Episode: 56721/101000 (56.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7709s / 13585.2666 s
agent0:                 episode reward: -0.0189,                 loss: nan
agent1:                 episode reward: 0.0189,                 loss: 0.1892
Episode: 56741/101000 (56.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1374s / 13589.4040 s
agent0:                 episode reward: 0.0114,                 loss: nan
agent1:                 episode reward: -0.0114,                 loss: 0.1874
Episode: 56761/101000 (56.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0005s / 13593.4045 s
agent0:                 episode reward: -0.2473,                 loss: nan
agent1:                 episode reward: 0.2473,                 loss: 0.1880
Episode: 56781/101000 (56.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7092s / 13597.1137 s
agent0:                 episode reward: 0.0712,                 loss: nan
agent1:                 episode reward: -0.0712,                 loss: 0.1907
Episode: 56801/101000 (56.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4625s / 13600.5762 s
agent0:                 episode reward: -0.4366,                 loss: 0.2473
agent1:                 episode reward: 0.4366,                 loss: 0.1875
Score delta: 1.7341186529856205, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/56364_1.
Episode: 56821/101000 (56.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5934s / 13604.1696 s
agent0:                 episode reward: 0.1071,                 loss: 0.2440
agent1:                 episode reward: -0.1071,                 loss: nan
Episode: 56841/101000 (56.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9641s / 13608.1337 s
agent0:                 episode reward: 0.0806,                 loss: 0.2410
agent1:                 episode reward: -0.0806,                 loss: nan
Episode: 56861/101000 (56.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7438s / 13611.8775 s
agent0:                 episode reward: 0.3342,                 loss: 0.2411
agent1:                 episode reward: -0.3342,                 loss: nan
Episode: 56881/101000 (56.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0235s / 13615.9010 s
agent0:                 episode reward: 0.0706,                 loss: 0.2418
agent1:                 episode reward: -0.0706,                 loss: nan
Episode: 56901/101000 (56.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9348s / 13619.8359 s
agent0:                 episode reward: 0.5471,                 loss: 0.2396
agent1:                 episode reward: -0.5471,                 loss: nan
Episode: 56921/101000 (56.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9504s / 13623.7863 s
agent0:                 episode reward: -0.0989,                 loss: 0.2408
agent1:                 episode reward: 0.0989,                 loss: nan
Episode: 56941/101000 (56.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6902s / 13627.4765 s
agent0:                 episode reward: -0.2451,                 loss: 0.2405
agent1:                 episode reward: 0.2451,                 loss: nan
Episode: 56961/101000 (56.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6348s / 13631.1112 s
agent0:                 episode reward: -0.0570,                 loss: 0.2213
agent1:                 episode reward: 0.0570,                 loss: nan
Episode: 56981/101000 (56.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6297s / 13634.7409 s
agent0:                 episode reward: -0.2792,                 loss: 0.2008
agent1:                 episode reward: 0.2792,                 loss: nan
Episode: 57001/101000 (56.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7182s / 13638.4591 s
agent0:                 episode reward: -0.1028,                 loss: 0.1995
agent1:                 episode reward: 0.1028,                 loss: nan
Episode: 57021/101000 (56.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9404s / 13642.3995 s
agent0:                 episode reward: 0.0262,                 loss: 0.1960
agent1:                 episode reward: -0.0262,                 loss: nan
Episode: 57041/101000 (56.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8034s / 13646.2029 s
agent0:                 episode reward: 0.0563,                 loss: 0.1987
agent1:                 episode reward: -0.0563,                 loss: nan
Episode: 57061/101000 (56.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7083s / 13649.9112 s
agent0:                 episode reward: 0.0574,                 loss: 0.1984
agent1:                 episode reward: -0.0574,                 loss: nan
Episode: 57081/101000 (56.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8737s / 13654.7849 s
agent0:                 episode reward: -0.0860,                 loss: 0.1974
agent1:                 episode reward: 0.0860,                 loss: nan
Episode: 57101/101000 (56.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0855s / 13658.8705 s
agent0:                 episode reward: 0.1285,                 loss: 0.1987
agent1:                 episode reward: -0.1285,                 loss: nan
Episode: 57121/101000 (56.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9071s / 13662.7775 s
agent0:                 episode reward: -0.0970,                 loss: 0.1964
agent1:                 episode reward: 0.0970,                 loss: nan
Episode: 57141/101000 (56.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8476s / 13665.6252 s
agent0:                 episode reward: 0.1110,                 loss: 0.1993
agent1:                 episode reward: -0.1110,                 loss: nan
Episode: 57161/101000 (56.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3694s / 13669.9945 s
agent0:                 episode reward: -0.1557,                 loss: 0.1989
agent1:                 episode reward: 0.1557,                 loss: nan
Episode: 57181/101000 (56.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0701s / 13674.0646 s
agent0:                 episode reward: -0.1897,                 loss: 0.1999
agent1:                 episode reward: 0.1897,                 loss: nan
Episode: 57201/101000 (56.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3259s / 13678.3905 s
agent0:                 episode reward: -0.0660,                 loss: 0.1980
agent1:                 episode reward: 0.0660,                 loss: nan
Episode: 57221/101000 (56.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8211s / 13682.2116 s
agent0:                 episode reward: 0.1185,                 loss: 0.1986
agent1:                 episode reward: -0.1185,                 loss: nan
Episode: 57241/101000 (56.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0922s / 13686.3038 s
agent0:                 episode reward: 0.0178,                 loss: 0.1959
agent1:                 episode reward: -0.0178,                 loss: nan
Episode: 57261/101000 (56.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6871s / 13689.9909 s
agent0:                 episode reward: 0.3718,                 loss: 0.1974
agent1:                 episode reward: -0.3718,                 loss: nan
Episode: 57281/101000 (56.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5228s / 13693.5137 s
agent0:                 episode reward: -0.5131,                 loss: 0.1990
agent1:                 episode reward: 0.5131,                 loss: 0.2182
Score delta: 1.532625079001289, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/56836_0.
Episode: 57301/101000 (56.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6232s / 13697.1369 s
agent0:                 episode reward: -0.2107,                 loss: nan
agent1:                 episode reward: 0.2107,                 loss: 0.2166
Episode: 57321/101000 (56.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6094s / 13700.7463 s
agent0:                 episode reward: -0.2299,                 loss: nan
agent1:                 episode reward: 0.2299,                 loss: 0.2143
Episode: 57341/101000 (56.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1266s / 13704.8729 s
agent0:                 episode reward: -0.6848,                 loss: nan
agent1:                 episode reward: 0.6848,                 loss: 0.2158
Episode: 57361/101000 (56.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4745s / 13708.3474 s
agent0:                 episode reward: 0.2091,                 loss: nan
agent1:                 episode reward: -0.2091,                 loss: 0.2167
Episode: 57381/101000 (56.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1823s / 13711.5297 s
agent0:                 episode reward: -0.2974,                 loss: nan
agent1:                 episode reward: 0.2974,                 loss: 0.2155
Episode: 57401/101000 (56.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8889s / 13715.4186 s
agent0:                 episode reward: -0.0368,                 loss: nan
agent1:                 episode reward: 0.0368,                 loss: 0.2163
Episode: 57421/101000 (56.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5136s / 13718.9322 s
agent0:                 episode reward: -0.3772,                 loss: nan
agent1:                 episode reward: 0.3772,                 loss: 0.2004
Episode: 57441/101000 (56.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7411s / 13722.6733 s
agent0:                 episode reward: -0.2401,                 loss: nan
agent1:                 episode reward: 0.2401,                 loss: 0.1943
Episode: 57461/101000 (56.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0830s / 13725.7564 s
agent0:                 episode reward: 0.0279,                 loss: 0.2077
agent1:                 episode reward: -0.0279,                 loss: 0.1936
Score delta: 1.5429713787720551, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/57022_1.
Episode: 57481/101000 (56.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8963s / 13729.6527 s
agent0:                 episode reward: -0.1759,                 loss: 0.2096
agent1:                 episode reward: 0.1759,                 loss: nan
Episode: 57501/101000 (56.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6295s / 13733.2822 s
agent0:                 episode reward: -0.5510,                 loss: 0.2076
agent1:                 episode reward: 0.5510,                 loss: nan
Episode: 57521/101000 (56.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1824s / 13737.4645 s
agent0:                 episode reward: -0.0502,                 loss: 0.2069
agent1:                 episode reward: 0.0502,                 loss: nan
Episode: 57541/101000 (56.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1203s / 13740.5848 s
agent0:                 episode reward: 0.4978,                 loss: 0.2079
agent1:                 episode reward: -0.4978,                 loss: nan
Episode: 57561/101000 (56.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4186s / 13745.0034 s
agent0:                 episode reward: 0.1099,                 loss: 0.2065
agent1:                 episode reward: -0.1099,                 loss: nan
Episode: 57581/101000 (57.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9545s / 13748.9579 s
agent0:                 episode reward: 0.2345,                 loss: 0.2069
agent1:                 episode reward: -0.2345,                 loss: 0.1685
Score delta: 1.9614634045565709, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/57142_0.
Episode: 57601/101000 (57.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0344s / 13752.9922 s
agent0:                 episode reward: -0.2422,                 loss: nan
agent1:                 episode reward: 0.2422,                 loss: 0.1667
Episode: 57621/101000 (57.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6906s / 13756.6829 s
agent0:                 episode reward: -0.5177,                 loss: nan
agent1:                 episode reward: 0.5177,                 loss: 0.1662
Episode: 57641/101000 (57.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1487s / 13759.8316 s
agent0:                 episode reward: -0.1861,                 loss: nan
agent1:                 episode reward: 0.1861,                 loss: 0.1684
Episode: 57661/101000 (57.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6809s / 13763.5125 s
agent0:                 episode reward: -0.4083,                 loss: nan
agent1:                 episode reward: 0.4083,                 loss: 0.1665
Episode: 57681/101000 (57.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0901s / 13766.6026 s
agent0:                 episode reward: -0.8205,                 loss: nan
agent1:                 episode reward: 0.8205,                 loss: 0.1661
Episode: 57701/101000 (57.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7612s / 13770.3638 s
agent0:                 episode reward: -0.2964,                 loss: 0.2201
agent1:                 episode reward: 0.2964,                 loss: 0.1673
Score delta: 1.7406907116667885, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/57273_1.
Episode: 57721/101000 (57.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8324s / 13774.1962 s
agent0:                 episode reward: -0.0694,                 loss: 0.2050
agent1:                 episode reward: 0.0694,                 loss: nan
Episode: 57741/101000 (57.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0561s / 13778.2523 s
agent0:                 episode reward: 0.0602,                 loss: 0.2031
agent1:                 episode reward: -0.0602,                 loss: nan
Episode: 57761/101000 (57.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0698s / 13782.3221 s
agent0:                 episode reward: 0.5115,                 loss: 0.2041
agent1:                 episode reward: -0.5115,                 loss: nan
Episode: 57781/101000 (57.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8578s / 13786.1799 s
agent0:                 episode reward: 0.2010,                 loss: 0.2049
agent1:                 episode reward: -0.2010,                 loss: nan
Episode: 57801/101000 (57.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9614s / 13790.1413 s
agent0:                 episode reward: 0.2689,                 loss: 0.2038
agent1:                 episode reward: -0.2689,                 loss: nan
Episode: 57821/101000 (57.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3033s / 13794.4446 s
agent0:                 episode reward: 0.1396,                 loss: 0.2029
agent1:                 episode reward: -0.1396,                 loss: nan
Episode: 57841/101000 (57.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3567s / 13798.8013 s
agent0:                 episode reward: -0.2317,                 loss: 0.2027
agent1:                 episode reward: 0.2317,                 loss: nan
Episode: 57861/101000 (57.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6643s / 13802.4657 s
agent0:                 episode reward: 0.1122,                 loss: 0.2027
agent1:                 episode reward: -0.1122,                 loss: nan
Episode: 57881/101000 (57.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6792s / 13806.1448 s
agent0:                 episode reward: 0.1028,                 loss: 0.2028
agent1:                 episode reward: -0.1028,                 loss: 0.2218
Score delta: 1.5098734749787837, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/57450_0.
Episode: 57901/101000 (57.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1898s / 13810.3347 s
agent0:                 episode reward: -0.5667,                 loss: nan
agent1:                 episode reward: 0.5667,                 loss: 0.2174
Episode: 57921/101000 (57.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9204s / 13814.2551 s
agent0:                 episode reward: 0.1174,                 loss: nan
agent1:                 episode reward: -0.1174,                 loss: 0.2175
Episode: 57941/101000 (57.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8601s / 13818.1152 s
agent0:                 episode reward: -0.8052,                 loss: nan
agent1:                 episode reward: 0.8052,                 loss: 0.2166
Episode: 57961/101000 (57.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5135s / 13821.6287 s
agent0:                 episode reward: -0.5261,                 loss: nan
agent1:                 episode reward: 0.5261,                 loss: 0.2175
Episode: 57981/101000 (57.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8907s / 13824.5194 s
agent0:                 episode reward: -0.5235,                 loss: nan
agent1:                 episode reward: 0.5235,                 loss: 0.2160
Episode: 58001/101000 (57.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4276s / 13827.9470 s
agent0:                 episode reward: -0.1761,                 loss: nan
agent1:                 episode reward: 0.1761,                 loss: 0.2150
Episode: 58021/101000 (57.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5168s / 13831.4638 s
agent0:                 episode reward: -0.5332,                 loss: nan
agent1:                 episode reward: 0.5332,                 loss: 0.2158
Episode: 58041/101000 (57.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4752s / 13835.9390 s
agent0:                 episode reward: -0.1563,                 loss: 0.1935
agent1:                 episode reward: 0.1563,                 loss: 0.2126
Score delta: 1.5077620431439338, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/57596_1.
Episode: 58061/101000 (57.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8550s / 13840.7940 s
agent0:                 episode reward: -0.1591,                 loss: 0.1921
agent1:                 episode reward: 0.1591,                 loss: nan
Episode: 58081/101000 (57.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0186s / 13844.8125 s
agent0:                 episode reward: 0.0256,                 loss: 0.1939
agent1:                 episode reward: -0.0256,                 loss: nan
Episode: 58101/101000 (57.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5208s / 13848.3334 s
agent0:                 episode reward: 0.3230,                 loss: 0.2006
agent1:                 episode reward: -0.3230,                 loss: nan
Episode: 58121/101000 (57.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5689s / 13851.9022 s
agent0:                 episode reward: 0.2561,                 loss: 0.2004
agent1:                 episode reward: -0.2561,                 loss: nan
Episode: 58141/101000 (57.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5987s / 13855.5010 s
agent0:                 episode reward: 0.0420,                 loss: 0.2027
agent1:                 episode reward: -0.0420,                 loss: nan
Episode: 58161/101000 (57.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1161s / 13859.6171 s
agent0:                 episode reward: 0.0292,                 loss: 0.2038
agent1:                 episode reward: -0.0292,                 loss: nan
Episode: 58181/101000 (57.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9115s / 13863.5286 s
agent0:                 episode reward: 0.0038,                 loss: 0.2031
agent1:                 episode reward: -0.0038,                 loss: nan
Episode: 58201/101000 (57.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3361s / 13866.8647 s
agent0:                 episode reward: 0.3695,                 loss: 0.2020
agent1:                 episode reward: -0.3695,                 loss: nan
Episode: 58221/101000 (57.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8745s / 13870.7392 s
agent0:                 episode reward: -0.5012,                 loss: 0.2013
agent1:                 episode reward: 0.5012,                 loss: nan
Episode: 58241/101000 (57.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0164s / 13874.7556 s
agent0:                 episode reward: 0.0634,                 loss: 0.2011
agent1:                 episode reward: -0.0634,                 loss: nan
Episode: 58261/101000 (57.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5595s / 13878.3151 s
agent0:                 episode reward: -0.1585,                 loss: 0.1992
agent1:                 episode reward: 0.1585,                 loss: nan
Episode: 58281/101000 (57.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9494s / 13882.2644 s
agent0:                 episode reward: 0.4316,                 loss: 0.2006
agent1:                 episode reward: -0.4316,                 loss: nan
Episode: 58301/101000 (57.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3889s / 13885.6533 s
agent0:                 episode reward: -0.3981,                 loss: 0.2009
agent1:                 episode reward: 0.3981,                 loss: nan
Episode: 58321/101000 (57.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5634s / 13889.2167 s
agent0:                 episode reward: 0.3941,                 loss: 0.1991
agent1:                 episode reward: -0.3941,                 loss: nan
Episode: 58341/101000 (57.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0902s / 13893.3069 s
agent0:                 episode reward: 0.2611,                 loss: 0.2011
agent1:                 episode reward: -0.2611,                 loss: nan
Episode: 58361/101000 (57.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0614s / 13897.3683 s
agent0:                 episode reward: 0.0324,                 loss: 0.2004
agent1:                 episode reward: -0.0324,                 loss: nan
Episode: 58381/101000 (57.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4444s / 13900.8127 s
agent0:                 episode reward: 0.0783,                 loss: 0.2003
agent1:                 episode reward: -0.0783,                 loss: nan
Episode: 58401/101000 (57.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4376s / 13905.2502 s
agent0:                 episode reward: -0.1219,                 loss: 0.2012
agent1:                 episode reward: 0.1219,                 loss: nan
Episode: 58421/101000 (57.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1489s / 13909.3991 s
agent0:                 episode reward: 0.1444,                 loss: 0.1994
agent1:                 episode reward: -0.1444,                 loss: nan
Episode: 58441/101000 (57.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6960s / 13913.0951 s
agent0:                 episode reward: 0.2024,                 loss: 0.1997
agent1:                 episode reward: -0.2024,                 loss: 0.1632
Score delta: 1.5303966688170452, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/58010_0.
Episode: 58461/101000 (57.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4564s / 13916.5515 s
agent0:                 episode reward: -0.4171,                 loss: nan
agent1:                 episode reward: 0.4171,                 loss: 0.1850
Episode: 58481/101000 (57.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0244s / 13920.5758 s
agent0:                 episode reward: 0.0436,                 loss: nan
agent1:                 episode reward: -0.0436,                 loss: 0.1951
Episode: 58501/101000 (57.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3070s / 13924.8828 s
agent0:                 episode reward: -0.2797,                 loss: nan
agent1:                 episode reward: 0.2797,                 loss: 0.1947
Episode: 58521/101000 (57.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6172s / 13928.5001 s
agent0:                 episode reward: 0.0840,                 loss: nan
agent1:                 episode reward: -0.0840,                 loss: 0.1933
Episode: 58541/101000 (57.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0261s / 13931.5261 s
agent0:                 episode reward: -0.3005,                 loss: nan
agent1:                 episode reward: 0.3005,                 loss: 0.1947
Episode: 58561/101000 (57.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4462s / 13934.9724 s
agent0:                 episode reward: 0.0636,                 loss: nan
agent1:                 episode reward: -0.0636,                 loss: 0.1935
Episode: 58581/101000 (58.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3514s / 13939.3238 s
agent0:                 episode reward: -0.2025,                 loss: nan
agent1:                 episode reward: 0.2025,                 loss: 0.1950
Episode: 58601/101000 (58.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5405s / 13942.8643 s
agent0:                 episode reward: -0.5962,                 loss: nan
agent1:                 episode reward: 0.5962,                 loss: 0.1926
Episode: 58621/101000 (58.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1010s / 13946.9653 s
agent0:                 episode reward: 0.1804,                 loss: 0.2501
agent1:                 episode reward: -0.1804,                 loss: 0.1941
Score delta: 1.5317373684198325, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/58176_1.
Episode: 58641/101000 (58.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7867s / 13950.7520 s
agent0:                 episode reward: -0.4911,                 loss: 0.2449
agent1:                 episode reward: 0.4911,                 loss: nan
Episode: 58661/101000 (58.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8666s / 13954.6186 s
agent0:                 episode reward: -0.1243,                 loss: 0.2410
agent1:                 episode reward: 0.1243,                 loss: nan
Episode: 58681/101000 (58.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6483s / 13958.2668 s
agent0:                 episode reward: 0.2935,                 loss: 0.2434
agent1:                 episode reward: -0.2935,                 loss: nan
Episode: 58701/101000 (58.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6664s / 13961.9332 s
agent0:                 episode reward: 0.1704,                 loss: 0.2431
agent1:                 episode reward: -0.1704,                 loss: nan
Episode: 58721/101000 (58.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9424s / 13965.8757 s
agent0:                 episode reward: 0.4500,                 loss: 0.2426
agent1:                 episode reward: -0.4500,                 loss: nan
Episode: 58741/101000 (58.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3401s / 13970.2158 s
agent0:                 episode reward: 0.0635,                 loss: 0.2427
agent1:                 episode reward: -0.0635,                 loss: nan
Episode: 58761/101000 (58.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2570s / 13973.4728 s
agent0:                 episode reward: -0.3393,                 loss: 0.2427
agent1:                 episode reward: 0.3393,                 loss: nan
Episode: 58781/101000 (58.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1768s / 13978.6495 s
agent0:                 episode reward: 0.0398,                 loss: 0.2403
agent1:                 episode reward: -0.0398,                 loss: nan
Episode: 58801/101000 (58.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5342s / 13982.1838 s
agent0:                 episode reward: 0.4798,                 loss: 0.2417
agent1:                 episode reward: -0.4798,                 loss: 0.1953
Score delta: 1.5526278991087263, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/58360_0.
Episode: 58821/101000 (58.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1015s / 13986.2853 s
agent0:                 episode reward: -0.4517,                 loss: nan
agent1:                 episode reward: 0.4517,                 loss: 0.1925
Episode: 58841/101000 (58.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2186s / 13989.5039 s
agent0:                 episode reward: -0.3920,                 loss: nan
agent1:                 episode reward: 0.3920,                 loss: 0.1921
Episode: 58861/101000 (58.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5524s / 13993.0563 s
agent0:                 episode reward: -0.0908,                 loss: nan
agent1:                 episode reward: 0.0908,                 loss: 0.1929
Episode: 58881/101000 (58.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0163s / 13996.0726 s
agent0:                 episode reward: -0.4138,                 loss: nan
agent1:                 episode reward: 0.4138,                 loss: 0.1923
Episode: 58901/101000 (58.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2494s / 13999.3220 s
agent0:                 episode reward: -0.4440,                 loss: nan
agent1:                 episode reward: 0.4440,                 loss: 0.1924
Episode: 58921/101000 (58.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8736s / 14002.1956 s
agent0:                 episode reward: -0.3027,                 loss: nan
agent1:                 episode reward: 0.3027,                 loss: 0.1905
Episode: 58941/101000 (58.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0789s / 14006.2744 s
agent0:                 episode reward: -0.4032,                 loss: nan
agent1:                 episode reward: 0.4032,                 loss: 0.1936
Episode: 58961/101000 (58.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9763s / 14010.2508 s
agent0:                 episode reward: -0.3564,                 loss: 0.2028
agent1:                 episode reward: 0.3564,                 loss: 0.1904
Score delta: 1.8235815470633217, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/58525_1.
Episode: 58981/101000 (58.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1767s / 14014.4274 s
agent0:                 episode reward: -0.0864,                 loss: 0.2022
agent1:                 episode reward: 0.0864,                 loss: nan
Episode: 59001/101000 (58.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7418s / 14018.1692 s
agent0:                 episode reward: -0.1745,                 loss: 0.2016
agent1:                 episode reward: 0.1745,                 loss: nan
Episode: 59021/101000 (58.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7208s / 14021.8901 s
agent0:                 episode reward: 0.0325,                 loss: 0.2014
agent1:                 episode reward: -0.0325,                 loss: nan
Episode: 59041/101000 (58.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9861s / 14025.8762 s
agent0:                 episode reward: 0.3918,                 loss: 0.1996
agent1:                 episode reward: -0.3918,                 loss: nan
Episode: 59061/101000 (58.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7681s / 14029.6443 s
agent0:                 episode reward: -0.2178,                 loss: 0.2013
agent1:                 episode reward: 0.2178,                 loss: nan
Episode: 59081/101000 (58.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9846s / 14034.6289 s
agent0:                 episode reward: 0.3674,                 loss: 0.2009
agent1:                 episode reward: -0.3674,                 loss: nan
Episode: 59101/101000 (58.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2832s / 14037.9121 s
agent0:                 episode reward: 0.0635,                 loss: 0.1962
agent1:                 episode reward: -0.0635,                 loss: nan
Episode: 59121/101000 (58.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6796s / 14041.5917 s
agent0:                 episode reward: 0.2362,                 loss: 0.1954
agent1:                 episode reward: -0.2362,                 loss: 0.1976
Score delta: 1.5634915844693673, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/58688_0.
Episode: 59141/101000 (58.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6533s / 14045.2451 s
agent0:                 episode reward: -0.5640,                 loss: nan
agent1:                 episode reward: 0.5640,                 loss: 0.1951
Episode: 59161/101000 (58.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7349s / 14048.9800 s
agent0:                 episode reward: -0.4302,                 loss: nan
agent1:                 episode reward: 0.4302,                 loss: 0.1908
Episode: 59181/101000 (58.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9438s / 14052.9238 s
agent0:                 episode reward: -0.2855,                 loss: nan
agent1:                 episode reward: 0.2855,                 loss: 0.1903
Episode: 59201/101000 (58.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2022s / 14057.1260 s
agent0:                 episode reward: -0.4050,                 loss: nan
agent1:                 episode reward: 0.4050,                 loss: 0.1903
Episode: 59221/101000 (58.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2768s / 14060.4028 s
agent0:                 episode reward: -0.5957,                 loss: nan
agent1:                 episode reward: 0.5957,                 loss: 0.1894
Episode: 59241/101000 (58.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0183s / 14064.4211 s
agent0:                 episode reward: -0.4810,                 loss: 0.2088
agent1:                 episode reward: 0.4810,                 loss: 0.1911
Score delta: 1.9295315070234675, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/58813_1.
Episode: 59261/101000 (58.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4859s / 14067.9070 s
agent0:                 episode reward: 0.2656,                 loss: 0.2073
agent1:                 episode reward: -0.2656,                 loss: nan
Episode: 59281/101000 (58.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8606s / 14071.7676 s
agent0:                 episode reward: 0.0521,                 loss: 0.2096
agent1:                 episode reward: -0.0521,                 loss: nan
Episode: 59301/101000 (58.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0309s / 14075.7985 s
agent0:                 episode reward: 0.0428,                 loss: 0.2065
agent1:                 episode reward: -0.0428,                 loss: nan
Episode: 59321/101000 (58.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7136s / 14079.5121 s
agent0:                 episode reward: 0.1539,                 loss: 0.2058
agent1:                 episode reward: -0.1539,                 loss: nan
Episode: 59341/101000 (58.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0621s / 14083.5741 s
agent0:                 episode reward: 0.1320,                 loss: 0.2052
agent1:                 episode reward: -0.1320,                 loss: nan
Episode: 59361/101000 (58.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6780s / 14087.2521 s
agent0:                 episode reward: 0.0420,                 loss: 0.2051
agent1:                 episode reward: -0.0420,                 loss: nan
Episode: 59381/101000 (58.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6571s / 14091.9092 s
agent0:                 episode reward: -0.3044,                 loss: 0.2068
agent1:                 episode reward: 0.3044,                 loss: nan
Episode: 59401/101000 (58.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5255s / 14095.4347 s
agent0:                 episode reward: 0.3645,                 loss: 0.2082
agent1:                 episode reward: -0.3645,                 loss: 0.1894
Score delta: 1.8004343984378373, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/58968_0.
Episode: 59421/101000 (58.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8270s / 14099.2617 s
agent0:                 episode reward: -0.3300,                 loss: nan
agent1:                 episode reward: 0.3300,                 loss: 0.1887
Episode: 59441/101000 (58.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2414s / 14103.5032 s
agent0:                 episode reward: -0.2750,                 loss: nan
agent1:                 episode reward: 0.2750,                 loss: 0.1878
Episode: 59461/101000 (58.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0742s / 14107.5773 s
agent0:                 episode reward: -0.4843,                 loss: nan
agent1:                 episode reward: 0.4843,                 loss: 0.1874
Episode: 59481/101000 (58.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0753s / 14111.6526 s
agent0:                 episode reward: -0.5790,                 loss: nan
agent1:                 episode reward: 0.5790,                 loss: 0.1891
Episode: 59501/101000 (58.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0086s / 14115.6612 s
agent0:                 episode reward: -0.2125,                 loss: nan
agent1:                 episode reward: 0.2125,                 loss: 0.1864
Episode: 59521/101000 (58.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3254s / 14119.9866 s
agent0:                 episode reward: 0.0879,                 loss: nan
agent1:                 episode reward: -0.0879,                 loss: 0.1874
Episode: 59541/101000 (58.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5443s / 14124.5309 s
agent0:                 episode reward: -0.5079,                 loss: 0.2414
agent1:                 episode reward: 0.5079,                 loss: 0.1856
Score delta: 1.8500850131840572, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/59106_1.
Episode: 59561/101000 (58.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9232s / 14128.4541 s
agent0:                 episode reward: 0.2583,                 loss: 0.2396
agent1:                 episode reward: -0.2583,                 loss: nan
Episode: 59581/101000 (58.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0335s / 14133.4876 s
agent0:                 episode reward: 0.0809,                 loss: 0.2406
agent1:                 episode reward: -0.0809,                 loss: nan
Episode: 59601/101000 (59.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8272s / 14137.3149 s
agent0:                 episode reward: 0.2299,                 loss: 0.2420
agent1:                 episode reward: -0.2299,                 loss: nan
Episode: 59621/101000 (59.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6109s / 14140.9258 s
agent0:                 episode reward: -0.3257,                 loss: 0.2383
agent1:                 episode reward: 0.3257,                 loss: nan
Episode: 59641/101000 (59.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8968s / 14144.8225 s
agent0:                 episode reward: 0.2429,                 loss: 0.2434
agent1:                 episode reward: -0.2429,                 loss: nan
Episode: 59661/101000 (59.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4585s / 14148.2810 s
agent0:                 episode reward: 0.2530,                 loss: 0.2407
agent1:                 episode reward: -0.2530,                 loss: nan
Episode: 59681/101000 (59.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9192s / 14152.2002 s
agent0:                 episode reward: 0.6233,                 loss: 0.2419
agent1:                 episode reward: -0.6233,                 loss: 0.2098
Score delta: 1.5098997364368603, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/59244_0.
Episode: 59701/101000 (59.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4268s / 14155.6270 s
agent0:                 episode reward: -0.2291,                 loss: nan
agent1:                 episode reward: 0.2291,                 loss: 0.1983
Episode: 59721/101000 (59.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8016s / 14159.4286 s
agent0:                 episode reward: -0.1121,                 loss: nan
agent1:                 episode reward: 0.1121,                 loss: 0.1976
Episode: 59741/101000 (59.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9544s / 14163.3830 s
agent0:                 episode reward: -0.2718,                 loss: nan
agent1:                 episode reward: 0.2718,                 loss: 0.1967
Episode: 59761/101000 (59.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1976s / 14167.5806 s
agent0:                 episode reward: -0.1398,                 loss: nan
agent1:                 episode reward: 0.1398,                 loss: 0.1944
Episode: 59781/101000 (59.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0888s / 14171.6693 s
agent0:                 episode reward: -0.5490,                 loss: nan
agent1:                 episode reward: 0.5490,                 loss: 0.1906
Episode: 59801/101000 (59.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1415s / 14175.8109 s
agent0:                 episode reward: -0.1447,                 loss: nan
agent1:                 episode reward: 0.1447,                 loss: 0.1869
Episode: 59821/101000 (59.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2373s / 14180.0481 s
agent0:                 episode reward: -0.5977,                 loss: 0.2010
agent1:                 episode reward: 0.5977,                 loss: 0.1864
Score delta: 1.5145946826788808, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/59385_1.
Episode: 59841/101000 (59.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9727s / 14184.0208 s
agent0:                 episode reward: 0.1370,                 loss: 0.1986
agent1:                 episode reward: -0.1370,                 loss: nan
Episode: 59861/101000 (59.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7284s / 14187.7493 s
agent0:                 episode reward: -0.1258,                 loss: 0.1957
agent1:                 episode reward: 0.1258,                 loss: nan
Episode: 59881/101000 (59.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7746s / 14192.5238 s
agent0:                 episode reward: -0.2572,                 loss: 0.1969
agent1:                 episode reward: 0.2572,                 loss: nan
Episode: 59901/101000 (59.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3632s / 14196.8870 s
agent0:                 episode reward: 0.0934,                 loss: 0.1962
agent1:                 episode reward: -0.0934,                 loss: nan
Episode: 59921/101000 (59.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9408s / 14200.8278 s
agent0:                 episode reward: -0.1530,                 loss: 0.1948
agent1:                 episode reward: 0.1530,                 loss: nan
Episode: 59941/101000 (59.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1610s / 14204.9889 s
agent0:                 episode reward: 0.2243,                 loss: 0.1968
agent1:                 episode reward: -0.2243,                 loss: nan
Episode: 59961/101000 (59.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9208s / 14208.9097 s
agent0:                 episode reward: -0.1834,                 loss: 0.1991
agent1:                 episode reward: 0.1834,                 loss: nan
Episode: 59981/101000 (59.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6682s / 14212.5778 s
agent0:                 episode reward: 0.2603,                 loss: 0.1968
agent1:                 episode reward: -0.2603,                 loss: 0.1972
Score delta: 1.5864974576693636, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/59548_0.
Episode: 60001/101000 (59.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1563s / 14215.7341 s
agent0:                 episode reward: -0.8156,                 loss: nan
agent1:                 episode reward: 0.8156,                 loss: 0.1963
Episode: 60021/101000 (59.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5402s / 14219.2743 s
agent0:                 episode reward: -0.6648,                 loss: nan
agent1:                 episode reward: 0.6648,                 loss: 0.1959
Episode: 60041/101000 (59.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5740s / 14222.8483 s
agent0:                 episode reward: -0.4174,                 loss: nan
agent1:                 episode reward: 0.4174,                 loss: 0.1939
Episode: 60061/101000 (59.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3707s / 14226.2190 s
agent0:                 episode reward: -0.3004,                 loss: nan
agent1:                 episode reward: 0.3004,                 loss: 0.1958
Episode: 60081/101000 (59.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5834s / 14229.8024 s
agent0:                 episode reward: -0.4521,                 loss: nan
agent1:                 episode reward: 0.4521,                 loss: 0.1949
Episode: 60101/101000 (59.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1929s / 14232.9953 s
agent0:                 episode reward: -0.3755,                 loss: nan
agent1:                 episode reward: 0.3755,                 loss: 0.1976
Episode: 60121/101000 (59.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2099s / 14237.2052 s
agent0:                 episode reward: -0.2422,                 loss: nan
agent1:                 episode reward: 0.2422,                 loss: 0.1954
Episode: 60141/101000 (59.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5549s / 14240.7602 s
agent0:                 episode reward: -0.1201,                 loss: nan
agent1:                 episode reward: 0.1201,                 loss: 0.1924
Episode: 60161/101000 (59.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1183s / 14244.8785 s
agent0:                 episode reward: -0.6677,                 loss: 0.3245
agent1:                 episode reward: 0.6677,                 loss: 0.1959
Score delta: 1.787099043333198, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/59729_1.
Episode: 60181/101000 (59.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3233s / 14249.2018 s
agent0:                 episode reward: -0.3305,                 loss: 0.3000
agent1:                 episode reward: 0.3305,                 loss: nan
Episode: 60201/101000 (59.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8100s / 14253.0117 s
agent0:                 episode reward: -0.1596,                 loss: 0.2951
agent1:                 episode reward: 0.1596,                 loss: nan
Episode: 60221/101000 (59.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0620s / 14257.0737 s
agent0:                 episode reward: -0.5976,                 loss: 0.2949
agent1:                 episode reward: 0.5976,                 loss: nan
Episode: 60241/101000 (59.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9318s / 14261.0055 s
agent0:                 episode reward: -0.1687,                 loss: 0.2932
agent1:                 episode reward: 0.1687,                 loss: nan
Episode: 60261/101000 (59.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2582s / 14265.2638 s
agent0:                 episode reward: -0.5625,                 loss: 0.2932
agent1:                 episode reward: 0.5625,                 loss: nan
Episode: 60281/101000 (59.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7993s / 14270.0631 s
agent0:                 episode reward: 0.4032,                 loss: 0.2914
agent1:                 episode reward: -0.4032,                 loss: nan
Episode: 60301/101000 (59.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6942s / 14273.7573 s
agent0:                 episode reward: -0.6142,                 loss: 0.2929
agent1:                 episode reward: 0.6142,                 loss: nan
Episode: 60321/101000 (59.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9689s / 14277.7262 s
agent0:                 episode reward: -0.2392,                 loss: 0.2911
agent1:                 episode reward: 0.2392,                 loss: nan
Episode: 60341/101000 (59.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1293s / 14281.8555 s
agent0:                 episode reward: 0.3701,                 loss: 0.2410
agent1:                 episode reward: -0.3701,                 loss: nan
Episode: 60361/101000 (59.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9507s / 14286.8062 s
agent0:                 episode reward: 0.0275,                 loss: 0.2033
agent1:                 episode reward: -0.0275,                 loss: nan
Episode: 60381/101000 (59.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8482s / 14290.6544 s
agent0:                 episode reward: -0.1003,                 loss: 0.2019
agent1:                 episode reward: 0.1003,                 loss: nan
Episode: 60401/101000 (59.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9642s / 14294.6186 s
agent0:                 episode reward: -0.2462,                 loss: 0.2012
agent1:                 episode reward: 0.2462,                 loss: nan
Episode: 60421/101000 (59.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8640s / 14298.4827 s
agent0:                 episode reward: -0.2021,                 loss: 0.1978
agent1:                 episode reward: 0.2021,                 loss: nan
Episode: 60441/101000 (59.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6481s / 14302.1308 s
agent0:                 episode reward: -0.5794,                 loss: 0.2011
agent1:                 episode reward: 0.5794,                 loss: nan
Episode: 60461/101000 (59.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7144s / 14305.8452 s
agent0:                 episode reward: -0.0569,                 loss: 0.2003
agent1:                 episode reward: 0.0569,                 loss: nan
Episode: 60481/101000 (59.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1178s / 14309.9630 s
agent0:                 episode reward: 0.3263,                 loss: 0.1984
agent1:                 episode reward: -0.3263,                 loss: 0.1888
Score delta: 1.6273936178053439, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/60043_0.
Episode: 60501/101000 (59.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6054s / 14312.5684 s
agent0:                 episode reward: -0.0365,                 loss: nan
agent1:                 episode reward: 0.0365,                 loss: 0.1885
Episode: 60521/101000 (59.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4057s / 14315.9741 s
agent0:                 episode reward: -0.4835,                 loss: nan
agent1:                 episode reward: 0.4835,                 loss: 0.1862
Episode: 60541/101000 (59.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6284s / 14319.6025 s
agent0:                 episode reward: -0.4389,                 loss: nan
agent1:                 episode reward: 0.4389,                 loss: 0.1872
Episode: 60561/101000 (59.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9076s / 14323.5101 s
agent0:                 episode reward: -0.3532,                 loss: nan
agent1:                 episode reward: 0.3532,                 loss: 0.1849
Episode: 60581/101000 (59.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4942s / 14327.0043 s
agent0:                 episode reward: 0.1876,                 loss: nan
agent1:                 episode reward: -0.1876,                 loss: 0.1906
Episode: 60601/101000 (60.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5096s / 14330.5139 s
agent0:                 episode reward: -0.8704,                 loss: 0.3394
agent1:                 episode reward: 0.8704,                 loss: 0.1927
Score delta: 1.6595670442209864, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/60172_1.
Episode: 60621/101000 (60.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8308s / 14335.3448 s
agent0:                 episode reward: -0.5464,                 loss: 0.2988
agent1:                 episode reward: 0.5464,                 loss: nan
Episode: 60641/101000 (60.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6079s / 14338.9527 s
agent0:                 episode reward: -0.3449,                 loss: 0.2968
agent1:                 episode reward: 0.3449,                 loss: nan
Episode: 60661/101000 (60.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4544s / 14342.4070 s
agent0:                 episode reward: -0.5622,                 loss: 0.2920
agent1:                 episode reward: 0.5622,                 loss: nan
Episode: 60681/101000 (60.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9543s / 14346.3613 s
agent0:                 episode reward: -0.7130,                 loss: 0.2931
agent1:                 episode reward: 0.7130,                 loss: nan
Episode: 60701/101000 (60.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1673s / 14350.5286 s
agent0:                 episode reward: 0.0863,                 loss: 0.2917
agent1:                 episode reward: -0.0863,                 loss: nan
Episode: 60721/101000 (60.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0419s / 14354.5706 s
agent0:                 episode reward: -0.5490,                 loss: 0.2911
agent1:                 episode reward: 0.5490,                 loss: nan
Episode: 60741/101000 (60.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2881s / 14358.8587 s
agent0:                 episode reward: -0.1585,                 loss: 0.2896
agent1:                 episode reward: 0.1585,                 loss: nan
Episode: 60761/101000 (60.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7282s / 14363.5868 s
agent0:                 episode reward: -0.5023,                 loss: 0.2919
agent1:                 episode reward: 0.5023,                 loss: nan
Episode: 60781/101000 (60.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7536s / 14367.3404 s
agent0:                 episode reward: 0.1337,                 loss: 0.2904
agent1:                 episode reward: -0.1337,                 loss: nan
Episode: 60801/101000 (60.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2930s / 14371.6334 s
agent0:                 episode reward: -0.3006,                 loss: 0.2514
agent1:                 episode reward: 0.3006,                 loss: nan
Episode: 60821/101000 (60.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4124s / 14376.0459 s
agent0:                 episode reward: 0.1420,                 loss: 0.2028
agent1:                 episode reward: -0.1420,                 loss: nan
Episode: 60841/101000 (60.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8024s / 14379.8483 s
agent0:                 episode reward: 0.2050,                 loss: 0.2019
agent1:                 episode reward: -0.2050,                 loss: nan
Episode: 60861/101000 (60.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3158s / 14384.1641 s
agent0:                 episode reward: -0.4483,                 loss: 0.2047
agent1:                 episode reward: 0.4483,                 loss: nan
Episode: 60881/101000 (60.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9510s / 14388.1151 s
agent0:                 episode reward: -0.0845,                 loss: 0.2013
agent1:                 episode reward: 0.0845,                 loss: nan
Episode: 60901/101000 (60.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7980s / 14392.9131 s
agent0:                 episode reward: -0.4262,                 loss: 0.2036
agent1:                 episode reward: 0.4262,                 loss: nan
Episode: 60921/101000 (60.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1381s / 14398.0512 s
agent0:                 episode reward: 0.1793,                 loss: 0.2023
agent1:                 episode reward: -0.1793,                 loss: nan
Episode: 60941/101000 (60.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2428s / 14402.2940 s
agent0:                 episode reward: -0.4300,                 loss: 0.2010
agent1:                 episode reward: 0.4300,                 loss: nan
Episode: 60961/101000 (60.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8320s / 14406.1259 s
agent0:                 episode reward: -0.5483,                 loss: 0.2018
agent1:                 episode reward: 0.5483,                 loss: nan
Episode: 60981/101000 (60.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9205s / 14409.0464 s
agent0:                 episode reward: 0.0927,                 loss: 0.2020
agent1:                 episode reward: -0.0927,                 loss: nan
Episode: 61001/101000 (60.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0922s / 14414.1386 s
agent0:                 episode reward: -0.2038,                 loss: 0.2005
agent1:                 episode reward: 0.2038,                 loss: nan
Episode: 61021/101000 (60.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1318s / 14419.2705 s
agent0:                 episode reward: -0.0807,                 loss: 0.2012
agent1:                 episode reward: 0.0807,                 loss: nan
Episode: 61041/101000 (60.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5047s / 14422.7751 s
agent0:                 episode reward: -0.1383,                 loss: 0.2017
agent1:                 episode reward: 0.1383,                 loss: nan
Episode: 61061/101000 (60.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6601s / 14426.4353 s
agent0:                 episode reward: -0.2739,                 loss: 0.2013
agent1:                 episode reward: 0.2739,                 loss: nan
Episode: 61081/101000 (60.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7161s / 14430.1513 s
agent0:                 episode reward: 0.2304,                 loss: 0.1986
agent1:                 episode reward: -0.2304,                 loss: nan
Episode: 61101/101000 (60.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8882s / 14434.0396 s
agent0:                 episode reward: -0.0912,                 loss: 0.2012
agent1:                 episode reward: 0.0912,                 loss: nan
Episode: 61121/101000 (60.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8770s / 14437.9165 s
agent0:                 episode reward: 0.1942,                 loss: 0.1978
agent1:                 episode reward: -0.1942,                 loss: nan
Episode: 61141/101000 (60.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3713s / 14442.2878 s
agent0:                 episode reward: -0.2489,                 loss: 0.2045
agent1:                 episode reward: 0.2489,                 loss: nan
Episode: 61161/101000 (60.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8996s / 14446.1875 s
agent0:                 episode reward: 0.1761,                 loss: 0.2043
agent1:                 episode reward: -0.1761,                 loss: nan
Episode: 61181/101000 (60.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2619s / 14450.4494 s
agent0:                 episode reward: -0.0822,                 loss: 0.2043
agent1:                 episode reward: 0.0822,                 loss: nan
Episode: 61201/101000 (60.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5668s / 14455.0162 s
agent0:                 episode reward: 0.2951,                 loss: 0.2034
agent1:                 episode reward: -0.2951,                 loss: nan
Episode: 61221/101000 (60.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7719s / 14458.7881 s
agent0:                 episode reward: 0.3063,                 loss: 0.2057
agent1:                 episode reward: -0.3063,                 loss: 0.1998
Score delta: 1.502158188198359, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/60777_0.
Episode: 61241/101000 (60.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2101s / 14462.9982 s
agent0:                 episode reward: -0.2870,                 loss: nan
agent1:                 episode reward: 0.2870,                 loss: 0.1950
Episode: 61261/101000 (60.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5982s / 14466.5964 s
agent0:                 episode reward: -0.5043,                 loss: nan
agent1:                 episode reward: 0.5043,                 loss: 0.1961
Episode: 61281/101000 (60.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6607s / 14470.2571 s
agent0:                 episode reward: -0.3555,                 loss: nan
agent1:                 episode reward: 0.3555,                 loss: 0.1958
Episode: 61301/101000 (60.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5786s / 14473.8357 s
agent0:                 episode reward: -0.5669,                 loss: nan
agent1:                 episode reward: 0.5669,                 loss: 0.1953
Episode: 61321/101000 (60.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6679s / 14477.5037 s
agent0:                 episode reward: -0.4353,                 loss: nan
agent1:                 episode reward: 0.4353,                 loss: 0.1954
Episode: 61341/101000 (60.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7080s / 14482.2117 s
agent0:                 episode reward: -0.4052,                 loss: nan
agent1:                 episode reward: 0.4052,                 loss: 0.1948
Episode: 61361/101000 (60.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7382s / 14485.9498 s
agent0:                 episode reward: -0.2386,                 loss: nan
agent1:                 episode reward: 0.2386,                 loss: 0.1941
Episode: 61381/101000 (60.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3787s / 14490.3285 s
agent0:                 episode reward: -0.5736,                 loss: 0.2039
agent1:                 episode reward: 0.5736,                 loss: 0.1933
Score delta: 1.5208726747940389, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/60950_1.
Episode: 61401/101000 (60.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3706s / 14494.6991 s
agent0:                 episode reward: -0.1347,                 loss: 0.2015
agent1:                 episode reward: 0.1347,                 loss: nan
Episode: 61421/101000 (60.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6077s / 14498.3067 s
agent0:                 episode reward: -0.2076,                 loss: 0.1972
agent1:                 episode reward: 0.2076,                 loss: nan
Episode: 61441/101000 (60.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9478s / 14502.2545 s
agent0:                 episode reward: -0.2828,                 loss: 0.1993
agent1:                 episode reward: 0.2828,                 loss: nan
Episode: 61461/101000 (60.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6001s / 14505.8546 s
agent0:                 episode reward: -0.0116,                 loss: 0.1977
agent1:                 episode reward: 0.0116,                 loss: nan
Episode: 61481/101000 (60.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3106s / 14510.1653 s
agent0:                 episode reward: -0.5024,                 loss: 0.1975
agent1:                 episode reward: 0.5024,                 loss: nan
Episode: 61501/101000 (60.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9938s / 14514.1590 s
agent0:                 episode reward: 0.0676,                 loss: 0.1987
agent1:                 episode reward: -0.0676,                 loss: nan
Episode: 61521/101000 (60.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0884s / 14518.2474 s
agent0:                 episode reward: -0.3276,                 loss: 0.2000
agent1:                 episode reward: 0.3276,                 loss: nan
Episode: 61541/101000 (60.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0007s / 14522.2482 s
agent0:                 episode reward: 0.2382,                 loss: 0.1967
agent1:                 episode reward: -0.2382,                 loss: nan
Episode: 61561/101000 (60.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5502s / 14525.7983 s
agent0:                 episode reward: 0.1066,                 loss: 0.1964
agent1:                 episode reward: -0.1066,                 loss: nan
Episode: 61581/101000 (60.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4148s / 14529.2132 s
agent0:                 episode reward: -0.1880,                 loss: 0.1957
agent1:                 episode reward: 0.1880,                 loss: nan
Episode: 61601/101000 (60.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2267s / 14533.4398 s
agent0:                 episode reward: 0.1547,                 loss: 0.1964
agent1:                 episode reward: -0.1547,                 loss: nan
Episode: 61621/101000 (61.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7878s / 14536.2277 s
agent0:                 episode reward: 0.0613,                 loss: 0.1945
agent1:                 episode reward: -0.0613,                 loss: nan
Episode: 61641/101000 (61.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1504s / 14540.3781 s
agent0:                 episode reward: -0.0716,                 loss: 0.2032
agent1:                 episode reward: 0.0716,                 loss: nan
Episode: 61661/101000 (61.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8335s / 14544.2116 s
agent0:                 episode reward: -0.3217,                 loss: 0.2076
agent1:                 episode reward: 0.3217,                 loss: nan
Episode: 61681/101000 (61.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8222s / 14548.0338 s
agent0:                 episode reward: -0.0226,                 loss: 0.2093
agent1:                 episode reward: 0.0226,                 loss: nan
Episode: 61701/101000 (61.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3088s / 14552.3425 s
agent0:                 episode reward: 0.0654,                 loss: 0.2092
agent1:                 episode reward: -0.0654,                 loss: nan
Episode: 61721/101000 (61.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9604s / 14556.3030 s
agent0:                 episode reward: -0.3353,                 loss: 0.2101
agent1:                 episode reward: 0.3353,                 loss: nan
Episode: 61741/101000 (61.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8750s / 14560.1780 s
agent0:                 episode reward: -0.1743,                 loss: 0.2086
agent1:                 episode reward: 0.1743,                 loss: nan
Episode: 61761/101000 (61.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0187s / 14565.1967 s
agent0:                 episode reward: -0.1079,                 loss: 0.2091
agent1:                 episode reward: 0.1079,                 loss: nan
Episode: 61781/101000 (61.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5138s / 14569.7105 s
agent0:                 episode reward: -0.1863,                 loss: 0.2100
agent1:                 episode reward: 0.1863,                 loss: nan
Episode: 61801/101000 (61.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7359s / 14574.4464 s
agent0:                 episode reward: -0.1908,                 loss: 0.2098
agent1:                 episode reward: 0.1908,                 loss: nan
Episode: 61821/101000 (61.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4935s / 14577.9399 s
agent0:                 episode reward: 0.0248,                 loss: 0.2096
agent1:                 episode reward: -0.0248,                 loss: 0.1939
Score delta: 1.6653296646725475, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/61385_0.
Episode: 61841/101000 (61.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9694s / 14581.9093 s
agent0:                 episode reward: -0.2503,                 loss: nan
agent1:                 episode reward: 0.2503,                 loss: 0.1960
Episode: 61861/101000 (61.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2567s / 14585.1659 s
agent0:                 episode reward: -0.7272,                 loss: nan
agent1:                 episode reward: 0.7272,                 loss: 0.1941
Episode: 61881/101000 (61.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3966s / 14588.5625 s
agent0:                 episode reward: -0.2221,                 loss: nan
agent1:                 episode reward: 0.2221,                 loss: 0.1948
Episode: 61901/101000 (61.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2078s / 14592.7703 s
agent0:                 episode reward: -0.5868,                 loss: nan
agent1:                 episode reward: 0.5868,                 loss: 0.1969
Episode: 61921/101000 (61.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3000s / 14596.0704 s
agent0:                 episode reward: -0.1187,                 loss: nan
agent1:                 episode reward: 0.1187,                 loss: 0.1931
Episode: 61941/101000 (61.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7542s / 14600.8245 s
agent0:                 episode reward: -0.4835,                 loss: nan
agent1:                 episode reward: 0.4835,                 loss: 0.1948
Episode: 61961/101000 (61.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1802s / 14605.0047 s
agent0:                 episode reward: -0.1790,                 loss: 0.2803
agent1:                 episode reward: 0.1790,                 loss: 0.1874
Score delta: 2.0489703207057155, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/61517_1.
Episode: 61981/101000 (61.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0326s / 14609.0373 s
agent0:                 episode reward: 0.3496,                 loss: 0.2750
agent1:                 episode reward: -0.3496,                 loss: nan
Episode: 62001/101000 (61.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5400s / 14613.5773 s
agent0:                 episode reward: -0.1528,                 loss: 0.2752
agent1:                 episode reward: 0.1528,                 loss: nan
Episode: 62021/101000 (61.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8963s / 14617.4736 s
agent0:                 episode reward: -0.0687,                 loss: 0.2740
agent1:                 episode reward: 0.0687,                 loss: nan
Episode: 62041/101000 (61.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0798s / 14621.5534 s
agent0:                 episode reward: 0.1121,                 loss: 0.2750
agent1:                 episode reward: -0.1121,                 loss: nan
Episode: 62061/101000 (61.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1628s / 14626.7162 s
agent0:                 episode reward: 0.4806,                 loss: 0.2738
agent1:                 episode reward: -0.4806,                 loss: nan
Episode: 62081/101000 (61.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5107s / 14630.2269 s
agent0:                 episode reward: -0.3904,                 loss: 0.2735
agent1:                 episode reward: 0.3904,                 loss: nan
Episode: 62101/101000 (61.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4272s / 14634.6542 s
agent0:                 episode reward: -0.1649,                 loss: 0.2613
agent1:                 episode reward: 0.1649,                 loss: nan
Episode: 62121/101000 (61.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9115s / 14638.5657 s
agent0:                 episode reward: 0.0207,                 loss: 0.2096
agent1:                 episode reward: -0.0207,                 loss: nan
Episode: 62141/101000 (61.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4547s / 14643.0204 s
agent0:                 episode reward: -0.0481,                 loss: 0.2082
agent1:                 episode reward: 0.0481,                 loss: nan
Episode: 62161/101000 (61.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1622s / 14647.1825 s
agent0:                 episode reward: 0.1659,                 loss: 0.2074
agent1:                 episode reward: -0.1659,                 loss: nan
Episode: 62181/101000 (61.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4964s / 14651.6789 s
agent0:                 episode reward: -0.2892,                 loss: 0.2035
agent1:                 episode reward: 0.2892,                 loss: nan
Episode: 62201/101000 (61.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5587s / 14656.2377 s
agent0:                 episode reward: -0.0145,                 loss: 0.2051
agent1:                 episode reward: 0.0145,                 loss: nan
Episode: 62221/101000 (61.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8493s / 14661.0870 s
agent0:                 episode reward: 0.1279,                 loss: 0.2053
agent1:                 episode reward: -0.1279,                 loss: nan
Episode: 62241/101000 (61.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1749s / 14666.2619 s
agent0:                 episode reward: 0.2771,                 loss: 0.2058
agent1:                 episode reward: -0.2771,                 loss: nan
Episode: 62261/101000 (61.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7249s / 14669.9867 s
agent0:                 episode reward: -0.0322,                 loss: 0.2057
agent1:                 episode reward: 0.0322,                 loss: nan
Episode: 62281/101000 (61.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1205s / 14675.1073 s
agent0:                 episode reward: 0.0681,                 loss: 0.2062
agent1:                 episode reward: -0.0681,                 loss: nan
Episode: 62301/101000 (61.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2987s / 14679.4060 s
agent0:                 episode reward: 0.2342,                 loss: 0.2085
agent1:                 episode reward: -0.2342,                 loss: 0.1989
Score delta: 1.6177612088728999, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/61869_0.
Episode: 62321/101000 (61.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8388s / 14683.2449 s
agent0:                 episode reward: -0.5545,                 loss: nan
agent1:                 episode reward: 0.5545,                 loss: 0.1957
Episode: 62341/101000 (61.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6739s / 14686.9187 s
agent0:                 episode reward: -0.2106,                 loss: nan
agent1:                 episode reward: 0.2106,                 loss: 0.1952
Episode: 62361/101000 (61.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2147s / 14691.1334 s
agent0:                 episode reward: -0.5967,                 loss: nan
agent1:                 episode reward: 0.5967,                 loss: 0.1960
Episode: 62381/101000 (61.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8961s / 14695.0295 s
agent0:                 episode reward: -0.7004,                 loss: nan
agent1:                 episode reward: 0.7004,                 loss: 0.1953
Episode: 62401/101000 (61.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7843s / 14698.8138 s
agent0:                 episode reward: -0.4743,                 loss: nan
agent1:                 episode reward: 0.4743,                 loss: 0.1961
Episode: 62421/101000 (61.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7612s / 14702.5750 s
agent0:                 episode reward: -0.6326,                 loss: nan
agent1:                 episode reward: 0.6326,                 loss: 0.1957
Episode: 62441/101000 (61.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8162s / 14706.3913 s
agent0:                 episode reward: -0.2481,                 loss: nan
agent1:                 episode reward: 0.2481,                 loss: 0.1959
Episode: 62461/101000 (61.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7796s / 14710.1709 s
agent0:                 episode reward: -0.4737,                 loss: nan
agent1:                 episode reward: 0.4737,                 loss: 0.1941
Episode: 62481/101000 (61.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8680s / 14714.0389 s
agent0:                 episode reward: 0.1167,                 loss: nan
agent1:                 episode reward: -0.1167,                 loss: 0.1948
Episode: 62501/101000 (61.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8365s / 14717.8753 s
agent0:                 episode reward: -0.6650,                 loss: 0.2906
agent1:                 episode reward: 0.6650,                 loss: 0.1949
Score delta: 1.818306418938287, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/62060_1.
Episode: 62521/101000 (61.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1179s / 14720.9933 s
agent0:                 episode reward: -0.3820,                 loss: 0.2752
agent1:                 episode reward: 0.3820,                 loss: nan
Episode: 62541/101000 (61.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2201s / 14725.2133 s
agent0:                 episode reward: -0.2124,                 loss: 0.2771
agent1:                 episode reward: 0.2124,                 loss: nan
Episode: 62561/101000 (61.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0711s / 14729.2845 s
agent0:                 episode reward: 0.0869,                 loss: 0.2765
agent1:                 episode reward: -0.0869,                 loss: nan
Episode: 62581/101000 (61.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5568s / 14733.8412 s
agent0:                 episode reward: -0.2758,                 loss: 0.2740
agent1:                 episode reward: 0.2758,                 loss: nan
Episode: 62601/101000 (61.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9654s / 14737.8066 s
agent0:                 episode reward: -0.0216,                 loss: 0.2739
agent1:                 episode reward: 0.0216,                 loss: nan
Episode: 62621/101000 (62.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2031s / 14742.0097 s
agent0:                 episode reward: -0.3744,                 loss: 0.2736
agent1:                 episode reward: 0.3744,                 loss: nan
Episode: 62641/101000 (62.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1003s / 14747.1100 s
agent0:                 episode reward: -0.0919,                 loss: 0.2029
agent1:                 episode reward: 0.0919,                 loss: nan
Episode: 62661/101000 (62.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4824s / 14751.5924 s
agent0:                 episode reward: 0.3013,                 loss: 0.1978
agent1:                 episode reward: -0.3013,                 loss: nan
Episode: 62681/101000 (62.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0208s / 14757.6132 s
agent0:                 episode reward: 0.0065,                 loss: 0.1968
agent1:                 episode reward: -0.0065,                 loss: nan
Episode: 62701/101000 (62.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4549s / 14762.0681 s
agent0:                 episode reward: -0.1493,                 loss: 0.1978
agent1:                 episode reward: 0.1493,                 loss: nan
Episode: 62721/101000 (62.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3465s / 14765.4145 s
agent0:                 episode reward: -0.4956,                 loss: 0.1978
agent1:                 episode reward: 0.4956,                 loss: nan
Episode: 62741/101000 (62.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1927s / 14769.6073 s
agent0:                 episode reward: 0.2118,                 loss: 0.1968
agent1:                 episode reward: -0.2118,                 loss: nan
Episode: 62761/101000 (62.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2721s / 14773.8794 s
agent0:                 episode reward: 0.3171,                 loss: 0.1954
agent1:                 episode reward: -0.3171,                 loss: nan
Episode: 62781/101000 (62.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7160s / 14777.5954 s
agent0:                 episode reward: 0.2233,                 loss: 0.2011
agent1:                 episode reward: -0.2233,                 loss: 0.2170
Score delta: 1.6716370944871966, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/62338_0.
Episode: 62801/101000 (62.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5322s / 14781.1276 s
agent0:                 episode reward: -0.5271,                 loss: nan
agent1:                 episode reward: 0.5271,                 loss: 0.2156
Episode: 62821/101000 (62.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1129s / 14785.2404 s
agent0:                 episode reward: -0.1380,                 loss: nan
agent1:                 episode reward: 0.1380,                 loss: 0.2151
Episode: 62841/101000 (62.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2229s / 14788.4634 s
agent0:                 episode reward: -0.5138,                 loss: nan
agent1:                 episode reward: 0.5138,                 loss: 0.2166
Episode: 62861/101000 (62.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1179s / 14791.5812 s
agent0:                 episode reward: -0.0103,                 loss: nan
agent1:                 episode reward: 0.0103,                 loss: 0.2162
Episode: 62881/101000 (62.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4906s / 14795.0718 s
agent0:                 episode reward: -0.3278,                 loss: nan
agent1:                 episode reward: 0.3278,                 loss: 0.2144
Episode: 62901/101000 (62.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0949s / 14799.1667 s
agent0:                 episode reward: -0.0393,                 loss: nan
agent1:                 episode reward: 0.0393,                 loss: 0.2153
Episode: 62921/101000 (62.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3410s / 14803.5077 s
agent0:                 episode reward: -0.4628,                 loss: nan
agent1:                 episode reward: 0.4628,                 loss: 0.1983
Episode: 62941/101000 (62.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1041s / 14807.6118 s
agent0:                 episode reward: 0.2848,                 loss: 0.1996
agent1:                 episode reward: -0.2848,                 loss: 0.1957
Score delta: 1.917447469061542, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/62497_1.
Episode: 62961/101000 (62.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8122s / 14811.4240 s
agent0:                 episode reward: 0.4990,                 loss: 0.2022
agent1:                 episode reward: -0.4990,                 loss: nan
Episode: 62981/101000 (62.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2346s / 14814.6586 s
agent0:                 episode reward: -0.1301,                 loss: 0.2014
agent1:                 episode reward: 0.1301,                 loss: nan
Episode: 63001/101000 (62.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1610s / 14818.8196 s
agent0:                 episode reward: 0.1941,                 loss: 0.2016
agent1:                 episode reward: -0.1941,                 loss: nan
Episode: 63021/101000 (62.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1377s / 14823.9573 s
agent0:                 episode reward: 0.2143,                 loss: 0.1978
agent1:                 episode reward: -0.2143,                 loss: nan
Episode: 63041/101000 (62.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5581s / 14827.5154 s
agent0:                 episode reward: 0.2444,                 loss: 0.2001
agent1:                 episode reward: -0.2444,                 loss: nan
Episode: 63061/101000 (62.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7115s / 14831.2269 s
agent0:                 episode reward: 0.1130,                 loss: 0.2026
agent1:                 episode reward: -0.1130,                 loss: nan
Episode: 63081/101000 (62.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9988s / 14835.2257 s
agent0:                 episode reward: -0.3151,                 loss: 0.2021
agent1:                 episode reward: 0.3151,                 loss: 0.1941
Score delta: 1.5323957746849781, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/62637_0.
Episode: 63101/101000 (62.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3870s / 14838.6127 s
agent0:                 episode reward: 0.0150,                 loss: nan
agent1:                 episode reward: -0.0150,                 loss: 0.1911
Episode: 63121/101000 (62.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9977s / 14842.6104 s
agent0:                 episode reward: -0.1894,                 loss: nan
agent1:                 episode reward: 0.1894,                 loss: 0.1907
Episode: 63141/101000 (62.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0986s / 14846.7090 s
agent0:                 episode reward: -0.2778,                 loss: nan
agent1:                 episode reward: 0.2778,                 loss: 0.1908
Episode: 63161/101000 (62.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9847s / 14850.6937 s
agent0:                 episode reward: -0.2339,                 loss: nan
agent1:                 episode reward: 0.2339,                 loss: 0.1914
Episode: 63181/101000 (62.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6514s / 14854.3451 s
agent0:                 episode reward: -0.5375,                 loss: nan
agent1:                 episode reward: 0.5375,                 loss: 0.1913
Episode: 63201/101000 (62.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0964s / 14857.4414 s
agent0:                 episode reward: -0.3725,                 loss: nan
agent1:                 episode reward: 0.3725,                 loss: 0.1883
Episode: 63221/101000 (62.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1721s / 14861.6136 s
agent0:                 episode reward: -0.1219,                 loss: nan
agent1:                 episode reward: 0.1219,                 loss: 0.1894
Episode: 63241/101000 (62.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4292s / 14866.0427 s
agent0:                 episode reward: -0.4435,                 loss: nan
agent1:                 episode reward: 0.4435,                 loss: 0.1891
Episode: 63261/101000 (62.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4912s / 14869.5340 s
agent0:                 episode reward: -0.3827,                 loss: nan
agent1:                 episode reward: 0.3827,                 loss: 0.1881
Episode: 63281/101000 (62.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0209s / 14873.5549 s
agent0:                 episode reward: -0.2898,                 loss: nan
agent1:                 episode reward: 0.2898,                 loss: 0.1899
Episode: 63301/101000 (62.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0452s / 14877.6001 s
agent0:                 episode reward: -0.4296,                 loss: nan
agent1:                 episode reward: 0.4296,                 loss: 0.1896
Episode: 63321/101000 (62.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4735s / 14882.0736 s
agent0:                 episode reward: -0.3566,                 loss: 0.2765
agent1:                 episode reward: 0.3566,                 loss: 0.1834
Score delta: 1.7173637703198732, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/62878_1.
Episode: 63341/101000 (62.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6304s / 14886.7040 s
agent0:                 episode reward: -0.0719,                 loss: 0.2720
agent1:                 episode reward: 0.0719,                 loss: nan
Episode: 63361/101000 (62.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1249s / 14890.8289 s
agent0:                 episode reward: -0.0347,                 loss: 0.2511
agent1:                 episode reward: 0.0347,                 loss: nan
Episode: 63381/101000 (62.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3247s / 14894.1535 s
agent0:                 episode reward: -0.0289,                 loss: 0.1957
agent1:                 episode reward: 0.0289,                 loss: nan
Episode: 63401/101000 (62.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5028s / 14898.6563 s
agent0:                 episode reward: 0.3009,                 loss: 0.1940
agent1:                 episode reward: -0.3009,                 loss: nan
Episode: 63421/101000 (62.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4529s / 14903.1092 s
agent0:                 episode reward: 0.2017,                 loss: 0.1931
agent1:                 episode reward: -0.2017,                 loss: nan
Episode: 63441/101000 (62.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3427s / 14907.4519 s
agent0:                 episode reward: 0.3080,                 loss: 0.1938
agent1:                 episode reward: -0.3080,                 loss: nan
Episode: 63461/101000 (62.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3597s / 14911.8115 s
agent0:                 episode reward: 0.1515,                 loss: 0.1940
agent1:                 episode reward: -0.1515,                 loss: nan
Episode: 63481/101000 (62.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1356s / 14915.9471 s
agent0:                 episode reward: 0.3172,                 loss: 0.1961
agent1:                 episode reward: -0.3172,                 loss: nan
Episode: 63501/101000 (62.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3053s / 14919.2525 s
agent0:                 episode reward: -0.3494,                 loss: 0.1931
agent1:                 episode reward: 0.3494,                 loss: nan
Episode: 63521/101000 (62.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8548s / 14923.1072 s
agent0:                 episode reward: 0.4687,                 loss: 0.1946
agent1:                 episode reward: -0.4687,                 loss: 0.1940
Score delta: 1.5510107541996492, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/63087_0.
Episode: 63541/101000 (62.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0767s / 14928.1840 s
agent0:                 episode reward: -1.1173,                 loss: nan
agent1:                 episode reward: 1.1173,                 loss: 0.1957
Episode: 63561/101000 (62.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5898s / 14931.7737 s
agent0:                 episode reward: -0.2765,                 loss: nan
agent1:                 episode reward: 0.2765,                 loss: 0.1925
Episode: 63581/101000 (62.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7977s / 14935.5714 s
agent0:                 episode reward: -0.0938,                 loss: nan
agent1:                 episode reward: 0.0938,                 loss: 0.1941
Episode: 63601/101000 (62.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7064s / 14939.2777 s
agent0:                 episode reward: -0.6079,                 loss: nan
agent1:                 episode reward: 0.6079,                 loss: 0.1947
Episode: 63621/101000 (62.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4230s / 14943.7007 s
agent0:                 episode reward: -0.2672,                 loss: nan
agent1:                 episode reward: 0.2672,                 loss: 0.1919
Episode: 63641/101000 (63.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8333s / 14947.5340 s
agent0:                 episode reward: -0.0863,                 loss: nan
agent1:                 episode reward: 0.0863,                 loss: 0.1935
Episode: 63661/101000 (63.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1975s / 14951.7315 s
agent0:                 episode reward: -0.2609,                 loss: 0.2408
agent1:                 episode reward: 0.2609,                 loss: 0.1929
Score delta: 1.6024323903488646, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/63226_1.
Episode: 63681/101000 (63.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1211s / 14955.8526 s
agent0:                 episode reward: -0.1533,                 loss: 0.2424
agent1:                 episode reward: 0.1533,                 loss: nan
Episode: 63701/101000 (63.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1913s / 14961.0439 s
agent0:                 episode reward: 0.0864,                 loss: 0.2399
agent1:                 episode reward: -0.0864,                 loss: nan
Episode: 63721/101000 (63.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7595s / 14965.8034 s
agent0:                 episode reward: 0.1116,                 loss: 0.2388
agent1:                 episode reward: -0.1116,                 loss: nan
Episode: 63741/101000 (63.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0592s / 14970.8626 s
agent0:                 episode reward: 0.2658,                 loss: 0.2408
agent1:                 episode reward: -0.2658,                 loss: nan
Episode: 63761/101000 (63.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2821s / 14975.1446 s
agent0:                 episode reward: 0.1432,                 loss: 0.2379
agent1:                 episode reward: -0.1432,                 loss: nan
Episode: 63781/101000 (63.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0326s / 14979.1772 s
agent0:                 episode reward: -0.1532,                 loss: 0.2397
agent1:                 episode reward: 0.1532,                 loss: nan
Episode: 63801/101000 (63.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5367s / 14982.7140 s
agent0:                 episode reward: -0.4135,                 loss: 0.2379
agent1:                 episode reward: 0.4135,                 loss: nan
Episode: 63821/101000 (63.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6291s / 14987.3430 s
agent0:                 episode reward: 0.2179,                 loss: 0.2397
agent1:                 episode reward: -0.2179,                 loss: nan
Episode: 63841/101000 (63.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6772s / 14992.0203 s
agent0:                 episode reward: 0.0111,                 loss: 0.2120
agent1:                 episode reward: -0.0111,                 loss: nan
Episode: 63861/101000 (63.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6826s / 14996.7029 s
agent0:                 episode reward: 0.0470,                 loss: 0.2010
agent1:                 episode reward: -0.0470,                 loss: nan
Episode: 63881/101000 (63.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2121s / 15000.9150 s
agent0:                 episode reward: 0.1213,                 loss: 0.2001
agent1:                 episode reward: -0.1213,                 loss: nan
Episode: 63901/101000 (63.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1997s / 15005.1147 s
agent0:                 episode reward: 0.1102,                 loss: 0.1979
agent1:                 episode reward: -0.1102,                 loss: nan
Episode: 63921/101000 (63.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0270s / 15009.1417 s
agent0:                 episode reward: -0.1454,                 loss: 0.1998
agent1:                 episode reward: 0.1454,                 loss: nan
Episode: 63941/101000 (63.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3771s / 15013.5188 s
agent0:                 episode reward: -0.1422,                 loss: 0.1977
agent1:                 episode reward: 0.1422,                 loss: nan
Episode: 63961/101000 (63.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2841s / 15017.8029 s
agent0:                 episode reward: 0.0397,                 loss: 0.1998
agent1:                 episode reward: -0.0397,                 loss: nan
Episode: 63981/101000 (63.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9617s / 15021.7646 s
agent0:                 episode reward: 0.2593,                 loss: 0.1968
agent1:                 episode reward: -0.2593,                 loss: nan
Episode: 64001/101000 (63.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7306s / 15025.4952 s
agent0:                 episode reward: -0.3254,                 loss: 0.1951
agent1:                 episode reward: 0.3254,                 loss: nan
Episode: 64021/101000 (63.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3777s / 15029.8729 s
agent0:                 episode reward: -0.1028,                 loss: 0.1986
agent1:                 episode reward: 0.1028,                 loss: nan
Episode: 64041/101000 (63.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5489s / 15034.4218 s
agent0:                 episode reward: 0.1547,                 loss: 0.1982
agent1:                 episode reward: -0.1547,                 loss: nan
Episode: 64061/101000 (63.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8364s / 15038.2582 s
agent0:                 episode reward: -0.1059,                 loss: 0.1970
agent1:                 episode reward: 0.1059,                 loss: nan
Episode: 64081/101000 (63.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9532s / 15042.2114 s
agent0:                 episode reward: 0.2161,                 loss: 0.2005
agent1:                 episode reward: -0.2161,                 loss: nan
Episode: 64101/101000 (63.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7084s / 15046.9197 s
agent0:                 episode reward: -0.2346,                 loss: 0.1970
agent1:                 episode reward: 0.2346,                 loss: nan
Episode: 64121/101000 (63.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2730s / 15051.1927 s
agent0:                 episode reward: -0.0957,                 loss: 0.1988
agent1:                 episode reward: 0.0957,                 loss: nan
Episode: 64141/101000 (63.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6609s / 15054.8536 s
agent0:                 episode reward: 0.2468,                 loss: 0.1954
agent1:                 episode reward: -0.2468,                 loss: 0.2204
Score delta: 1.9151198976769706, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/63705_0.
Episode: 64161/101000 (63.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2849s / 15059.1385 s
agent0:                 episode reward: -0.2719,                 loss: nan
agent1:                 episode reward: 0.2719,                 loss: 0.2173
Episode: 64181/101000 (63.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3864s / 15063.5249 s
agent0:                 episode reward: -0.3886,                 loss: nan
agent1:                 episode reward: 0.3886,                 loss: 0.2178
Episode: 64201/101000 (63.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0598s / 15067.5846 s
agent0:                 episode reward: -0.3315,                 loss: nan
agent1:                 episode reward: 0.3315,                 loss: 0.2168
Episode: 64221/101000 (63.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4331s / 15072.0177 s
agent0:                 episode reward: -0.6393,                 loss: nan
agent1:                 episode reward: 0.6393,                 loss: 0.2165
Episode: 64241/101000 (63.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6304s / 15075.6482 s
agent0:                 episode reward: -0.7888,                 loss: nan
agent1:                 episode reward: 0.7888,                 loss: 0.2159
Episode: 64261/101000 (63.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2644s / 15079.9125 s
agent0:                 episode reward: -0.4262,                 loss: nan
agent1:                 episode reward: 0.4262,                 loss: 0.2169
Episode: 64281/101000 (63.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6917s / 15084.6043 s
agent0:                 episode reward: -0.3049,                 loss: 0.2025
agent1:                 episode reward: 0.3049,                 loss: 0.2158
Score delta: 1.5157848957275768, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/63841_1.
Episode: 64301/101000 (63.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3018s / 15087.9060 s
agent0:                 episode reward: 0.3958,                 loss: 0.2005
agent1:                 episode reward: -0.3958,                 loss: nan
Episode: 64321/101000 (63.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0164s / 15092.9224 s
agent0:                 episode reward: -0.4891,                 loss: 0.2062
agent1:                 episode reward: 0.4891,                 loss: nan
Episode: 64341/101000 (63.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8805s / 15096.8029 s
agent0:                 episode reward: 0.1128,                 loss: 0.2036
agent1:                 episode reward: -0.1128,                 loss: nan
Episode: 64361/101000 (63.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6817s / 15101.4846 s
agent0:                 episode reward: -0.0343,                 loss: 0.2052
agent1:                 episode reward: 0.0343,                 loss: nan
Episode: 64381/101000 (63.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0490s / 15105.5336 s
agent0:                 episode reward: 0.0723,                 loss: 0.2054
agent1:                 episode reward: -0.0723,                 loss: nan
Episode: 64401/101000 (63.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6639s / 15110.1975 s
agent0:                 episode reward: 0.1334,                 loss: 0.2044
agent1:                 episode reward: -0.1334,                 loss: nan
Episode: 64421/101000 (63.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0362s / 15114.2337 s
agent0:                 episode reward: 0.0815,                 loss: 0.2047
agent1:                 episode reward: -0.0815,                 loss: nan
Episode: 64441/101000 (63.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5363s / 15118.7700 s
agent0:                 episode reward: 0.1621,                 loss: 0.2018
agent1:                 episode reward: -0.1621,                 loss: nan
Episode: 64461/101000 (63.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2270s / 15122.9971 s
agent0:                 episode reward: 0.1196,                 loss: 0.2042
agent1:                 episode reward: -0.1196,                 loss: nan
Episode: 64481/101000 (63.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3557s / 15127.3528 s
agent0:                 episode reward: 0.2901,                 loss: 0.2061
agent1:                 episode reward: -0.2901,                 loss: nan
Score delta: 1.6400752765762874, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/64055_0.
Episode: 64501/101000 (63.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8447s / 15131.1975 s
agent0:                 episode reward: -0.5083,                 loss: nan
agent1:                 episode reward: 0.5083,                 loss: 0.1986
Episode: 64521/101000 (63.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0306s / 15136.2280 s
agent0:                 episode reward: -0.2363,                 loss: nan
agent1:                 episode reward: 0.2363,                 loss: 0.1943
Episode: 64541/101000 (63.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8721s / 15140.1001 s
agent0:                 episode reward: -0.5875,                 loss: nan
agent1:                 episode reward: 0.5875,                 loss: 0.1949
Episode: 64561/101000 (63.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8332s / 15143.9333 s
agent0:                 episode reward: -0.2254,                 loss: nan
agent1:                 episode reward: 0.2254,                 loss: 0.1969
Episode: 64581/101000 (63.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3428s / 15148.2761 s
agent0:                 episode reward: -0.7446,                 loss: nan
agent1:                 episode reward: 0.7446,                 loss: 0.1936
Episode: 64601/101000 (63.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2661s / 15152.5421 s
agent0:                 episode reward: 0.3921,                 loss: nan
agent1:                 episode reward: -0.3921,                 loss: 0.1953
Episode: 64621/101000 (63.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9470s / 15155.4891 s
agent0:                 episode reward: -0.6543,                 loss: 0.1956
agent1:                 episode reward: 0.6543,                 loss: 0.1916
Score delta: 1.659467659190809, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/64192_1.
Episode: 64641/101000 (64.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6415s / 15160.1307 s
agent0:                 episode reward: -0.1853,                 loss: 0.1970
agent1:                 episode reward: 0.1853,                 loss: nan
Episode: 64661/101000 (64.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1151s / 15164.2458 s
agent0:                 episode reward: 0.0326,                 loss: 0.1980
agent1:                 episode reward: -0.0326,                 loss: nan
Episode: 64681/101000 (64.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5858s / 15168.8315 s
agent0:                 episode reward: 0.0273,                 loss: 0.1968
agent1:                 episode reward: -0.0273,                 loss: nan
Episode: 64701/101000 (64.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2416s / 15173.0731 s
agent0:                 episode reward: -0.1098,                 loss: 0.1983
agent1:                 episode reward: 0.1098,                 loss: nan
Episode: 64721/101000 (64.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9203s / 15176.9934 s
agent0:                 episode reward: -0.1045,                 loss: 0.1955
agent1:                 episode reward: 0.1045,                 loss: nan
Episode: 64741/101000 (64.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9849s / 15180.9783 s
agent0:                 episode reward: 0.2247,                 loss: 0.1971
agent1:                 episode reward: -0.2247,                 loss: nan
Episode: 64761/101000 (64.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5219s / 15185.5002 s
agent0:                 episode reward: 0.0669,                 loss: 0.1963
agent1:                 episode reward: -0.0669,                 loss: nan
Episode: 64781/101000 (64.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3876s / 15188.8878 s
agent0:                 episode reward: 0.4511,                 loss: 0.1994
agent1:                 episode reward: -0.4511,                 loss: nan
Episode: 64801/101000 (64.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9701s / 15192.8578 s
agent0:                 episode reward: -0.0712,                 loss: 0.2008
agent1:                 episode reward: 0.0712,                 loss: nan
Episode: 64821/101000 (64.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4482s / 15197.3060 s
agent0:                 episode reward: -0.3716,                 loss: 0.2007
agent1:                 episode reward: 0.3716,                 loss: nan
Episode: 64841/101000 (64.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7730s / 15201.0790 s
agent0:                 episode reward: -0.0173,                 loss: 0.2033
agent1:                 episode reward: 0.0173,                 loss: nan
Episode: 64861/101000 (64.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4423s / 15205.5213 s
agent0:                 episode reward: -0.1508,                 loss: 0.2031
agent1:                 episode reward: 0.1508,                 loss: nan
Episode: 64881/101000 (64.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4939s / 15209.0152 s
agent0:                 episode reward: 0.1869,                 loss: 0.2010
agent1:                 episode reward: -0.1869,                 loss: nan
Episode: 64901/101000 (64.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9397s / 15212.9549 s
agent0:                 episode reward: -0.0641,                 loss: 0.2015
agent1:                 episode reward: 0.0641,                 loss: nan
Episode: 64921/101000 (64.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8748s / 15217.8297 s
agent0:                 episode reward: 0.2324,                 loss: 0.2008
agent1:                 episode reward: -0.2324,                 loss: nan
Episode: 64941/101000 (64.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0219s / 15221.8516 s
agent0:                 episode reward: -0.1047,                 loss: 0.2000
agent1:                 episode reward: 0.1047,                 loss: 0.1962
Score delta: 1.53176549131658, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/64497_0.
Episode: 64961/101000 (64.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2551s / 15227.1068 s
agent0:                 episode reward: -0.6547,                 loss: nan
agent1:                 episode reward: 0.6547,                 loss: 0.1964
Episode: 64981/101000 (64.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1536s / 15231.2604 s
agent0:                 episode reward: -0.1500,                 loss: nan
agent1:                 episode reward: 0.1500,                 loss: 0.1964
Episode: 65001/101000 (64.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3547s / 15235.6151 s
agent0:                 episode reward: -0.6921,                 loss: nan
agent1:                 episode reward: 0.6921,                 loss: 0.1953
Episode: 65021/101000 (64.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0732s / 15239.6883 s
agent0:                 episode reward: -0.5675,                 loss: nan
agent1:                 episode reward: 0.5675,                 loss: 0.1954
Episode: 65041/101000 (64.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7176s / 15243.4059 s
agent0:                 episode reward: -0.1383,                 loss: nan
agent1:                 episode reward: 0.1383,                 loss: 0.1951
Episode: 65061/101000 (64.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1784s / 15247.5843 s
agent0:                 episode reward: -0.2876,                 loss: nan
agent1:                 episode reward: 0.2876,                 loss: 0.1941
Episode: 65081/101000 (64.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2852s / 15251.8695 s
agent0:                 episode reward: -0.4709,                 loss: 0.2061
agent1:                 episode reward: 0.4709,                 loss: 0.1905
Score delta: 1.5309024598115413, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/64640_1.
Episode: 65101/101000 (64.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6821s / 15256.5516 s
agent0:                 episode reward: 0.1471,                 loss: 0.2029
agent1:                 episode reward: -0.1471,                 loss: nan
Episode: 65121/101000 (64.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4992s / 15262.0507 s
agent0:                 episode reward: 0.1378,                 loss: 0.2009
agent1:                 episode reward: -0.1378,                 loss: nan
Episode: 65141/101000 (64.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4142s / 15266.4649 s
agent0:                 episode reward: -0.0063,                 loss: 0.2029
agent1:                 episode reward: 0.0063,                 loss: nan
Episode: 65161/101000 (64.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9578s / 15270.4227 s
agent0:                 episode reward: -0.1191,                 loss: 0.2007
agent1:                 episode reward: 0.1191,                 loss: nan
Episode: 65181/101000 (64.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9052s / 15275.3280 s
agent0:                 episode reward: -0.3311,                 loss: 0.2017
agent1:                 episode reward: 0.3311,                 loss: nan
Episode: 65201/101000 (64.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7606s / 15279.0886 s
agent0:                 episode reward: -0.0034,                 loss: 0.2027
agent1:                 episode reward: 0.0034,                 loss: nan
Episode: 65221/101000 (64.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4079s / 15283.4964 s
agent0:                 episode reward: -0.3233,                 loss: 0.1999
agent1:                 episode reward: 0.3233,                 loss: nan
Episode: 65241/101000 (64.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6179s / 15287.1143 s
agent0:                 episode reward: -0.2227,                 loss: 0.1996
agent1:                 episode reward: 0.2227,                 loss: nan
Episode: 65261/101000 (64.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3558s / 15292.4702 s
agent0:                 episode reward: -0.3436,                 loss: 0.2011
agent1:                 episode reward: 0.3436,                 loss: nan
Episode: 65281/101000 (64.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4107s / 15296.8809 s
agent0:                 episode reward: 0.2821,                 loss: 0.1954
agent1:                 episode reward: -0.2821,                 loss: 0.2060
Score delta: 1.6433257329902962, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/64842_0.
Episode: 65301/101000 (64.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7851s / 15300.6661 s
agent0:                 episode reward: -0.6578,                 loss: nan
agent1:                 episode reward: 0.6578,                 loss: 0.1974
Episode: 65321/101000 (64.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8809s / 15304.5470 s
agent0:                 episode reward: -0.2510,                 loss: nan
agent1:                 episode reward: 0.2510,                 loss: 0.1975
Episode: 65341/101000 (64.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5103s / 15308.0573 s
agent0:                 episode reward: -0.2896,                 loss: nan
agent1:                 episode reward: 0.2896,                 loss: 0.1978
Episode: 65361/101000 (64.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8029s / 15311.8603 s
agent0:                 episode reward: -0.5481,                 loss: nan
agent1:                 episode reward: 0.5481,                 loss: 0.1971
Episode: 65381/101000 (64.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2132s / 15316.0735 s
agent0:                 episode reward: -0.5188,                 loss: nan
agent1:                 episode reward: 0.5188,                 loss: 0.1964
Episode: 65401/101000 (64.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0246s / 15320.0981 s
agent0:                 episode reward: -0.2511,                 loss: nan
agent1:                 episode reward: 0.2511,                 loss: 0.1954
Episode: 65421/101000 (64.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9511s / 15324.0491 s
agent0:                 episode reward: -0.4565,                 loss: nan
agent1:                 episode reward: 0.4565,                 loss: 0.1946
Score delta: 1.7350180642431525, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/64995_1.
Episode: 65441/101000 (64.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9400s / 15329.9891 s
agent0:                 episode reward: -0.0960,                 loss: 0.1949
agent1:                 episode reward: 0.0960,                 loss: nan
Episode: 65461/101000 (64.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0156s / 15334.0047 s
agent0:                 episode reward: -0.0706,                 loss: 0.1921
agent1:                 episode reward: 0.0706,                 loss: nan
Episode: 65481/101000 (64.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2187s / 15338.2234 s
agent0:                 episode reward: -0.3610,                 loss: 0.1943
agent1:                 episode reward: 0.3610,                 loss: nan
Episode: 65501/101000 (64.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5484s / 15342.7718 s
agent0:                 episode reward: -0.1056,                 loss: 0.1938
agent1:                 episode reward: 0.1056,                 loss: nan
Episode: 65521/101000 (64.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9263s / 15346.6981 s
agent0:                 episode reward: -0.1144,                 loss: 0.1931
agent1:                 episode reward: 0.1144,                 loss: nan
Episode: 65541/101000 (64.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3511s / 15351.0493 s
agent0:                 episode reward: -0.1883,                 loss: 0.1932
agent1:                 episode reward: 0.1883,                 loss: nan
Episode: 65561/101000 (64.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6984s / 15355.7477 s
agent0:                 episode reward: 0.0413,                 loss: 0.1933
agent1:                 episode reward: -0.0413,                 loss: nan
Episode: 65581/101000 (64.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6517s / 15359.3994 s
agent0:                 episode reward: 0.1354,                 loss: 0.1940
agent1:                 episode reward: -0.1354,                 loss: nan
Episode: 65601/101000 (64.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6542s / 15363.0536 s
agent0:                 episode reward: -0.0382,                 loss: 0.1929
agent1:                 episode reward: 0.0382,                 loss: nan
Episode: 65621/101000 (64.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4315s / 15366.4851 s
agent0:                 episode reward: -0.0015,                 loss: 0.1911
agent1:                 episode reward: 0.0015,                 loss: nan
Episode: 65641/101000 (64.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2165s / 15370.7017 s
agent0:                 episode reward: 0.3930,                 loss: 0.1921
agent1:                 episode reward: -0.3930,                 loss: nan
Episode: 65661/101000 (65.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3582s / 15374.0598 s
agent0:                 episode reward: -0.3078,                 loss: 0.1907
agent1:                 episode reward: 0.3078,                 loss: 0.1874
Score delta: 1.5473187472827927, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/65218_0.
Episode: 65681/101000 (65.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4539s / 15377.5138 s
agent0:                 episode reward: -0.8622,                 loss: nan
agent1:                 episode reward: 0.8622,                 loss: 0.1877
Episode: 65701/101000 (65.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5288s / 15382.0426 s
agent0:                 episode reward: -0.6757,                 loss: nan
agent1:                 episode reward: 0.6757,                 loss: 0.1837
Episode: 65721/101000 (65.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8109s / 15385.8535 s
agent0:                 episode reward: -0.6143,                 loss: nan
agent1:                 episode reward: 0.6143,                 loss: 0.1847
Episode: 65741/101000 (65.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4902s / 15389.3438 s
agent0:                 episode reward: -0.5725,                 loss: nan
agent1:                 episode reward: 0.5725,                 loss: 0.1824
Episode: 65761/101000 (65.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5595s / 15392.9033 s
agent0:                 episode reward: -0.3968,                 loss: nan
agent1:                 episode reward: 0.3968,                 loss: 0.1830
Episode: 65781/101000 (65.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5942s / 15396.4975 s
agent0:                 episode reward: -0.3774,                 loss: nan
agent1:                 episode reward: 0.3774,                 loss: 0.1822
Episode: 65801/101000 (65.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6236s / 15400.1211 s
agent0:                 episode reward: -0.3138,                 loss: 0.2049
agent1:                 episode reward: 0.3138,                 loss: 0.1816
Score delta: 1.5882718429884113, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/65367_1.
Episode: 65821/101000 (65.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1129s / 15403.2340 s
agent0:                 episode reward: -0.3905,                 loss: 0.2043
agent1:                 episode reward: 0.3905,                 loss: nan
Episode: 65841/101000 (65.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9979s / 15407.2319 s
agent0:                 episode reward: -0.2806,                 loss: 0.2024
agent1:                 episode reward: 0.2806,                 loss: nan
Episode: 65861/101000 (65.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2787s / 15411.5106 s
agent0:                 episode reward: 0.1141,                 loss: 0.2043
agent1:                 episode reward: -0.1141,                 loss: nan
Episode: 65881/101000 (65.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5175s / 15416.0281 s
agent0:                 episode reward: 0.0319,                 loss: 0.2002
agent1:                 episode reward: -0.0319,                 loss: nan
Episode: 65901/101000 (65.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1151s / 15420.1432 s
agent0:                 episode reward: -0.0628,                 loss: 0.1953
agent1:                 episode reward: 0.0628,                 loss: nan
Episode: 65921/101000 (65.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0706s / 15424.2138 s
agent0:                 episode reward: -0.0665,                 loss: 0.1958
agent1:                 episode reward: 0.0665,                 loss: nan
Episode: 65941/101000 (65.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4885s / 15428.7023 s
agent0:                 episode reward: -0.1062,                 loss: 0.1953
agent1:                 episode reward: 0.1062,                 loss: nan
Episode: 65961/101000 (65.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3929s / 15432.0952 s
agent0:                 episode reward: -0.1309,                 loss: 0.1955
agent1:                 episode reward: 0.1309,                 loss: nan
Episode: 65981/101000 (65.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2157s / 15436.3109 s
agent0:                 episode reward: 0.1436,                 loss: 0.1947
agent1:                 episode reward: -0.1436,                 loss: 0.1963
Score delta: 1.8670271032788293, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/65551_0.
Episode: 66001/101000 (65.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6755s / 15439.9863 s
agent0:                 episode reward: -0.0471,                 loss: nan
agent1:                 episode reward: 0.0471,                 loss: 0.1938
Episode: 66021/101000 (65.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8734s / 15443.8598 s
agent0:                 episode reward: -0.1355,                 loss: nan
agent1:                 episode reward: 0.1355,                 loss: 0.1932
Episode: 66041/101000 (65.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8044s / 15448.6642 s
agent0:                 episode reward: -0.4553,                 loss: nan
agent1:                 episode reward: 0.4553,                 loss: 0.1939
Episode: 66061/101000 (65.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7360s / 15452.4002 s
agent0:                 episode reward: -0.1708,                 loss: nan
agent1:                 episode reward: 0.1708,                 loss: 0.1934
Episode: 66081/101000 (65.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2972s / 15456.6973 s
agent0:                 episode reward: -0.1716,                 loss: nan
agent1:                 episode reward: 0.1716,                 loss: 0.1940
Episode: 66101/101000 (65.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8280s / 15460.5253 s
agent0:                 episode reward: -0.3495,                 loss: nan
agent1:                 episode reward: 0.3495,                 loss: 0.1942
Episode: 66121/101000 (65.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8917s / 15464.4170 s
agent0:                 episode reward: -0.4224,                 loss: nan
agent1:                 episode reward: 0.4224,                 loss: 0.1938
Episode: 66141/101000 (65.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4364s / 15467.8533 s
agent0:                 episode reward: -0.0269,                 loss: nan
agent1:                 episode reward: 0.0269,                 loss: 0.1927
Episode: 66161/101000 (65.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9453s / 15471.7986 s
agent0:                 episode reward: -0.5381,                 loss: 0.2068
agent1:                 episode reward: 0.5381,                 loss: 0.1943
Score delta: 1.6983363414316386, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/65731_1.
Episode: 66181/101000 (65.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4130s / 15476.2116 s
agent0:                 episode reward: 0.0014,                 loss: 0.2067
agent1:                 episode reward: -0.0014,                 loss: nan
Episode: 66201/101000 (65.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9291s / 15480.1407 s
agent0:                 episode reward: 0.4548,                 loss: 0.2063
agent1:                 episode reward: -0.4548,                 loss: nan
Episode: 66221/101000 (65.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6134s / 15484.7541 s
agent0:                 episode reward: -0.2882,                 loss: 0.2048
agent1:                 episode reward: 0.2882,                 loss: nan
Episode: 66241/101000 (65.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3217s / 15489.0758 s
agent0:                 episode reward: -0.0429,                 loss: 0.2040
agent1:                 episode reward: 0.0429,                 loss: nan
Episode: 66261/101000 (65.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0756s / 15492.1513 s
agent0:                 episode reward: -0.0354,                 loss: 0.2058
agent1:                 episode reward: 0.0354,                 loss: nan
Episode: 66281/101000 (65.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1287s / 15496.2800 s
agent0:                 episode reward: -0.0098,                 loss: 0.2066
agent1:                 episode reward: 0.0098,                 loss: nan
Episode: 66301/101000 (65.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7777s / 15500.0577 s
agent0:                 episode reward: -0.1052,                 loss: 0.2054
agent1:                 episode reward: 0.1052,                 loss: nan
Episode: 66321/101000 (65.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2011s / 15504.2589 s
agent0:                 episode reward: -0.1599,                 loss: 0.2069
agent1:                 episode reward: 0.1599,                 loss: nan
Episode: 66341/101000 (65.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7919s / 15508.0508 s
agent0:                 episode reward: 0.0831,                 loss: 0.2064
agent1:                 episode reward: -0.0831,                 loss: 0.1895
Score delta: 1.7083223803469614, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/65908_0.
Episode: 66361/101000 (65.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8123s / 15512.8631 s
agent0:                 episode reward: -0.4630,                 loss: nan
agent1:                 episode reward: 0.4630,                 loss: 0.1913
Episode: 66381/101000 (65.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8439s / 15515.7070 s
agent0:                 episode reward: -0.6240,                 loss: nan
agent1:                 episode reward: 0.6240,                 loss: 0.1877
Episode: 66401/101000 (65.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1877s / 15518.8947 s
agent0:                 episode reward: -0.1056,                 loss: nan
agent1:                 episode reward: 0.1056,                 loss: 0.1852
Episode: 66421/101000 (65.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7695s / 15522.6642 s
agent0:                 episode reward: -0.6411,                 loss: nan
agent1:                 episode reward: 0.6411,                 loss: 0.1833
Episode: 66441/101000 (65.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9993s / 15526.6635 s
agent0:                 episode reward: -0.1418,                 loss: nan
agent1:                 episode reward: 0.1418,                 loss: 0.1826
Episode: 66461/101000 (65.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3658s / 15531.0293 s
agent0:                 episode reward: 0.3803,                 loss: nan
agent1:                 episode reward: -0.3803,                 loss: 0.1832
Episode: 66481/101000 (65.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2511s / 15535.2804 s
agent0:                 episode reward: 0.1667,                 loss: nan
agent1:                 episode reward: -0.1667,                 loss: 0.1838
Episode: 66501/101000 (65.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5569s / 15539.8373 s
agent0:                 episode reward: -0.1975,                 loss: 0.1988
agent1:                 episode reward: 0.1975,                 loss: 0.1821
Score delta: 1.6442606991982802, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/66066_1.
Episode: 66521/101000 (65.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5915s / 15544.4289 s
agent0:                 episode reward: -0.2812,                 loss: 0.1978
agent1:                 episode reward: 0.2812,                 loss: nan
Episode: 66541/101000 (65.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7990s / 15549.2278 s
agent0:                 episode reward: -0.0058,                 loss: 0.1978
agent1:                 episode reward: 0.0058,                 loss: nan
Episode: 66561/101000 (65.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2150s / 15553.4428 s
agent0:                 episode reward: -0.0678,                 loss: 0.1934
agent1:                 episode reward: 0.0678,                 loss: nan
Episode: 66581/101000 (65.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8994s / 15557.3422 s
agent0:                 episode reward: -0.0321,                 loss: 0.1903
agent1:                 episode reward: 0.0321,                 loss: nan
Episode: 66601/101000 (65.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4628s / 15560.8050 s
agent0:                 episode reward: 0.0454,                 loss: 0.1907
agent1:                 episode reward: -0.0454,                 loss: nan
Episode: 66621/101000 (65.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0127s / 15564.8177 s
agent0:                 episode reward: 0.0716,                 loss: 0.1929
agent1:                 episode reward: -0.0716,                 loss: nan
Episode: 66641/101000 (65.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0502s / 15568.8679 s
agent0:                 episode reward: 0.1443,                 loss: 0.1913
agent1:                 episode reward: -0.1443,                 loss: nan
Episode: 66661/101000 (66.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1108s / 15572.9787 s
agent0:                 episode reward: 0.0418,                 loss: 0.1897
agent1:                 episode reward: -0.0418,                 loss: 0.1948
Score delta: 1.5432323799383887, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/66224_0.
Episode: 66681/101000 (66.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5104s / 15577.4892 s
agent0:                 episode reward: -0.2923,                 loss: nan
agent1:                 episode reward: 0.2923,                 loss: 0.1954
Episode: 66701/101000 (66.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4168s / 15581.9060 s
agent0:                 episode reward: -0.1408,                 loss: nan
agent1:                 episode reward: 0.1408,                 loss: 0.1941
Episode: 66721/101000 (66.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2518s / 15586.1578 s
agent0:                 episode reward: -0.0088,                 loss: nan
agent1:                 episode reward: 0.0088,                 loss: 0.1951
Episode: 66741/101000 (66.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4892s / 15590.6470 s
agent0:                 episode reward: -0.6234,                 loss: nan
agent1:                 episode reward: 0.6234,                 loss: 0.1951
Episode: 66761/101000 (66.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4737s / 15595.1207 s
agent0:                 episode reward: -0.2946,                 loss: nan
agent1:                 episode reward: 0.2946,                 loss: 0.1929
Episode: 66781/101000 (66.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3117s / 15599.4324 s
agent0:                 episode reward: 0.0940,                 loss: nan
agent1:                 episode reward: -0.0940,                 loss: 0.1925
Episode: 66801/101000 (66.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8297s / 15603.2621 s
agent0:                 episode reward: -0.5946,                 loss: 0.3900
agent1:                 episode reward: 0.5946,                 loss: 0.1914
Score delta: 1.5644169775250185, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/66374_1.
Episode: 66821/101000 (66.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0877s / 15606.3498 s
agent0:                 episode reward: -0.7395,                 loss: 0.3035
agent1:                 episode reward: 0.7395,                 loss: nan
Episode: 66841/101000 (66.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2149s / 15610.5647 s
agent0:                 episode reward: -0.5063,                 loss: 0.2974
agent1:                 episode reward: 0.5063,                 loss: nan
Episode: 66861/101000 (66.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7299s / 15614.2945 s
agent0:                 episode reward: -0.2356,                 loss: 0.2965
agent1:                 episode reward: 0.2356,                 loss: nan
Episode: 66881/101000 (66.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0781s / 15618.3726 s
agent0:                 episode reward: -0.0439,                 loss: 0.2931
agent1:                 episode reward: 0.0439,                 loss: nan
Episode: 66901/101000 (66.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5511s / 15622.9237 s
agent0:                 episode reward: -0.5093,                 loss: 0.2957
agent1:                 episode reward: 0.5093,                 loss: nan
Episode: 66921/101000 (66.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0521s / 15626.9758 s
agent0:                 episode reward: -0.4160,                 loss: 0.2952
agent1:                 episode reward: 0.4160,                 loss: nan
Episode: 66941/101000 (66.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0932s / 15631.0690 s
agent0:                 episode reward: -0.8217,                 loss: 0.2939
agent1:                 episode reward: 0.8217,                 loss: nan
Episode: 66961/101000 (66.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5866s / 15634.6555 s
agent0:                 episode reward: -0.4448,                 loss: 0.2936
agent1:                 episode reward: 0.4448,                 loss: nan
Episode: 66981/101000 (66.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2037s / 15638.8593 s
agent0:                 episode reward: -0.2054,                 loss: 0.2925
agent1:                 episode reward: 0.2054,                 loss: nan
Episode: 67001/101000 (66.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1609s / 15642.0201 s
agent0:                 episode reward: -0.0661,                 loss: 0.2915
agent1:                 episode reward: 0.0661,                 loss: nan
Episode: 67021/101000 (66.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0404s / 15646.0605 s
agent0:                 episode reward: 0.0054,                 loss: 0.2892
agent1:                 episode reward: -0.0054,                 loss: nan
Episode: 67041/101000 (66.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3481s / 15649.4086 s
agent0:                 episode reward: -0.1982,                 loss: 0.2520
agent1:                 episode reward: 0.1982,                 loss: nan
Episode: 67061/101000 (66.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5875s / 15653.9961 s
agent0:                 episode reward: -0.5223,                 loss: 0.1890
agent1:                 episode reward: 0.5223,                 loss: nan
Episode: 67081/101000 (66.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1053s / 15659.1014 s
agent0:                 episode reward: 0.1840,                 loss: 0.1902
agent1:                 episode reward: -0.1840,                 loss: nan
Episode: 67101/101000 (66.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5973s / 15662.6987 s
agent0:                 episode reward: -0.2432,                 loss: 0.1891
agent1:                 episode reward: 0.2432,                 loss: nan
Episode: 67121/101000 (66.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0399s / 15667.7386 s
agent0:                 episode reward: -0.0280,                 loss: 0.1861
agent1:                 episode reward: 0.0280,                 loss: nan
Episode: 67141/101000 (66.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2936s / 15672.0322 s
agent0:                 episode reward: -0.1462,                 loss: 0.1868
agent1:                 episode reward: 0.1462,                 loss: nan
Episode: 67161/101000 (66.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0215s / 15677.0537 s
agent0:                 episode reward: -0.3137,                 loss: 0.1879
agent1:                 episode reward: 0.3137,                 loss: nan
Episode: 67181/101000 (66.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9964s / 15681.0502 s
agent0:                 episode reward: -0.1426,                 loss: 0.1881
agent1:                 episode reward: 0.1426,                 loss: nan
Episode: 67201/101000 (66.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5613s / 15685.6115 s
agent0:                 episode reward: 0.0303,                 loss: 0.1858
agent1:                 episode reward: -0.0303,                 loss: nan
Episode: 67221/101000 (66.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2209s / 15688.8324 s
agent0:                 episode reward: 0.0270,                 loss: 0.1871
agent1:                 episode reward: -0.0270,                 loss: 0.2192
Score delta: 1.59508766320069, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/66786_0.
Episode: 67241/101000 (66.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7639s / 15692.5963 s
agent0:                 episode reward: -0.2160,                 loss: nan
agent1:                 episode reward: 0.2160,                 loss: 0.2173
Episode: 67261/101000 (66.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4105s / 15698.0068 s
agent0:                 episode reward: -0.6839,                 loss: nan
agent1:                 episode reward: 0.6839,                 loss: 0.2155
Episode: 67281/101000 (66.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4363s / 15702.4430 s
agent0:                 episode reward: -0.3568,                 loss: nan
agent1:                 episode reward: 0.3568,                 loss: 0.2043
Episode: 67301/101000 (66.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0841s / 15706.5271 s
agent0:                 episode reward: -0.1263,                 loss: nan
agent1:                 episode reward: 0.1263,                 loss: 0.1868
Episode: 67321/101000 (66.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4272s / 15709.9543 s
agent0:                 episode reward: -0.5558,                 loss: nan
agent1:                 episode reward: 0.5558,                 loss: 0.1858
Episode: 67341/101000 (66.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1019s / 15714.0561 s
agent0:                 episode reward: -0.5429,                 loss: nan
agent1:                 episode reward: 0.5429,                 loss: 0.1876
Episode: 67361/101000 (66.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3205s / 15717.3766 s
agent0:                 episode reward: -0.4802,                 loss: nan
agent1:                 episode reward: 0.4802,                 loss: 0.1840
Episode: 67381/101000 (66.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9166s / 15721.2932 s
agent0:                 episode reward: -0.4364,                 loss: 0.2007
agent1:                 episode reward: 0.4364,                 loss: 0.1844
Score delta: 1.8140236576111686, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/66953_1.
Episode: 67401/101000 (66.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2533s / 15724.5464 s
agent0:                 episode reward: 0.1447,                 loss: 0.1987
agent1:                 episode reward: -0.1447,                 loss: nan
Episode: 67421/101000 (66.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7220s / 15728.2685 s
agent0:                 episode reward: -0.2863,                 loss: 0.1975
agent1:                 episode reward: 0.2863,                 loss: nan
Episode: 67441/101000 (66.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4476s / 15732.7160 s
agent0:                 episode reward: -0.0898,                 loss: 0.1976
agent1:                 episode reward: 0.0898,                 loss: nan
Episode: 67461/101000 (66.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5995s / 15737.3155 s
agent0:                 episode reward: 0.1632,                 loss: 0.1976
agent1:                 episode reward: -0.1632,                 loss: nan
Episode: 67481/101000 (66.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0058s / 15741.3212 s
agent0:                 episode reward: 0.0271,                 loss: 0.1983
agent1:                 episode reward: -0.0271,                 loss: nan
Episode: 67501/101000 (66.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8708s / 15745.1920 s
agent0:                 episode reward: -0.3449,                 loss: 0.1974
agent1:                 episode reward: 0.3449,                 loss: nan
Episode: 67521/101000 (66.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2201s / 15749.4121 s
agent0:                 episode reward: -0.0871,                 loss: 0.1984
agent1:                 episode reward: 0.0871,                 loss: nan
Episode: 67541/101000 (66.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4447s / 15752.8568 s
agent0:                 episode reward: 0.1531,                 loss: 0.1961
agent1:                 episode reward: -0.1531,                 loss: nan
Episode: 67561/101000 (66.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2609s / 15756.1177 s
agent0:                 episode reward: 0.4525,                 loss: 0.2023
agent1:                 episode reward: -0.4525,                 loss: 0.2103
Score delta: 1.6268573348579622, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/67134_0.
Episode: 67581/101000 (66.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2305s / 15760.3482 s
agent0:                 episode reward: 0.1024,                 loss: nan
agent1:                 episode reward: -0.1024,                 loss: 0.1968
Episode: 67601/101000 (66.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2830s / 15764.6311 s
agent0:                 episode reward: -0.3893,                 loss: nan
agent1:                 episode reward: 0.3893,                 loss: 0.1980
Episode: 67621/101000 (66.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7297s / 15769.3608 s
agent0:                 episode reward: -0.2221,                 loss: nan
agent1:                 episode reward: 0.2221,                 loss: 0.1934
Episode: 67641/101000 (66.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3122s / 15773.6731 s
agent0:                 episode reward: -0.2634,                 loss: nan
agent1:                 episode reward: 0.2634,                 loss: 0.1974
Episode: 67661/101000 (66.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7506s / 15778.4237 s
agent0:                 episode reward: -0.2232,                 loss: nan
agent1:                 episode reward: 0.2232,                 loss: 0.1952
Episode: 67681/101000 (67.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0506s / 15782.4743 s
agent0:                 episode reward: -0.3662,                 loss: nan
agent1:                 episode reward: 0.3662,                 loss: 0.1958
Episode: 67701/101000 (67.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0275s / 15786.5018 s
agent0:                 episode reward: -0.4040,                 loss: nan
agent1:                 episode reward: 0.4040,                 loss: 0.1949
Episode: 67721/101000 (67.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8190s / 15790.3209 s
agent0:                 episode reward: -0.6700,                 loss: 0.2006
agent1:                 episode reward: 0.6700,                 loss: 0.1945
Score delta: 1.5726012255323263, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/67290_1.
Episode: 67741/101000 (67.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6837s / 15795.0046 s
agent0:                 episode reward: -0.0631,                 loss: 0.2040
agent1:                 episode reward: 0.0631,                 loss: nan
Episode: 67761/101000 (67.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4541s / 15799.4587 s
agent0:                 episode reward: -0.0143,                 loss: 0.2029
agent1:                 episode reward: 0.0143,                 loss: nan
Episode: 67781/101000 (67.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2975s / 15803.7562 s
agent0:                 episode reward: 0.4380,                 loss: 0.1997
agent1:                 episode reward: -0.4380,                 loss: nan
Episode: 67801/101000 (67.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6714s / 15808.4276 s
agent0:                 episode reward: -0.1805,                 loss: 0.2030
agent1:                 episode reward: 0.1805,                 loss: nan
Episode: 67821/101000 (67.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2970s / 15812.7246 s
agent0:                 episode reward: 0.2098,                 loss: 0.2033
agent1:                 episode reward: -0.2098,                 loss: nan
Episode: 67841/101000 (67.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8580s / 15817.5826 s
agent0:                 episode reward: -0.1564,                 loss: 0.2012
agent1:                 episode reward: 0.1564,                 loss: nan
Episode: 67861/101000 (67.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8436s / 15821.4262 s
agent0:                 episode reward: -0.6843,                 loss: 0.2026
agent1:                 episode reward: 0.6843,                 loss: nan
Episode: 67881/101000 (67.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0702s / 15825.4964 s
agent0:                 episode reward: -0.5195,                 loss: 0.2014
agent1:                 episode reward: 0.5195,                 loss: nan
Episode: 67901/101000 (67.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2360s / 15829.7323 s
agent0:                 episode reward: 0.4382,                 loss: 0.2008
agent1:                 episode reward: -0.4382,                 loss: nan
Episode: 67921/101000 (67.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1786s / 15833.9109 s
agent0:                 episode reward: -0.2574,                 loss: 0.2023
agent1:                 episode reward: 0.2574,                 loss: nan
Episode: 67941/101000 (67.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2773s / 15838.1882 s
agent0:                 episode reward: -0.2368,                 loss: 0.2007
agent1:                 episode reward: 0.2368,                 loss: nan
Episode: 67961/101000 (67.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0655s / 15843.2537 s
agent0:                 episode reward: 0.2918,                 loss: 0.2016
agent1:                 episode reward: -0.2918,                 loss: nan
Episode: 67981/101000 (67.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4134s / 15847.6671 s
agent0:                 episode reward: -0.4406,                 loss: 0.1982
agent1:                 episode reward: 0.4406,                 loss: 0.1871
Score delta: 1.5275123424081958, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/67538_0.
Episode: 68001/101000 (67.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1858s / 15851.8530 s
agent0:                 episode reward: 0.0029,                 loss: nan
agent1:                 episode reward: -0.0029,                 loss: 0.1851
Episode: 68021/101000 (67.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0419s / 15855.8949 s
agent0:                 episode reward: -0.2250,                 loss: nan
agent1:                 episode reward: 0.2250,                 loss: 0.1844
Episode: 68041/101000 (67.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6956s / 15860.5905 s
agent0:                 episode reward: -0.2033,                 loss: nan
agent1:                 episode reward: 0.2033,                 loss: 0.1842
Episode: 68061/101000 (67.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5112s / 15865.1017 s
agent0:                 episode reward: 0.2857,                 loss: nan
agent1:                 episode reward: -0.2857,                 loss: 0.1849
Episode: 68081/101000 (67.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6661s / 15867.7678 s
agent0:                 episode reward: -0.2471,                 loss: nan
agent1:                 episode reward: 0.2471,                 loss: 0.1852
Episode: 68101/101000 (67.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7167s / 15871.4845 s
agent0:                 episode reward: -0.1766,                 loss: nan
agent1:                 episode reward: 0.1766,                 loss: 0.1833
Episode: 68121/101000 (67.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2369s / 15875.7214 s
agent0:                 episode reward: -0.3237,                 loss: nan
agent1:                 episode reward: 0.3237,                 loss: 0.1850
Episode: 68141/101000 (67.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6685s / 15880.3899 s
agent0:                 episode reward: 0.4140,                 loss: 0.1906
agent1:                 episode reward: -0.4140,                 loss: 0.1816
Score delta: 1.5204655974708665, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/67698_1.
Episode: 68161/101000 (67.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5164s / 15883.9063 s
agent0:                 episode reward: 0.1203,                 loss: 0.1891
agent1:                 episode reward: -0.1203,                 loss: nan
Episode: 68181/101000 (67.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7165s / 15887.6228 s
agent0:                 episode reward: 0.0172,                 loss: 0.1903
agent1:                 episode reward: -0.0172,                 loss: nan
Episode: 68201/101000 (67.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5205s / 15892.1433 s
agent0:                 episode reward: -0.5590,                 loss: 0.1848
agent1:                 episode reward: 0.5590,                 loss: nan
Episode: 68221/101000 (67.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6972s / 15896.8404 s
agent0:                 episode reward: 0.2705,                 loss: 0.1843
agent1:                 episode reward: -0.2705,                 loss: nan
Episode: 68241/101000 (67.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0210s / 15901.8615 s
agent0:                 episode reward: 0.2258,                 loss: 0.1864
agent1:                 episode reward: -0.2258,                 loss: nan
Episode: 68261/101000 (67.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6983s / 15906.5597 s
agent0:                 episode reward: -0.3359,                 loss: 0.1859
agent1:                 episode reward: 0.3359,                 loss: nan
Episode: 68281/101000 (67.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9437s / 15910.5034 s
agent0:                 episode reward: -0.1216,                 loss: 0.1865
agent1:                 episode reward: 0.1216,                 loss: nan
Episode: 68301/101000 (67.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0902s / 15914.5936 s
agent0:                 episode reward: 0.2503,                 loss: 0.1866
agent1:                 episode reward: -0.2503,                 loss: nan
Episode: 68321/101000 (67.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5403s / 15919.1339 s
agent0:                 episode reward: -0.1859,                 loss: 0.1862
agent1:                 episode reward: 0.1859,                 loss: nan
Episode: 68341/101000 (67.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1722s / 15923.3061 s
agent0:                 episode reward: 0.2734,                 loss: 0.1862
agent1:                 episode reward: -0.2734,                 loss: nan
Episode: 68361/101000 (67.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7829s / 15927.0890 s
agent0:                 episode reward: 0.3581,                 loss: 0.1847
agent1:                 episode reward: -0.3581,                 loss: nan
Episode: 68381/101000 (67.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1364s / 15931.2254 s
agent0:                 episode reward: 0.3078,                 loss: 0.1842
agent1:                 episode reward: -0.3078,                 loss: 0.1996
Score delta: 1.5579439861900126, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/67942_0.
Episode: 68401/101000 (67.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5885s / 15935.8140 s
agent0:                 episode reward: 0.0685,                 loss: nan
agent1:                 episode reward: -0.0685,                 loss: 0.1978
Episode: 68421/101000 (67.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6544s / 15939.4683 s
agent0:                 episode reward: -0.6402,                 loss: nan
agent1:                 episode reward: 0.6402,                 loss: 0.1945
Episode: 68441/101000 (67.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9174s / 15943.3857 s
agent0:                 episode reward: -0.3498,                 loss: nan
agent1:                 episode reward: 0.3498,                 loss: 0.1949
Episode: 68461/101000 (67.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0385s / 15947.4243 s
agent0:                 episode reward: -0.5478,                 loss: nan
agent1:                 episode reward: 0.5478,                 loss: 0.1960
Episode: 68481/101000 (67.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1347s / 15951.5590 s
agent0:                 episode reward: -0.5380,                 loss: nan
agent1:                 episode reward: 0.5380,                 loss: 0.1947
Episode: 68501/101000 (67.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6533s / 15956.2123 s
agent0:                 episode reward: -0.2643,                 loss: nan
agent1:                 episode reward: 0.2643,                 loss: 0.1980
Episode: 68521/101000 (67.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7363s / 15960.9486 s
agent0:                 episode reward: -0.2542,                 loss: nan
agent1:                 episode reward: 0.2542,                 loss: 0.1959
Episode: 68541/101000 (67.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2903s / 15965.2389 s
agent0:                 episode reward: -0.2715,                 loss: nan
agent1:                 episode reward: 0.2715,                 loss: 0.1953
Episode: 68561/101000 (67.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2511s / 15969.4900 s
agent0:                 episode reward: -0.5935,                 loss: nan
agent1:                 episode reward: 0.5935,                 loss: 0.1965
Score delta: 1.8322279036674753, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/68135_1.
Episode: 68581/101000 (67.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4108s / 15973.9008 s
agent0:                 episode reward: -0.3424,                 loss: 0.2063
agent1:                 episode reward: 0.3424,                 loss: nan
Episode: 68601/101000 (67.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4774s / 15978.3782 s
agent0:                 episode reward: -0.3296,                 loss: 0.2066
agent1:                 episode reward: 0.3296,                 loss: nan
Episode: 68621/101000 (67.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2341s / 15982.6123 s
agent0:                 episode reward: 0.4036,                 loss: 0.2086
agent1:                 episode reward: -0.4036,                 loss: nan
Episode: 68641/101000 (67.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8853s / 15986.4976 s
agent0:                 episode reward: 0.1647,                 loss: 0.2046
agent1:                 episode reward: -0.1647,                 loss: nan
Episode: 68661/101000 (67.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9712s / 15991.4688 s
agent0:                 episode reward: 0.1330,                 loss: 0.2060
agent1:                 episode reward: -0.1330,                 loss: nan
Episode: 68681/101000 (68.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0681s / 15995.5369 s
agent0:                 episode reward: -0.5039,                 loss: 0.2039
agent1:                 episode reward: 0.5039,                 loss: nan
Episode: 68701/101000 (68.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7377s / 15999.2746 s
agent0:                 episode reward: -0.1213,                 loss: 0.2054
agent1:                 episode reward: 0.1213,                 loss: nan
Episode: 68721/101000 (68.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1979s / 16003.4725 s
agent0:                 episode reward: 0.2826,                 loss: 0.2012
agent1:                 episode reward: -0.2826,                 loss: nan
Score delta: 1.567083344312096, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/68295_0.
Episode: 68741/101000 (68.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3652s / 16007.8377 s
agent0:                 episode reward: -0.0931,                 loss: nan
agent1:                 episode reward: 0.0931,                 loss: 0.1922
Episode: 68761/101000 (68.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8117s / 16011.6494 s
agent0:                 episode reward: -0.2595,                 loss: nan
agent1:                 episode reward: 0.2595,                 loss: 0.1906
Episode: 68781/101000 (68.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4368s / 16015.0863 s
agent0:                 episode reward: -0.4332,                 loss: nan
agent1:                 episode reward: 0.4332,                 loss: 0.1868
Episode: 68801/101000 (68.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9026s / 16018.9889 s
agent0:                 episode reward: -0.3059,                 loss: nan
agent1:                 episode reward: 0.3059,                 loss: 0.1770
Episode: 68821/101000 (68.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3276s / 16023.3164 s
agent0:                 episode reward: -0.2630,                 loss: nan
agent1:                 episode reward: 0.2630,                 loss: 0.1768
Episode: 68841/101000 (68.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7915s / 16027.1079 s
agent0:                 episode reward: -0.3377,                 loss: nan
agent1:                 episode reward: 0.3377,                 loss: 0.1764
Episode: 68861/101000 (68.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5848s / 16031.6927 s
agent0:                 episode reward: -0.1096,                 loss: nan
agent1:                 episode reward: 0.1096,                 loss: 0.1748
Episode: 68881/101000 (68.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0406s / 16034.7333 s
agent0:                 episode reward: -0.1375,                 loss: nan
agent1:                 episode reward: 0.1375,                 loss: 0.1747
Episode: 68901/101000 (68.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7297s / 16038.4629 s
agent0:                 episode reward: -0.7718,                 loss: 0.2447
agent1:                 episode reward: 0.7718,                 loss: 0.1758
Score delta: 1.6327847233161965, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/68463_1.
Episode: 68921/101000 (68.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3912s / 16042.8541 s
agent0:                 episode reward: -0.6364,                 loss: 0.2335
agent1:                 episode reward: 0.6364,                 loss: nan
Episode: 68941/101000 (68.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6422s / 16047.4962 s
agent0:                 episode reward: -0.2510,                 loss: 0.2310
agent1:                 episode reward: 0.2510,                 loss: nan
Episode: 68961/101000 (68.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2217s / 16051.7180 s
agent0:                 episode reward: -0.3045,                 loss: 0.2276
agent1:                 episode reward: 0.3045,                 loss: nan
Episode: 68981/101000 (68.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5879s / 16055.3059 s
agent0:                 episode reward: 0.0300,                 loss: 0.2268
agent1:                 episode reward: -0.0300,                 loss: nan
Episode: 69001/101000 (68.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6076s / 16058.9135 s
agent0:                 episode reward: -0.3491,                 loss: 0.2284
agent1:                 episode reward: 0.3491,                 loss: nan
Episode: 69021/101000 (68.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3764s / 16063.2899 s
agent0:                 episode reward: 0.2702,                 loss: 0.2250
agent1:                 episode reward: -0.2702,                 loss: nan
Episode: 69041/101000 (68.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4582s / 16067.7481 s
agent0:                 episode reward: -0.2001,                 loss: 0.2251
agent1:                 episode reward: 0.2001,                 loss: nan
Episode: 69061/101000 (68.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4046s / 16072.1527 s
agent0:                 episode reward: -0.5336,                 loss: 0.2240
agent1:                 episode reward: 0.5336,                 loss: nan
Episode: 69081/101000 (68.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5995s / 16075.7522 s
agent0:                 episode reward: -0.3615,                 loss: 0.2243
agent1:                 episode reward: 0.3615,                 loss: nan
Episode: 69101/101000 (68.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0779s / 16079.8301 s
agent0:                 episode reward: 0.2302,                 loss: 0.2265
agent1:                 episode reward: -0.2302,                 loss: 0.2024
Score delta: 1.7744086752228756, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/68670_0.
Episode: 69121/101000 (68.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8043s / 16083.6344 s
agent0:                 episode reward: -0.2648,                 loss: nan
agent1:                 episode reward: 0.2648,                 loss: 0.1952
Episode: 69141/101000 (68.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3743s / 16088.0087 s
agent0:                 episode reward: -0.5819,                 loss: nan
agent1:                 episode reward: 0.5819,                 loss: 0.1950
Episode: 69161/101000 (68.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0018s / 16092.0105 s
agent0:                 episode reward: -0.5510,                 loss: nan
agent1:                 episode reward: 0.5510,                 loss: 0.1950
Episode: 69181/101000 (68.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9231s / 16095.9336 s
agent0:                 episode reward: -0.5400,                 loss: nan
agent1:                 episode reward: 0.5400,                 loss: 0.1937
Episode: 69201/101000 (68.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7267s / 16099.6603 s
agent0:                 episode reward: -0.3483,                 loss: nan
agent1:                 episode reward: 0.3483,                 loss: 0.1953
Episode: 69221/101000 (68.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4956s / 16103.1559 s
agent0:                 episode reward: -0.4192,                 loss: nan
agent1:                 episode reward: 0.4192,                 loss: 0.1935
Episode: 69241/101000 (68.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8104s / 16106.9663 s
agent0:                 episode reward: -0.5128,                 loss: nan
agent1:                 episode reward: 0.5128,                 loss: 0.1936
Episode: 69261/101000 (68.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7483s / 16110.7146 s
agent0:                 episode reward: -0.1673,                 loss: nan
agent1:                 episode reward: 0.1673,                 loss: 0.1944
Episode: 69281/101000 (68.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7304s / 16114.4450 s
agent0:                 episode reward: -0.4649,                 loss: 0.2059
agent1:                 episode reward: 0.4649,                 loss: 0.1915
Score delta: 1.8802826098288374, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/68839_1.
Episode: 69301/101000 (68.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1086s / 16118.5537 s
agent0:                 episode reward: -0.0245,                 loss: 0.1993
agent1:                 episode reward: 0.0245,                 loss: nan
Episode: 69321/101000 (68.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8928s / 16122.4464 s
agent0:                 episode reward: 0.1122,                 loss: 0.1994
agent1:                 episode reward: -0.1122,                 loss: nan
Episode: 69341/101000 (68.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2668s / 16126.7132 s
agent0:                 episode reward: -0.3924,                 loss: 0.2000
agent1:                 episode reward: 0.3924,                 loss: nan
Episode: 69361/101000 (68.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4592s / 16131.1724 s
agent0:                 episode reward: 0.2283,                 loss: 0.2019
agent1:                 episode reward: -0.2283,                 loss: nan
Episode: 69381/101000 (68.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7857s / 16134.9581 s
agent0:                 episode reward: 0.1025,                 loss: 0.2037
agent1:                 episode reward: -0.1025,                 loss: nan
Episode: 69401/101000 (68.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9271s / 16139.8852 s
agent0:                 episode reward: 0.2891,                 loss: 0.2069
agent1:                 episode reward: -0.2891,                 loss: nan
Episode: 69421/101000 (68.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8642s / 16143.7494 s
agent0:                 episode reward: 0.1484,                 loss: 0.2031
agent1:                 episode reward: -0.1484,                 loss: nan
Episode: 69441/101000 (68.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3662s / 16148.1156 s
agent0:                 episode reward: -0.0934,                 loss: 0.2048
agent1:                 episode reward: 0.0934,                 loss: 0.1950
Score delta: 1.6510047779337262, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/69002_0.
Episode: 69461/101000 (68.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6429s / 16152.7585 s
agent0:                 episode reward: -0.3999,                 loss: nan
agent1:                 episode reward: 0.3999,                 loss: 0.1907
Episode: 69481/101000 (68.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0506s / 16156.8091 s
agent0:                 episode reward: -0.2627,                 loss: nan
agent1:                 episode reward: 0.2627,                 loss: 0.1923
Episode: 69501/101000 (68.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9032s / 16160.7123 s
agent0:                 episode reward: 0.0949,                 loss: nan
agent1:                 episode reward: -0.0949,                 loss: 0.1818
Episode: 69521/101000 (68.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6839s / 16164.3962 s
agent0:                 episode reward: -0.5528,                 loss: nan
agent1:                 episode reward: 0.5528,                 loss: 0.1826
Episode: 69541/101000 (68.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5578s / 16168.9539 s
agent0:                 episode reward: -0.5129,                 loss: nan
agent1:                 episode reward: 0.5129,                 loss: 0.1813
Episode: 69561/101000 (68.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2525s / 16173.2065 s
agent0:                 episode reward: -0.4001,                 loss: nan
agent1:                 episode reward: 0.4001,                 loss: 0.1818
Episode: 69581/101000 (68.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9192s / 16177.1257 s
agent0:                 episode reward: -0.1035,                 loss: nan
agent1:                 episode reward: 0.1035,                 loss: 0.1810
Episode: 69601/101000 (68.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5172s / 16181.6429 s
agent0:                 episode reward: -0.4143,                 loss: nan
agent1:                 episode reward: 0.4143,                 loss: 0.1834
Episode: 69621/101000 (68.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2131s / 16185.8560 s
agent0:                 episode reward: -0.1267,                 loss: nan
agent1:                 episode reward: 0.1267,                 loss: 0.1818
Episode: 69641/101000 (68.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4153s / 16190.2713 s
agent0:                 episode reward: -0.4217,                 loss: nan
agent1:                 episode reward: 0.4217,                 loss: 0.1805
Episode: 69661/101000 (68.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6936s / 16193.9648 s
agent0:                 episode reward: -0.5225,                 loss: nan
agent1:                 episode reward: 0.5225,                 loss: 0.1826
Episode: 69681/101000 (68.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1189s / 16199.0838 s
agent0:                 episode reward: -0.5547,                 loss: 0.2068
agent1:                 episode reward: 0.5547,                 loss: 0.1826
Score delta: 1.6481435094816521, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/69237_1.
Episode: 69701/101000 (69.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6757s / 16203.7594 s
agent0:                 episode reward: 0.4920,                 loss: 0.2066
agent1:                 episode reward: -0.4920,                 loss: nan
Episode: 69721/101000 (69.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4310s / 16208.1904 s
agent0:                 episode reward: 0.2685,                 loss: 0.2054
agent1:                 episode reward: -0.2685,                 loss: nan
Episode: 69741/101000 (69.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5861s / 16212.7765 s
agent0:                 episode reward: -0.0291,                 loss: 0.2069
agent1:                 episode reward: 0.0291,                 loss: nan
Episode: 69761/101000 (69.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8548s / 16217.6313 s
agent0:                 episode reward: 0.2478,                 loss: 0.2058
agent1:                 episode reward: -0.2478,                 loss: nan
Episode: 69781/101000 (69.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5352s / 16221.1665 s
agent0:                 episode reward: -0.2210,                 loss: 0.2054
agent1:                 episode reward: 0.2210,                 loss: nan
Episode: 69801/101000 (69.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6645s / 16224.8310 s
agent0:                 episode reward: -0.5251,                 loss: 0.2057
agent1:                 episode reward: 0.5251,                 loss: nan
Episode: 69821/101000 (69.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3381s / 16229.1690 s
agent0:                 episode reward: 0.1093,                 loss: 0.2063
agent1:                 episode reward: -0.1093,                 loss: nan
Episode: 69841/101000 (69.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4544s / 16233.6234 s
agent0:                 episode reward: 0.0938,                 loss: 0.2042
agent1:                 episode reward: -0.0938,                 loss: nan
Episode: 69861/101000 (69.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3856s / 16238.0090 s
agent0:                 episode reward: 0.1077,                 loss: 0.2042
agent1:                 episode reward: -0.1077,                 loss: nan
Episode: 69881/101000 (69.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1344s / 16242.1434 s
agent0:                 episode reward: -0.1701,                 loss: 0.2037
agent1:                 episode reward: 0.1701,                 loss: nan
Episode: 69901/101000 (69.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3117s / 16246.4551 s
agent0:                 episode reward: -0.2592,                 loss: 0.2042
agent1:                 episode reward: 0.2592,                 loss: nan
Episode: 69921/101000 (69.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9536s / 16251.4087 s
agent0:                 episode reward: 0.1010,                 loss: 0.2059
agent1:                 episode reward: -0.1010,                 loss: nan
Episode: 69941/101000 (69.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7245s / 16256.1332 s
agent0:                 episode reward: 0.1104,                 loss: 0.2045
agent1:                 episode reward: -0.1104,                 loss: nan
Episode: 69961/101000 (69.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1826s / 16259.3158 s
agent0:                 episode reward: 0.1956,                 loss: 0.2048
agent1:                 episode reward: -0.1956,                 loss: nan
Episode: 69981/101000 (69.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9627s / 16264.2785 s
agent0:                 episode reward: 0.4000,                 loss: 0.2063
agent1:                 episode reward: -0.4000,                 loss: nan
Episode: 70001/101000 (69.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0602s / 16268.3387 s
agent0:                 episode reward: -0.4698,                 loss: 0.1995
agent1:                 episode reward: 0.4698,                 loss: 0.1969
Score delta: 1.685578935707938, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/69559_0.
Episode: 70021/101000 (69.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6527s / 16271.9914 s
agent0:                 episode reward: -0.0971,                 loss: nan
agent1:                 episode reward: 0.0971,                 loss: 0.1951
Episode: 70041/101000 (69.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1115s / 16276.1029 s
agent0:                 episode reward: -0.3770,                 loss: nan
agent1:                 episode reward: 0.3770,                 loss: 0.1953
Episode: 70061/101000 (69.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7343s / 16279.8372 s
agent0:                 episode reward: -0.3533,                 loss: nan
agent1:                 episode reward: 0.3533,                 loss: 0.1932
Episode: 70081/101000 (69.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8783s / 16283.7154 s
agent0:                 episode reward: -0.1239,                 loss: nan
agent1:                 episode reward: 0.1239,                 loss: 0.1946
Episode: 70101/101000 (69.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6878s / 16287.4032 s
agent0:                 episode reward: -0.4305,                 loss: nan
agent1:                 episode reward: 0.4305,                 loss: 0.1936
Episode: 70121/101000 (69.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9519s / 16291.3551 s
agent0:                 episode reward: -0.3498,                 loss: nan
agent1:                 episode reward: 0.3498,                 loss: 0.1949
Episode: 70141/101000 (69.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8529s / 16295.2080 s
agent0:                 episode reward: -0.8329,                 loss: 0.2051
agent1:                 episode reward: 0.8329,                 loss: 0.1858
Score delta: 1.874687351559611, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/69714_1.
Episode: 70161/101000 (69.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7543s / 16298.9623 s
agent0:                 episode reward: 0.1920,                 loss: 0.2055
agent1:                 episode reward: -0.1920,                 loss: nan
Episode: 70181/101000 (69.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7420s / 16303.7043 s
agent0:                 episode reward: 0.1100,                 loss: 0.2052
agent1:                 episode reward: -0.1100,                 loss: nan
Episode: 70201/101000 (69.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0314s / 16308.7357 s
agent0:                 episode reward: 0.0786,                 loss: 0.2029
agent1:                 episode reward: -0.0786,                 loss: nan
Episode: 70221/101000 (69.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7762s / 16313.5119 s
agent0:                 episode reward: 0.4542,                 loss: 0.2047
agent1:                 episode reward: -0.4542,                 loss: nan
Episode: 70241/101000 (69.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3674s / 16316.8793 s
agent0:                 episode reward: 0.2503,                 loss: 0.2044
agent1:                 episode reward: -0.2503,                 loss: nan
Episode: 70261/101000 (69.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6920s / 16320.5713 s
agent0:                 episode reward: 0.1133,                 loss: 0.2048
agent1:                 episode reward: -0.1133,                 loss: nan
Episode: 70281/101000 (69.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7421s / 16325.3134 s
agent0:                 episode reward: 0.5150,                 loss: 0.2038
agent1:                 episode reward: -0.5150,                 loss: nan
Episode: 70301/101000 (69.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5067s / 16329.8201 s
agent0:                 episode reward: 0.3454,                 loss: 0.2043
agent1:                 episode reward: -0.3454,                 loss: nan
Episode: 70321/101000 (69.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8736s / 16333.6937 s
agent0:                 episode reward: -0.2957,                 loss: 0.2034
agent1:                 episode reward: 0.2957,                 loss: nan
Episode: 70341/101000 (69.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6850s / 16337.3787 s
agent0:                 episode reward: 0.0795,                 loss: 0.2038
agent1:                 episode reward: -0.0795,                 loss: nan
Episode: 70361/101000 (69.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9376s / 16341.3163 s
agent0:                 episode reward: 0.0877,                 loss: 0.2060
agent1:                 episode reward: -0.0877,                 loss: 0.2140
Score delta: 1.8373750006578224, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/69922_0.
Episode: 70381/101000 (69.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5866s / 16345.9029 s
agent0:                 episode reward: -0.3906,                 loss: nan
agent1:                 episode reward: 0.3906,                 loss: 0.2154
Episode: 70401/101000 (69.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2152s / 16350.1181 s
agent0:                 episode reward: -0.3952,                 loss: nan
agent1:                 episode reward: 0.3952,                 loss: 0.2158
Episode: 70421/101000 (69.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4404s / 16353.5585 s
agent0:                 episode reward: -0.1981,                 loss: nan
agent1:                 episode reward: 0.1981,                 loss: 0.2140
Episode: 70441/101000 (69.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9678s / 16358.5263 s
agent0:                 episode reward: -0.0522,                 loss: nan
agent1:                 episode reward: 0.0522,                 loss: 0.2124
Episode: 70461/101000 (69.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0276s / 16362.5539 s
agent0:                 episode reward: -0.3101,                 loss: nan
agent1:                 episode reward: 0.3101,                 loss: 0.2143
Episode: 70481/101000 (69.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8271s / 16366.3810 s
agent0:                 episode reward: -0.2610,                 loss: nan
agent1:                 episode reward: 0.2610,                 loss: 0.2176
Episode: 70501/101000 (69.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9033s / 16370.2843 s
agent0:                 episode reward: -0.3060,                 loss: nan
agent1:                 episode reward: 0.3060,                 loss: 0.2110
Episode: 70521/101000 (69.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9916s / 16374.2759 s
agent0:                 episode reward: -0.5546,                 loss: 0.2440
agent1:                 episode reward: 0.5546,                 loss: 0.2121
Score delta: 1.5254718657001698, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/70083_1.
Episode: 70541/101000 (69.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2127s / 16378.4886 s
agent0:                 episode reward: 0.1174,                 loss: 0.2422
agent1:                 episode reward: -0.1174,                 loss: nan
Episode: 70561/101000 (69.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8418s / 16384.3304 s
agent0:                 episode reward: 0.2575,                 loss: 0.2402
agent1:                 episode reward: -0.2575,                 loss: nan
Episode: 70581/101000 (69.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2166s / 16389.5470 s
agent0:                 episode reward: -0.0300,                 loss: 0.2427
agent1:                 episode reward: 0.0300,                 loss: nan
Episode: 70601/101000 (69.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4105s / 16393.9575 s
agent0:                 episode reward: 0.0362,                 loss: 0.2337
agent1:                 episode reward: -0.0362,                 loss: nan
Episode: 70621/101000 (69.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7911s / 16397.7486 s
agent0:                 episode reward: -0.2751,                 loss: 0.2015
agent1:                 episode reward: 0.2751,                 loss: nan
Episode: 70641/101000 (69.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4079s / 16402.1565 s
agent0:                 episode reward: 0.1221,                 loss: 0.2029
agent1:                 episode reward: -0.1221,                 loss: nan
Episode: 70661/101000 (69.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8027s / 16406.9592 s
agent0:                 episode reward: -0.0014,                 loss: 0.2004
agent1:                 episode reward: 0.0014,                 loss: nan
Episode: 70681/101000 (69.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3228s / 16411.2820 s
agent0:                 episode reward: -0.0403,                 loss: 0.2027
agent1:                 episode reward: 0.0403,                 loss: nan
Episode: 70701/101000 (70.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5365s / 16415.8185 s
agent0:                 episode reward: 0.1638,                 loss: 0.2021
agent1:                 episode reward: -0.1638,                 loss: nan
Episode: 70721/101000 (70.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8352s / 16419.6537 s
agent0:                 episode reward: 0.1438,                 loss: 0.2018
agent1:                 episode reward: -0.1438,                 loss: nan
Episode: 70741/101000 (70.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7624s / 16424.4161 s
agent0:                 episode reward: 0.3730,                 loss: 0.1995
agent1:                 episode reward: -0.3730,                 loss: nan
Episode: 70761/101000 (70.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8043s / 16427.2203 s
agent0:                 episode reward: -0.0076,                 loss: 0.2019
agent1:                 episode reward: 0.0076,                 loss: nan
Episode: 70781/101000 (70.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8432s / 16431.0635 s
agent0:                 episode reward: -0.1396,                 loss: 0.2014
agent1:                 episode reward: 0.1396,                 loss: nan
Episode: 70801/101000 (70.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6189s / 16435.6824 s
agent0:                 episode reward: 0.0595,                 loss: 0.2008
agent1:                 episode reward: -0.0595,                 loss: nan
Episode: 70821/101000 (70.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5139s / 16441.1963 s
agent0:                 episode reward: -0.2659,                 loss: 0.2010
agent1:                 episode reward: 0.2659,                 loss: nan
Episode: 70841/101000 (70.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8102s / 16445.0065 s
agent0:                 episode reward: 0.0676,                 loss: 0.2011
agent1:                 episode reward: -0.0676,                 loss: nan
Episode: 70861/101000 (70.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4369s / 16449.4434 s
agent0:                 episode reward: -0.1264,                 loss: 0.2008
agent1:                 episode reward: 0.1264,                 loss: nan
Episode: 70881/101000 (70.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7417s / 16453.1851 s
agent0:                 episode reward: 0.4673,                 loss: 0.2030
agent1:                 episode reward: -0.4673,                 loss: nan
Score delta: 1.5117273676398288, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/70455_0.
Episode: 70901/101000 (70.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0073s / 16457.1925 s
agent0:                 episode reward: -0.1817,                 loss: nan
agent1:                 episode reward: 0.1817,                 loss: 0.1904
Episode: 70921/101000 (70.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7319s / 16460.9244 s
agent0:                 episode reward: -0.1640,                 loss: nan
agent1:                 episode reward: 0.1640,                 loss: 0.1924
Episode: 70941/101000 (70.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7195s / 16464.6439 s
agent0:                 episode reward: -0.4726,                 loss: nan
agent1:                 episode reward: 0.4726,                 loss: 0.1900
Episode: 70961/101000 (70.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1257s / 16468.7696 s
agent0:                 episode reward: -0.1038,                 loss: nan
agent1:                 episode reward: 0.1038,                 loss: 0.1917
Episode: 70981/101000 (70.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1520s / 16472.9216 s
agent0:                 episode reward: -0.2960,                 loss: nan
agent1:                 episode reward: 0.2960,                 loss: 0.1908
Episode: 71001/101000 (70.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9305s / 16477.8521 s
agent0:                 episode reward: -0.4213,                 loss: nan
agent1:                 episode reward: 0.4213,                 loss: 0.1913
Episode: 71021/101000 (70.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5367s / 16482.3887 s
agent0:                 episode reward: -0.6165,                 loss: nan
agent1:                 episode reward: 0.6165,                 loss: 0.1903
Episode: 71041/101000 (70.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2070s / 16486.5957 s
agent0:                 episode reward: -0.4765,                 loss: nan
agent1:                 episode reward: 0.4765,                 loss: 0.1889
Episode: 71061/101000 (70.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0611s / 16491.6568 s
agent0:                 episode reward: -0.1638,                 loss: nan
agent1:                 episode reward: 0.1638,                 loss: 0.1814
Episode: 71081/101000 (70.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3971s / 16496.0539 s
agent0:                 episode reward: -0.3948,                 loss: nan
agent1:                 episode reward: 0.3948,                 loss: 0.1765
Episode: 71101/101000 (70.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6106s / 16500.6645 s
agent0:                 episode reward: -0.7398,                 loss: 0.3456
agent1:                 episode reward: 0.7398,                 loss: 0.1760
Score delta: 1.6874589343728439, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/70671_1.
Episode: 71121/101000 (70.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1632s / 16504.8277 s
agent0:                 episode reward: -0.4464,                 loss: 0.2977
agent1:                 episode reward: 0.4464,                 loss: nan
Episode: 71141/101000 (70.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5017s / 16510.3294 s
agent0:                 episode reward: -0.5892,                 loss: 0.2962
agent1:                 episode reward: 0.5892,                 loss: nan
Episode: 71161/101000 (70.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2734s / 16514.6028 s
agent0:                 episode reward: -0.3866,                 loss: 0.2319
agent1:                 episode reward: 0.3866,                 loss: nan
Episode: 71181/101000 (70.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3156s / 16517.9184 s
agent0:                 episode reward: -0.5364,                 loss: 0.2053
agent1:                 episode reward: 0.5364,                 loss: nan
Episode: 71201/101000 (70.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9379s / 16522.8563 s
agent0:                 episode reward: -0.1575,                 loss: 0.2038
agent1:                 episode reward: 0.1575,                 loss: nan
Episode: 71221/101000 (70.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0549s / 16526.9112 s
agent0:                 episode reward: -0.2652,                 loss: 0.2022
agent1:                 episode reward: 0.2652,                 loss: nan
Episode: 71241/101000 (70.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2100s / 16531.1212 s
agent0:                 episode reward: -0.1515,                 loss: 0.2019
agent1:                 episode reward: 0.1515,                 loss: nan
Episode: 71261/101000 (70.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5473s / 16536.6685 s
agent0:                 episode reward: -0.1637,                 loss: 0.2001
agent1:                 episode reward: 0.1637,                 loss: nan
Episode: 71281/101000 (70.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3249s / 16540.9934 s
agent0:                 episode reward: -0.2026,                 loss: 0.2011
agent1:                 episode reward: 0.2026,                 loss: nan
Episode: 71301/101000 (70.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5117s / 16546.5051 s
agent0:                 episode reward: -0.1790,                 loss: 0.2009
agent1:                 episode reward: 0.1790,                 loss: nan
Episode: 71321/101000 (70.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7171s / 16551.2222 s
agent0:                 episode reward: 0.1805,                 loss: 0.1996
agent1:                 episode reward: -0.1805,                 loss: nan
Episode: 71341/101000 (70.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4194s / 16555.6416 s
agent0:                 episode reward: 0.0601,                 loss: 0.2012
agent1:                 episode reward: -0.0601,                 loss: nan
Episode: 71361/101000 (70.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3205s / 16559.9621 s
agent0:                 episode reward: 0.3054,                 loss: 0.1990
agent1:                 episode reward: -0.3054,                 loss: nan
Episode: 71381/101000 (70.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5305s / 16565.4926 s
agent0:                 episode reward: -0.2698,                 loss: 0.1983
agent1:                 episode reward: 0.2698,                 loss: nan
Episode: 71401/101000 (70.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8691s / 16569.3617 s
agent0:                 episode reward: 0.1208,                 loss: 0.1996
agent1:                 episode reward: -0.1208,                 loss: nan
Episode: 71421/101000 (70.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3837s / 16573.7455 s
agent0:                 episode reward: -0.1145,                 loss: 0.1987
agent1:                 episode reward: 0.1145,                 loss: nan
Episode: 71441/101000 (70.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2112s / 16577.9567 s
agent0:                 episode reward: 0.0985,                 loss: 0.1992
agent1:                 episode reward: -0.0985,                 loss: nan
Episode: 71461/101000 (70.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7674s / 16582.7241 s
agent0:                 episode reward: -0.1797,                 loss: 0.1988
agent1:                 episode reward: 0.1797,                 loss: nan
Episode: 71481/101000 (70.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2730s / 16586.9972 s
agent0:                 episode reward: -0.0210,                 loss: 0.2001
agent1:                 episode reward: 0.0210,                 loss: nan
Episode: 71501/101000 (70.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5719s / 16591.5690 s
agent0:                 episode reward: -0.2924,                 loss: 0.2093
agent1:                 episode reward: 0.2924,                 loss: nan
Episode: 71521/101000 (70.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7333s / 16595.3023 s
agent0:                 episode reward: 0.0501,                 loss: 0.2071
agent1:                 episode reward: -0.0501,                 loss: nan
Episode: 71541/101000 (70.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7433s / 16600.0457 s
agent0:                 episode reward: 0.1915,                 loss: 0.2082
agent1:                 episode reward: -0.1915,                 loss: nan
Episode: 71561/101000 (70.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9953s / 16604.0410 s
agent0:                 episode reward: 0.0773,                 loss: 0.2073
agent1:                 episode reward: -0.0773,                 loss: nan
Episode: 71581/101000 (70.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9350s / 16607.9760 s
agent0:                 episode reward: 0.0606,                 loss: 0.2068
agent1:                 episode reward: -0.0606,                 loss: nan
Episode: 71601/101000 (70.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7177s / 16612.6937 s
agent0:                 episode reward: -0.0951,                 loss: 0.2061
agent1:                 episode reward: 0.0951,                 loss: nan
Episode: 71621/101000 (70.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7339s / 16617.4276 s
agent0:                 episode reward: 0.2284,                 loss: 0.2062
agent1:                 episode reward: -0.2284,                 loss: nan
Episode: 71641/101000 (70.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7993s / 16621.2269 s
agent0:                 episode reward: -0.0899,                 loss: 0.2068
agent1:                 episode reward: 0.0899,                 loss: nan
Episode: 71661/101000 (70.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7414s / 16624.9683 s
agent0:                 episode reward: 0.2111,                 loss: 0.2066
agent1:                 episode reward: -0.2111,                 loss: nan
Episode: 71681/101000 (70.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3456s / 16629.3139 s
agent0:                 episode reward: 0.3576,                 loss: 0.2055
agent1:                 episode reward: -0.3576,                 loss: nan
Episode: 71701/101000 (70.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2430s / 16633.5568 s
agent0:                 episode reward: 0.0409,                 loss: 0.2055
agent1:                 episode reward: -0.0409,                 loss: nan
Episode: 71721/101000 (71.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0817s / 16637.6385 s
agent0:                 episode reward: -0.0216,                 loss: 0.2071
agent1:                 episode reward: 0.0216,                 loss: nan
Episode: 71741/101000 (71.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5182s / 16642.1567 s
agent0:                 episode reward: 0.0642,                 loss: 0.2027
agent1:                 episode reward: -0.0642,                 loss: 0.1671
Score delta: 1.546673326277255, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/71301_0.
Episode: 71761/101000 (71.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5952s / 16646.7519 s
agent0:                 episode reward: -0.1207,                 loss: nan
agent1:                 episode reward: 0.1207,                 loss: 0.1636
Episode: 71781/101000 (71.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4989s / 16650.2508 s
agent0:                 episode reward: -0.0757,                 loss: nan
agent1:                 episode reward: 0.0757,                 loss: 0.1619
Episode: 71801/101000 (71.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4097s / 16654.6604 s
agent0:                 episode reward: -0.7668,                 loss: nan
agent1:                 episode reward: 0.7668,                 loss: 0.1644
Episode: 71821/101000 (71.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6802s / 16658.3406 s
agent0:                 episode reward: -0.7214,                 loss: nan
agent1:                 episode reward: 0.7214,                 loss: 0.1622
Episode: 71841/101000 (71.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8612s / 16662.2018 s
agent0:                 episode reward: -0.6019,                 loss: nan
agent1:                 episode reward: 0.6019,                 loss: 0.1630
Episode: 71861/101000 (71.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9729s / 16666.1747 s
agent0:                 episode reward: -0.5849,                 loss: nan
agent1:                 episode reward: 0.5849,                 loss: 0.1624
Episode: 71881/101000 (71.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2484s / 16670.4231 s
agent0:                 episode reward: -0.4223,                 loss: nan
agent1:                 episode reward: 0.4223,                 loss: 0.1641
Episode: 71901/101000 (71.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2440s / 16674.6671 s
agent0:                 episode reward: -0.7951,                 loss: 0.3075
agent1:                 episode reward: 0.7951,                 loss: 0.1614
Score delta: 1.8079012177774356, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/71470_1.
Episode: 71921/101000 (71.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4142s / 16679.0812 s
agent0:                 episode reward: -0.7357,                 loss: 0.2780
agent1:                 episode reward: 0.7357,                 loss: nan
Episode: 71941/101000 (71.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2485s / 16683.3297 s
agent0:                 episode reward: -0.1642,                 loss: 0.2756
agent1:                 episode reward: 0.1642,                 loss: nan
Episode: 71961/101000 (71.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6482s / 16687.9779 s
agent0:                 episode reward: -0.7678,                 loss: 0.2707
agent1:                 episode reward: 0.7678,                 loss: nan
Episode: 71981/101000 (71.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6620s / 16691.6398 s
agent0:                 episode reward: -0.4219,                 loss: 0.2712
agent1:                 episode reward: 0.4219,                 loss: nan
Episode: 72001/101000 (71.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3853s / 16696.0251 s
agent0:                 episode reward: -0.3464,                 loss: 0.2104
agent1:                 episode reward: 0.3464,                 loss: nan
Episode: 72021/101000 (71.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2747s / 16700.2998 s
agent0:                 episode reward: -0.1791,                 loss: 0.2064
agent1:                 episode reward: 0.1791,                 loss: nan
Episode: 72041/101000 (71.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5355s / 16704.8353 s
agent0:                 episode reward: -0.3129,                 loss: 0.2036
agent1:                 episode reward: 0.3129,                 loss: nan
Episode: 72061/101000 (71.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8842s / 16709.7195 s
agent0:                 episode reward: -0.1687,                 loss: 0.2032
agent1:                 episode reward: 0.1687,                 loss: nan
Episode: 72081/101000 (71.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4432s / 16714.1627 s
agent0:                 episode reward: 0.1627,                 loss: 0.2040
agent1:                 episode reward: -0.1627,                 loss: nan
Episode: 72101/101000 (71.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3367s / 16718.4994 s
agent0:                 episode reward: -0.0030,                 loss: 0.2034
agent1:                 episode reward: 0.0030,                 loss: nan
Episode: 72121/101000 (71.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6797s / 16723.1791 s
agent0:                 episode reward: 0.0941,                 loss: 0.2040
agent1:                 episode reward: -0.0941,                 loss: 0.1947
Score delta: 2.008395586482857, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/71684_0.
Episode: 72141/101000 (71.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7428s / 16727.9219 s
agent0:                 episode reward: -0.2784,                 loss: nan
agent1:                 episode reward: 0.2784,                 loss: 0.1916
Episode: 72161/101000 (71.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8531s / 16731.7750 s
agent0:                 episode reward: -0.3606,                 loss: nan
agent1:                 episode reward: 0.3606,                 loss: 0.1922
Episode: 72181/101000 (71.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9810s / 16735.7560 s
agent0:                 episode reward: -0.1210,                 loss: nan
agent1:                 episode reward: 0.1210,                 loss: 0.1920
Episode: 72201/101000 (71.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2963s / 16740.0522 s
agent0:                 episode reward: -0.0244,                 loss: nan
agent1:                 episode reward: 0.0244,                 loss: 0.1935
Episode: 72221/101000 (71.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1254s / 16743.1776 s
agent0:                 episode reward: -0.0580,                 loss: nan
agent1:                 episode reward: 0.0580,                 loss: 0.1918
Episode: 72241/101000 (71.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9845s / 16747.1621 s
agent0:                 episode reward: 0.0743,                 loss: nan
agent1:                 episode reward: -0.0743,                 loss: 0.1776
Episode: 72261/101000 (71.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6139s / 16750.7760 s
agent0:                 episode reward: -0.2706,                 loss: nan
agent1:                 episode reward: 0.2706,                 loss: 0.1777
Episode: 72281/101000 (71.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6214s / 16755.3974 s
agent0:                 episode reward: -0.0049,                 loss: nan
agent1:                 episode reward: 0.0049,                 loss: 0.1775
Episode: 72301/101000 (71.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1904s / 16759.5878 s
agent0:                 episode reward: -0.4788,                 loss: nan
agent1:                 episode reward: 0.4788,                 loss: 0.1763
Episode: 72321/101000 (71.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3303s / 16763.9181 s
agent0:                 episode reward: -0.1180,                 loss: nan
agent1:                 episode reward: 0.1180,                 loss: 0.1769
Episode: 72341/101000 (71.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2708s / 16768.1889 s
agent0:                 episode reward: -0.1081,                 loss: 0.1965
agent1:                 episode reward: 0.1081,                 loss: 0.1774
Score delta: 1.5052800555355514, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/71910_1.
Episode: 72361/101000 (71.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0900s / 16772.2789 s
agent0:                 episode reward: -0.0475,                 loss: 0.1994
agent1:                 episode reward: 0.0475,                 loss: nan
Episode: 72381/101000 (71.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5627s / 16776.8417 s
agent0:                 episode reward: -0.0163,                 loss: 0.1980
agent1:                 episode reward: 0.0163,                 loss: nan
Episode: 72401/101000 (71.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5790s / 16781.4207 s
agent0:                 episode reward: 0.6041,                 loss: 0.1955
agent1:                 episode reward: -0.6041,                 loss: nan
Episode: 72421/101000 (71.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8689s / 16785.2896 s
agent0:                 episode reward: 0.1952,                 loss: 0.1963
agent1:                 episode reward: -0.1952,                 loss: nan
Episode: 72441/101000 (71.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5767s / 16790.8663 s
agent0:                 episode reward: 0.3404,                 loss: 0.1963
agent1:                 episode reward: -0.3404,                 loss: nan
Episode: 72461/101000 (71.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0250s / 16795.8913 s
agent0:                 episode reward: 0.3067,                 loss: 0.1952
agent1:                 episode reward: -0.3067,                 loss: nan
Episode: 72481/101000 (71.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1796s / 16800.0710 s
agent0:                 episode reward: 0.2707,                 loss: 0.1955
agent1:                 episode reward: -0.2707,                 loss: nan
Episode: 72501/101000 (71.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3275s / 16805.3984 s
agent0:                 episode reward: 0.0558,                 loss: 0.1944
agent1:                 episode reward: -0.0558,                 loss: nan
Episode: 72521/101000 (71.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6860s / 16810.0845 s
agent0:                 episode reward: 0.2532,                 loss: 0.1940
agent1:                 episode reward: -0.2532,                 loss: nan
Episode: 72541/101000 (71.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3801s / 16814.4646 s
agent0:                 episode reward: 0.3812,                 loss: 0.1960
agent1:                 episode reward: -0.3812,                 loss: nan
Episode: 72561/101000 (71.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8716s / 16818.3361 s
agent0:                 episode reward: 0.3397,                 loss: 0.2087
agent1:                 episode reward: -0.3397,                 loss: nan
Episode: 72581/101000 (71.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1880s / 16822.5241 s
agent0:                 episode reward: 0.1183,                 loss: 0.2059
agent1:                 episode reward: -0.1183,                 loss: nan
Episode: 72601/101000 (71.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8976s / 16826.4217 s
agent0:                 episode reward: 0.1286,                 loss: 0.2041
agent1:                 episode reward: -0.1286,                 loss: nan
Episode: 72621/101000 (71.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7425s / 16830.1641 s
agent0:                 episode reward: 0.4504,                 loss: 0.2074
agent1:                 episode reward: -0.4504,                 loss: nan
Episode: 72641/101000 (71.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5417s / 16834.7058 s
agent0:                 episode reward: 0.0306,                 loss: 0.2057
agent1:                 episode reward: -0.0306,                 loss: nan
Episode: 72661/101000 (71.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7537s / 16839.4595 s
agent0:                 episode reward: -0.1633,                 loss: 0.2072
agent1:                 episode reward: 0.1633,                 loss: nan
Episode: 72681/101000 (71.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3444s / 16843.8039 s
agent0:                 episode reward: 0.0656,                 loss: 0.2056
agent1:                 episode reward: -0.0656,                 loss: nan
Episode: 72701/101000 (71.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0892s / 16847.8931 s
agent0:                 episode reward: 0.0258,                 loss: 0.2064
agent1:                 episode reward: -0.0258,                 loss: nan
Episode: 72721/101000 (72.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7061s / 16851.5991 s
agent0:                 episode reward: -0.0093,                 loss: 0.2058
agent1:                 episode reward: 0.0093,                 loss: nan
Episode: 72741/101000 (72.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9728s / 16856.5720 s
agent0:                 episode reward: -0.1859,                 loss: 0.2051
agent1:                 episode reward: 0.1859,                 loss: nan
Episode: 72761/101000 (72.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6145s / 16862.1865 s
agent0:                 episode reward: -0.1955,                 loss: 0.2049
agent1:                 episode reward: 0.1955,                 loss: nan
Episode: 72781/101000 (72.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6502s / 16866.8367 s
agent0:                 episode reward: 0.3939,                 loss: 0.2067
agent1:                 episode reward: -0.3939,                 loss: 0.1918
Score delta: 1.6800458202572959, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/72345_0.
Episode: 72801/101000 (72.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9009s / 16871.7376 s
agent0:                 episode reward: -0.4435,                 loss: nan
agent1:                 episode reward: 0.4435,                 loss: 0.1864
Episode: 72821/101000 (72.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6594s / 16875.3970 s
agent0:                 episode reward: -0.1069,                 loss: nan
agent1:                 episode reward: 0.1069,                 loss: 0.1896
Episode: 72841/101000 (72.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7989s / 16879.1959 s
agent0:                 episode reward: -0.4313,                 loss: nan
agent1:                 episode reward: 0.4313,                 loss: 0.1880
Episode: 72861/101000 (72.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1456s / 16883.3415 s
agent0:                 episode reward: -0.5641,                 loss: nan
agent1:                 episode reward: 0.5641,                 loss: 0.1885
Episode: 72881/101000 (72.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7621s / 16888.1036 s
agent0:                 episode reward: -0.4545,                 loss: nan
agent1:                 episode reward: 0.4545,                 loss: 0.1880
Episode: 72901/101000 (72.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9136s / 16893.0172 s
agent0:                 episode reward: -0.0105,                 loss: nan
agent1:                 episode reward: 0.0105,                 loss: 0.1879
Episode: 72921/101000 (72.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2822s / 16897.2994 s
agent0:                 episode reward: -0.2644,                 loss: nan
agent1:                 episode reward: 0.2644,                 loss: 0.1864
Episode: 72941/101000 (72.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9922s / 16901.2916 s
agent0:                 episode reward: -0.1883,                 loss: nan
agent1:                 episode reward: 0.1883,                 loss: 0.1909
Episode: 72961/101000 (72.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8303s / 16906.1219 s
agent0:                 episode reward: -0.4880,                 loss: 0.1928
agent1:                 episode reward: 0.4880,                 loss: 0.1868
Score delta: 2.023718480974873, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/72529_1.
Episode: 72981/101000 (72.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3800s / 16910.5019 s
agent0:                 episode reward: 0.2578,                 loss: 0.1948
agent1:                 episode reward: -0.2578,                 loss: nan
Episode: 73001/101000 (72.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3365s / 16914.8383 s
agent0:                 episode reward: -0.0018,                 loss: 0.1944
agent1:                 episode reward: 0.0018,                 loss: nan
Episode: 73021/101000 (72.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0928s / 16918.9311 s
agent0:                 episode reward: 0.2467,                 loss: 0.1930
agent1:                 episode reward: -0.2467,                 loss: nan
Episode: 73041/101000 (72.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3835s / 16923.3146 s
agent0:                 episode reward: 0.3989,                 loss: 0.1949
agent1:                 episode reward: -0.3989,                 loss: nan
Episode: 73061/101000 (72.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0047s / 16928.3193 s
agent0:                 episode reward: 0.0745,                 loss: 0.1926
agent1:                 episode reward: -0.0745,                 loss: nan
Episode: 73081/101000 (72.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8134s / 16932.1327 s
agent0:                 episode reward: -0.0361,                 loss: 0.1997
agent1:                 episode reward: 0.0361,                 loss: nan
Episode: 73101/101000 (72.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2769s / 16936.4096 s
agent0:                 episode reward: 0.0894,                 loss: 0.1969
agent1:                 episode reward: -0.0894,                 loss: nan
Episode: 73121/101000 (72.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2013s / 16940.6108 s
agent0:                 episode reward: -0.0541,                 loss: 0.1973
agent1:                 episode reward: 0.0541,                 loss: nan
Episode: 73141/101000 (72.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8880s / 16944.4988 s
agent0:                 episode reward: 0.3369,                 loss: 0.1997
agent1:                 episode reward: -0.3369,                 loss: nan
Episode: 73161/101000 (72.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1642s / 16949.6631 s
agent0:                 episode reward: 0.3042,                 loss: 0.1971
agent1:                 episode reward: -0.3042,                 loss: nan
Episode: 73181/101000 (72.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7470s / 16954.4101 s
agent0:                 episode reward: -0.2719,                 loss: 0.1991
agent1:                 episode reward: 0.2719,                 loss: nan
Episode: 73201/101000 (72.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1767s / 16958.5868 s
agent0:                 episode reward: 0.1107,                 loss: 0.2004
agent1:                 episode reward: -0.1107,                 loss: nan
Episode: 73221/101000 (72.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4121s / 16962.9989 s
agent0:                 episode reward: 0.0261,                 loss: 0.1994
agent1:                 episode reward: -0.0261,                 loss: nan
Episode: 73241/101000 (72.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7148s / 16966.7137 s
agent0:                 episode reward: 0.4669,                 loss: 0.1971
agent1:                 episode reward: -0.4669,                 loss: 0.2172
Score delta: 1.5676769076241226, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/72811_0.
Episode: 73261/101000 (72.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6352s / 16970.3489 s
agent0:                 episode reward: -0.2207,                 loss: nan
agent1:                 episode reward: 0.2207,                 loss: 0.2159
Episode: 73281/101000 (72.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1425s / 16974.4914 s
agent0:                 episode reward: 0.1186,                 loss: nan
agent1:                 episode reward: -0.1186,                 loss: 0.2016
Episode: 73301/101000 (72.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3928s / 16978.8842 s
agent0:                 episode reward: -0.4620,                 loss: nan
agent1:                 episode reward: 0.4620,                 loss: 0.1827
Episode: 73321/101000 (72.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4095s / 16983.2937 s
agent0:                 episode reward: -0.3713,                 loss: nan
agent1:                 episode reward: 0.3713,                 loss: 0.1830
Episode: 73341/101000 (72.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1888s / 16988.4825 s
agent0:                 episode reward: -0.1266,                 loss: nan
agent1:                 episode reward: 0.1266,                 loss: 0.1807
Episode: 73361/101000 (72.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3982s / 16992.8807 s
agent0:                 episode reward: -0.2563,                 loss: nan
agent1:                 episode reward: 0.2563,                 loss: 0.1824
Episode: 73381/101000 (72.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4460s / 16996.3266 s
agent0:                 episode reward: -0.3364,                 loss: nan
agent1:                 episode reward: 0.3364,                 loss: 0.1817
Episode: 73401/101000 (72.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4546s / 17000.7812 s
agent0:                 episode reward: -0.3770,                 loss: nan
agent1:                 episode reward: 0.3770,                 loss: 0.1806
Episode: 73421/101000 (72.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2467s / 17005.0279 s
agent0:                 episode reward: -0.3403,                 loss: 0.2112
agent1:                 episode reward: 0.3403,                 loss: 0.1777
Score delta: 1.5025349656777303, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/72981_1.
Episode: 73441/101000 (72.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5365s / 17009.5644 s
agent0:                 episode reward: -0.3878,                 loss: 0.2023
agent1:                 episode reward: 0.3878,                 loss: nan
Episode: 73461/101000 (72.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9689s / 17014.5333 s
agent0:                 episode reward: 0.1192,                 loss: 0.1998
agent1:                 episode reward: -0.1192,                 loss: nan
Episode: 73481/101000 (72.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4632s / 17018.9965 s
agent0:                 episode reward: -0.1066,                 loss: 0.2000
agent1:                 episode reward: 0.1066,                 loss: nan
Episode: 73501/101000 (72.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0179s / 17023.0144 s
agent0:                 episode reward: 0.3624,                 loss: 0.2000
agent1:                 episode reward: -0.3624,                 loss: nan
Episode: 73521/101000 (72.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9908s / 17028.0052 s
agent0:                 episode reward: 0.4068,                 loss: 0.1985
agent1:                 episode reward: -0.4068,                 loss: nan
Episode: 73541/101000 (72.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1516s / 17033.1568 s
agent0:                 episode reward: -0.2336,                 loss: 0.1996
agent1:                 episode reward: 0.2336,                 loss: nan
Episode: 73561/101000 (72.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3169s / 17037.4736 s
agent0:                 episode reward: 0.0126,                 loss: 0.1987
agent1:                 episode reward: -0.0126,                 loss: nan
Episode: 73581/101000 (72.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0303s / 17041.5040 s
agent0:                 episode reward: -0.1282,                 loss: 0.2012
agent1:                 episode reward: 0.1282,                 loss: nan
Episode: 73601/101000 (72.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4973s / 17046.0012 s
agent0:                 episode reward: -0.1445,                 loss: 0.1998
agent1:                 episode reward: 0.1445,                 loss: nan
Episode: 73621/101000 (72.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8801s / 17050.8813 s
agent0:                 episode reward: -0.0413,                 loss: 0.2005
agent1:                 episode reward: 0.0413,                 loss: nan
Episode: 73641/101000 (72.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8156s / 17054.6969 s
agent0:                 episode reward: -0.1378,                 loss: 0.1998
agent1:                 episode reward: 0.1378,                 loss: nan
Episode: 73661/101000 (72.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3121s / 17059.0090 s
agent0:                 episode reward: 0.0638,                 loss: 0.2014
agent1:                 episode reward: -0.0638,                 loss: nan
Episode: 73681/101000 (72.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0508s / 17063.0598 s
agent0:                 episode reward: -0.4940,                 loss: 0.2002
agent1:                 episode reward: 0.4940,                 loss: nan
Episode: 73701/101000 (72.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6306s / 17068.6904 s
agent0:                 episode reward: 0.1633,                 loss: 0.1993
agent1:                 episode reward: -0.1633,                 loss: nan
Episode: 73721/101000 (72.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8050s / 17072.4953 s
agent0:                 episode reward: 0.0729,                 loss: 0.1979
agent1:                 episode reward: -0.0729,                 loss: nan
Episode: 73741/101000 (73.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6646s / 17077.1600 s
agent0:                 episode reward: -0.2850,                 loss: 0.1984
agent1:                 episode reward: 0.2850,                 loss: 0.1667
Score delta: 1.5468186021754005, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/73301_0.
Episode: 73761/101000 (73.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2685s / 17082.4284 s
agent0:                 episode reward: -0.1993,                 loss: nan
agent1:                 episode reward: 0.1993,                 loss: 0.1645
Episode: 73781/101000 (73.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5016s / 17086.9300 s
agent0:                 episode reward: 0.1024,                 loss: nan
agent1:                 episode reward: -0.1024,                 loss: 0.1634
Episode: 73801/101000 (73.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4723s / 17091.4023 s
agent0:                 episode reward: -0.6466,                 loss: nan
agent1:                 episode reward: 0.6466,                 loss: 0.1645
Episode: 73821/101000 (73.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9739s / 17096.3762 s
agent0:                 episode reward: -0.3142,                 loss: nan
agent1:                 episode reward: 0.3142,                 loss: 0.1624
Episode: 73841/101000 (73.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3079s / 17100.6841 s
agent0:                 episode reward: -0.3290,                 loss: nan
agent1:                 episode reward: 0.3290,                 loss: 0.1632
Episode: 73861/101000 (73.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5196s / 17105.2037 s
agent0:                 episode reward: -0.3759,                 loss: nan
agent1:                 episode reward: 0.3759,                 loss: 0.1619
Episode: 73881/101000 (73.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0862s / 17109.2899 s
agent0:                 episode reward: -0.2690,                 loss: nan
agent1:                 episode reward: 0.2690,                 loss: 0.1610
Episode: 73901/101000 (73.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5660s / 17112.8559 s
agent0:                 episode reward: -0.3759,                 loss: 0.1931
agent1:                 episode reward: 0.3759,                 loss: 0.1610
Score delta: 1.5778821890501182, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/73470_1.
Episode: 73921/101000 (73.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3588s / 17117.2147 s
agent0:                 episode reward: -0.7004,                 loss: 0.1850
agent1:                 episode reward: 0.7004,                 loss: nan
Episode: 73941/101000 (73.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5668s / 17120.7815 s
agent0:                 episode reward: -0.6895,                 loss: 0.1838
agent1:                 episode reward: 0.6895,                 loss: nan
Episode: 73961/101000 (73.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0145s / 17125.7960 s
agent0:                 episode reward: -0.7979,                 loss: 0.1826
agent1:                 episode reward: 0.7979,                 loss: nan
Episode: 73981/101000 (73.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7413s / 17130.5374 s
agent0:                 episode reward: -0.6930,                 loss: 0.1799
agent1:                 episode reward: 0.6930,                 loss: nan
Episode: 74001/101000 (73.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7802s / 17134.3176 s
agent0:                 episode reward: -0.9128,                 loss: 0.1771
agent1:                 episode reward: 0.9128,                 loss: nan
Episode: 74021/101000 (73.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3709s / 17138.6885 s
agent0:                 episode reward: -0.8686,                 loss: 0.1766
agent1:                 episode reward: 0.8686,                 loss: nan
Episode: 74041/101000 (73.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8989s / 17143.5874 s
agent0:                 episode reward: -0.5165,                 loss: 0.1731
agent1:                 episode reward: 0.5165,                 loss: nan
Episode: 74061/101000 (73.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7184s / 17148.3058 s
agent0:                 episode reward: -0.4768,                 loss: 0.1711
agent1:                 episode reward: 0.4768,                 loss: nan
Episode: 74081/101000 (73.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5575s / 17152.8633 s
agent0:                 episode reward: -0.5943,                 loss: 0.1779
agent1:                 episode reward: 0.5943,                 loss: nan
Episode: 74101/101000 (73.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1504s / 17157.0137 s
agent0:                 episode reward: -0.4651,                 loss: 0.1768
agent1:                 episode reward: 0.4651,                 loss: nan
Episode: 74121/101000 (73.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1173s / 17161.1310 s
agent0:                 episode reward: -0.9733,                 loss: 0.1730
agent1:                 episode reward: 0.9733,                 loss: nan
Episode: 74141/101000 (73.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2450s / 17166.3760 s
agent0:                 episode reward: -0.9051,                 loss: 0.1734
agent1:                 episode reward: 0.9051,                 loss: nan
Episode: 74161/101000 (73.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4659s / 17170.8419 s
agent0:                 episode reward: -0.2489,                 loss: 0.1720
agent1:                 episode reward: 0.2489,                 loss: nan
Episode: 74181/101000 (73.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6872s / 17174.5290 s
agent0:                 episode reward: -0.7834,                 loss: 0.1708
agent1:                 episode reward: 0.7834,                 loss: nan
Episode: 74201/101000 (73.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7856s / 17180.3146 s
agent0:                 episode reward: -0.7267,                 loss: 0.1691
agent1:                 episode reward: 0.7267,                 loss: nan
Episode: 74221/101000 (73.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1056s / 17184.4202 s
agent0:                 episode reward: -0.7211,                 loss: 0.1690
agent1:                 episode reward: 0.7211,                 loss: nan
Episode: 74241/101000 (73.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3230s / 17188.7432 s
agent0:                 episode reward: -0.1472,                 loss: 0.1683
agent1:                 episode reward: 0.1472,                 loss: nan
Episode: 74261/101000 (73.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7454s / 17193.4887 s
agent0:                 episode reward: -0.4521,                 loss: 0.1677
agent1:                 episode reward: 0.4521,                 loss: nan
Episode: 74281/101000 (73.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8962s / 17198.3849 s
agent0:                 episode reward: 0.1279,                 loss: 0.1681
agent1:                 episode reward: -0.1279,                 loss: nan
Episode: 74301/101000 (73.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8579s / 17203.2428 s
agent0:                 episode reward: -0.1440,                 loss: 0.1673
agent1:                 episode reward: 0.1440,                 loss: nan
Episode: 74321/101000 (73.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5784s / 17206.8212 s
agent0:                 episode reward: -0.4959,                 loss: 0.1672
agent1:                 episode reward: 0.4959,                 loss: nan
Episode: 74341/101000 (73.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3822s / 17211.2034 s
agent0:                 episode reward: -0.4750,                 loss: 0.1663
agent1:                 episode reward: 0.4750,                 loss: nan
Episode: 74361/101000 (73.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4486s / 17215.6520 s
agent0:                 episode reward: -0.0087,                 loss: 0.1662
agent1:                 episode reward: 0.0087,                 loss: nan
Episode: 74381/101000 (73.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9595s / 17220.6116 s
agent0:                 episode reward: -0.3394,                 loss: 0.1655
agent1:                 episode reward: 0.3394,                 loss: nan
Episode: 74401/101000 (73.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0858s / 17224.6974 s
agent0:                 episode reward: 0.0225,                 loss: 0.1734
agent1:                 episode reward: -0.0225,                 loss: nan
Episode: 74421/101000 (73.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8483s / 17229.5456 s
agent0:                 episode reward: -0.2692,                 loss: 0.1980
agent1:                 episode reward: 0.2692,                 loss: nan
Episode: 74441/101000 (73.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2027s / 17233.7484 s
agent0:                 episode reward: 0.3938,                 loss: 0.1978
agent1:                 episode reward: -0.3938,                 loss: 0.1824
Score delta: 1.5624097198194005, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/74007_0.
Episode: 74461/101000 (73.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1665s / 17237.9149 s
agent0:                 episode reward: -0.0225,                 loss: nan
agent1:                 episode reward: 0.0225,                 loss: 0.1845
Episode: 74481/101000 (73.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3779s / 17242.2928 s
agent0:                 episode reward: -0.1581,                 loss: nan
agent1:                 episode reward: 0.1581,                 loss: 0.1799
Episode: 74501/101000 (73.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7152s / 17247.0080 s
agent0:                 episode reward: -0.2148,                 loss: nan
agent1:                 episode reward: 0.2148,                 loss: 0.1775
Episode: 74521/101000 (73.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5910s / 17250.5990 s
agent0:                 episode reward: -0.3057,                 loss: nan
agent1:                 episode reward: 0.3057,                 loss: 0.1776
Episode: 74541/101000 (73.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2153s / 17254.8144 s
agent0:                 episode reward: -0.4607,                 loss: nan
agent1:                 episode reward: 0.4607,                 loss: 0.1790
Episode: 74561/101000 (73.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5312s / 17258.3456 s
agent0:                 episode reward: -0.0795,                 loss: nan
agent1:                 episode reward: 0.0795,                 loss: 0.1780
Episode: 74581/101000 (73.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4646s / 17262.8102 s
agent0:                 episode reward: 0.0479,                 loss: nan
agent1:                 episode reward: -0.0479,                 loss: 0.1781
Episode: 74601/101000 (73.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4838s / 17267.2940 s
agent0:                 episode reward: -0.4078,                 loss: 0.1966
agent1:                 episode reward: 0.4078,                 loss: 0.1778
Score delta: 2.0896241756847114, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/74168_1.
Episode: 74621/101000 (73.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8522s / 17272.1462 s
agent0:                 episode reward: 0.2263,                 loss: 0.2005
agent1:                 episode reward: -0.2263,                 loss: nan
Episode: 74641/101000 (73.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7081s / 17276.8543 s
agent0:                 episode reward: 0.2629,                 loss: 0.1995
agent1:                 episode reward: -0.2629,                 loss: nan
Episode: 74661/101000 (73.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6032s / 17281.4575 s
agent0:                 episode reward: 0.6597,                 loss: 0.1992
agent1:                 episode reward: -0.6597,                 loss: nan
Episode: 74681/101000 (73.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1623s / 17285.6198 s
agent0:                 episode reward: 0.1421,                 loss: 0.1983
agent1:                 episode reward: -0.1421,                 loss: nan
Episode: 74701/101000 (73.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3923s / 17290.0121 s
agent0:                 episode reward: 0.2357,                 loss: 0.2005
agent1:                 episode reward: -0.2357,                 loss: nan
Episode: 74721/101000 (73.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7743s / 17294.7864 s
agent0:                 episode reward: -0.1341,                 loss: 0.1991
agent1:                 episode reward: 0.1341,                 loss: nan
Episode: 74741/101000 (74.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9373s / 17299.7237 s
agent0:                 episode reward: 0.1882,                 loss: 0.2004
agent1:                 episode reward: -0.1882,                 loss: nan
Episode: 74761/101000 (74.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5156s / 17304.2393 s
agent0:                 episode reward: 0.2905,                 loss: 0.1998
agent1:                 episode reward: -0.2905,                 loss: nan
Episode: 74781/101000 (74.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9352s / 17310.1746 s
agent0:                 episode reward: 0.0548,                 loss: 0.1981
agent1:                 episode reward: -0.0548,                 loss: nan
Episode: 74801/101000 (74.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8989s / 17315.0735 s
agent0:                 episode reward: 0.2507,                 loss: 0.1992
agent1:                 episode reward: -0.2507,                 loss: nan
Episode: 74821/101000 (74.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1869s / 17320.2604 s
agent0:                 episode reward: 0.1960,                 loss: 0.1997
agent1:                 episode reward: -0.1960,                 loss: nan
Episode: 74841/101000 (74.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6286s / 17323.8889 s
agent0:                 episode reward: 0.1245,                 loss: 0.1982
agent1:                 episode reward: -0.1245,                 loss: nan
Episode: 74861/101000 (74.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9329s / 17327.8218 s
agent0:                 episode reward: 0.0346,                 loss: 0.1980
agent1:                 episode reward: -0.0346,                 loss: nan
Episode: 74881/101000 (74.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5218s / 17332.3437 s
agent0:                 episode reward: 0.3269,                 loss: 0.1985
agent1:                 episode reward: -0.3269,                 loss: 0.1864
Score delta: 1.5175530584800254, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/74446_0.
Episode: 74901/101000 (74.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4830s / 17336.8266 s
agent0:                 episode reward: -0.1364,                 loss: nan
agent1:                 episode reward: 0.1364,                 loss: 0.1875
Episode: 74921/101000 (74.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8060s / 17341.6326 s
agent0:                 episode reward: -0.0646,                 loss: nan
agent1:                 episode reward: 0.0646,                 loss: 0.1871
Episode: 74941/101000 (74.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0982s / 17346.7308 s
agent0:                 episode reward: -0.1881,                 loss: nan
agent1:                 episode reward: 0.1881,                 loss: 0.1877
Episode: 74961/101000 (74.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1506s / 17350.8814 s
agent0:                 episode reward: 0.0350,                 loss: nan
agent1:                 episode reward: -0.0350,                 loss: 0.1878
Episode: 74981/101000 (74.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3892s / 17355.2707 s
agent0:                 episode reward: -0.1456,                 loss: nan
agent1:                 episode reward: 0.1456,                 loss: 0.1862
Episode: 75001/101000 (74.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8653s / 17360.1360 s
agent0:                 episode reward: -0.3016,                 loss: nan
agent1:                 episode reward: 0.3016,                 loss: 0.1875
Episode: 75021/101000 (74.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8011s / 17363.9371 s
agent0:                 episode reward: -0.2296,                 loss: nan
agent1:                 episode reward: 0.2296,                 loss: 0.1866
Episode: 75041/101000 (74.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2194s / 17368.1565 s
agent0:                 episode reward: -0.5372,                 loss: nan
agent1:                 episode reward: 0.5372,                 loss: 0.1847
Score delta: 1.5743977090693912, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/74615_1.
Episode: 75061/101000 (74.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0492s / 17373.2057 s
agent0:                 episode reward: 0.0262,                 loss: 0.2168
agent1:                 episode reward: -0.0262,                 loss: nan
Episode: 75081/101000 (74.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5566s / 17377.7623 s
agent0:                 episode reward: -0.1257,                 loss: 0.1947
agent1:                 episode reward: 0.1257,                 loss: nan
Episode: 75101/101000 (74.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0199s / 17382.7822 s
agent0:                 episode reward: 0.0053,                 loss: 0.1923
agent1:                 episode reward: -0.0053,                 loss: nan
Episode: 75121/101000 (74.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6480s / 17387.4302 s
agent0:                 episode reward: 0.1220,                 loss: 0.1937
agent1:                 episode reward: -0.1220,                 loss: nan
Episode: 75141/101000 (74.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5367s / 17391.9669 s
agent0:                 episode reward: 0.2927,                 loss: 0.1920
agent1:                 episode reward: -0.2927,                 loss: nan
Episode: 75161/101000 (74.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9278s / 17396.8947 s
agent0:                 episode reward: -0.1213,                 loss: 0.1901
agent1:                 episode reward: 0.1213,                 loss: nan
Episode: 75181/101000 (74.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3214s / 17401.2161 s
agent0:                 episode reward: -0.1659,                 loss: 0.1911
agent1:                 episode reward: 0.1659,                 loss: nan
Episode: 75201/101000 (74.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7151s / 17405.9312 s
agent0:                 episode reward: -0.0086,                 loss: 0.1899
agent1:                 episode reward: 0.0086,                 loss: nan
Episode: 75221/101000 (74.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3355s / 17410.2667 s
agent0:                 episode reward: 0.3088,                 loss: 0.1872
agent1:                 episode reward: -0.3088,                 loss: nan
Episode: 75241/101000 (74.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6980s / 17413.9647 s
agent0:                 episode reward: -0.2199,                 loss: 0.1907
agent1:                 episode reward: 0.2199,                 loss: nan
Episode: 75261/101000 (74.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8269s / 17418.7916 s
agent0:                 episode reward: -0.0244,                 loss: 0.1887
agent1:                 episode reward: 0.0244,                 loss: nan
Episode: 75281/101000 (74.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9849s / 17422.7764 s
agent0:                 episode reward: 0.1528,                 loss: 0.1905
agent1:                 episode reward: -0.1528,                 loss: nan
Episode: 75301/101000 (74.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4969s / 17426.2733 s
agent0:                 episode reward: 0.0611,                 loss: 0.1883
agent1:                 episode reward: -0.0611,                 loss: nan
Episode: 75321/101000 (74.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2075s / 17430.4808 s
agent0:                 episode reward: 0.2494,                 loss: 0.1870
agent1:                 episode reward: -0.2494,                 loss: nan
Episode: 75341/101000 (74.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7706s / 17435.2514 s
agent0:                 episode reward: -0.0656,                 loss: 0.1871
agent1:                 episode reward: 0.0656,                 loss: nan
Episode: 75361/101000 (74.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2268s / 17439.4783 s
agent0:                 episode reward: 0.5438,                 loss: 0.1888
agent1:                 episode reward: -0.5438,                 loss: nan
Episode: 75381/101000 (74.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4641s / 17443.9424 s
agent0:                 episode reward: 0.4200,                 loss: 0.1848
agent1:                 episode reward: -0.4200,                 loss: 0.1961
Score delta: 1.8232807543716985, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/74945_0.
Episode: 75401/101000 (74.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2378s / 17448.1801 s
agent0:                 episode reward: -0.2260,                 loss: nan
agent1:                 episode reward: 0.2260,                 loss: 0.1938
Episode: 75421/101000 (74.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2440s / 17450.4242 s
agent0:                 episode reward: 0.0139,                 loss: nan
agent1:                 episode reward: -0.0139,                 loss: 0.1840
Episode: 75441/101000 (74.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0568s / 17454.4810 s
agent0:                 episode reward: -0.1654,                 loss: nan
agent1:                 episode reward: 0.1654,                 loss: 0.1789
Episode: 75461/101000 (74.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3064s / 17458.7873 s
agent0:                 episode reward: -0.2922,                 loss: nan
agent1:                 episode reward: 0.2922,                 loss: 0.1775
Episode: 75481/101000 (74.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6191s / 17463.4065 s
agent0:                 episode reward: -0.4277,                 loss: nan
agent1:                 episode reward: 0.4277,                 loss: 0.1791
Episode: 75501/101000 (74.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5099s / 17467.9164 s
agent0:                 episode reward: -0.1023,                 loss: nan
agent1:                 episode reward: 0.1023,                 loss: 0.1790
Episode: 75521/101000 (74.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7188s / 17472.6352 s
agent0:                 episode reward: -0.5209,                 loss: nan
agent1:                 episode reward: 0.5209,                 loss: 0.1761
Episode: 75541/101000 (74.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7397s / 17477.3749 s
agent0:                 episode reward: -0.3360,                 loss: nan
agent1:                 episode reward: 0.3360,                 loss: 0.1764
Episode: 75561/101000 (74.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8644s / 17481.2393 s
agent0:                 episode reward: -0.4651,                 loss: nan
agent1:                 episode reward: 0.4651,                 loss: 0.1776
Episode: 75581/101000 (74.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3135s / 17486.5528 s
agent0:                 episode reward: -0.0649,                 loss: 0.2426
agent1:                 episode reward: 0.0649,                 loss: 0.1817
Score delta: 1.6732856089531467, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/75140_1.
Episode: 75601/101000 (74.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5114s / 17492.0642 s
agent0:                 episode reward: -0.0117,                 loss: 0.2076
agent1:                 episode reward: 0.0117,                 loss: nan
Episode: 75621/101000 (74.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0419s / 17496.1061 s
agent0:                 episode reward: 0.0404,                 loss: 0.1783
agent1:                 episode reward: -0.0404,                 loss: nan
Episode: 75641/101000 (74.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9673s / 17501.0734 s
agent0:                 episode reward: 0.0194,                 loss: 0.1785
agent1:                 episode reward: -0.0194,                 loss: nan
Episode: 75661/101000 (74.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0061s / 17505.0795 s
agent0:                 episode reward: 0.0036,                 loss: 0.1757
agent1:                 episode reward: -0.0036,                 loss: nan
Episode: 75681/101000 (74.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9336s / 17510.0130 s
agent0:                 episode reward: 0.3721,                 loss: 0.1747
agent1:                 episode reward: -0.3721,                 loss: nan
Episode: 75701/101000 (74.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1442s / 17513.1572 s
agent0:                 episode reward: -0.1508,                 loss: 0.1752
agent1:                 episode reward: 0.1508,                 loss: nan
Episode: 75721/101000 (74.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0003s / 17518.1576 s
agent0:                 episode reward: 0.1909,                 loss: 0.1763
agent1:                 episode reward: -0.1909,                 loss: nan
Episode: 75741/101000 (74.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1572s / 17523.3148 s
agent0:                 episode reward: 0.0383,                 loss: 0.1762
agent1:                 episode reward: -0.0383,                 loss: nan
Episode: 75761/101000 (75.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1431s / 17528.4579 s
agent0:                 episode reward: 0.2649,                 loss: 0.1753
agent1:                 episode reward: -0.2649,                 loss: nan
Episode: 75781/101000 (75.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3086s / 17532.7665 s
agent0:                 episode reward: 0.0732,                 loss: 0.1758
agent1:                 episode reward: -0.0732,                 loss: nan
Episode: 75801/101000 (75.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5551s / 17537.3216 s
agent0:                 episode reward: 0.3156,                 loss: 0.1747
agent1:                 episode reward: -0.3156,                 loss: nan
Episode: 75821/101000 (75.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8112s / 17542.1328 s
agent0:                 episode reward: 0.0408,                 loss: 0.1749
agent1:                 episode reward: -0.0408,                 loss: nan
Episode: 75841/101000 (75.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4868s / 17547.6196 s
agent0:                 episode reward: -0.3800,                 loss: 0.1759
agent1:                 episode reward: 0.3800,                 loss: nan
Episode: 75861/101000 (75.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6857s / 17552.3053 s
agent0:                 episode reward: 0.0283,                 loss: 0.1745
agent1:                 episode reward: -0.0283,                 loss: nan
Episode: 75881/101000 (75.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7214s / 17557.0267 s
agent0:                 episode reward: 0.2427,                 loss: 0.1749
agent1:                 episode reward: -0.2427,                 loss: nan
Episode: 75901/101000 (75.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3230s / 17562.3497 s
agent0:                 episode reward: -0.2171,                 loss: 0.1736
agent1:                 episode reward: 0.2171,                 loss: nan
Episode: 75921/101000 (75.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6107s / 17566.9604 s
agent0:                 episode reward: -0.0310,                 loss: 0.1752
agent1:                 episode reward: 0.0310,                 loss: nan
Episode: 75941/101000 (75.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7476s / 17571.7079 s
agent0:                 episode reward: 0.1167,                 loss: 0.1935
agent1:                 episode reward: -0.1167,                 loss: nan
Episode: 75961/101000 (75.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3371s / 17576.0450 s
agent0:                 episode reward: 0.0837,                 loss: 0.1933
agent1:                 episode reward: -0.0837,                 loss: nan
Episode: 75981/101000 (75.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8715s / 17580.9165 s
agent0:                 episode reward: -0.0062,                 loss: 0.1943
agent1:                 episode reward: 0.0062,                 loss: nan
Episode: 76001/101000 (75.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9218s / 17585.8383 s
agent0:                 episode reward: 0.0936,                 loss: 0.1919
agent1:                 episode reward: -0.0936,                 loss: nan
Episode: 76021/101000 (75.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6456s / 17590.4839 s
agent0:                 episode reward: -0.1077,                 loss: 0.1942
agent1:                 episode reward: 0.1077,                 loss: nan
Episode: 76041/101000 (75.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8776s / 17596.3615 s
agent0:                 episode reward: 0.2383,                 loss: 0.1948
agent1:                 episode reward: -0.2383,                 loss: nan
Episode: 76061/101000 (75.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8188s / 17601.1803 s
agent0:                 episode reward: 0.3988,                 loss: 0.1945
agent1:                 episode reward: -0.3988,                 loss: nan
Episode: 76081/101000 (75.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1082s / 17606.2885 s
agent0:                 episode reward: -0.3202,                 loss: 0.1904
agent1:                 episode reward: 0.3202,                 loss: 0.1940
Score delta: 1.7179065729570748, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/75636_0.
Episode: 76101/101000 (75.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4539s / 17611.7424 s
agent0:                 episode reward: -0.4225,                 loss: nan
agent1:                 episode reward: 0.4225,                 loss: 0.1924
Episode: 76121/101000 (75.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4658s / 17616.2082 s
agent0:                 episode reward: -0.5100,                 loss: nan
agent1:                 episode reward: 0.5100,                 loss: 0.1919
Episode: 76141/101000 (75.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7008s / 17620.9090 s
agent0:                 episode reward: -0.4184,                 loss: nan
agent1:                 episode reward: 0.4184,                 loss: 0.1929
Episode: 76161/101000 (75.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2075s / 17625.1165 s
agent0:                 episode reward: -0.4649,                 loss: nan
agent1:                 episode reward: 0.4649,                 loss: 0.1922
Episode: 76181/101000 (75.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7945s / 17628.9110 s
agent0:                 episode reward: -0.1854,                 loss: nan
agent1:                 episode reward: 0.1854,                 loss: 0.1934
Episode: 76201/101000 (75.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1558s / 17633.0668 s
agent0:                 episode reward: -0.5201,                 loss: nan
agent1:                 episode reward: 0.5201,                 loss: 0.1905
Episode: 76221/101000 (75.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4331s / 17637.4999 s
agent0:                 episode reward: -0.4575,                 loss: nan
agent1:                 episode reward: 0.4575,                 loss: 0.1923
Episode: 76241/101000 (75.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7095s / 17642.2095 s
agent0:                 episode reward: -0.2507,                 loss: nan
agent1:                 episode reward: 0.2507,                 loss: 0.1882
Episode: 76261/101000 (75.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3158s / 17646.5253 s
agent0:                 episode reward: -0.2987,                 loss: nan
agent1:                 episode reward: 0.2987,                 loss: 0.1825
Episode: 76281/101000 (75.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0080s / 17651.5333 s
agent0:                 episode reward: -0.1518,                 loss: 0.1921
agent1:                 episode reward: 0.1518,                 loss: 0.1804
Score delta: 1.7667159497169322, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/75850_1.
Episode: 76301/101000 (75.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3820s / 17656.9153 s
agent0:                 episode reward: 0.0294,                 loss: 0.1894
agent1:                 episode reward: -0.0294,                 loss: nan
Episode: 76321/101000 (75.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5172s / 17662.4324 s
agent0:                 episode reward: -0.1218,                 loss: 0.1890
agent1:                 episode reward: 0.1218,                 loss: nan
Episode: 76341/101000 (75.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3867s / 17666.8192 s
agent0:                 episode reward: 0.1079,                 loss: 0.1909
agent1:                 episode reward: -0.1079,                 loss: nan
Episode: 76361/101000 (75.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3918s / 17671.2110 s
agent0:                 episode reward: 0.0541,                 loss: 0.1884
agent1:                 episode reward: -0.0541,                 loss: nan
Episode: 76381/101000 (75.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2414s / 17675.4524 s
agent0:                 episode reward: 0.1926,                 loss: 0.1883
agent1:                 episode reward: -0.1926,                 loss: nan
Episode: 76401/101000 (75.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1700s / 17680.6225 s
agent0:                 episode reward: 0.1343,                 loss: 0.1887
agent1:                 episode reward: -0.1343,                 loss: nan
Episode: 76421/101000 (75.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1439s / 17685.7664 s
agent0:                 episode reward: 0.2565,                 loss: 0.1883
agent1:                 episode reward: -0.2565,                 loss: nan
Episode: 76441/101000 (75.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0428s / 17690.8092 s
agent0:                 episode reward: 0.3444,                 loss: 0.1897
agent1:                 episode reward: -0.3444,                 loss: nan
Episode: 76461/101000 (75.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8094s / 17694.6186 s
agent0:                 episode reward: -0.3015,                 loss: 0.1898
agent1:                 episode reward: 0.3015,                 loss: nan
Episode: 76481/101000 (75.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7612s / 17699.3798 s
agent0:                 episode reward: 0.2620,                 loss: 0.1951
agent1:                 episode reward: -0.2620,                 loss: nan
Episode: 76501/101000 (75.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9697s / 17704.3495 s
agent0:                 episode reward: 0.3954,                 loss: 0.1993
agent1:                 episode reward: -0.3954,                 loss: nan
Episode: 76521/101000 (75.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6275s / 17709.9770 s
agent0:                 episode reward: 0.1795,                 loss: 0.1986
agent1:                 episode reward: -0.1795,                 loss: 0.1861
Score delta: 1.5016377139678936, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/76090_0.
Episode: 76541/101000 (75.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2321s / 17714.2091 s
agent0:                 episode reward: -0.3469,                 loss: nan
agent1:                 episode reward: 0.3469,                 loss: 0.1841
Episode: 76561/101000 (75.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3680s / 17718.5771 s
agent0:                 episode reward: -0.1657,                 loss: nan
agent1:                 episode reward: 0.1657,                 loss: 0.1844
Episode: 76581/101000 (75.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9270s / 17723.5041 s
agent0:                 episode reward: -0.1238,                 loss: nan
agent1:                 episode reward: 0.1238,                 loss: 0.1828
Episode: 76601/101000 (75.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5695s / 17728.0736 s
agent0:                 episode reward: -0.2074,                 loss: nan
agent1:                 episode reward: 0.2074,                 loss: 0.1835
Episode: 76621/101000 (75.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4335s / 17732.5071 s
agent0:                 episode reward: -0.3102,                 loss: nan
agent1:                 episode reward: 0.3102,                 loss: 0.1834
Episode: 76641/101000 (75.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4895s / 17736.9966 s
agent0:                 episode reward: -0.5933,                 loss: nan
agent1:                 episode reward: 0.5933,                 loss: 0.1831
Episode: 76661/101000 (75.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9639s / 17742.9604 s
agent0:                 episode reward: -0.4709,                 loss: nan
agent1:                 episode reward: 0.4709,                 loss: 0.1846
Episode: 76681/101000 (75.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1898s / 17748.1502 s
agent0:                 episode reward: -0.1223,                 loss: nan
agent1:                 episode reward: 0.1223,                 loss: 0.1840
Episode: 76701/101000 (75.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1793s / 17753.3296 s
agent0:                 episode reward: -0.2487,                 loss: nan
agent1:                 episode reward: 0.2487,                 loss: 0.1824
Episode: 76721/101000 (75.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0121s / 17758.3417 s
agent0:                 episode reward: -0.4860,                 loss: 0.2030
agent1:                 episode reward: 0.4860,                 loss: 0.1827
Score delta: 1.5213775697752636, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/76283_1.
Episode: 76741/101000 (75.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0857s / 17762.4274 s
agent0:                 episode reward: -0.1782,                 loss: 0.1964
agent1:                 episode reward: 0.1782,                 loss: nan
Episode: 76761/101000 (76.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4458s / 17767.8732 s
agent0:                 episode reward: 0.0975,                 loss: 0.1966
agent1:                 episode reward: -0.0975,                 loss: nan
Episode: 76781/101000 (76.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4711s / 17772.3443 s
agent0:                 episode reward: 0.5344,                 loss: 0.1964
agent1:                 episode reward: -0.5344,                 loss: nan
Episode: 76801/101000 (76.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1956s / 17776.5399 s
agent0:                 episode reward: -0.0001,                 loss: 0.1916
agent1:                 episode reward: 0.0001,                 loss: nan
Episode: 76821/101000 (76.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2012s / 17781.7410 s
agent0:                 episode reward: 0.4030,                 loss: 0.1956
agent1:                 episode reward: -0.4030,                 loss: nan
Episode: 76841/101000 (76.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8386s / 17786.5797 s
agent0:                 episode reward: -0.2735,                 loss: 0.1942
agent1:                 episode reward: 0.2735,                 loss: nan
Episode: 76861/101000 (76.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8923s / 17791.4720 s
agent0:                 episode reward: 0.1736,                 loss: 0.1952
agent1:                 episode reward: -0.1736,                 loss: nan
Episode: 76881/101000 (76.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1695s / 17796.6415 s
agent0:                 episode reward: 0.0342,                 loss: 0.1943
agent1:                 episode reward: -0.0342,                 loss: nan
Episode: 76901/101000 (76.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8775s / 17801.5190 s
agent0:                 episode reward: 0.0912,                 loss: 0.1958
agent1:                 episode reward: -0.0912,                 loss: nan
Episode: 76921/101000 (76.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4046s / 17806.9236 s
agent0:                 episode reward: -0.1602,                 loss: 0.1938
agent1:                 episode reward: 0.1602,                 loss: nan
Episode: 76941/101000 (76.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2153s / 17812.1389 s
agent0:                 episode reward: -0.0781,                 loss: 0.1919
agent1:                 episode reward: 0.0781,                 loss: nan
Episode: 76961/101000 (76.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0978s / 17817.2367 s
agent0:                 episode reward: 0.2524,                 loss: 0.1949
agent1:                 episode reward: -0.2524,                 loss: nan
Episode: 76981/101000 (76.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8896s / 17822.1263 s
agent0:                 episode reward: -0.0537,                 loss: 0.1931
agent1:                 episode reward: 0.0537,                 loss: nan
Episode: 77001/101000 (76.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5063s / 17826.6326 s
agent0:                 episode reward: 0.3631,                 loss: 0.1941
agent1:                 episode reward: -0.3631,                 loss: 0.2121
Score delta: 1.6614342714771753, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/76567_0.
Episode: 77021/101000 (76.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2292s / 17831.8618 s
agent0:                 episode reward: -0.0167,                 loss: nan
agent1:                 episode reward: 0.0167,                 loss: 0.2130
Episode: 77041/101000 (76.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4183s / 17837.2801 s
agent0:                 episode reward: -0.4425,                 loss: nan
agent1:                 episode reward: 0.4425,                 loss: 0.2144
Episode: 77061/101000 (76.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0008s / 17842.2809 s
agent0:                 episode reward: -0.3910,                 loss: nan
agent1:                 episode reward: 0.3910,                 loss: 0.2134
Episode: 77081/101000 (76.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6580s / 17846.9388 s
agent0:                 episode reward: -0.4351,                 loss: nan
agent1:                 episode reward: 0.4351,                 loss: 0.2102
Episode: 77101/101000 (76.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8227s / 17850.7615 s
agent0:                 episode reward: 0.0669,                 loss: nan
agent1:                 episode reward: -0.0669,                 loss: 0.1972
Episode: 77121/101000 (76.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5960s / 17855.3575 s
agent0:                 episode reward: -0.3754,                 loss: nan
agent1:                 episode reward: 0.3754,                 loss: 0.1824
Episode: 77141/101000 (76.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0659s / 17859.4234 s
agent0:                 episode reward: -0.3702,                 loss: nan
agent1:                 episode reward: 0.3702,                 loss: 0.1795
Episode: 77161/101000 (76.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0245s / 17864.4478 s
agent0:                 episode reward: -0.1843,                 loss: nan
agent1:                 episode reward: 0.1843,                 loss: 0.1778
Episode: 77181/101000 (76.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3049s / 17868.7527 s
agent0:                 episode reward: -0.1181,                 loss: nan
agent1:                 episode reward: 0.1181,                 loss: 0.1787
Episode: 77201/101000 (76.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5172s / 17873.2699 s
agent0:                 episode reward: -0.3464,                 loss: 0.1990
agent1:                 episode reward: 0.3464,                 loss: 0.1793
Score delta: 1.6531325851936078, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/76761_1.
Episode: 77221/101000 (76.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5758s / 17877.8457 s
agent0:                 episode reward: -0.0777,                 loss: 0.1996
agent1:                 episode reward: 0.0777,                 loss: nan
Episode: 77241/101000 (76.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4868s / 17882.3325 s
agent0:                 episode reward: -0.3260,                 loss: 0.1991
agent1:                 episode reward: 0.3260,                 loss: nan
Episode: 77261/101000 (76.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2584s / 17886.5908 s
agent0:                 episode reward: -0.0601,                 loss: 0.1996
agent1:                 episode reward: 0.0601,                 loss: nan
Episode: 77281/101000 (76.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3910s / 17890.9818 s
agent0:                 episode reward: 0.2738,                 loss: 0.2001
agent1:                 episode reward: -0.2738,                 loss: nan
Episode: 77301/101000 (76.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3788s / 17896.3606 s
agent0:                 episode reward: 0.4326,                 loss: 0.1985
agent1:                 episode reward: -0.4326,                 loss: nan
Episode: 77321/101000 (76.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7607s / 17901.1213 s
agent0:                 episode reward: -0.4664,                 loss: 0.2002
agent1:                 episode reward: 0.4664,                 loss: nan
Episode: 77341/101000 (76.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5018s / 17906.6230 s
agent0:                 episode reward: -0.0297,                 loss: 0.1984
agent1:                 episode reward: 0.0297,                 loss: nan
Episode: 77361/101000 (76.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3733s / 17910.9963 s
agent0:                 episode reward: 0.4690,                 loss: 0.1979
agent1:                 episode reward: -0.4690,                 loss: nan
Episode: 77381/101000 (76.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2518s / 17915.2482 s
agent0:                 episode reward: -0.1875,                 loss: 0.1983
agent1:                 episode reward: 0.1875,                 loss: nan
Episode: 77401/101000 (76.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1609s / 17919.4091 s
agent0:                 episode reward: 0.3692,                 loss: 0.1973
agent1:                 episode reward: -0.3692,                 loss: nan
Episode: 77421/101000 (76.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6328s / 17924.0418 s
agent0:                 episode reward: -0.0168,                 loss: 0.1970
agent1:                 episode reward: 0.0168,                 loss: 0.1955
Score delta: 1.5940293442359306, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/76978_0.
Episode: 77441/101000 (76.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6959s / 17928.7378 s
agent0:                 episode reward: -0.4857,                 loss: nan
agent1:                 episode reward: 0.4857,                 loss: 0.1922
Episode: 77461/101000 (76.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3726s / 17933.1103 s
agent0:                 episode reward: -0.3304,                 loss: nan
agent1:                 episode reward: 0.3304,                 loss: 0.1925
Episode: 77481/101000 (76.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2746s / 17937.3849 s
agent0:                 episode reward: -0.2262,                 loss: nan
agent1:                 episode reward: 0.2262,                 loss: 0.1900
Episode: 77501/101000 (76.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2786s / 17941.6636 s
agent0:                 episode reward: -0.6183,                 loss: nan
agent1:                 episode reward: 0.6183,                 loss: 0.1913
Episode: 77521/101000 (76.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1087s / 17945.7723 s
agent0:                 episode reward: -0.1505,                 loss: nan
agent1:                 episode reward: 0.1505,                 loss: 0.1905
Episode: 77541/101000 (76.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6025s / 17949.3748 s
agent0:                 episode reward: -0.2905,                 loss: nan
agent1:                 episode reward: 0.2905,                 loss: 0.1917
Episode: 77561/101000 (76.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8482s / 17954.2229 s
agent0:                 episode reward: -0.3413,                 loss: nan
agent1:                 episode reward: 0.3413,                 loss: 0.1913
Episode: 77581/101000 (76.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8624s / 17959.0854 s
agent0:                 episode reward: -0.6139,                 loss: nan
agent1:                 episode reward: 0.6139,                 loss: 0.1901
Episode: 77601/101000 (76.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9449s / 17963.0303 s
agent0:                 episode reward: -0.3420,                 loss: 0.2090
agent1:                 episode reward: 0.3420,                 loss: 0.1863
Score delta: 1.5827266411749268, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/77158_1.
Episode: 77621/101000 (76.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5188s / 17967.5491 s
agent0:                 episode reward: -0.1274,                 loss: 0.2041
agent1:                 episode reward: 0.1274,                 loss: nan
Episode: 77641/101000 (76.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4430s / 17971.9920 s
agent0:                 episode reward: 0.0859,                 loss: 0.2052
agent1:                 episode reward: -0.0859,                 loss: nan
Episode: 77661/101000 (76.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2929s / 17976.2849 s
agent0:                 episode reward: 0.1912,                 loss: 0.2025
agent1:                 episode reward: -0.1912,                 loss: nan
Episode: 77681/101000 (76.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0333s / 17980.3182 s
agent0:                 episode reward: -0.0664,                 loss: 0.2024
agent1:                 episode reward: 0.0664,                 loss: nan
Episode: 77701/101000 (76.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3469s / 17985.6651 s
agent0:                 episode reward: 0.0295,                 loss: 0.2021
agent1:                 episode reward: -0.0295,                 loss: nan
Episode: 77721/101000 (76.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7002s / 17991.3653 s
agent0:                 episode reward: 0.2366,                 loss: 0.1994
agent1:                 episode reward: -0.2366,                 loss: nan
Episode: 77741/101000 (76.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0656s / 17995.4310 s
agent0:                 episode reward: 0.2436,                 loss: 0.2002
agent1:                 episode reward: -0.2436,                 loss: nan
Episode: 77761/101000 (76.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3353s / 18000.7663 s
agent0:                 episode reward: 0.2583,                 loss: 0.1997
agent1:                 episode reward: -0.2583,                 loss: nan
Episode: 77781/101000 (77.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8072s / 18005.5735 s
agent0:                 episode reward: -0.0752,                 loss: 0.1992
agent1:                 episode reward: 0.0752,                 loss: nan
Episode: 77801/101000 (77.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2442s / 18009.8177 s
agent0:                 episode reward: 0.4063,                 loss: 0.2039
agent1:                 episode reward: -0.4063,                 loss: 0.2167
Score delta: 1.8622007013776993, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/77366_0.
Episode: 77821/101000 (77.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9534s / 18013.7711 s
agent0:                 episode reward: -0.7701,                 loss: nan
agent1:                 episode reward: 0.7701,                 loss: 0.2132
Episode: 77841/101000 (77.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6415s / 18018.4126 s
agent0:                 episode reward: -0.0184,                 loss: nan
agent1:                 episode reward: 0.0184,                 loss: 0.2144
Episode: 77861/101000 (77.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6571s / 18023.0698 s
agent0:                 episode reward: -0.2414,                 loss: nan
agent1:                 episode reward: 0.2414,                 loss: 0.1933
Episode: 77881/101000 (77.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5751s / 18027.6449 s
agent0:                 episode reward: -0.7759,                 loss: nan
agent1:                 episode reward: 0.7759,                 loss: 0.1774
Episode: 77901/101000 (77.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6390s / 18032.2839 s
agent0:                 episode reward: -0.0587,                 loss: nan
agent1:                 episode reward: 0.0587,                 loss: 0.1786
Episode: 77921/101000 (77.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9313s / 18036.2152 s
agent0:                 episode reward: -0.0569,                 loss: nan
agent1:                 episode reward: 0.0569,                 loss: 0.1774
Episode: 77941/101000 (77.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7300s / 18040.9452 s
agent0:                 episode reward: -0.1733,                 loss: nan
agent1:                 episode reward: 0.1733,                 loss: 0.1772
Episode: 77961/101000 (77.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4071s / 18046.3524 s
agent0:                 episode reward: -0.3660,                 loss: 0.2071
agent1:                 episode reward: 0.3660,                 loss: 0.1755
Score delta: 1.610986255361977, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/77532_1.
Episode: 77981/101000 (77.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7587s / 18051.1111 s
agent0:                 episode reward: 0.0840,                 loss: 0.1986
agent1:                 episode reward: -0.0840,                 loss: nan
Episode: 78001/101000 (77.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8821s / 18054.9932 s
agent0:                 episode reward: 0.4339,                 loss: 0.1986
agent1:                 episode reward: -0.4339,                 loss: nan
Episode: 78021/101000 (77.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6989s / 18059.6921 s
agent0:                 episode reward: -0.1032,                 loss: 0.1971
agent1:                 episode reward: 0.1032,                 loss: nan
Episode: 78041/101000 (77.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9302s / 18063.6223 s
agent0:                 episode reward: -0.2574,                 loss: 0.1973
agent1:                 episode reward: 0.2574,                 loss: nan
Episode: 78061/101000 (77.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6053s / 18067.2275 s
agent0:                 episode reward: -0.1248,                 loss: 0.1963
agent1:                 episode reward: 0.1248,                 loss: nan
Episode: 78081/101000 (77.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4490s / 18072.6765 s
agent0:                 episode reward: -0.3666,                 loss: 0.1946
agent1:                 episode reward: 0.3666,                 loss: nan
Episode: 78101/101000 (77.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3402s / 18076.0167 s
agent0:                 episode reward: -0.0671,                 loss: 0.1956
agent1:                 episode reward: 0.0671,                 loss: nan
Episode: 78121/101000 (77.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7483s / 18080.7650 s
agent0:                 episode reward: 0.0806,                 loss: 0.1957
agent1:                 episode reward: -0.0806,                 loss: nan
Episode: 78141/101000 (77.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8042s / 18085.5692 s
agent0:                 episode reward: 0.1086,                 loss: 0.1955
agent1:                 episode reward: -0.1086,                 loss: nan
Episode: 78161/101000 (77.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7207s / 18090.2899 s
agent0:                 episode reward: 0.0574,                 loss: 0.1961
agent1:                 episode reward: -0.0574,                 loss: 0.1999
Score delta: 1.5927481184011594, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/77733_0.
Episode: 78181/101000 (77.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9780s / 18095.2680 s
agent0:                 episode reward: -0.1027,                 loss: nan
agent1:                 episode reward: 0.1027,                 loss: 0.1922
Episode: 78201/101000 (77.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1413s / 18099.4093 s
agent0:                 episode reward: -0.1199,                 loss: nan
agent1:                 episode reward: 0.1199,                 loss: 0.1915
Episode: 78221/101000 (77.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9216s / 18103.3309 s
agent0:                 episode reward: -0.3343,                 loss: nan
agent1:                 episode reward: 0.3343,                 loss: 0.1897
Episode: 78241/101000 (77.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0527s / 18108.3836 s
agent0:                 episode reward: -0.4458,                 loss: nan
agent1:                 episode reward: 0.4458,                 loss: 0.1920
Episode: 78261/101000 (77.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1654s / 18112.5490 s
agent0:                 episode reward: -0.6758,                 loss: nan
agent1:                 episode reward: 0.6758,                 loss: 0.1923
Episode: 78281/101000 (77.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0429s / 18116.5919 s
agent0:                 episode reward: -0.0517,                 loss: nan
agent1:                 episode reward: 0.0517,                 loss: 0.1898
Episode: 78301/101000 (77.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3056s / 18120.8975 s
agent0:                 episode reward: -0.3080,                 loss: nan
agent1:                 episode reward: 0.3080,                 loss: 0.1907
Episode: 78321/101000 (77.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3716s / 18125.2690 s
agent0:                 episode reward: -0.2212,                 loss: nan
agent1:                 episode reward: 0.2212,                 loss: 0.1906
Episode: 78341/101000 (77.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4071s / 18129.6761 s
agent0:                 episode reward: -0.4225,                 loss: nan
agent1:                 episode reward: 0.4225,                 loss: 0.1874
Episode: 78361/101000 (77.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2828s / 18134.9590 s
agent0:                 episode reward: -0.2381,                 loss: nan
agent1:                 episode reward: 0.2381,                 loss: 0.1893
Episode: 78381/101000 (77.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5034s / 18139.4623 s
agent0:                 episode reward: -0.5940,                 loss: 0.1946
agent1:                 episode reward: 0.5940,                 loss: 0.1907
Score delta: 1.5381382860885926, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/77944_1.
Episode: 78401/101000 (77.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5556s / 18143.0180 s
agent0:                 episode reward: 0.0143,                 loss: 0.1887
agent1:                 episode reward: -0.0143,                 loss: nan
Episode: 78421/101000 (77.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4831s / 18147.5011 s
agent0:                 episode reward: -0.2738,                 loss: 0.1916
agent1:                 episode reward: 0.2738,                 loss: nan
Episode: 78441/101000 (77.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0582s / 18152.5593 s
agent0:                 episode reward: -0.2197,                 loss: 0.1937
agent1:                 episode reward: 0.2197,                 loss: nan
Episode: 78461/101000 (77.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7551s / 18158.3144 s
agent0:                 episode reward: -0.0987,                 loss: 0.1935
agent1:                 episode reward: 0.0987,                 loss: nan
Episode: 78481/101000 (77.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3145s / 18162.6289 s
agent0:                 episode reward: 0.3791,                 loss: 0.1936
agent1:                 episode reward: -0.3791,                 loss: nan
Episode: 78501/101000 (77.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2066s / 18166.8356 s
agent0:                 episode reward: 0.1216,                 loss: 0.1933
agent1:                 episode reward: -0.1216,                 loss: nan
Episode: 78521/101000 (77.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6437s / 18171.4793 s
agent0:                 episode reward: -0.1017,                 loss: 0.1914
agent1:                 episode reward: 0.1017,                 loss: nan
Episode: 78541/101000 (77.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8977s / 18175.3770 s
agent0:                 episode reward: -0.4950,                 loss: 0.1936
agent1:                 episode reward: 0.4950,                 loss: nan
Episode: 78561/101000 (77.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2224s / 18179.5994 s
agent0:                 episode reward: 0.2964,                 loss: 0.1930
agent1:                 episode reward: -0.2964,                 loss: nan
Episode: 78581/101000 (77.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2094s / 18184.8088 s
agent0:                 episode reward: -0.1476,                 loss: 0.1930
agent1:                 episode reward: 0.1476,                 loss: nan
Episode: 78601/101000 (77.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9209s / 18189.7297 s
agent0:                 episode reward: 0.3805,                 loss: 0.1924
agent1:                 episode reward: -0.3805,                 loss: nan
Episode: 78621/101000 (77.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3002s / 18195.0299 s
agent0:                 episode reward: 0.0737,                 loss: 0.1931
agent1:                 episode reward: -0.0737,                 loss: 0.1893
Score delta: 1.7156171254428405, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/78182_0.
Episode: 78641/101000 (77.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5419s / 18199.5717 s
agent0:                 episode reward: -0.3513,                 loss: nan
agent1:                 episode reward: 0.3513,                 loss: 0.1799
Episode: 78661/101000 (77.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5206s / 18203.0924 s
agent0:                 episode reward: -0.2513,                 loss: nan
agent1:                 episode reward: 0.2513,                 loss: 0.1791
Episode: 78681/101000 (77.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2064s / 18208.2988 s
agent0:                 episode reward: -0.4025,                 loss: nan
agent1:                 episode reward: 0.4025,                 loss: 0.1802
Episode: 78701/101000 (77.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6446s / 18212.9434 s
agent0:                 episode reward: -0.1337,                 loss: nan
agent1:                 episode reward: 0.1337,                 loss: 0.1802
Episode: 78721/101000 (77.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1061s / 18217.0495 s
agent0:                 episode reward: -0.2494,                 loss: nan
agent1:                 episode reward: 0.2494,                 loss: 0.1794
Episode: 78741/101000 (77.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7911s / 18220.8406 s
agent0:                 episode reward: -0.4491,                 loss: nan
agent1:                 episode reward: 0.4491,                 loss: 0.1794
Episode: 78761/101000 (77.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0350s / 18224.8757 s
agent0:                 episode reward: -0.5080,                 loss: nan
agent1:                 episode reward: 0.5080,                 loss: 0.1789
Episode: 78781/101000 (78.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7722s / 18229.6478 s
agent0:                 episode reward: -0.5185,                 loss: 0.1998
agent1:                 episode reward: 0.5185,                 loss: 0.1770
Score delta: 1.7242565941494399, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/78351_1.
Episode: 78801/101000 (78.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3207s / 18233.9685 s
agent0:                 episode reward: 0.4098,                 loss: 0.1972
agent1:                 episode reward: -0.4098,                 loss: nan
Episode: 78821/101000 (78.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4374s / 18238.4059 s
agent0:                 episode reward: 0.1759,                 loss: 0.1971
agent1:                 episode reward: -0.1759,                 loss: nan
Episode: 78841/101000 (78.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2388s / 18242.6447 s
agent0:                 episode reward: 0.2033,                 loss: 0.1966
agent1:                 episode reward: -0.2033,                 loss: nan
Episode: 78861/101000 (78.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7479s / 18247.3926 s
agent0:                 episode reward: 0.3670,                 loss: 0.1962
agent1:                 episode reward: -0.3670,                 loss: nan
Episode: 78881/101000 (78.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9473s / 18251.3399 s
agent0:                 episode reward: -0.5688,                 loss: 0.1961
agent1:                 episode reward: 0.5688,                 loss: nan
Episode: 78901/101000 (78.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8458s / 18256.1857 s
agent0:                 episode reward: -0.1855,                 loss: 0.1952
agent1:                 episode reward: 0.1855,                 loss: nan
Episode: 78921/101000 (78.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1750s / 18261.3607 s
agent0:                 episode reward: -0.0645,                 loss: 0.1962
agent1:                 episode reward: 0.0645,                 loss: nan
Episode: 78941/101000 (78.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7269s / 18266.0877 s
agent0:                 episode reward: -0.1979,                 loss: 0.2055
agent1:                 episode reward: 0.1979,                 loss: nan
Episode: 78961/101000 (78.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5112s / 18271.5988 s
agent0:                 episode reward: 0.1751,                 loss: 0.2085
agent1:                 episode reward: -0.1751,                 loss: nan
Episode: 78981/101000 (78.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8632s / 18276.4620 s
agent0:                 episode reward: 0.2334,                 loss: 0.2054
agent1:                 episode reward: -0.2334,                 loss: 0.1954
Score delta: 1.7662956571594854, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/78551_0.
Episode: 79001/101000 (78.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2572s / 18281.7192 s
agent0:                 episode reward: -0.5345,                 loss: nan
agent1:                 episode reward: 0.5345,                 loss: 0.1934
Episode: 79021/101000 (78.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3540s / 18286.0732 s
agent0:                 episode reward: -0.2771,                 loss: nan
agent1:                 episode reward: 0.2771,                 loss: 0.1917
Episode: 79041/101000 (78.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0952s / 18290.1684 s
agent0:                 episode reward: -0.3582,                 loss: nan
agent1:                 episode reward: 0.3582,                 loss: 0.1914
Episode: 79061/101000 (78.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0588s / 18294.2273 s
agent0:                 episode reward: -0.0805,                 loss: nan
agent1:                 episode reward: 0.0805,                 loss: 0.1910
Episode: 79081/101000 (78.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4543s / 18298.6816 s
agent0:                 episode reward: -0.2032,                 loss: nan
agent1:                 episode reward: 0.2032,                 loss: 0.1917
Episode: 79101/101000 (78.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1941s / 18303.8757 s
agent0:                 episode reward: -0.4420,                 loss: nan
agent1:                 episode reward: 0.4420,                 loss: 0.1913
Episode: 79121/101000 (78.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8368s / 18308.7124 s
agent0:                 episode reward: -0.2460,                 loss: nan
agent1:                 episode reward: 0.2460,                 loss: 0.1903
Episode: 79141/101000 (78.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4686s / 18313.1810 s
agent0:                 episode reward: -0.7668,                 loss: nan
agent1:                 episode reward: 0.7668,                 loss: 0.1897
Episode: 79161/101000 (78.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7801s / 18317.9611 s
agent0:                 episode reward: -0.0517,                 loss: nan
agent1:                 episode reward: 0.0517,                 loss: 0.1882
Episode: 79181/101000 (78.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4956s / 18322.4567 s
agent0:                 episode reward: -0.4843,                 loss: nan
agent1:                 episode reward: 0.4843,                 loss: 0.1746
Score delta: 1.5101662619127505, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/78755_1.
Episode: 79201/101000 (78.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0432s / 18326.4998 s
agent0:                 episode reward: -0.0868,                 loss: 0.2464
agent1:                 episode reward: 0.0868,                 loss: nan
Episode: 79221/101000 (78.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4100s / 18330.9098 s
agent0:                 episode reward: -0.0799,                 loss: 0.2419
agent1:                 episode reward: 0.0799,                 loss: nan
Episode: 79241/101000 (78.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2799s / 18335.1897 s
agent0:                 episode reward: -0.0247,                 loss: 0.2385
agent1:                 episode reward: 0.0247,                 loss: nan
Episode: 79261/101000 (78.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0337s / 18340.2234 s
agent0:                 episode reward: -0.3246,                 loss: 0.2383
agent1:                 episode reward: 0.3246,                 loss: nan
Episode: 79281/101000 (78.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9489s / 18346.1723 s
agent0:                 episode reward: -0.3836,                 loss: 0.2385
agent1:                 episode reward: 0.3836,                 loss: nan
Episode: 79301/101000 (78.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9522s / 18351.1245 s
agent0:                 episode reward: -0.1015,                 loss: 0.2377
agent1:                 episode reward: 0.1015,                 loss: nan
Episode: 79321/101000 (78.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0683s / 18355.1928 s
agent0:                 episode reward: -0.1213,                 loss: 0.2381
agent1:                 episode reward: 0.1213,                 loss: nan
Episode: 79341/101000 (78.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1122s / 18360.3050 s
agent0:                 episode reward: -0.1196,                 loss: 0.2380
agent1:                 episode reward: 0.1196,                 loss: nan
Episode: 79361/101000 (78.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9152s / 18365.2203 s
agent0:                 episode reward: -0.1532,                 loss: 0.2361
agent1:                 episode reward: 0.1532,                 loss: nan
Episode: 79381/101000 (78.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7058s / 18369.9260 s
agent0:                 episode reward: 0.2240,                 loss: 0.2358
agent1:                 episode reward: -0.2240,                 loss: nan
Episode: 79401/101000 (78.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7151s / 18374.6412 s
agent0:                 episode reward: 0.3833,                 loss: 0.2390
agent1:                 episode reward: -0.3833,                 loss: nan
Episode: 79421/101000 (78.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3981s / 18380.0393 s
agent0:                 episode reward: -0.1974,                 loss: 0.2370
agent1:                 episode reward: 0.1974,                 loss: nan
Episode: 79441/101000 (78.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8208s / 18384.8601 s
agent0:                 episode reward: -0.2954,                 loss: 0.2364
agent1:                 episode reward: 0.2954,                 loss: nan
Episode: 79461/101000 (78.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0683s / 18389.9284 s
agent0:                 episode reward: 0.1398,                 loss: 0.2241
agent1:                 episode reward: -0.1398,                 loss: nan
Episode: 79481/101000 (78.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0480s / 18394.9764 s
agent0:                 episode reward: 0.0062,                 loss: 0.1990
agent1:                 episode reward: -0.0062,                 loss: nan
Episode: 79501/101000 (78.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4875s / 18400.4639 s
agent0:                 episode reward: -0.1689,                 loss: 0.1971
agent1:                 episode reward: 0.1689,                 loss: nan
Episode: 79521/101000 (78.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1815s / 18405.6454 s
agent0:                 episode reward: 0.2686,                 loss: 0.1965
agent1:                 episode reward: -0.2686,                 loss: nan
Episode: 79541/101000 (78.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1555s / 18409.8009 s
agent0:                 episode reward: 0.2694,                 loss: 0.1986
agent1:                 episode reward: -0.2694,                 loss: nan
Episode: 79561/101000 (78.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5705s / 18414.3714 s
agent0:                 episode reward: -0.1615,                 loss: 0.1946
agent1:                 episode reward: 0.1615,                 loss: nan
Episode: 79581/101000 (78.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0672s / 18418.4386 s
agent0:                 episode reward: -0.0088,                 loss: 0.1962
agent1:                 episode reward: 0.0088,                 loss: nan
Episode: 79601/101000 (78.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3598s / 18424.7984 s
agent0:                 episode reward: 0.1366,                 loss: 0.1969
agent1:                 episode reward: -0.1366,                 loss: nan
Episode: 79621/101000 (78.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0730s / 18429.8714 s
agent0:                 episode reward: -0.2506,                 loss: 0.1964
agent1:                 episode reward: 0.2506,                 loss: nan
Episode: 79641/101000 (78.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4007s / 18434.2721 s
agent0:                 episode reward: 0.0949,                 loss: 0.1948
agent1:                 episode reward: -0.0949,                 loss: nan
Episode: 79661/101000 (78.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1039s / 18438.3759 s
agent0:                 episode reward: 0.0531,                 loss: 0.1959
agent1:                 episode reward: -0.0531,                 loss: 0.1834
Score delta: 1.5786643562016, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/79218_0.
Episode: 79681/101000 (78.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2988s / 18442.6747 s
agent0:                 episode reward: -0.2171,                 loss: nan
agent1:                 episode reward: 0.2171,                 loss: 0.1851
Episode: 79701/101000 (78.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4103s / 18447.0850 s
agent0:                 episode reward: -0.0267,                 loss: nan
agent1:                 episode reward: 0.0267,                 loss: 0.1830
Episode: 79721/101000 (78.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9201s / 18451.0051 s
agent0:                 episode reward: 0.1708,                 loss: nan
agent1:                 episode reward: -0.1708,                 loss: 0.1834
Episode: 79741/101000 (78.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7930s / 18455.7981 s
agent0:                 episode reward: 0.0443,                 loss: nan
agent1:                 episode reward: -0.0443,                 loss: 0.1827
Episode: 79761/101000 (78.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3077s / 18460.1058 s
agent0:                 episode reward: -0.2237,                 loss: nan
agent1:                 episode reward: 0.2237,                 loss: 0.1823
Episode: 79781/101000 (78.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0015s / 18464.1073 s
agent0:                 episode reward: -0.3279,                 loss: nan
agent1:                 episode reward: 0.3279,                 loss: 0.1808
Episode: 79801/101000 (79.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1558s / 18468.2631 s
agent0:                 episode reward: -0.2338,                 loss: nan
agent1:                 episode reward: 0.2338,                 loss: 0.1826
Episode: 79821/101000 (79.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8147s / 18473.0779 s
agent0:                 episode reward: -0.1973,                 loss: nan
agent1:                 episode reward: 0.1973,                 loss: 0.1799
Episode: 79841/101000 (79.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4241s / 18478.5020 s
agent0:                 episode reward: -0.3882,                 loss: nan
agent1:                 episode reward: 0.3882,                 loss: 0.1795
Episode: 79861/101000 (79.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3191s / 18482.8210 s
agent0:                 episode reward: -0.3305,                 loss: nan
agent1:                 episode reward: 0.3305,                 loss: 0.1818
Episode: 79881/101000 (79.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1332s / 18486.9542 s
agent0:                 episode reward: -0.4250,                 loss: nan
agent1:                 episode reward: 0.4250,                 loss: 0.1818
Episode: 79901/101000 (79.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3699s / 18491.3241 s
agent0:                 episode reward: -0.4486,                 loss: nan
agent1:                 episode reward: 0.4486,                 loss: 0.1808
Episode: 79921/101000 (79.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2795s / 18495.6036 s
agent0:                 episode reward: -0.1783,                 loss: nan
agent1:                 episode reward: 0.1783,                 loss: 0.1814
Episode: 79941/101000 (79.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9571s / 18499.5607 s
agent0:                 episode reward: -0.5226,                 loss: nan
agent1:                 episode reward: 0.5226,                 loss: 0.1801
Episode: 79961/101000 (79.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6667s / 18504.2274 s
agent0:                 episode reward: -0.1911,                 loss: nan
agent1:                 episode reward: 0.1911,                 loss: 0.1803
Episode: 79981/101000 (79.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2424s / 18509.4698 s
agent0:                 episode reward: -0.0176,                 loss: nan
agent1:                 episode reward: 0.0176,                 loss: 0.1783
Episode: 80001/101000 (79.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1380s / 18514.6078 s
agent0:                 episode reward: -0.5291,                 loss: 0.2019
agent1:                 episode reward: 0.5291,                 loss: 0.1764
Score delta: 1.6902201909416914, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/79567_1.
Episode: 80021/101000 (79.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6152s / 18519.2230 s
agent0:                 episode reward: 0.0648,                 loss: 0.2010
agent1:                 episode reward: -0.0648,                 loss: nan
Episode: 80041/101000 (79.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3377s / 18524.5607 s
agent0:                 episode reward: 0.3568,                 loss: 0.2002
agent1:                 episode reward: -0.3568,                 loss: nan
Episode: 80061/101000 (79.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7262s / 18529.2869 s
agent0:                 episode reward: 0.0106,                 loss: 0.2009
agent1:                 episode reward: -0.0106,                 loss: nan
Episode: 80081/101000 (79.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4932s / 18534.7801 s
agent0:                 episode reward: 0.6952,                 loss: 0.2028
agent1:                 episode reward: -0.6952,                 loss: nan
Episode: 80101/101000 (79.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7406s / 18538.5206 s
agent0:                 episode reward: 0.2507,                 loss: 0.2018
agent1:                 episode reward: -0.2507,                 loss: nan
Episode: 80121/101000 (79.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5266s / 18543.0472 s
agent0:                 episode reward: -0.2869,                 loss: 0.2004
agent1:                 episode reward: 0.2869,                 loss: nan
Episode: 80141/101000 (79.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1040s / 18548.1512 s
agent0:                 episode reward: 0.2505,                 loss: 0.2052
agent1:                 episode reward: -0.2505,                 loss: nan
Episode: 80161/101000 (79.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3259s / 18552.4771 s
agent0:                 episode reward: 0.1926,                 loss: 0.2076
agent1:                 episode reward: -0.1926,                 loss: nan
Episode: 80181/101000 (79.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9145s / 18556.3915 s
agent0:                 episode reward: -0.0647,                 loss: 0.2097
agent1:                 episode reward: 0.0647,                 loss: nan
Episode: 80201/101000 (79.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0951s / 18561.4867 s
agent0:                 episode reward: -0.2637,                 loss: 0.2075
agent1:                 episode reward: 0.2637,                 loss: nan
Episode: 80221/101000 (79.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9612s / 18568.4479 s
agent0:                 episode reward: 0.2596,                 loss: 0.2067
agent1:                 episode reward: -0.2596,                 loss: nan
Episode: 80241/101000 (79.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8452s / 18573.2931 s
agent0:                 episode reward: 0.1141,                 loss: 0.2062
agent1:                 episode reward: -0.1141,                 loss: nan
Episode: 80261/101000 (79.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3083s / 18578.6014 s
agent0:                 episode reward: 0.3164,                 loss: 0.2070
agent1:                 episode reward: -0.3164,                 loss: nan
Episode: 80281/101000 (79.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7542s / 18583.3556 s
agent0:                 episode reward: 0.1174,                 loss: 0.2057
agent1:                 episode reward: -0.1174,                 loss: nan
Episode: 80301/101000 (79.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5463s / 18587.9019 s
agent0:                 episode reward: 0.0625,                 loss: 0.2036
agent1:                 episode reward: -0.0625,                 loss: nan
Episode: 80321/101000 (79.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2584s / 18594.1603 s
agent0:                 episode reward: -0.6595,                 loss: 0.2072
agent1:                 episode reward: 0.6595,                 loss: nan
Episode: 80341/101000 (79.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8944s / 18599.0547 s
agent0:                 episode reward: -0.2830,                 loss: 0.2060
agent1:                 episode reward: 0.2830,                 loss: nan
Episode: 80361/101000 (79.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0617s / 18604.1164 s
agent0:                 episode reward: -0.1143,                 loss: 0.2057
agent1:                 episode reward: 0.1143,                 loss: nan
Episode: 80381/101000 (79.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3563s / 18609.4726 s
agent0:                 episode reward: 0.1345,                 loss: 0.2088
agent1:                 episode reward: -0.1345,                 loss: nan
Episode: 80401/101000 (79.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7459s / 18614.2185 s
agent0:                 episode reward: 0.0763,                 loss: 0.2072
agent1:                 episode reward: -0.0763,                 loss: nan
Episode: 80421/101000 (79.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1056s / 18618.3241 s
agent0:                 episode reward: 0.0342,                 loss: 0.2060
agent1:                 episode reward: -0.0342,                 loss: 0.1737
Score delta: 1.5987530188315104, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/79980_0.
Episode: 80441/101000 (79.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7802s / 18623.1043 s
agent0:                 episode reward: -0.3631,                 loss: nan
agent1:                 episode reward: 0.3631,                 loss: 0.1727
Episode: 80461/101000 (79.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8414s / 18627.9456 s
agent0:                 episode reward: -0.4964,                 loss: nan
agent1:                 episode reward: 0.4964,                 loss: 0.1743
Episode: 80481/101000 (79.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1262s / 18633.0718 s
agent0:                 episode reward: -0.1530,                 loss: nan
agent1:                 episode reward: 0.1530,                 loss: 0.1737
Episode: 80501/101000 (79.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3306s / 18638.4024 s
agent0:                 episode reward: -0.0395,                 loss: nan
agent1:                 episode reward: 0.0395,                 loss: 0.1721
Episode: 80521/101000 (79.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5963s / 18642.9987 s
agent0:                 episode reward: -0.2127,                 loss: nan
agent1:                 episode reward: 0.2127,                 loss: 0.1750
Episode: 80541/101000 (79.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4709s / 18648.4697 s
agent0:                 episode reward: -0.3681,                 loss: nan
agent1:                 episode reward: 0.3681,                 loss: 0.1728
Episode: 80561/101000 (79.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4613s / 18653.9310 s
agent0:                 episode reward: -0.0904,                 loss: nan
agent1:                 episode reward: 0.0904,                 loss: 0.1746
Episode: 80581/101000 (79.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7413s / 18658.6722 s
agent0:                 episode reward: -0.3926,                 loss: nan
agent1:                 episode reward: 0.3926,                 loss: 0.1745
Episode: 80601/101000 (79.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2824s / 18662.9546 s
agent0:                 episode reward: -0.5596,                 loss: 0.2377
agent1:                 episode reward: 0.5596,                 loss: 0.1726
Score delta: 1.5057123201077325, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/80173_1.
Episode: 80621/101000 (79.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3904s / 18667.3451 s
agent0:                 episode reward: 0.0207,                 loss: 0.2389
agent1:                 episode reward: -0.0207,                 loss: nan
Episode: 80641/101000 (79.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1638s / 18671.5088 s
agent0:                 episode reward: 0.1013,                 loss: 0.2392
agent1:                 episode reward: -0.1013,                 loss: nan
Episode: 80661/101000 (79.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0422s / 18675.5511 s
agent0:                 episode reward: -0.1075,                 loss: 0.2392
agent1:                 episode reward: 0.1075,                 loss: nan
Episode: 80681/101000 (79.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1588s / 18680.7099 s
agent0:                 episode reward: 0.0354,                 loss: 0.1928
agent1:                 episode reward: -0.0354,                 loss: nan
Episode: 80701/101000 (79.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5432s / 18686.2531 s
agent0:                 episode reward: -0.2421,                 loss: 0.1881
agent1:                 episode reward: 0.2421,                 loss: nan
Episode: 80721/101000 (79.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9289s / 18690.1820 s
agent0:                 episode reward: 0.0089,                 loss: 0.1876
agent1:                 episode reward: -0.0089,                 loss: nan
Episode: 80741/101000 (79.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1107s / 18694.2927 s
agent0:                 episode reward: -0.2894,                 loss: 0.1867
agent1:                 episode reward: 0.2894,                 loss: nan
Episode: 80761/101000 (79.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8590s / 18699.1518 s
agent0:                 episode reward: -0.1218,                 loss: 0.1882
agent1:                 episode reward: 0.1218,                 loss: nan
Episode: 80781/101000 (79.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6175s / 18703.7693 s
agent0:                 episode reward: 0.1683,                 loss: 0.1870
agent1:                 episode reward: -0.1683,                 loss: nan
Episode: 80801/101000 (80.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5117s / 18708.2810 s
agent0:                 episode reward: -0.1050,                 loss: 0.1862
agent1:                 episode reward: 0.1050,                 loss: 0.1868
Score delta: 1.657854897044534, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/80361_0.
Episode: 80821/101000 (80.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0258s / 18713.3069 s
agent0:                 episode reward: -0.2545,                 loss: nan
agent1:                 episode reward: 0.2545,                 loss: 0.1844
Episode: 80841/101000 (80.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2916s / 18718.5985 s
agent0:                 episode reward: -0.2526,                 loss: nan
agent1:                 episode reward: 0.2526,                 loss: 0.1841
Episode: 80861/101000 (80.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0490s / 18723.6475 s
agent0:                 episode reward: -0.0357,                 loss: nan
agent1:                 episode reward: 0.0357,                 loss: 0.1853
Episode: 80881/101000 (80.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0971s / 18728.7446 s
agent0:                 episode reward: -0.2394,                 loss: nan
agent1:                 episode reward: 0.2394,                 loss: 0.1839
Episode: 80901/101000 (80.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0148s / 18733.7594 s
agent0:                 episode reward: -0.6919,                 loss: nan
agent1:                 episode reward: 0.6919,                 loss: 0.1769
Episode: 80921/101000 (80.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8288s / 18738.5882 s
agent0:                 episode reward: -0.0184,                 loss: nan
agent1:                 episode reward: 0.0184,                 loss: 0.1741
Episode: 80941/101000 (80.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6487s / 18743.2369 s
agent0:                 episode reward: -0.3546,                 loss: nan
agent1:                 episode reward: 0.3546,                 loss: 0.1752
Episode: 80961/101000 (80.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4985s / 18748.7355 s
agent0:                 episode reward: 0.0043,                 loss: nan
agent1:                 episode reward: -0.0043,                 loss: 0.1745
Episode: 80981/101000 (80.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0673s / 18753.8027 s
agent0:                 episode reward: -0.4492,                 loss: nan
agent1:                 episode reward: 0.4492,                 loss: 0.1731
Episode: 81001/101000 (80.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9566s / 18758.7594 s
agent0:                 episode reward: -0.1148,                 loss: 0.2000
agent1:                 episode reward: 0.1148,                 loss: 0.1742
Score delta: 1.5338873739795373, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/80559_1.
Episode: 81021/101000 (80.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8966s / 18762.6560 s
agent0:                 episode reward: 0.0913,                 loss: 0.1969
agent1:                 episode reward: -0.0913,                 loss: nan
Episode: 81041/101000 (80.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7631s / 18767.4191 s
agent0:                 episode reward: -0.2332,                 loss: 0.1961
agent1:                 episode reward: 0.2332,                 loss: nan
Episode: 81061/101000 (80.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8011s / 18772.2201 s
agent0:                 episode reward: -0.0269,                 loss: 0.1990
agent1:                 episode reward: 0.0269,                 loss: nan
Episode: 81081/101000 (80.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2040s / 18777.4241 s
agent0:                 episode reward: -0.2256,                 loss: 0.1970
agent1:                 episode reward: 0.2256,                 loss: nan
Episode: 81101/101000 (80.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0557s / 18782.4799 s
agent0:                 episode reward: 0.2067,                 loss: 0.1990
agent1:                 episode reward: -0.2067,                 loss: nan
Episode: 81121/101000 (80.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4686s / 18786.9484 s
agent0:                 episode reward: 0.2038,                 loss: 0.1973
agent1:                 episode reward: -0.2038,                 loss: nan
Episode: 81141/101000 (80.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5836s / 18791.5321 s
agent0:                 episode reward: 0.6485,                 loss: 0.1974
agent1:                 episode reward: -0.6485,                 loss: nan
Episode: 81161/101000 (80.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8083s / 18796.3403 s
agent0:                 episode reward: -0.0655,                 loss: 0.1964
agent1:                 episode reward: 0.0655,                 loss: nan
Episode: 81181/101000 (80.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6582s / 18800.9986 s
agent0:                 episode reward: 0.3172,                 loss: 0.1978
agent1:                 episode reward: -0.3172,                 loss: 0.1920
Score delta: 1.5086209406677777, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/80744_0.
Episode: 81201/101000 (80.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1441s / 18806.1427 s
agent0:                 episode reward: 0.0374,                 loss: nan
agent1:                 episode reward: -0.0374,                 loss: 0.1879
Episode: 81221/101000 (80.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3325s / 18810.4752 s
agent0:                 episode reward: -0.1709,                 loss: nan
agent1:                 episode reward: 0.1709,                 loss: 0.1858
Episode: 81241/101000 (80.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7850s / 18814.2601 s
agent0:                 episode reward: -0.4202,                 loss: nan
agent1:                 episode reward: 0.4202,                 loss: 0.1870
Episode: 81261/101000 (80.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7667s / 18819.0269 s
agent0:                 episode reward: -0.0982,                 loss: nan
agent1:                 episode reward: 0.0982,                 loss: 0.1894
Episode: 81281/101000 (80.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7550s / 18823.7818 s
agent0:                 episode reward: -0.6875,                 loss: nan
agent1:                 episode reward: 0.6875,                 loss: 0.1865
Episode: 81301/101000 (80.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5177s / 18828.2996 s
agent0:                 episode reward: -0.4282,                 loss: nan
agent1:                 episode reward: 0.4282,                 loss: 0.1842
Episode: 81321/101000 (80.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4731s / 18832.7727 s
agent0:                 episode reward: -0.6602,                 loss: nan
agent1:                 episode reward: 0.6602,                 loss: 0.1860
Episode: 81341/101000 (80.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8660s / 18836.6387 s
agent0:                 episode reward: -0.1225,                 loss: nan
agent1:                 episode reward: 0.1225,                 loss: 0.1864
Episode: 81361/101000 (80.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3982s / 18842.0369 s
agent0:                 episode reward: -0.1369,                 loss: nan
agent1:                 episode reward: 0.1369,                 loss: 0.1852
Episode: 81381/101000 (80.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5327s / 18846.5696 s
agent0:                 episode reward: 0.0897,                 loss: nan
agent1:                 episode reward: -0.0897,                 loss: 0.1845
Episode: 81401/101000 (80.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8697s / 18851.4393 s
agent0:                 episode reward: -0.2209,                 loss: 0.1918
agent1:                 episode reward: 0.2209,                 loss: 0.1825
Score delta: 1.7461489131495653, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/80964_1.
Episode: 81421/101000 (80.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9063s / 18857.3456 s
agent0:                 episode reward: -0.2618,                 loss: 0.1951
agent1:                 episode reward: 0.2618,                 loss: nan
Episode: 81441/101000 (80.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1301s / 18862.4757 s
agent0:                 episode reward: -0.2108,                 loss: 0.1991
agent1:                 episode reward: 0.2108,                 loss: nan
Episode: 81461/101000 (80.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4046s / 18866.8803 s
agent0:                 episode reward: 0.1944,                 loss: 0.2000
agent1:                 episode reward: -0.1944,                 loss: nan
Episode: 81481/101000 (80.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6803s / 18871.5605 s
agent0:                 episode reward: -0.0407,                 loss: 0.1995
agent1:                 episode reward: 0.0407,                 loss: nan
Episode: 81501/101000 (80.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3323s / 18875.8929 s
agent0:                 episode reward: 0.4410,                 loss: 0.1978
agent1:                 episode reward: -0.4410,                 loss: nan
Episode: 81521/101000 (80.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5307s / 18880.4236 s
agent0:                 episode reward: -0.0401,                 loss: 0.1981
agent1:                 episode reward: 0.0401,                 loss: nan
Episode: 81541/101000 (80.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9607s / 18885.3843 s
agent0:                 episode reward: -0.1038,                 loss: 0.2015
agent1:                 episode reward: 0.1038,                 loss: nan
Episode: 81561/101000 (80.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5173s / 18890.9016 s
agent0:                 episode reward: -0.0890,                 loss: 0.1984
agent1:                 episode reward: 0.0890,                 loss: nan
Episode: 81581/101000 (80.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1993s / 18895.1009 s
agent0:                 episode reward: 0.0143,                 loss: 0.1970
agent1:                 episode reward: -0.0143,                 loss: nan
Episode: 81601/101000 (80.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2717s / 18900.3726 s
agent0:                 episode reward: -0.3162,                 loss: 0.1975
agent1:                 episode reward: 0.3162,                 loss: nan
Episode: 81621/101000 (80.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7048s / 18905.0774 s
agent0:                 episode reward: -0.0358,                 loss: 0.1995
agent1:                 episode reward: 0.0358,                 loss: nan
Episode: 81641/101000 (80.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4911s / 18908.5685 s
agent0:                 episode reward: 0.0078,                 loss: 0.1972
agent1:                 episode reward: -0.0078,                 loss: nan
Episode: 81661/101000 (80.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9798s / 18913.5483 s
agent0:                 episode reward: 0.3013,                 loss: 0.1966
agent1:                 episode reward: -0.3013,                 loss: nan
Episode: 81681/101000 (80.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6263s / 18918.1747 s
agent0:                 episode reward: 0.2091,                 loss: 0.1970
agent1:                 episode reward: -0.2091,                 loss: nan
Episode: 81701/101000 (80.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0983s / 18923.2730 s
agent0:                 episode reward: 0.0522,                 loss: 0.1956
agent1:                 episode reward: -0.0522,                 loss: nan
Episode: 81721/101000 (80.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5386s / 18927.8116 s
agent0:                 episode reward: 0.1340,                 loss: 0.1931
agent1:                 episode reward: -0.1340,                 loss: nan
Episode: 81741/101000 (80.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6260s / 18932.4376 s
agent0:                 episode reward: 0.1607,                 loss: 0.1986
agent1:                 episode reward: -0.1607,                 loss: nan
Episode: 81761/101000 (80.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6596s / 18938.0972 s
agent0:                 episode reward: -0.3427,                 loss: 0.2032
agent1:                 episode reward: 0.3427,                 loss: nan
Episode: 81781/101000 (80.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5410s / 18942.6383 s
agent0:                 episode reward: -0.3220,                 loss: 0.2020
agent1:                 episode reward: 0.3220,                 loss: nan
Episode: 81801/101000 (80.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0055s / 18948.6438 s
agent0:                 episode reward: 0.4526,                 loss: 0.2035
agent1:                 episode reward: -0.4526,                 loss: 0.1888
Score delta: 1.5511091312908938, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/81370_0.
Episode: 81821/101000 (81.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0308s / 18953.6746 s
agent0:                 episode reward: -0.5116,                 loss: nan
agent1:                 episode reward: 0.5116,                 loss: 0.1826
Episode: 81841/101000 (81.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4798s / 18959.1544 s
agent0:                 episode reward: -0.3080,                 loss: nan
agent1:                 episode reward: 0.3080,                 loss: 0.1748
Episode: 81861/101000 (81.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0282s / 18964.1826 s
agent0:                 episode reward: -0.1710,                 loss: nan
agent1:                 episode reward: 0.1710,                 loss: 0.1742
Episode: 81881/101000 (81.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7328s / 18968.9153 s
agent0:                 episode reward: -0.3902,                 loss: nan
agent1:                 episode reward: 0.3902,                 loss: 0.1734
Episode: 81901/101000 (81.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7365s / 18973.6518 s
agent0:                 episode reward: -0.2756,                 loss: nan
agent1:                 episode reward: 0.2756,                 loss: 0.1752
Episode: 81921/101000 (81.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1381s / 18977.7900 s
agent0:                 episode reward: -0.4679,                 loss: nan
agent1:                 episode reward: 0.4679,                 loss: 0.1738
Episode: 81941/101000 (81.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0118s / 18982.8018 s
agent0:                 episode reward: -0.5640,                 loss: nan
agent1:                 episode reward: 0.5640,                 loss: 0.1735
Episode: 81961/101000 (81.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3983s / 18987.2001 s
agent0:                 episode reward: -0.3498,                 loss: nan
agent1:                 episode reward: 0.3498,                 loss: 0.1725
Episode: 81981/101000 (81.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3378s / 18991.5379 s
agent0:                 episode reward: -0.3274,                 loss: nan
agent1:                 episode reward: 0.3274,                 loss: 0.1737
Episode: 82001/101000 (81.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7325s / 18996.2704 s
agent0:                 episode reward: -0.3843,                 loss: nan
agent1:                 episode reward: 0.3843,                 loss: 0.1747
Episode: 82021/101000 (81.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0021s / 19000.2724 s
agent0:                 episode reward: -0.4318,                 loss: nan
agent1:                 episode reward: 0.4318,                 loss: 0.1746
Episode: 82041/101000 (81.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2977s / 19004.5701 s
agent0:                 episode reward: -0.1467,                 loss: 0.1971
agent1:                 episode reward: 0.1467,                 loss: 0.1752
Score delta: 1.5723738855521057, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/81601_1.
Episode: 82061/101000 (81.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4310s / 19010.0011 s
agent0:                 episode reward: -0.0955,                 loss: 0.1979
agent1:                 episode reward: 0.0955,                 loss: nan
Episode: 82081/101000 (81.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9868s / 19013.9880 s
agent0:                 episode reward: 0.2861,                 loss: 0.1960
agent1:                 episode reward: -0.2861,                 loss: nan
Episode: 82101/101000 (81.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8873s / 19018.8753 s
agent0:                 episode reward: -0.1536,                 loss: 0.1967
agent1:                 episode reward: 0.1536,                 loss: nan
Episode: 82121/101000 (81.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9434s / 19023.8186 s
agent0:                 episode reward: -0.0426,                 loss: 0.1933
agent1:                 episode reward: 0.0426,                 loss: nan
Episode: 82141/101000 (81.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1621s / 19028.9807 s
agent0:                 episode reward: 0.4986,                 loss: 0.1967
agent1:                 episode reward: -0.4986,                 loss: nan
Episode: 82161/101000 (81.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6065s / 19033.5872 s
agent0:                 episode reward: 0.0264,                 loss: 0.1953
agent1:                 episode reward: -0.0264,                 loss: nan
Episode: 82181/101000 (81.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4580s / 19038.0453 s
agent0:                 episode reward: 0.1693,                 loss: 0.1948
agent1:                 episode reward: -0.1693,                 loss: nan
Episode: 82201/101000 (81.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9794s / 19043.0247 s
agent0:                 episode reward: 0.3004,                 loss: 0.1944
agent1:                 episode reward: -0.3004,                 loss: nan
Episode: 82221/101000 (81.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3838s / 19047.4085 s
agent0:                 episode reward: -0.4061,                 loss: 0.1955
agent1:                 episode reward: 0.4061,                 loss: nan
Episode: 82241/101000 (81.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8858s / 19052.2942 s
agent0:                 episode reward: -0.2115,                 loss: 0.1928
agent1:                 episode reward: 0.2115,                 loss: nan
Episode: 82261/101000 (81.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3824s / 19057.6766 s
agent0:                 episode reward: -0.0058,                 loss: 0.1960
agent1:                 episode reward: 0.0058,                 loss: nan
Episode: 82281/101000 (81.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0898s / 19062.7665 s
agent0:                 episode reward: -0.2664,                 loss: 0.1933
agent1:                 episode reward: 0.2664,                 loss: nan
Episode: 82301/101000 (81.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3284s / 19067.0949 s
agent0:                 episode reward: -0.0716,                 loss: 0.1934
agent1:                 episode reward: 0.0716,                 loss: nan
Episode: 82321/101000 (81.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4282s / 19072.5231 s
agent0:                 episode reward: 0.0149,                 loss: 0.1966
agent1:                 episode reward: -0.0149,                 loss: nan
Episode: 82341/101000 (81.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2336s / 19077.7567 s
agent0:                 episode reward: 0.3811,                 loss: 0.1971
agent1:                 episode reward: -0.3811,                 loss: nan
Episode: 82361/101000 (81.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1681s / 19082.9248 s
agent0:                 episode reward: 0.2619,                 loss: 0.2007
agent1:                 episode reward: -0.2619,                 loss: nan
Episode: 82381/101000 (81.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6121s / 19088.5369 s
agent0:                 episode reward: 0.1861,                 loss: 0.1985
agent1:                 episode reward: -0.1861,                 loss: nan
Episode: 82401/101000 (81.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7986s / 19094.3356 s
agent0:                 episode reward: 0.2418,                 loss: 0.1994
agent1:                 episode reward: -0.2418,                 loss: 0.1909
Score delta: 2.044720011104431, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/81963_0.
Episode: 82421/101000 (81.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3186s / 19099.6542 s
agent0:                 episode reward: -0.6274,                 loss: nan
agent1:                 episode reward: 0.6274,                 loss: 0.1899
Episode: 82441/101000 (81.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3710s / 19104.0252 s
agent0:                 episode reward: -0.4255,                 loss: nan
agent1:                 episode reward: 0.4255,                 loss: 0.1912
Episode: 82461/101000 (81.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7898s / 19108.8149 s
agent0:                 episode reward: -0.6282,                 loss: nan
agent1:                 episode reward: 0.6282,                 loss: 0.1903
Episode: 82481/101000 (81.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6927s / 19113.5076 s
agent0:                 episode reward: -0.2588,                 loss: nan
agent1:                 episode reward: 0.2588,                 loss: 0.1929
Episode: 82501/101000 (81.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0167s / 19118.5242 s
agent0:                 episode reward: -0.0358,                 loss: nan
agent1:                 episode reward: 0.0358,                 loss: 0.1891
Episode: 82521/101000 (81.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7309s / 19122.2552 s
agent0:                 episode reward: -0.5056,                 loss: nan
agent1:                 episode reward: 0.5056,                 loss: 0.1811
Episode: 82541/101000 (81.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1518s / 19127.4070 s
agent0:                 episode reward: -0.1519,                 loss: nan
agent1:                 episode reward: 0.1519,                 loss: 0.1778
Episode: 82561/101000 (81.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9872s / 19132.3942 s
agent0:                 episode reward: -0.4600,                 loss: nan
agent1:                 episode reward: 0.4600,                 loss: 0.1759
Episode: 82581/101000 (81.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0425s / 19136.4367 s
agent0:                 episode reward: -0.2121,                 loss: nan
agent1:                 episode reward: 0.2121,                 loss: 0.1746
Episode: 82601/101000 (81.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9032s / 19140.3399 s
agent0:                 episode reward: -0.0592,                 loss: nan
agent1:                 episode reward: 0.0592,                 loss: 0.1761
Episode: 82621/101000 (81.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3428s / 19144.6827 s
agent0:                 episode reward: -0.2188,                 loss: nan
agent1:                 episode reward: 0.2188,                 loss: 0.1748
Episode: 82641/101000 (81.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0094s / 19149.6921 s
agent0:                 episode reward: 0.1110,                 loss: nan
agent1:                 episode reward: -0.1110,                 loss: 0.1755
Episode: 82661/101000 (81.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5866s / 19154.2787 s
agent0:                 episode reward: -0.1546,                 loss: nan
agent1:                 episode reward: 0.1546,                 loss: 0.1759
Episode: 82681/101000 (81.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3321s / 19158.6109 s
agent0:                 episode reward: -0.4476,                 loss: nan
agent1:                 episode reward: 0.4476,                 loss: 0.1751
Episode: 82701/101000 (81.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8695s / 19163.4804 s
agent0:                 episode reward: -0.0587,                 loss: nan
agent1:                 episode reward: 0.0587,                 loss: 0.1746
Episode: 82721/101000 (81.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6479s / 19168.1283 s
agent0:                 episode reward: -0.4043,                 loss: nan
agent1:                 episode reward: 0.4043,                 loss: 0.1759
Episode: 82741/101000 (81.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2701s / 19172.3983 s
agent0:                 episode reward: -0.1915,                 loss: nan
agent1:                 episode reward: 0.1915,                 loss: 0.1747
Episode: 82761/101000 (81.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7468s / 19177.1451 s
agent0:                 episode reward: -0.4816,                 loss: nan
agent1:                 episode reward: 0.4816,                 loss: 0.1739
Episode: 82781/101000 (81.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6414s / 19181.7865 s
agent0:                 episode reward: -0.2738,                 loss: nan
agent1:                 episode reward: 0.2738,                 loss: 0.1738
Episode: 82801/101000 (81.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9066s / 19186.6931 s
agent0:                 episode reward: -0.1809,                 loss: nan
agent1:                 episode reward: 0.1809,                 loss: 0.1744
Episode: 82821/101000 (82.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4806s / 19191.1737 s
agent0:                 episode reward: -0.5458,                 loss: nan
agent1:                 episode reward: 0.5458,                 loss: 0.1747
Episode: 82841/101000 (82.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0470s / 19196.2207 s
agent0:                 episode reward: -0.4063,                 loss: nan
agent1:                 episode reward: 0.4063,                 loss: 0.1727
Episode: 82861/101000 (82.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8638s / 19201.0845 s
agent0:                 episode reward: 0.2734,                 loss: 0.1977
agent1:                 episode reward: -0.2734,                 loss: 0.1746
Score delta: 1.5983536435861077, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/82417_1.
Episode: 82881/101000 (82.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2772s / 19205.3617 s
agent0:                 episode reward: -0.2389,                 loss: 0.1991
agent1:                 episode reward: 0.2389,                 loss: nan
Episode: 82901/101000 (82.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7734s / 19210.1351 s
agent0:                 episode reward: 0.0122,                 loss: 0.1987
agent1:                 episode reward: -0.0122,                 loss: nan
Episode: 82921/101000 (82.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2072s / 19214.3423 s
agent0:                 episode reward: -0.3230,                 loss: 0.1970
agent1:                 episode reward: 0.3230,                 loss: nan
Episode: 82941/101000 (82.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5738s / 19218.9161 s
agent0:                 episode reward: 0.1031,                 loss: 0.1987
agent1:                 episode reward: -0.1031,                 loss: nan
Episode: 82961/101000 (82.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3628s / 19223.2789 s
agent0:                 episode reward: 0.5995,                 loss: 0.2001
agent1:                 episode reward: -0.5995,                 loss: nan
Episode: 82981/101000 (82.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0639s / 19229.3428 s
agent0:                 episode reward: -0.2157,                 loss: 0.1996
agent1:                 episode reward: 0.2157,                 loss: nan
Episode: 83001/101000 (82.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7155s / 19234.0583 s
agent0:                 episode reward: 0.0829,                 loss: 0.1968
agent1:                 episode reward: -0.0829,                 loss: nan
Episode: 83021/101000 (82.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7262s / 19238.7845 s
agent0:                 episode reward: 0.1453,                 loss: 0.2001
agent1:                 episode reward: -0.1453,                 loss: nan
Episode: 83041/101000 (82.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7038s / 19243.4884 s
agent0:                 episode reward: 0.1020,                 loss: 0.1979
agent1:                 episode reward: -0.1020,                 loss: 0.1934
Score delta: 1.5577736421971915, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/82606_0.
Episode: 83061/101000 (82.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9241s / 19247.4125 s
agent0:                 episode reward: 0.0882,                 loss: nan
agent1:                 episode reward: -0.0882,                 loss: 0.1919
Episode: 83081/101000 (82.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7105s / 19251.1230 s
agent0:                 episode reward: -0.8205,                 loss: nan
agent1:                 episode reward: 0.8205,                 loss: 0.1901
Episode: 83101/101000 (82.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1434s / 19256.2665 s
agent0:                 episode reward: -0.3053,                 loss: nan
agent1:                 episode reward: 0.3053,                 loss: 0.1899
Episode: 83121/101000 (82.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3981s / 19260.6645 s
agent0:                 episode reward: -0.3608,                 loss: nan
agent1:                 episode reward: 0.3608,                 loss: 0.1907
Episode: 83141/101000 (82.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5223s / 19265.1869 s
agent0:                 episode reward: -0.1089,                 loss: nan
agent1:                 episode reward: 0.1089,                 loss: 0.1906
Episode: 83161/101000 (82.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3821s / 19269.5690 s
agent0:                 episode reward: -0.2942,                 loss: nan
agent1:                 episode reward: 0.2942,                 loss: 0.1912
Episode: 83181/101000 (82.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0603s / 19273.6293 s
agent0:                 episode reward: -0.4026,                 loss: nan
agent1:                 episode reward: 0.4026,                 loss: 0.1893
Episode: 83201/101000 (82.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3663s / 19278.9956 s
agent0:                 episode reward: -0.1242,                 loss: nan
agent1:                 episode reward: 0.1242,                 loss: 0.1888
Episode: 83221/101000 (82.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0062s / 19283.0017 s
agent0:                 episode reward: -0.4531,                 loss: 0.1949
agent1:                 episode reward: 0.4531,                 loss: 0.1881
Score delta: 1.5351675379243355, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/82793_1.
Episode: 83241/101000 (82.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8392s / 19287.8409 s
agent0:                 episode reward: 0.3974,                 loss: 0.1951
agent1:                 episode reward: -0.3974,                 loss: nan
Episode: 83261/101000 (82.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8677s / 19292.7086 s
agent0:                 episode reward: 0.2447,                 loss: 0.1950
agent1:                 episode reward: -0.2447,                 loss: nan
Episode: 83281/101000 (82.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6862s / 19297.3947 s
agent0:                 episode reward: 0.4119,                 loss: 0.1941
agent1:                 episode reward: -0.4119,                 loss: nan
Episode: 83301/101000 (82.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8467s / 19302.2414 s
agent0:                 episode reward: -0.2132,                 loss: 0.1957
agent1:                 episode reward: 0.2132,                 loss: nan
Episode: 83321/101000 (82.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9062s / 19307.1477 s
agent0:                 episode reward: -0.0558,                 loss: 0.1955
agent1:                 episode reward: 0.0558,                 loss: nan
Episode: 83341/101000 (82.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6543s / 19311.8020 s
agent0:                 episode reward: 0.2620,                 loss: 0.1942
agent1:                 episode reward: -0.2620,                 loss: nan
Episode: 83361/101000 (82.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7149s / 19316.5169 s
agent0:                 episode reward: -0.2043,                 loss: 0.1941
agent1:                 episode reward: 0.2043,                 loss: nan
Episode: 83381/101000 (82.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1453s / 19320.6622 s
agent0:                 episode reward: -0.4659,                 loss: 0.1955
agent1:                 episode reward: 0.4659,                 loss: nan
Episode: 83401/101000 (82.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2807s / 19325.9429 s
agent0:                 episode reward: 0.1090,                 loss: 0.1964
agent1:                 episode reward: -0.1090,                 loss: nan
Episode: 83421/101000 (82.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9214s / 19331.8642 s
agent0:                 episode reward: -0.2248,                 loss: 0.1957
agent1:                 episode reward: 0.2248,                 loss: nan
Episode: 83441/101000 (82.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6768s / 19336.5411 s
agent0:                 episode reward: -0.0696,                 loss: 0.1951
agent1:                 episode reward: 0.0696,                 loss: nan
Episode: 83461/101000 (82.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5687s / 19342.1097 s
agent0:                 episode reward: -0.2163,                 loss: 0.1935
agent1:                 episode reward: 0.2163,                 loss: nan
Episode: 83481/101000 (82.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2385s / 19346.3482 s
agent0:                 episode reward: -0.2552,                 loss: 0.1930
agent1:                 episode reward: 0.2552,                 loss: nan
Episode: 83501/101000 (82.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9640s / 19351.3122 s
agent0:                 episode reward: 0.1605,                 loss: 0.1938
agent1:                 episode reward: -0.1605,                 loss: nan
Episode: 83521/101000 (82.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4981s / 19356.8103 s
agent0:                 episode reward: -0.1739,                 loss: 0.1949
agent1:                 episode reward: 0.1739,                 loss: nan
Episode: 83541/101000 (82.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9907s / 19361.8010 s
agent0:                 episode reward: -0.0271,                 loss: 0.1916
agent1:                 episode reward: 0.0271,                 loss: nan
Episode: 83561/101000 (82.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8964s / 19366.6974 s
agent0:                 episode reward: 0.3363,                 loss: 0.1916
agent1:                 episode reward: -0.3363,                 loss: nan
Episode: 83581/101000 (82.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0395s / 19371.7369 s
agent0:                 episode reward: -0.0429,                 loss: 0.1930
agent1:                 episode reward: 0.0429,                 loss: nan
Episode: 83601/101000 (82.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6381s / 19376.3750 s
agent0:                 episode reward: 0.0607,                 loss: 0.1925
agent1:                 episode reward: -0.0607,                 loss: nan
Episode: 83621/101000 (82.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4831s / 19380.8581 s
agent0:                 episode reward: -0.0440,                 loss: 0.1909
agent1:                 episode reward: 0.0440,                 loss: nan
Episode: 83641/101000 (82.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0038s / 19384.8619 s
agent0:                 episode reward: -0.1390,                 loss: 0.1929
agent1:                 episode reward: 0.1390,                 loss: nan
Episode: 83661/101000 (82.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2840s / 19389.1459 s
agent0:                 episode reward: 0.2107,                 loss: 0.1935
agent1:                 episode reward: -0.2107,                 loss: nan
Episode: 83681/101000 (82.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8342s / 19393.9801 s
agent0:                 episode reward: 0.1354,                 loss: 0.1925
agent1:                 episode reward: -0.1354,                 loss: nan
Episode: 83701/101000 (82.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7953s / 19398.7754 s
agent0:                 episode reward: 0.1973,                 loss: 0.1907
agent1:                 episode reward: -0.1973,                 loss: nan
Episode: 83721/101000 (82.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6234s / 19403.3988 s
agent0:                 episode reward: -0.1913,                 loss: 0.1932
agent1:                 episode reward: 0.1913,                 loss: nan
Episode: 83741/101000 (82.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4116s / 19408.8104 s
agent0:                 episode reward: -0.3416,                 loss: 0.1909
agent1:                 episode reward: 0.3416,                 loss: nan
Episode: 83761/101000 (82.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1893s / 19413.9997 s
agent0:                 episode reward: -0.2791,                 loss: 0.1912
agent1:                 episode reward: 0.2791,                 loss: nan
Episode: 83781/101000 (82.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9875s / 19419.9872 s
agent0:                 episode reward: -0.0992,                 loss: 0.1907
agent1:                 episode reward: 0.0992,                 loss: nan
Episode: 83801/101000 (82.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8241s / 19424.8113 s
agent0:                 episode reward: 0.0556,                 loss: 0.1925
agent1:                 episode reward: -0.0556,                 loss: nan
Score delta: 1.6359381779701043, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/83375_0.
Episode: 83821/101000 (82.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9981s / 19429.8094 s
agent0:                 episode reward: -0.1590,                 loss: nan
agent1:                 episode reward: 0.1590,                 loss: 0.1897
Episode: 83841/101000 (83.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4814s / 19433.2909 s
agent0:                 episode reward: 0.0076,                 loss: nan
agent1:                 episode reward: -0.0076,                 loss: 0.1889
Episode: 83861/101000 (83.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1738s / 19437.4647 s
agent0:                 episode reward: -0.3110,                 loss: nan
agent1:                 episode reward: 0.3110,                 loss: 0.1890
Episode: 83881/101000 (83.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3937s / 19442.8584 s
agent0:                 episode reward: -0.0542,                 loss: nan
agent1:                 episode reward: 0.0542,                 loss: 0.1877
Episode: 83901/101000 (83.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4068s / 19447.2652 s
agent0:                 episode reward: -0.3820,                 loss: nan
agent1:                 episode reward: 0.3820,                 loss: 0.1876
Episode: 83921/101000 (83.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5049s / 19451.7701 s
agent0:                 episode reward: -0.5287,                 loss: nan
agent1:                 episode reward: 0.5287,                 loss: 0.1885
Episode: 83941/101000 (83.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7084s / 19456.4785 s
agent0:                 episode reward: -0.2455,                 loss: nan
agent1:                 episode reward: 0.2455,                 loss: 0.1886
Episode: 83961/101000 (83.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4937s / 19461.9722 s
agent0:                 episode reward: -0.0415,                 loss: nan
agent1:                 episode reward: 0.0415,                 loss: 0.1803
Episode: 83981/101000 (83.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8465s / 19466.8186 s
agent0:                 episode reward: 0.0397,                 loss: nan
agent1:                 episode reward: -0.0397,                 loss: 0.1766
Episode: 84001/101000 (83.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2391s / 19472.0577 s
agent0:                 episode reward: 0.0216,                 loss: nan
agent1:                 episode reward: -0.0216,                 loss: 0.1795
Episode: 84021/101000 (83.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6944s / 19476.7521 s
agent0:                 episode reward: -0.3015,                 loss: 0.1904
agent1:                 episode reward: 0.3015,                 loss: 0.1804
Score delta: 1.6115882931077945, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/83578_1.
Episode: 84041/101000 (83.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7037s / 19482.4558 s
agent0:                 episode reward: -0.2477,                 loss: 0.1885
agent1:                 episode reward: 0.2477,                 loss: nan
Episode: 84061/101000 (83.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5932s / 19487.0491 s
agent0:                 episode reward: 0.6498,                 loss: 0.1881
agent1:                 episode reward: -0.6498,                 loss: nan
Episode: 84081/101000 (83.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8106s / 19491.8597 s
agent0:                 episode reward: 0.5920,                 loss: 0.1858
agent1:                 episode reward: -0.5920,                 loss: nan
Episode: 84101/101000 (83.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5572s / 19496.4169 s
agent0:                 episode reward: 0.4878,                 loss: 0.1876
agent1:                 episode reward: -0.4878,                 loss: nan
Episode: 84121/101000 (83.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6823s / 19501.0991 s
agent0:                 episode reward: -0.0125,                 loss: 0.1872
agent1:                 episode reward: 0.0125,                 loss: nan
Episode: 84141/101000 (83.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3441s / 19506.4432 s
agent0:                 episode reward: 0.1587,                 loss: 0.1875
agent1:                 episode reward: -0.1587,                 loss: nan
Episode: 84161/101000 (83.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1754s / 19511.6186 s
agent0:                 episode reward: 0.6573,                 loss: 0.1921
agent1:                 episode reward: -0.6573,                 loss: nan
Episode: 84181/101000 (83.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8979s / 19516.5165 s
agent0:                 episode reward: 0.1746,                 loss: 0.1984
agent1:                 episode reward: -0.1746,                 loss: nan
Episode: 84201/101000 (83.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4756s / 19520.9922 s
agent0:                 episode reward: -0.6946,                 loss: 0.1974
agent1:                 episode reward: 0.6946,                 loss: nan
Episode: 84221/101000 (83.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5032s / 19525.4954 s
agent0:                 episode reward: -0.0290,                 loss: 0.1980
agent1:                 episode reward: 0.0290,                 loss: nan
Episode: 84241/101000 (83.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0819s / 19530.5773 s
agent0:                 episode reward: 0.6583,                 loss: 0.1960
agent1:                 episode reward: -0.6583,                 loss: 0.1943
Score delta: 1.9040507257009183, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/83809_0.
Episode: 84261/101000 (83.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5914s / 19535.1687 s
agent0:                 episode reward: -0.1196,                 loss: nan
agent1:                 episode reward: 0.1196,                 loss: 0.1881
Episode: 84281/101000 (83.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7475s / 19539.9162 s
agent0:                 episode reward: -0.0767,                 loss: nan
agent1:                 episode reward: 0.0767,                 loss: 0.1880
Episode: 84301/101000 (83.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2643s / 19545.1804 s
agent0:                 episode reward: -0.1336,                 loss: nan
agent1:                 episode reward: 0.1336,                 loss: 0.1891
Episode: 84321/101000 (83.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8968s / 19550.0772 s
agent0:                 episode reward: -0.3977,                 loss: nan
agent1:                 episode reward: 0.3977,                 loss: 0.1853
Episode: 84341/101000 (83.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7270s / 19554.8042 s
agent0:                 episode reward: -0.4333,                 loss: nan
agent1:                 episode reward: 0.4333,                 loss: 0.1884
Episode: 84361/101000 (83.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9225s / 19559.7267 s
agent0:                 episode reward: -0.2265,                 loss: nan
agent1:                 episode reward: 0.2265,                 loss: 0.1872
Episode: 84381/101000 (83.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8760s / 19563.6027 s
agent0:                 episode reward: -0.0955,                 loss: nan
agent1:                 episode reward: 0.0955,                 loss: 0.1867
Episode: 84401/101000 (83.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1047s / 19567.7075 s
agent0:                 episode reward: -0.0303,                 loss: nan
agent1:                 episode reward: 0.0303,                 loss: 0.1863
Episode: 84421/101000 (83.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1360s / 19572.8434 s
agent0:                 episode reward: -0.2352,                 loss: nan
agent1:                 episode reward: 0.2352,                 loss: 0.1850
Episode: 84441/101000 (83.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2786s / 19577.1221 s
agent0:                 episode reward: -0.1542,                 loss: nan
agent1:                 episode reward: 0.1542,                 loss: 0.1852
Episode: 84461/101000 (83.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3056s / 19581.4276 s
agent0:                 episode reward: -0.1249,                 loss: nan
agent1:                 episode reward: 0.1249,                 loss: 0.1857
Episode: 84481/101000 (83.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1219s / 19586.5495 s
agent0:                 episode reward: -0.5757,                 loss: 0.2054
agent1:                 episode reward: 0.5757,                 loss: 0.1831
Score delta: 1.8916889070454777, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/84047_1.
Episode: 84501/101000 (83.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6452s / 19592.1948 s
agent0:                 episode reward: -0.4306,                 loss: 0.2001
agent1:                 episode reward: 0.4306,                 loss: nan
Episode: 84521/101000 (83.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7598s / 19596.9546 s
agent0:                 episode reward: -0.5878,                 loss: 0.1958
agent1:                 episode reward: 0.5878,                 loss: nan
Episode: 84541/101000 (83.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3258s / 19602.2804 s
agent0:                 episode reward: -0.0345,                 loss: 0.1934
agent1:                 episode reward: 0.0345,                 loss: nan
Episode: 84561/101000 (83.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7644s / 19607.0448 s
agent0:                 episode reward: 0.1515,                 loss: 0.1945
agent1:                 episode reward: -0.1515,                 loss: nan
Episode: 84581/101000 (83.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4519s / 19611.4967 s
agent0:                 episode reward: 0.0256,                 loss: 0.1935
agent1:                 episode reward: -0.0256,                 loss: nan
Episode: 84601/101000 (83.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4854s / 19616.9822 s
agent0:                 episode reward: 0.8093,                 loss: 0.1944
agent1:                 episode reward: -0.8093,                 loss: nan
Episode: 84621/101000 (83.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0719s / 19622.0541 s
agent0:                 episode reward: -0.3295,                 loss: 0.1942
agent1:                 episode reward: 0.3295,                 loss: nan
Episode: 84641/101000 (83.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0951s / 19626.1492 s
agent0:                 episode reward: 0.2538,                 loss: 0.1925
agent1:                 episode reward: -0.2538,                 loss: nan
Episode: 84661/101000 (83.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2225s / 19631.3717 s
agent0:                 episode reward: -0.1883,                 loss: 0.1933
agent1:                 episode reward: 0.1883,                 loss: nan
Episode: 84681/101000 (83.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2355s / 19636.6072 s
agent0:                 episode reward: -0.0074,                 loss: 0.1895
agent1:                 episode reward: 0.0074,                 loss: nan
Episode: 84701/101000 (83.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1596s / 19641.7667 s
agent0:                 episode reward: 0.0389,                 loss: 0.1910
agent1:                 episode reward: -0.0389,                 loss: nan
Episode: 84721/101000 (83.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8087s / 19647.5754 s
agent0:                 episode reward: 0.2763,                 loss: 0.1926
agent1:                 episode reward: -0.2763,                 loss: nan
Episode: 84741/101000 (83.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3293s / 19651.9047 s
agent0:                 episode reward: -0.1517,                 loss: 0.2005
agent1:                 episode reward: 0.1517,                 loss: nan
Episode: 84761/101000 (83.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1683s / 19656.0731 s
agent0:                 episode reward: 0.0431,                 loss: 0.1995
agent1:                 episode reward: -0.0431,                 loss: nan
Episode: 84781/101000 (83.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3469s / 19661.4200 s
agent0:                 episode reward: -0.1724,                 loss: 0.1990
agent1:                 episode reward: 0.1724,                 loss: nan
Episode: 84801/101000 (83.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9374s / 19665.3574 s
agent0:                 episode reward: -0.2896,                 loss: 0.1996
agent1:                 episode reward: 0.2896,                 loss: nan
Episode: 84821/101000 (83.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4040s / 19669.7613 s
agent0:                 episode reward: 0.4052,                 loss: 0.1986
agent1:                 episode reward: -0.4052,                 loss: 0.1876
Score delta: 1.595777271727438, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/84389_0.
Episode: 84841/101000 (84.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3772s / 19675.1385 s
agent0:                 episode reward: -0.0731,                 loss: nan
agent1:                 episode reward: 0.0731,                 loss: 0.1840
Episode: 84861/101000 (84.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8835s / 19680.0221 s
agent0:                 episode reward: -0.6496,                 loss: nan
agent1:                 episode reward: 0.6496,                 loss: 0.1794
Episode: 84881/101000 (84.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8461s / 19685.8681 s
agent0:                 episode reward: -0.2262,                 loss: nan
agent1:                 episode reward: 0.2262,                 loss: 0.1754
Episode: 84901/101000 (84.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9952s / 19690.8634 s
agent0:                 episode reward: -0.2667,                 loss: nan
agent1:                 episode reward: 0.2667,                 loss: 0.1731
Episode: 84921/101000 (84.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3362s / 19695.1996 s
agent0:                 episode reward: -0.1325,                 loss: nan
agent1:                 episode reward: 0.1325,                 loss: 0.1745
Episode: 84941/101000 (84.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3937s / 19700.5932 s
agent0:                 episode reward: -0.1751,                 loss: nan
agent1:                 episode reward: 0.1751,                 loss: 0.1746
Episode: 84961/101000 (84.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9574s / 19705.5507 s
agent0:                 episode reward: -0.0279,                 loss: nan
agent1:                 episode reward: 0.0279,                 loss: 0.1746
Episode: 84981/101000 (84.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0344s / 19711.5851 s
agent0:                 episode reward: -0.3513,                 loss: nan
agent1:                 episode reward: 0.3513,                 loss: 0.1736
Episode: 85001/101000 (84.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2808s / 19715.8659 s
agent0:                 episode reward: -0.3876,                 loss: 0.2085
agent1:                 episode reward: 0.3876,                 loss: 0.1728
Score delta: 2.0332194409902105, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/84568_1.
Episode: 85021/101000 (84.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6294s / 19720.4954 s
agent0:                 episode reward: 0.6378,                 loss: 0.2030
agent1:                 episode reward: -0.6378,                 loss: nan
Episode: 85041/101000 (84.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3530s / 19725.8484 s
agent0:                 episode reward: -0.2871,                 loss: 0.2012
agent1:                 episode reward: 0.2871,                 loss: nan
Episode: 85061/101000 (84.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3566s / 19731.2050 s
agent0:                 episode reward: -0.0589,                 loss: 0.2009
agent1:                 episode reward: 0.0589,                 loss: nan
Episode: 85081/101000 (84.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7833s / 19736.9882 s
agent0:                 episode reward: 0.0225,                 loss: 0.1981
agent1:                 episode reward: -0.0225,                 loss: nan
Episode: 85101/101000 (84.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4994s / 19741.4876 s
agent0:                 episode reward: -0.2184,                 loss: 0.2000
agent1:                 episode reward: 0.2184,                 loss: nan
Episode: 85121/101000 (84.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3613s / 19746.8489 s
agent0:                 episode reward: 0.3903,                 loss: 0.1995
agent1:                 episode reward: -0.3903,                 loss: nan
Episode: 85141/101000 (84.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7606s / 19751.6095 s
agent0:                 episode reward: 0.1544,                 loss: 0.1995
agent1:                 episode reward: -0.1544,                 loss: nan
Episode: 85161/101000 (84.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7952s / 19756.4047 s
agent0:                 episode reward: 0.1357,                 loss: 0.1989
agent1:                 episode reward: -0.1357,                 loss: nan
Episode: 85181/101000 (84.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8219s / 19761.2266 s
agent0:                 episode reward: 0.0484,                 loss: 0.1980
agent1:                 episode reward: -0.0484,                 loss: 0.1653
Score delta: 1.7521300170102159, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/84747_0.
Episode: 85201/101000 (84.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9975s / 19766.2240 s
agent0:                 episode reward: -0.4612,                 loss: nan
agent1:                 episode reward: 0.4612,                 loss: 0.1608
Episode: 85221/101000 (84.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9369s / 19771.1609 s
agent0:                 episode reward: -0.7041,                 loss: nan
agent1:                 episode reward: 0.7041,                 loss: 0.1595
Episode: 85241/101000 (84.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9384s / 19776.0993 s
agent0:                 episode reward: -0.5931,                 loss: nan
agent1:                 episode reward: 0.5931,                 loss: 0.1593
Episode: 85261/101000 (84.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3972s / 19780.4965 s
agent0:                 episode reward: -0.2483,                 loss: nan
agent1:                 episode reward: 0.2483,                 loss: 0.1605
Episode: 85281/101000 (84.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6959s / 19785.1924 s
agent0:                 episode reward: -0.3102,                 loss: nan
agent1:                 episode reward: 0.3102,                 loss: 0.1585
Episode: 85301/101000 (84.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8880s / 19790.0804 s
agent0:                 episode reward: -0.2009,                 loss: nan
agent1:                 episode reward: 0.2009,                 loss: 0.1575
Episode: 85321/101000 (84.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9455s / 19795.0259 s
agent0:                 episode reward: 0.2233,                 loss: nan
agent1:                 episode reward: -0.2233,                 loss: 0.1575
Episode: 85341/101000 (84.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2136s / 19799.2395 s
agent0:                 episode reward: -0.5003,                 loss: nan
agent1:                 episode reward: 0.5003,                 loss: 0.1584
Episode: 85361/101000 (84.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2964s / 19803.5359 s
agent0:                 episode reward: -0.0337,                 loss: nan
agent1:                 episode reward: 0.0337,                 loss: 0.1573
Episode: 85381/101000 (84.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3880s / 19808.9239 s
agent0:                 episode reward: -0.5387,                 loss: 0.3177
agent1:                 episode reward: 0.5387,                 loss: 0.1839
Score delta: 1.5342104330920416, save the model to .//data/model/20220117154800/mdp_arbitrary_mdp_fictitious_selfplay2/84940_1.
Episode: 85401/101000 (84.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1112s / 19814.0351 s
agent0:                 episode reward: -0.3776,                 loss: 0.2935
agent1:                 episode reward: 0.3776,                 loss: nan
Episode: 85421/101000 (84.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2033s / 19819.2384 s
agent0:                 episode reward: -0.0839,                 loss: 0.2952
agent1:                 episode reward: 0.0839,                 loss: nan
Episode: 85441/101000 (84.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8767s / 19824.1151 s
agent0:                 episode reward: 0.0128,                 loss: 0.2537
agent1:                 episode reward: -0.0128,                 loss: nan
Episode: 85461/101000 (84.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1958s / 19828.3109 s