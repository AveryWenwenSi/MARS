pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
double_dunk_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'double_dunk_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220204_1545/pettingzoo_double_dunk_v2_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220204_1545/pettingzoo_double_dunk_v2_nash_ppo.
Episode: 1/30000 (0.0033%),                 avg. length: 4331.0,                last time consumption/overall running time: 25.3540s / 25.3540 s
env0_first_0:                 episode reward: -29.0000,                 loss: -0.0616
env0_second_0:                 episode reward: 29.0000,                 loss: -0.0598
env1_first_0:                 episode reward: -25.0000,                 loss: nan
env1_second_0:                 episode reward: 25.0000,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 3945.65,                last time consumption/overall running time: 488.7447s / 514.0987 s
env0_first_0:                 episode reward: -26.2000,                 loss: -0.0363
env0_second_0:                 episode reward: 26.2000,                 loss: -0.0330
env1_first_0:                 episode reward: -29.2500,                 loss: nan
env1_second_0:                 episode reward: 29.2500,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 3793.5,                last time consumption/overall running time: 470.3007s / 984.3995 s
env0_first_0:                 episode reward: -24.9000,                 loss: 0.0223
env0_second_0:                 episode reward: 24.9000,                 loss: 0.0257
env1_first_0:                 episode reward: -24.3000,                 loss: nan
env1_second_0:                 episode reward: 24.3000,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 3776.4,                last time consumption/overall running time: 446.1071s / 1430.5065 s
env0_first_0:                 episode reward: -17.5000,                 loss: 0.0376
env0_second_0:                 episode reward: 17.5000,                 loss: 0.0362
env1_first_0:                 episode reward: -24.8000,                 loss: nan
env1_second_0:                 episode reward: 24.8000,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 3778.25,                last time consumption/overall running time: 479.2045s / 1909.7111 s
env0_first_0:                 episode reward: -25.0000,                 loss: 0.0572
env0_second_0:                 episode reward: 25.0000,                 loss: 0.0586
env1_first_0:                 episode reward: -25.2000,                 loss: nan
env1_second_0:                 episode reward: 25.2000,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 4139.25,                last time consumption/overall running time: 503.6592s / 2413.3703 s
env0_first_0:                 episode reward: -23.9000,                 loss: 0.0659
env0_second_0:                 episode reward: 23.9000,                 loss: 0.0698
env1_first_0:                 episode reward: -23.3500,                 loss: nan
env1_second_0:                 episode reward: 23.3500,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 4073.1,                last time consumption/overall running time: 462.1391s / 2875.5093 s
env0_first_0:                 episode reward: -22.9500,                 loss: 0.0711
env0_second_0:                 episode reward: 22.9500,                 loss: 0.0742
env1_first_0:                 episode reward: -21.1500,                 loss: nan
env1_second_0:                 episode reward: 21.1500,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 4071.9,                last time consumption/overall running time: 468.4880s / 3343.9973 s
env0_first_0:                 episode reward: -20.4500,                 loss: 0.0759
env0_second_0:                 episode reward: 20.4500,                 loss: 0.0838
env1_first_0:                 episode reward: -22.5500,                 loss: nan
env1_second_0:                 episode reward: 22.5500,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 4199.1,                last time consumption/overall running time: 515.3768s / 3859.3741 s
env0_first_0:                 episode reward: -22.6500,                 loss: 0.0933
env0_second_0:                 episode reward: 22.6500,                 loss: 0.0943
env1_first_0:                 episode reward: -23.1000,                 loss: nan
env1_second_0:                 episode reward: 23.1000,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 4061.55,                last time consumption/overall running time: 512.3635s / 4371.7376 s
env0_first_0:                 episode reward: -24.3500,                 loss: 0.0943
env0_second_0:                 episode reward: 24.3500,                 loss: 0.0852
env1_first_0:                 episode reward: -25.1000,                 loss: nan
env1_second_0:                 episode reward: 25.1000,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 4394.4,                last time consumption/overall running time: 550.9981s / 4922.7357 s
env0_first_0:                 episode reward: -25.6500,                 loss: 0.0709
env0_second_0:                 episode reward: 25.6500,                 loss: 0.0649
env1_first_0:                 episode reward: -25.8500,                 loss: nan
env1_second_0:                 episode reward: 25.8500,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 4367.1,                last time consumption/overall running time: 541.2058s / 5463.9415 s
env0_first_0:                 episode reward: -27.8500,                 loss: 0.0685
env0_second_0:                 episode reward: 27.8500,                 loss: 0.0677
env1_first_0:                 episode reward: -26.5500,                 loss: nan
env1_second_0:                 episode reward: 26.5500,                 loss: nan
Episode: 241/30000 (0.8033%),                 avg. length: 4164.5,                last time consumption/overall running time: 502.7117s / 5966.6533 s
env0_first_0:                 episode reward: -22.7000,                 loss: 0.0574
env0_second_0:                 episode reward: 22.7000,                 loss: 0.0538
env1_first_0:                 episode reward: -24.3500,                 loss: nan
env1_second_0:                 episode reward: 24.3500,                 loss: nan
Episode: 261/30000 (0.8700%),                 avg. length: 4697.45,                last time consumption/overall running time: 569.4095s / 6536.0628 s
env0_first_0:                 episode reward: -30.2500,                 loss: 0.0615
env0_second_0:                 episode reward: 30.2500,                 loss: 0.0547
env1_first_0:                 episode reward: -32.7000,                 loss: nan
env1_second_0:                 episode reward: 32.7000,                 loss: nan
Episode: 281/30000 (0.9367%),                 avg. length: 4433.95,                last time consumption/overall running time: 538.4987s / 7074.5614 s
env0_first_0:                 episode reward: -27.4500,                 loss: 0.0491
env0_second_0:                 episode reward: 27.4500,                 loss: 0.0473
env1_first_0:                 episode reward: -29.2000,                 loss: nan
env1_second_0:                 episode reward: 29.2000,                 loss: nan
Episode: 301/30000 (1.0033%),                 avg. length: 4932.2,                last time consumption/overall running time: 603.6742s / 7678.2356 s
env0_first_0:                 episode reward: -32.2000,                 loss: 0.0112
env0_second_0:                 episode reward: 32.2000,                 loss: 0.0085
env1_first_0:                 episode reward: -33.0500,                 loss: nan
env1_second_0:                 episode reward: 33.0500,                 loss: nan
Episode: 321/30000 (1.0700%),                 avg. length: 4479.35,                last time consumption/overall running time: 552.7431s / 8230.9787 s
env0_first_0:                 episode reward: -29.2000,                 loss: 0.0154
env0_second_0:                 episode reward: 29.2000,                 loss: 0.0206
env1_first_0:                 episode reward: -30.1500,                 loss: nan
env1_second_0:                 episode reward: 30.1500,                 loss: nan
Episode: 341/30000 (1.1367%),                 avg. length: 4177.5,                last time consumption/overall running time: 532.7284s / 8763.7071 s
env0_first_0:                 episode reward: -27.6500,                 loss: 0.0114
env0_second_0:                 episode reward: 27.6500,                 loss: 0.0117
env1_first_0:                 episode reward: -27.9000,                 loss: nan
env1_second_0:                 episode reward: 27.9000,                 loss: nan
Episode: 361/30000 (1.2033%),                 avg. length: 4495.45,                last time consumption/overall running time: 583.7611s / 9347.4682 s
env0_first_0:                 episode reward: -29.7000,                 loss: -0.0393
env0_second_0:                 episode reward: 29.7000,                 loss: -0.0372
env1_first_0:                 episode reward: -30.3500,                 loss: nan
env1_second_0:                 episode reward: 30.3500,                 loss: nan
Episode: 381/30000 (1.2700%),                 avg. length: 4161.35,                last time consumption/overall running time: 523.8614s / 9871.3296 s
env0_first_0:                 episode reward: -23.4000,                 loss: -0.1271
env0_second_0:                 episode reward: 23.4000,                 loss: -0.1267
env1_first_0:                 episode reward: -24.2000,                 loss: nan
env1_second_0:                 episode reward: 24.2000,                 loss: nan
Episode: 401/30000 (1.3367%),                 avg. length: 4198.55,                last time consumption/overall running time: 514.3966s / 10385.7262 s
env0_first_0:                 episode reward: -16.6500,                 loss: -0.2026
env0_second_0:                 episode reward: 16.6500,                 loss: -0.2016
env1_first_0:                 episode reward: -16.3000,                 loss: nan
env1_second_0:                 episode reward: 16.3000,                 loss: nan
Episode: 421/30000 (1.4033%),                 avg. length: 4189.7,                last time consumption/overall running time: 549.4157s / 10935.1418 s
env0_first_0:                 episode reward: -11.8000,                 loss: -0.1929
env0_second_0:                 episode reward: 11.8000,                 loss: -0.1849
env1_first_0:                 episode reward: -12.1500,                 loss: nan
env1_second_0:                 episode reward: 12.1500,                 loss: nan
Episode: 441/30000 (1.4700%),                 avg. length: 5357.6,                last time consumption/overall running time: 665.2516s / 11600.3934 s
env0_first_0:                 episode reward: -11.8000,                 loss: -0.2109
env0_second_0:                 episode reward: 11.8000,                 loss: -0.2065
env1_first_0:                 episode reward: -10.8500,                 loss: nan
env1_second_0:                 episode reward: 10.8500,                 loss: nan
Episode: 461/30000 (1.5367%),                 avg. length: 6107.7,                last time consumption/overall running time: 741.9387s / 12342.3322 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.2236
env0_second_0:                 episode reward: 7.0000,                 loss: -0.2180
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 481/30000 (1.6033%),                 avg. length: 5685.0,                last time consumption/overall running time: 682.0195s / 13024.3517 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.2013
env0_second_0:                 episode reward: 3.9500,                 loss: -0.1978
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 501/30000 (1.6700%),                 avg. length: 5258.9,                last time consumption/overall running time: 645.5184s / 13669.8701 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1923
env0_second_0:                 episode reward: -1.2500,                 loss: -0.1838
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 521/30000 (1.7367%),                 avg. length: 4532.25,                last time consumption/overall running time: 548.4836s / 14218.3537 s
env0_first_0:                 episode reward: 8.4000,                 loss: -0.1915
env0_second_0:                 episode reward: -8.4000,                 loss: -0.1827
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 541/30000 (1.8033%),                 avg. length: 3441.55,                last time consumption/overall running time: 415.7965s / 14634.1501 s
env0_first_0:                 episode reward: 13.4000,                 loss: -0.1915
env0_second_0:                 episode reward: -13.4000,                 loss: -0.1836
env1_first_0:                 episode reward: 13.6500,                 loss: nan
env1_second_0:                 episode reward: -13.6500,                 loss: nan
Episode: 561/30000 (1.8700%),                 avg. length: 2889.15,                last time consumption/overall running time: 346.7505s / 14980.9006 s
env0_first_0:                 episode reward: 16.8000,                 loss: -0.1964
env0_second_0:                 episode reward: -16.8000,                 loss: -0.1842
env1_first_0:                 episode reward: 15.3500,                 loss: nan
env1_second_0:                 episode reward: -15.3500,                 loss: nan
Episode: 581/30000 (1.9367%),                 avg. length: 2938.4,                last time consumption/overall running time: 363.9662s / 15344.8669 s
env0_first_0:                 episode reward: 13.6000,                 loss: -0.2077
env0_second_0:                 episode reward: -13.6000,                 loss: -0.1971
env1_first_0:                 episode reward: 15.3500,                 loss: nan
env1_second_0:                 episode reward: -15.3500,                 loss: nan
Episode: 601/30000 (2.0033%),                 avg. length: 2706.4,                last time consumption/overall running time: 315.1645s / 15660.0314 s
env0_first_0:                 episode reward: 16.7500,                 loss: -0.2065
env0_second_0:                 episode reward: -16.7500,                 loss: -0.1991
env1_first_0:                 episode reward: 16.2500,                 loss: nan
env1_second_0:                 episode reward: -16.2500,                 loss: nan
Episode: 621/30000 (2.0700%),                 avg. length: 2327.75,                last time consumption/overall running time: 289.5294s / 15949.5608 s
env0_first_0:                 episode reward: 17.0500,                 loss: -0.1964
env0_second_0:                 episode reward: -17.0500,                 loss: -0.1903
env1_first_0:                 episode reward: 16.7500,                 loss: nan
env1_second_0:                 episode reward: -16.7500,                 loss: nan
Episode: 641/30000 (2.1367%),                 avg. length: 2262.95,                last time consumption/overall running time: 281.4688s / 16231.0295 s
env0_first_0:                 episode reward: 17.7500,                 loss: -0.2039
env0_second_0:                 episode reward: -17.7500,                 loss: -0.1952
env1_first_0:                 episode reward: 16.4500,                 loss: nan
env1_second_0:                 episode reward: -16.4500,                 loss: nan
Episode: 661/30000 (2.2033%),                 avg. length: 2259.2,                last time consumption/overall running time: 286.3637s / 16517.3932 s
env0_first_0:                 episode reward: 17.3500,                 loss: -0.2077
env0_second_0:                 episode reward: -17.3500,                 loss: -0.2032
env1_first_0:                 episode reward: 15.1500,                 loss: nan
env1_second_0:                 episode reward: -15.1500,                 loss: nan
Episode: 681/30000 (2.2700%),                 avg. length: 2111.95,                last time consumption/overall running time: 266.5009s / 16783.8941 s
env0_first_0:                 episode reward: 16.4000,                 loss: -0.2025
env0_second_0:                 episode reward: -16.4000,                 loss: -0.1952
env1_first_0:                 episode reward: 17.2000,                 loss: nan
env1_second_0:                 episode reward: -17.2000,                 loss: nan
Episode: 701/30000 (2.3367%),                 avg. length: 1936.55,                last time consumption/overall running time: 231.3666s / 17015.2607 s
env0_first_0:                 episode reward: 16.2500,                 loss: -0.1816
env0_second_0:                 episode reward: -16.2500,                 loss: -0.1714
env1_first_0:                 episode reward: 16.7500,                 loss: nan
env1_second_0:                 episode reward: -16.7500,                 loss: nan
Episode: 721/30000 (2.4033%),                 avg. length: 1898.25,                last time consumption/overall running time: 237.7067s / 17252.9674 s
env0_first_0:                 episode reward: 18.4500,                 loss: -0.1626
env0_second_0:                 episode reward: -18.4500,                 loss: -0.1467
env1_first_0:                 episode reward: 15.9000,                 loss: nan
env1_second_0:                 episode reward: -15.9000,                 loss: nan
Episode: 741/30000 (2.4700%),                 avg. length: 1729.95,                last time consumption/overall running time: 204.7245s / 17457.6920 s
env0_first_0:                 episode reward: 14.5500,                 loss: -0.1130
env0_second_0:                 episode reward: -14.5500,                 loss: -0.0967
env1_first_0:                 episode reward: 15.0000,                 loss: nan
env1_second_0:                 episode reward: -15.0000,                 loss: nan
Episode: 761/30000 (2.5367%),                 avg. length: 1684.85,                last time consumption/overall running time: 209.1361s / 17666.8281 s
env0_first_0:                 episode reward: 10.3000,                 loss: -0.0778
env0_second_0:                 episode reward: -10.3000,                 loss: -0.0609
env1_first_0:                 episode reward: 12.2000,                 loss: nan
env1_second_0:                 episode reward: -12.2000,                 loss: nan
Episode: 781/30000 (2.6033%),                 avg. length: 1759.4,                last time consumption/overall running time: 212.6263s / 17879.4544 s
env0_first_0:                 episode reward: 9.2000,                 loss: -0.1138
env0_second_0:                 episode reward: -9.2000,                 loss: -0.0978
env1_first_0:                 episode reward: 8.3500,                 loss: nan
env1_second_0:                 episode reward: -8.3500,                 loss: nan
Episode: 801/30000 (2.6700%),                 avg. length: 1729.1,                last time consumption/overall running time: 210.9354s / 18090.3898 s
env0_first_0:                 episode reward: 8.8500,                 loss: -0.1070
env0_second_0:                 episode reward: -8.8500,                 loss: -0.0900
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 821/30000 (2.7367%),                 avg. length: 1608.0,                last time consumption/overall running time: 193.8373s / 18284.2271 s
env0_first_0:                 episode reward: 11.2000,                 loss: -0.0853
env0_second_0:                 episode reward: -11.2000,                 loss: -0.0733
env1_first_0:                 episode reward: 12.5500,                 loss: nan
env1_second_0:                 episode reward: -12.5500,                 loss: nan
Episode: 841/30000 (2.8033%),                 avg. length: 2139.85,                last time consumption/overall running time: 273.0845s / 18557.3117 s
env0_first_0:                 episode reward: 12.7500,                 loss: -0.1054
env0_second_0:                 episode reward: -12.7500,                 loss: -0.0911
env1_first_0:                 episode reward: 13.8000,                 loss: nan
env1_second_0:                 episode reward: -13.8000,                 loss: nan
Episode: 861/30000 (2.8700%),                 avg. length: 2101.4,                last time consumption/overall running time: 255.4750s / 18812.7867 s
env0_first_0:                 episode reward: 12.4500,                 loss: -0.0995
env0_second_0:                 episode reward: -12.4500,                 loss: -0.0883
env1_first_0:                 episode reward: 12.6000,                 loss: nan
env1_second_0:                 episode reward: -12.6000,                 loss: nan
Episode: 881/30000 (2.9367%),                 avg. length: 1799.4,                last time consumption/overall running time: 231.9159s / 19044.7025 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0879
env0_second_0:                 episode reward: -10.0000,                 loss: -0.0673
env1_first_0:                 episode reward: 11.4000,                 loss: nan
env1_second_0:                 episode reward: -11.4000,                 loss: nan
Episode: 901/30000 (3.0033%),                 avg. length: 1824.2,                last time consumption/overall running time: 227.6215s / 19272.3241 s
env0_first_0:                 episode reward: 11.9500,                 loss: -0.1083
env0_second_0:                 episode reward: -11.9500,                 loss: -0.0895
env1_first_0:                 episode reward: 11.9500,                 loss: nan
env1_second_0:                 episode reward: -11.9500,                 loss: nan
Episode: 921/30000 (3.0700%),                 avg. length: 1978.9,                last time consumption/overall running time: 251.1141s / 19523.4382 s
env0_first_0:                 episode reward: 10.5500,                 loss: -0.1073
env0_second_0:                 episode reward: -10.5500,                 loss: -0.0894
env1_first_0:                 episode reward: 10.5500,                 loss: nan
env1_second_0:                 episode reward: -10.5500,                 loss: nan
Episode: 941/30000 (3.1367%),                 avg. length: 1969.65,                last time consumption/overall running time: 247.2334s / 19770.6717 s
env0_first_0:                 episode reward: 11.8500,                 loss: -0.0975
env0_second_0:                 episode reward: -11.8500,                 loss: -0.0737
env1_first_0:                 episode reward: 10.5000,                 loss: nan
env1_second_0:                 episode reward: -10.5000,                 loss: nan
Episode: 961/30000 (3.2033%),                 avg. length: 1782.2,                last time consumption/overall running time: 223.2058s / 19993.8774 s
env0_first_0:                 episode reward: 10.5000,                 loss: -0.0922
env0_second_0:                 episode reward: -10.5000,                 loss: -0.0716
env1_first_0:                 episode reward: 9.1500,                 loss: nan
env1_second_0:                 episode reward: -9.1500,                 loss: nan
Episode: 981/30000 (3.2700%),                 avg. length: 1770.75,                last time consumption/overall running time: 216.0032s / 20209.8806 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.1418
env0_second_0:                 episode reward: -9.5500,                 loss: -0.1217
env1_first_0:                 episode reward: 8.6500,                 loss: nan
env1_second_0:                 episode reward: -8.6500,                 loss: nan
Episode: 1001/30000 (3.3367%),                 avg. length: 1760.7,                last time consumption/overall running time: 229.8101s / 20439.6907 s
env0_first_0:                 episode reward: 8.0500,                 loss: -0.1479
env0_second_0:                 episode reward: -8.0500,                 loss: -0.1262
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 1021/30000 (3.4033%),                 avg. length: 1801.15,                last time consumption/overall running time: 223.0325s / 20662.7232 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.1767
env0_second_0:                 episode reward: -7.7500,                 loss: -0.1549
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 1041/30000 (3.4700%),                 avg. length: 1793.35,                last time consumption/overall running time: 224.5177s / 20887.2409 s
env0_first_0:                 episode reward: 7.3500,                 loss: -0.1413
env0_second_0:                 episode reward: -7.3500,                 loss: -0.1190
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 1061/30000 (3.5367%),                 avg. length: 1790.1,                last time consumption/overall running time: 226.4350s / 21113.6758 s
env0_first_0:                 episode reward: 8.4000,                 loss: -0.1275
env0_second_0:                 episode reward: -8.4000,                 loss: -0.0973
env1_first_0:                 episode reward: 8.4000,                 loss: nan
env1_second_0:                 episode reward: -8.4000,                 loss: nan
Episode: 1081/30000 (3.6033%),                 avg. length: 2019.4,                last time consumption/overall running time: 255.6684s / 21369.3442 s
env0_first_0:                 episode reward: 8.8000,                 loss: -0.1417
env0_second_0:                 episode reward: -8.8000,                 loss: -0.1182
env1_first_0:                 episode reward: 10.2000,                 loss: nan
env1_second_0:                 episode reward: -10.2000,                 loss: nan
Episode: 1101/30000 (3.6700%),                 avg. length: 2288.9,                last time consumption/overall running time: 289.5848s / 21658.9290 s
env0_first_0:                 episode reward: 10.3500,                 loss: -0.1187
env0_second_0:                 episode reward: -10.3500,                 loss: -0.1067
env1_first_0:                 episode reward: 9.2000,                 loss: nan
env1_second_0:                 episode reward: -9.2000,                 loss: nan
Episode: 1121/30000 (3.7367%),                 avg. length: 2674.05,                last time consumption/overall running time: 330.4376s / 21989.3666 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0578
env0_second_0:                 episode reward: -10.0000,                 loss: -0.0396
env1_first_0:                 episode reward: 10.9500,                 loss: nan
env1_second_0:                 episode reward: -10.9500,                 loss: nan
Episode: 1141/30000 (3.8033%),                 avg. length: 2464.9,                last time consumption/overall running time: 290.6177s / 22279.9843 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0591
env0_second_0:                 episode reward: -9.4500,                 loss: -0.0364
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 1161/30000 (3.8700%),                 avg. length: 2814.1,                last time consumption/overall running time: 335.9502s / 22615.9345 s
env0_first_0:                 episode reward: 9.0500,                 loss: -0.0403
env0_second_0:                 episode reward: -9.0500,                 loss: -0.0257
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 1181/30000 (3.9367%),                 avg. length: 3240.6,                last time consumption/overall running time: 396.0343s / 23011.9688 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0227
env0_second_0:                 episode reward: -9.9500,                 loss: -0.0058
env1_first_0:                 episode reward: 8.3000,                 loss: nan
env1_second_0:                 episode reward: -8.3000,                 loss: nan
Episode: 1201/30000 (4.0033%),                 avg. length: 2918.4,                last time consumption/overall running time: 362.9993s / 23374.9681 s
env0_first_0:                 episode reward: 10.1000,                 loss: -0.0268
env0_second_0:                 episode reward: -10.1000,                 loss: -0.0158
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 1221/30000 (4.0700%),                 avg. length: 2777.45,                last time consumption/overall running time: 337.9294s / 23712.8976 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0678
env0_second_0:                 episode reward: -9.9000,                 loss: -0.0539
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 1241/30000 (4.1367%),                 avg. length: 2551.9,                last time consumption/overall running time: 320.6202s / 24033.5178 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.0881
env0_second_0:                 episode reward: -5.8000,                 loss: -0.0702
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 1261/30000 (4.2033%),                 avg. length: 3004.9,                last time consumption/overall running time: 357.6965s / 24391.2142 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.1065
env0_second_0:                 episode reward: -3.6000,                 loss: -0.0866
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 1281/30000 (4.2700%),                 avg. length: 2817.05,                last time consumption/overall running time: 346.2358s / 24737.4500 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.1022
env0_second_0:                 episode reward: -5.3000,                 loss: -0.0823
env1_first_0:                 episode reward: 8.3500,                 loss: nan
env1_second_0:                 episode reward: -8.3500,                 loss: nan
Episode: 1301/30000 (4.3367%),                 avg. length: 3054.6,                last time consumption/overall running time: 371.5087s / 25108.9587 s
env0_first_0:                 episode reward: 9.1000,                 loss: -0.1014
env0_second_0:                 episode reward: -9.1000,                 loss: -0.0855
env1_first_0:                 episode reward: 10.3000,                 loss: nan
env1_second_0:                 episode reward: -10.3000,                 loss: nan
Episode: 1321/30000 (4.4033%),                 avg. length: 2956.15,                last time consumption/overall running time: 373.4047s / 25482.3634 s
env0_first_0:                 episode reward: 10.3500,                 loss: -0.1012
env0_second_0:                 episode reward: -10.3500,                 loss: -0.0844
env1_first_0:                 episode reward: 8.6000,                 loss: nan
env1_second_0:                 episode reward: -8.6000,                 loss: nan
Episode: 1341/30000 (4.4700%),                 avg. length: 2874.6,                last time consumption/overall running time: 352.1159s / 25834.4794 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.1041
env0_second_0:                 episode reward: -7.1500,                 loss: -0.0881
env1_first_0:                 episode reward: 8.0500,                 loss: nan
env1_second_0:                 episode reward: -8.0500,                 loss: nan
Episode: 1361/30000 (4.5367%),                 avg. length: 2951.5,                last time consumption/overall running time: 353.4194s / 26187.8987 s
env0_first_0:                 episode reward: 5.5000,                 loss: -0.0976
env0_second_0:                 episode reward: -5.5000,                 loss: -0.0797
env1_first_0:                 episode reward: 8.5500,                 loss: nan
env1_second_0:                 episode reward: -8.5500,                 loss: nan
Episode: 1381/30000 (4.6033%),                 avg. length: 3072.05,                last time consumption/overall running time: 366.5137s / 26554.4124 s
env0_first_0:                 episode reward: 7.8000,                 loss: -0.0390
env0_second_0:                 episode reward: -7.8000,                 loss: -0.0166
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 1401/30000 (4.6700%),                 avg. length: 3284.4,                last time consumption/overall running time: 408.7697s / 26963.1822 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.0005
env0_second_0:                 episode reward: -7.1500,                 loss: 0.0143
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 1421/30000 (4.7367%),                 avg. length: 3308.45,                last time consumption/overall running time: 399.9916s / 27363.1738 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.0730
env0_second_0:                 episode reward: -5.8500,                 loss: -0.0510
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 1441/30000 (4.8033%),                 avg. length: 3405.35,                last time consumption/overall running time: 428.0031s / 27791.1769 s
env0_first_0:                 episode reward: 6.4500,                 loss: -0.0439
env0_second_0:                 episode reward: -6.4500,                 loss: -0.0286
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 1461/30000 (4.8700%),                 avg. length: 3132.75,                last time consumption/overall running time: 396.8687s / 28188.0456 s
env0_first_0:                 episode reward: 10.7500,                 loss: -0.0796
env0_second_0:                 episode reward: -10.7500,                 loss: -0.0555
env1_first_0:                 episode reward: 8.4500,                 loss: nan
env1_second_0:                 episode reward: -8.4500,                 loss: nan
Episode: 1481/30000 (4.9367%),                 avg. length: 3018.7,                last time consumption/overall running time: 382.4976s / 28570.5432 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0963
env0_second_0:                 episode reward: -9.8000,                 loss: -0.0774
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 1501/30000 (5.0033%),                 avg. length: 2286.2,                last time consumption/overall running time: 288.6165s / 28859.1597 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.0912
env0_second_0:                 episode reward: -9.6000,                 loss: -0.0688
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 1521/30000 (5.0700%),                 avg. length: 2324.7,                last time consumption/overall running time: 288.8988s / 29148.0585 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.1011
env0_second_0:                 episode reward: -9.6000,                 loss: -0.0703
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 1541/30000 (5.1367%),                 avg. length: 2348.8,                last time consumption/overall running time: 280.0940s / 29428.1525 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.1055
env0_second_0:                 episode reward: -10.0000,                 loss: -0.0785
env1_first_0:                 episode reward: 10.1500,                 loss: nan
env1_second_0:                 episode reward: -10.1500,                 loss: nan
Episode: 1561/30000 (5.2033%),                 avg. length: 2369.05,                last time consumption/overall running time: 270.7585s / 29698.9109 s
env0_first_0:                 episode reward: 10.5000,                 loss: -0.1252
env0_second_0:                 episode reward: -10.5000,                 loss: -0.0969
env1_first_0:                 episode reward: 11.3500,                 loss: nan
env1_second_0:                 episode reward: -11.3500,                 loss: nan
Episode: 1581/30000 (5.2700%),                 avg. length: 2116.6,                last time consumption/overall running time: 247.8555s / 29946.7664 s
env0_first_0:                 episode reward: 11.2500,                 loss: -0.1170
env0_second_0:                 episode reward: -11.2500,                 loss: -0.0835
env1_first_0:                 episode reward: 10.6500,                 loss: nan
env1_second_0:                 episode reward: -10.6500,                 loss: nan
Episode: 1601/30000 (5.3367%),                 avg. length: 1955.95,                last time consumption/overall running time: 245.5555s / 30192.3219 s
env0_first_0:                 episode reward: 11.4000,                 loss: -0.1089
env0_second_0:                 episode reward: -11.4000,                 loss: -0.0750
env1_first_0:                 episode reward: 12.2500,                 loss: nan
env1_second_0:                 episode reward: -12.2500,                 loss: nan
Episode: 1621/30000 (5.4033%),                 avg. length: 2425.35,                last time consumption/overall running time: 301.4996s / 30493.8215 s
env0_first_0:                 episode reward: 10.9500,                 loss: -0.0706
env0_second_0:                 episode reward: -10.9500,                 loss: -0.0357
env1_first_0:                 episode reward: 11.0000,                 loss: nan
env1_second_0:                 episode reward: -11.0000,                 loss: nan
Episode: 1641/30000 (5.4700%),                 avg. length: 2028.15,                last time consumption/overall running time: 256.1128s / 30749.9343 s
env0_first_0:                 episode reward: 9.3000,                 loss: -0.1417
env0_second_0:                 episode reward: -9.3000,                 loss: -0.1140
env1_first_0:                 episode reward: 10.4000,                 loss: nan
env1_second_0:                 episode reward: -10.4000,                 loss: nan
Episode: 1661/30000 (5.5367%),                 avg. length: 2350.6,                last time consumption/overall running time: 291.5139s / 31041.4482 s
env0_first_0:                 episode reward: 10.8000,                 loss: -0.1440
env0_second_0:                 episode reward: -10.8000,                 loss: -0.1107
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 1681/30000 (5.6033%),                 avg. length: 2689.7,                last time consumption/overall running time: 335.2310s / 31376.6792 s
env0_first_0:                 episode reward: 19.2500,                 loss: -0.0166
env0_second_0:                 episode reward: -19.2500,                 loss: 0.0975
env1_first_0:                 episode reward: 19.7000,                 loss: nan
env1_second_0:                 episode reward: -19.7000,                 loss: nan
Episode: 1701/30000 (5.6700%),                 avg. length: 2353.95,                last time consumption/overall running time: 291.1824s / 31667.8616 s
env0_first_0:                 episode reward: 15.2000,                 loss: -0.1232
env0_second_0:                 episode reward: -15.2000,                 loss: -0.0752
env1_first_0:                 episode reward: 15.1000,                 loss: nan
env1_second_0:                 episode reward: -15.1000,                 loss: nan
Episode: 1721/30000 (5.7367%),                 avg. length: 2408.1,                last time consumption/overall running time: 296.8930s / 31964.7546 s
env0_first_0:                 episode reward: 14.3000,                 loss: -0.1573
env0_second_0:                 episode reward: -14.3000,                 loss: -0.1146
env1_first_0:                 episode reward: 14.1000,                 loss: nan
env1_second_0:                 episode reward: -14.1000,                 loss: nan
Episode: 1741/30000 (5.8033%),                 avg. length: 2152.3,                last time consumption/overall running time: 261.8649s / 32226.6195 s
env0_first_0:                 episode reward: 12.6000,                 loss: -0.1505
env0_second_0:                 episode reward: -12.6000,                 loss: -0.1014
env1_first_0:                 episode reward: 10.8500,                 loss: nan
env1_second_0:                 episode reward: -10.8500,                 loss: nan
Episode: 1761/30000 (5.8700%),                 avg. length: 2281.2,                last time consumption/overall running time: 287.9925s / 32514.6120 s
env0_first_0:                 episode reward: 12.8000,                 loss: -0.1446
env0_second_0:                 episode reward: -12.8000,                 loss: -0.0957
env1_first_0:                 episode reward: 12.5000,                 loss: nan
env1_second_0:                 episode reward: -12.5000,                 loss: nan
Episode: 1781/30000 (5.9367%),                 avg. length: 2081.05,                last time consumption/overall running time: 263.9699s / 32778.5819 s
env0_first_0:                 episode reward: 12.5500,                 loss: -0.1409
env0_second_0:                 episode reward: -12.5500,                 loss: -0.0891
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 1801/30000 (6.0033%),                 avg. length: 1818.45,                last time consumption/overall running time: 230.3912s / 33008.9731 s
env0_first_0:                 episode reward: 9.0500,                 loss: -0.1324
env0_second_0:                 episode reward: -9.0500,                 loss: -0.0864
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 1821/30000 (6.0700%),                 avg. length: 1915.75,                last time consumption/overall running time: 242.7286s / 33251.7017 s
env0_first_0:                 episode reward: 8.4500,                 loss: -0.1594
env0_second_0:                 episode reward: -8.4500,                 loss: -0.1222
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 1841/30000 (6.1367%),                 avg. length: 2329.8,                last time consumption/overall running time: 291.3240s / 33543.0257 s
env0_first_0:                 episode reward: 11.0000,                 loss: -0.1582
env0_second_0:                 episode reward: -11.0000,                 loss: -0.1287
env1_first_0:                 episode reward: 10.7000,                 loss: nan
env1_second_0:                 episode reward: -10.7000,                 loss: nan
Episode: 1861/30000 (6.2033%),                 avg. length: 2096.65,                last time consumption/overall running time: 266.1670s / 33809.1927 s
env0_first_0:                 episode reward: 11.4500,                 loss: -0.1340
env0_second_0:                 episode reward: -11.4500,                 loss: 0.1003
env1_first_0:                 episode reward: 10.8500,                 loss: nan
env1_second_0:                 episode reward: -10.8500,                 loss: nan
Episode: 1881/30000 (6.2700%),                 avg. length: 2007.75,                last time consumption/overall running time: 253.6150s / 34062.8076 s
env0_first_0:                 episode reward: 17.6500,                 loss: -0.1801
env0_second_0:                 episode reward: -17.6500,                 loss: -0.1370
env1_first_0:                 episode reward: 16.9500,                 loss: nan
env1_second_0:                 episode reward: -16.9500,                 loss: nan
Episode: 1901/30000 (6.3367%),                 avg. length: 1921.35,                last time consumption/overall running time: 244.6136s / 34307.4212 s
env0_first_0:                 episode reward: 18.0000,                 loss: -0.1636
env0_second_0:                 episode reward: -18.0000,                 loss: -0.1198
env1_first_0:                 episode reward: 18.4000,                 loss: nan
env1_second_0:                 episode reward: -18.4000,                 loss: nan
Episode: 1921/30000 (6.4033%),                 avg. length: 1956.35,                last time consumption/overall running time: 245.7225s / 34553.1437 s
env0_first_0:                 episode reward: 17.3000,                 loss: -0.1001
env0_second_0:                 episode reward: -17.3000,                 loss: -0.0645
env1_first_0:                 episode reward: 15.2000,                 loss: nan
env1_second_0:                 episode reward: -15.2000,                 loss: nan
Episode: 1941/30000 (6.4700%),                 avg. length: 2145.9,                last time consumption/overall running time: 272.2359s / 34825.3797 s
env0_first_0:                 episode reward: 16.3500,                 loss: -0.1384
env0_second_0:                 episode reward: -16.3500,                 loss: -0.0394
env1_first_0:                 episode reward: 16.9000,                 loss: nan
env1_second_0:                 episode reward: -16.9000,                 loss: nan
Episode: 1961/30000 (6.5367%),                 avg. length: 2116.65,                last time consumption/overall running time: 267.8734s / 35093.2531 s
env0_first_0:                 episode reward: 15.9000,                 loss: -0.1314
env0_second_0:                 episode reward: -15.9000,                 loss: -0.0881
env1_first_0:                 episode reward: 17.8000,                 loss: nan
env1_second_0:                 episode reward: -17.8000,                 loss: nan
Episode: 1981/30000 (6.6033%),                 avg. length: 2039.5,                last time consumption/overall running time: 249.1577s / 35342.4108 s
env0_first_0:                 episode reward: 15.8500,                 loss: -0.1263
env0_second_0:                 episode reward: -15.8500,                 loss: -0.0977
env1_first_0:                 episode reward: 15.0500,                 loss: nan
env1_second_0:                 episode reward: -15.0500,                 loss: nan
Episode: 2001/30000 (6.6700%),                 avg. length: 2100.95,                last time consumption/overall running time: 261.8644s / 35604.2752 s
env0_first_0:                 episode reward: 15.1500,                 loss: -0.1026
env0_second_0:                 episode reward: -15.1500,                 loss: -0.0875
env1_first_0:                 episode reward: 16.6000,                 loss: nan
env1_second_0:                 episode reward: -16.6000,                 loss: nan
Episode: 2021/30000 (6.7367%),                 avg. length: 2107.2,                last time consumption/overall running time: 247.6231s / 35851.8984 s
env0_first_0:                 episode reward: 19.8000,                 loss: -0.1872
env0_second_0:                 episode reward: -19.8000,                 loss: -0.1689
env1_first_0:                 episode reward: 19.6000,                 loss: nan
env1_second_0:                 episode reward: -19.6000,                 loss: nan
Episode: 2041/30000 (6.8033%),                 avg. length: 2099.9,                last time consumption/overall running time: 266.4935s / 36118.3919 s
env0_first_0:                 episode reward: 16.9500,                 loss: -0.1582
env0_second_0:                 episode reward: -16.9500,                 loss: -0.1338
env1_first_0:                 episode reward: 17.6500,                 loss: nan
env1_second_0:                 episode reward: -17.6500,                 loss: nan
Episode: 2061/30000 (6.8700%),                 avg. length: 2049.35,                last time consumption/overall running time: 249.6726s / 36368.0645 s
env0_first_0:                 episode reward: 15.5500,                 loss: -0.1526
env0_second_0:                 episode reward: -15.5500,                 loss: -0.1234
env1_first_0:                 episode reward: 15.6500,                 loss: nan
env1_second_0:                 episode reward: -15.6500,                 loss: nan
Episode: 2081/30000 (6.9367%),                 avg. length: 2077.85,                last time consumption/overall running time: 260.7632s / 36628.8278 s
env0_first_0:                 episode reward: 19.5500,                 loss: -0.1777
env0_second_0:                 episode reward: -19.5500,                 loss: -0.1503
env1_first_0:                 episode reward: 18.7500,                 loss: nan
env1_second_0:                 episode reward: -18.7500,                 loss: nan
Episode: 2101/30000 (7.0033%),                 avg. length: 2219.7,                last time consumption/overall running time: 272.9294s / 36901.7571 s
env0_first_0:                 episode reward: 17.4000,                 loss: -0.1300
env0_second_0:                 episode reward: -17.4000,                 loss: -0.0503
env1_first_0:                 episode reward: 19.1500,                 loss: nan
env1_second_0:                 episode reward: -19.1500,                 loss: nan
Episode: 2121/30000 (7.0700%),                 avg. length: 2034.35,                last time consumption/overall running time: 244.8231s / 37146.5803 s
env0_first_0:                 episode reward: 16.2500,                 loss: -0.1281
env0_second_0:                 episode reward: -16.2500,                 loss: -0.1027
env1_first_0:                 episode reward: 15.8000,                 loss: nan
env1_second_0:                 episode reward: -15.8000,                 loss: nan
Episode: 2141/30000 (7.1367%),                 avg. length: 2306.65,                last time consumption/overall running time: 276.4696s / 37423.0499 s
env0_first_0:                 episode reward: 13.6500,                 loss: -0.0939
env0_second_0:                 episode reward: -13.6500,                 loss: -0.0630
env1_first_0:                 episode reward: 14.7500,                 loss: nan
env1_second_0:                 episode reward: -14.7500,                 loss: nan
Episode: 2161/30000 (7.2033%),                 avg. length: 2345.55,                last time consumption/overall running time: 294.3495s / 37717.3994 s
env0_first_0:                 episode reward: 14.7500,                 loss: -0.1195
env0_second_0:                 episode reward: -14.7500,                 loss: -0.0911
env1_first_0:                 episode reward: 15.7000,                 loss: nan
env1_second_0:                 episode reward: -15.7000,                 loss: nan
Episode: 2181/30000 (7.2700%),                 avg. length: 2199.85,                last time consumption/overall running time: 270.8063s / 37988.2057 s
env0_first_0:                 episode reward: 13.8500,                 loss: -0.1368
env0_second_0:                 episode reward: -13.8500,                 loss: -0.1041
env1_first_0:                 episode reward: 15.4000,                 loss: nan
env1_second_0:                 episode reward: -15.4000,                 loss: nan
Episode: 2201/30000 (7.3367%),                 avg. length: 1965.65,                last time consumption/overall running time: 249.9213s / 38238.1269 s
env0_first_0:                 episode reward: 12.1000,                 loss: -0.1485
env0_second_0:                 episode reward: -12.1000,                 loss: -0.1180
env1_first_0:                 episode reward: 12.3000,                 loss: nan
env1_second_0:                 episode reward: -12.3000,                 loss: nan
Episode: 2221/30000 (7.4033%),                 avg. length: 2030.45,                last time consumption/overall running time: 253.6899s / 38491.8169 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.1503
env0_second_0:                 episode reward: -9.6000,                 loss: -0.0999
env1_first_0:                 episode reward: 10.2000,                 loss: nan
env1_second_0:                 episode reward: -10.2000,                 loss: nan
Episode: 2241/30000 (7.4700%),                 avg. length: 2000.4,                last time consumption/overall running time: 251.2837s / 38743.1006 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.1489
env0_second_0:                 episode reward: -9.5500,                 loss: -0.1003
env1_first_0:                 episode reward: 7.7500,                 loss: nan
env1_second_0:                 episode reward: -7.7500,                 loss: nan
Episode: 2261/30000 (7.5367%),                 avg. length: 1950.05,                last time consumption/overall running time: 245.4857s / 38988.5863 s
env0_first_0:                 episode reward: 7.8000,                 loss: -0.1422
env0_second_0:                 episode reward: -7.8000,                 loss: -0.1107
env1_first_0:                 episode reward: 8.1000,                 loss: nan
env1_second_0:                 episode reward: -8.1000,                 loss: nan
Episode: 2281/30000 (7.6033%),                 avg. length: 1913.85,                last time consumption/overall running time: 242.0315s / 39230.6178 s
env0_first_0:                 episode reward: 8.3000,                 loss: -0.1493
env0_second_0:                 episode reward: -8.3000,                 loss: -0.1121
env1_first_0:                 episode reward: 8.1500,                 loss: nan
env1_second_0:                 episode reward: -8.1500,                 loss: nan
Episode: 2301/30000 (7.6700%),                 avg. length: 1793.05,                last time consumption/overall running time: 227.2988s / 39457.9166 s
env0_first_0:                 episode reward: 8.7500,                 loss: -0.1309
env0_second_0:                 episode reward: -8.7500,                 loss: -0.1024
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 2321/30000 (7.7367%),                 avg. length: 1968.75,                last time consumption/overall running time: 239.6084s / 39697.5250 s
env0_first_0:                 episode reward: 6.1500,                 loss: -0.1533
env0_second_0:                 episode reward: -6.1500,                 loss: -0.1151
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 2341/30000 (7.8033%),                 avg. length: 1777.05,                last time consumption/overall running time: 227.6567s / 39925.1816 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.1425
env0_second_0:                 episode reward: -6.5000,                 loss: -0.1226
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 2361/30000 (7.8700%),                 avg. length: 1735.65,                last time consumption/overall running time: 223.5342s / 40148.7159 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.1611
env0_second_0:                 episode reward: -6.6000,                 loss: -0.1172
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 2381/30000 (7.9367%),                 avg. length: 1728.85,                last time consumption/overall running time: 217.8931s / 40366.6089 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.1637
env0_second_0:                 episode reward: -5.3000,                 loss: -0.1300
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 2401/30000 (8.0033%),                 avg. length: 1790.8,                last time consumption/overall running time: 219.7462s / 40586.3551 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.1756
env0_second_0:                 episode reward: -4.5500,                 loss: -0.1350
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 2421/30000 (8.0700%),                 avg. length: 1766.0,                last time consumption/overall running time: 197.3956s / 40783.7507 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1747
env0_second_0:                 episode reward: -3.4000,                 loss: -0.1420
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 2441/30000 (8.1367%),                 avg. length: 1787.8,                last time consumption/overall running time: 221.8037s / 41005.5544 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.1361
env0_second_0:                 episode reward: -6.5000,                 loss: -0.0899
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 2461/30000 (8.2033%),                 avg. length: 1833.0,                last time consumption/overall running time: 212.7108s / 41218.2652 s
env0_first_0:                 episode reward: 8.6000,                 loss: -0.1333
env0_second_0:                 episode reward: -8.6000,                 loss: -0.0912
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 2481/30000 (8.2700%),                 avg. length: 1955.95,                last time consumption/overall running time: 224.4780s / 41442.7432 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.1500
env0_second_0:                 episode reward: -6.8000,                 loss: -0.0974
env1_first_0:                 episode reward: 8.4000,                 loss: nan
env1_second_0:                 episode reward: -8.4000,                 loss: nan
Episode: 2501/30000 (8.3367%),                 avg. length: 1879.9,                last time consumption/overall running time: 240.2791s / 41683.0223 s
env0_first_0:                 episode reward: 7.7000,                 loss: -0.1482
env0_second_0:                 episode reward: -7.7000,                 loss: -0.1062
env1_first_0:                 episode reward: 8.1000,                 loss: nan
env1_second_0:                 episode reward: -8.1000,                 loss: nan
Episode: 2521/30000 (8.4033%),                 avg. length: 1944.85,                last time consumption/overall running time: 246.4024s / 41929.4247 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.1275
env0_second_0:                 episode reward: -6.6000,                 loss: -0.0569
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 2541/30000 (8.4700%),                 avg. length: 2012.55,                last time consumption/overall running time: 255.3246s / 42184.7493 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.1521
env0_second_0:                 episode reward: -5.3000,                 loss: -0.0990
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 2561/30000 (8.5367%),                 avg. length: 1871.4,                last time consumption/overall running time: 234.3901s / 42419.1393 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.1583
env0_second_0:                 episode reward: -5.4500,                 loss: -0.1227
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 2581/30000 (8.6033%),                 avg. length: 1916.25,                last time consumption/overall running time: 240.2125s / 42659.3519 s
env0_first_0:                 episode reward: 6.9500,                 loss: -0.1458
env0_second_0:                 episode reward: -6.9500,                 loss: -0.1016
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 2601/30000 (8.6700%),                 avg. length: 2001.65,                last time consumption/overall running time: 253.3438s / 42912.6956 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.1321
env0_second_0:                 episode reward: -7.1500,                 loss: -0.0014
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 2621/30000 (8.7367%),                 avg. length: 2065.45,                last time consumption/overall running time: 261.5830s / 43174.2787 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.1573
env0_second_0:                 episode reward: -7.0000,                 loss: -0.1052
env1_first_0:                 episode reward: 8.0500,                 loss: nan
env1_second_0:                 episode reward: -8.0500,                 loss: nan
Episode: 2641/30000 (8.8033%),                 avg. length: 2121.7,                last time consumption/overall running time: 267.1796s / 43441.4583 s
env0_first_0:                 episode reward: 7.2500,                 loss: -0.1799
env0_second_0:                 episode reward: -7.2500,                 loss: -0.1402
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 2661/30000 (8.8700%),                 avg. length: 4471.55,                last time consumption/overall running time: 556.3592s / 43997.8175 s
env0_first_0:                 episode reward: 99.3500,                 loss: 0.0931
env0_second_0:                 episode reward: -99.3500,                 loss: 0.1189
env1_first_0:                 episode reward: 99.6000,                 loss: nan
env1_second_0:                 episode reward: -99.6000,                 loss: nan
Episode: 2681/30000 (8.9367%),                 avg. length: 5348.85,                last time consumption/overall running time: 662.8993s / 44660.7168 s
env0_first_0:                 episode reward: 138.6000,                 loss: 0.2699
env0_second_0:                 episode reward: -138.6000,                 loss: 1.8274
env1_first_0:                 episode reward: 140.5500,                 loss: nan
env1_second_0:                 episode reward: -140.5500,                 loss: nan
Episode: 2701/30000 (9.0033%),                 avg. length: 1687.8,                last time consumption/overall running time: 212.6528s / 44873.3696 s
env0_first_0:                 episode reward: 6.0000,                 loss: -0.1776
env0_second_0:                 episode reward: -6.0000,                 loss: 0.4721
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 2721/30000 (9.0700%),                 avg. length: 1768.8,                last time consumption/overall running time: 226.3706s / 45099.7403 s
env0_first_0:                 episode reward: 6.3500,                 loss: -0.1765
env0_second_0:                 episode reward: -6.3500,                 loss: 0.0954
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 2741/30000 (9.1367%),                 avg. length: 1795.1,                last time consumption/overall running time: 224.8941s / 45324.6344 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.1873
env0_second_0:                 episode reward: -4.5000,                 loss: -0.0182
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 2761/30000 (9.2033%),                 avg. length: 1794.95,                last time consumption/overall running time: 227.2417s / 45551.8761 s
env0_first_0:                 episode reward: 6.3000,                 loss: -0.1736
env0_second_0:                 episode reward: -6.3000,                 loss: -0.0809
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 2781/30000 (9.2700%),                 avg. length: 1968.45,                last time consumption/overall running time: 250.2428s / 45802.1189 s
env0_first_0:                 episode reward: 7.2500,                 loss: -0.1746
env0_second_0:                 episode reward: -7.2500,                 loss: -0.1140
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 2801/30000 (9.3367%),                 avg. length: 1710.3,                last time consumption/overall running time: 215.0849s / 46017.2039 s
env0_first_0:                 episode reward: 6.2000,                 loss: -0.1478
env0_second_0:                 episode reward: -6.2000,                 loss: -0.0566
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 2821/30000 (9.4033%),                 avg. length: 2004.75,                last time consumption/overall running time: 250.9089s / 46268.1128 s
env0_first_0:                 episode reward: 6.4500,                 loss: -0.1282
env0_second_0:                 episode reward: -6.4500,                 loss: -0.0585
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 2841/30000 (9.4700%),                 avg. length: 1831.7,                last time consumption/overall running time: 230.8532s / 46498.9659 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.1476
env0_second_0:                 episode reward: -5.7500,                 loss: -0.0909
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 2861/30000 (9.5367%),                 avg. length: 1741.45,                last time consumption/overall running time: 221.3921s / 46720.3580 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.1609
env0_second_0:                 episode reward: -5.1000,                 loss: -0.0386
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 2881/30000 (9.6033%),                 avg. length: 1679.0,                last time consumption/overall running time: 213.9080s / 46934.2660 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.1123
env0_second_0:                 episode reward: -3.3500,                 loss: -0.0334
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 2901/30000 (9.6700%),                 avg. length: 1571.35,                last time consumption/overall running time: 199.3191s / 47133.5851 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1250
env0_second_0:                 episode reward: -2.6500,                 loss: -0.0765
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 2921/30000 (9.7367%),                 avg. length: 1739.55,                last time consumption/overall running time: 217.1031s / 47350.6882 s
env0_first_0:                 episode reward: 6.7500,                 loss: -0.1517
env0_second_0:                 episode reward: -6.7500,                 loss: 0.1394
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 2941/30000 (9.8033%),                 avg. length: 1469.65,                last time consumption/overall running time: 187.5749s / 47538.2631 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2003
env0_second_0:                 episode reward: -2.1500,                 loss: -0.1328
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 2961/30000 (9.8700%),                 avg. length: 1548.15,                last time consumption/overall running time: 192.0125s / 47730.2756 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.2140
env0_second_0:                 episode reward: -2.8000,                 loss: -0.1342
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 2981/30000 (9.9367%),                 avg. length: 1525.0,                last time consumption/overall running time: 191.6773s / 47921.9529 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1931
env0_second_0:                 episode reward: -3.1500,                 loss: -0.1013
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 3001/30000 (10.0033%),                 avg. length: 1890.65,                last time consumption/overall running time: 236.6576s / 48158.6106 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.1332
env0_second_0:                 episode reward: -4.9000,                 loss: -0.0713
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 3021/30000 (10.0700%),                 avg. length: 1748.75,                last time consumption/overall running time: 215.0531s / 48373.6637 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.1750
env0_second_0:                 episode reward: -4.3500,                 loss: -0.1264
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 3041/30000 (10.1367%),                 avg. length: 2187.1,                last time consumption/overall running time: 277.9103s / 48651.5740 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.1066
env0_second_0:                 episode reward: -5.1000,                 loss: -0.0651
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 3061/30000 (10.2033%),                 avg. length: 2029.75,                last time consumption/overall running time: 250.0172s / 48901.5912 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.1738
env0_second_0:                 episode reward: -3.4500,                 loss: -0.1014
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 3081/30000 (10.2700%),                 avg. length: 2165.55,                last time consumption/overall running time: 268.9677s / 49170.5589 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1708
env0_second_0:                 episode reward: -3.9000,                 loss: -0.1099
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 3101/30000 (10.3367%),                 avg. length: 1924.2,                last time consumption/overall running time: 244.6597s / 49415.2185 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.1515
env0_second_0:                 episode reward: -6.8000,                 loss: -0.0802
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 3121/30000 (10.4033%),                 avg. length: 1777.75,                last time consumption/overall running time: 222.4285s / 49637.6470 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1567
env0_second_0:                 episode reward: -3.9000,                 loss: -0.0936
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 3141/30000 (10.4700%),                 avg. length: 1942.4,                last time consumption/overall running time: 238.0240s / 49875.6710 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1548
env0_second_0:                 episode reward: -1.4000,                 loss: -0.1059
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 3161/30000 (10.5367%),                 avg. length: 1724.7,                last time consumption/overall running time: 216.7897s / 50092.4607 s
env0_first_0:                 episode reward: 8.8500,                 loss: -0.0808
env0_second_0:                 episode reward: -8.8500,                 loss: -0.0389
env1_first_0:                 episode reward: 8.3500,                 loss: nan
env1_second_0:                 episode reward: -8.3500,                 loss: nan
Episode: 3181/30000 (10.6033%),                 avg. length: 1677.55,                last time consumption/overall running time: 211.2207s / 50303.6815 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.1748
env0_second_0:                 episode reward: -5.9000,                 loss: -0.1098
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 3201/30000 (10.6700%),                 avg. length: 1655.45,                last time consumption/overall running time: 208.8175s / 50512.4990 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1803
env0_second_0:                 episode reward: -1.8500,                 loss: -0.1063
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 3221/30000 (10.7367%),                 avg. length: 1573.35,                last time consumption/overall running time: 188.0969s / 50700.5959 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1896
env0_second_0:                 episode reward: -2.9500,                 loss: -0.1116
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 3241/30000 (10.8033%),                 avg. length: 1737.1,                last time consumption/overall running time: 195.3009s / 50895.8968 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.2077
env0_second_0:                 episode reward: -2.8500,                 loss: -0.1427
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 3261/30000 (10.8700%),                 avg. length: 1585.6,                last time consumption/overall running time: 201.2144s / 51097.1113 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2226
env0_second_0:                 episode reward: -2.6500,                 loss: -0.0963
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 3281/30000 (10.9367%),                 avg. length: 1562.85,                last time consumption/overall running time: 199.8846s / 51296.9959 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2088
env0_second_0:                 episode reward: -2.0500,                 loss: -0.0925
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 3301/30000 (11.0033%),                 avg. length: 1534.35,                last time consumption/overall running time: 195.6967s / 51492.6925 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.2025
env0_second_0:                 episode reward: -2.7500,                 loss: -0.0888
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 3321/30000 (11.0700%),                 avg. length: 1640.45,                last time consumption/overall running time: 205.5131s / 51698.2057 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1179
env0_second_0:                 episode reward: -2.3500,                 loss: -0.0475
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 3341/30000 (11.1367%),                 avg. length: 1508.7,                last time consumption/overall running time: 189.2864s / 51887.4921 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.0989
env0_second_0:                 episode reward: -3.3000,                 loss: -0.0341
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 3361/30000 (11.2033%),                 avg. length: 1612.85,                last time consumption/overall running time: 204.4330s / 52091.9251 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1293
env0_second_0:                 episode reward: -3.4000,                 loss: -0.0471
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 3381/30000 (11.2700%),                 avg. length: 1652.05,                last time consumption/overall running time: 194.0494s / 52285.9746 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.0999
env0_second_0:                 episode reward: -2.2000,                 loss: 0.0160
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 3401/30000 (11.3367%),                 avg. length: 1678.4,                last time consumption/overall running time: 208.6252s / 52494.5997 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.1134
env0_second_0:                 episode reward: -4.7000,                 loss: -0.0029
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 3421/30000 (11.4033%),                 avg. length: 2020.8,                last time consumption/overall running time: 260.1654s / 52754.7651 s
env0_first_0:                 episode reward: 8.6500,                 loss: -0.1573
env0_second_0:                 episode reward: -8.6500,                 loss: -0.0891
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 3441/30000 (11.4700%),                 avg. length: 2137.6,                last time consumption/overall running time: 258.9087s / 53013.6738 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.1423
env0_second_0:                 episode reward: -4.9500,                 loss: -0.0954
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 3461/30000 (11.5367%),                 avg. length: 2247.45,                last time consumption/overall running time: 285.4884s / 53299.1622 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.1030
env0_second_0:                 episode reward: -4.4000,                 loss: -0.0306
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 3481/30000 (11.6033%),                 avg. length: 2252.0,                last time consumption/overall running time: 285.3664s / 53584.5286 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.1539
env0_second_0:                 episode reward: -4.3000,                 loss: -0.0821
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 3501/30000 (11.6700%),                 avg. length: 1831.25,                last time consumption/overall running time: 219.4021s / 53803.9308 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.1383
env0_second_0:                 episode reward: -4.6500,                 loss: -0.0522
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 3521/30000 (11.7367%),                 avg. length: 1692.3,                last time consumption/overall running time: 215.0399s / 54018.9707 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.1776
env0_second_0:                 episode reward: -4.1000,                 loss: -0.0902
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 3541/30000 (11.8033%),                 avg. length: 1600.55,                last time consumption/overall running time: 203.0553s / 54222.0260 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1455
env0_second_0:                 episode reward: -2.7500,                 loss: -0.0851
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 3561/30000 (11.8700%),                 avg. length: 1568.45,                last time consumption/overall running time: 199.3009s / 54421.3269 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.1772
env0_second_0:                 episode reward: -4.1500,                 loss: -0.0881
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 3581/30000 (11.9367%),                 avg. length: 1728.65,                last time consumption/overall running time: 217.4936s / 54638.8204 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1899
env0_second_0:                 episode reward: -3.9000,                 loss: -0.1229
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 3601/30000 (12.0033%),                 avg. length: 1705.3,                last time consumption/overall running time: 217.1111s / 54855.9315 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.1641
env0_second_0:                 episode reward: -5.0500,                 loss: -0.0999
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 3621/30000 (12.0700%),                 avg. length: 1798.3,                last time consumption/overall running time: 225.7752s / 55081.7067 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1694
env0_second_0:                 episode reward: -1.8500,                 loss: -0.0949
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 3641/30000 (12.1367%),                 avg. length: 1553.1,                last time consumption/overall running time: 196.4849s / 55278.1917 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1569
env0_second_0:                 episode reward: -2.4500,                 loss: -0.0224
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 3661/30000 (12.2033%),                 avg. length: 1562.4,                last time consumption/overall running time: 200.5313s / 55478.7230 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.2035
env0_second_0:                 episode reward: -2.7000,                 loss: -0.1411
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 3681/30000 (12.2700%),                 avg. length: 1734.55,                last time consumption/overall running time: 218.8636s / 55697.5866 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.2053
env0_second_0:                 episode reward: -3.0500,                 loss: -0.1139
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 3701/30000 (12.3367%),                 avg. length: 1575.85,                last time consumption/overall running time: 201.6483s / 55899.2349 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.2264
env0_second_0:                 episode reward: -3.3000,                 loss: -0.0899
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 3721/30000 (12.4033%),                 avg. length: 1700.0,                last time consumption/overall running time: 208.4738s / 56107.7086 s
env0_first_0:                 episode reward: 7.2000,                 loss: -0.2046
env0_second_0:                 episode reward: -7.2000,                 loss: -0.1355
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 3741/30000 (12.4700%),                 avg. length: 1708.75,                last time consumption/overall running time: 214.3868s / 56322.0955 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.1854
env0_second_0:                 episode reward: -5.4500,                 loss: -0.1087
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 3761/30000 (12.5367%),                 avg. length: 2012.25,                last time consumption/overall running time: 257.0627s / 56579.1582 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.1992
env0_second_0:                 episode reward: -5.9000,                 loss: -0.1205
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 3781/30000 (12.6033%),                 avg. length: 2027.4,                last time consumption/overall running time: 257.4202s / 56836.5784 s
env0_first_0:                 episode reward: 5.0000,                 loss: -0.2078
env0_second_0:                 episode reward: -5.0000,                 loss: -0.1555
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 3801/30000 (12.6700%),                 avg. length: 1709.0,                last time consumption/overall running time: 214.6483s / 57051.2268 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.2151
env0_second_0:                 episode reward: -4.3000,                 loss: -0.1211
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 3821/30000 (12.7367%),                 avg. length: 1770.7,                last time consumption/overall running time: 225.0813s / 57276.3080 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.1894
env0_second_0:                 episode reward: -5.6000,                 loss: -0.0741
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 3841/30000 (12.8033%),                 avg. length: 1779.5,                last time consumption/overall running time: 219.1399s / 57495.4479 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.1849
env0_second_0:                 episode reward: -5.0500,                 loss: -0.1391
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 3861/30000 (12.8700%),                 avg. length: 1728.7,                last time consumption/overall running time: 218.7342s / 57714.1822 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.1954
env0_second_0:                 episode reward: -3.8500,                 loss: -0.0993
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 3881/30000 (12.9367%),                 avg. length: 1728.75,                last time consumption/overall running time: 208.5702s / 57922.7524 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.2137
env0_second_0:                 episode reward: -3.0000,                 loss: -0.1443
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 3901/30000 (13.0033%),                 avg. length: 1556.05,                last time consumption/overall running time: 197.3879s / 58120.1403 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2450
env0_second_0:                 episode reward: -2.1500,                 loss: -0.1591
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 3921/30000 (13.0700%),                 avg. length: 1639.7,                last time consumption/overall running time: 190.3976s / 58310.5379 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2135
env0_second_0:                 episode reward: -2.6500,                 loss: -0.0167
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 3941/30000 (13.1367%),                 avg. length: 1730.85,                last time consumption/overall running time: 218.1113s / 58528.6492 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.1910
env0_second_0:                 episode reward: -3.0500,                 loss: -0.0955
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 3961/30000 (13.2033%),                 avg. length: 2053.85,                last time consumption/overall running time: 249.1527s / 58777.8019 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.1695
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0720
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 3981/30000 (13.2700%),                 avg. length: 1615.1,                last time consumption/overall running time: 190.4725s / 58968.2743 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.2342
env0_second_0:                 episode reward: -4.6500,                 loss: -0.1032
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 4001/30000 (13.3367%),                 avg. length: 1822.85,                last time consumption/overall running time: 224.0918s / 59192.3661 s
env0_first_0:                 episode reward: 12.2000,                 loss: -0.1346
env0_second_0:                 episode reward: -12.2000,                 loss: -0.0373
env1_first_0:                 episode reward: 10.8500,                 loss: nan
env1_second_0:                 episode reward: -10.8500,                 loss: nan
Episode: 4021/30000 (13.4033%),                 avg. length: 1614.9,                last time consumption/overall running time: 198.0739s / 59390.4401 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.2152
env0_second_0:                 episode reward: -6.6500,                 loss: -0.1365
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 4041/30000 (13.4700%),                 avg. length: 1572.8,                last time consumption/overall running time: 197.0724s / 59587.5125 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.2214
env0_second_0:                 episode reward: -2.9000,                 loss: -0.1675
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 4061/30000 (13.5367%),                 avg. length: 1515.65,                last time consumption/overall running time: 197.3670s / 59784.8795 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2454
env0_second_0:                 episode reward: -1.6000,                 loss: -0.1707
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 4081/30000 (13.6033%),                 avg. length: 1654.95,                last time consumption/overall running time: 207.6545s / 59992.5339 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1937
env0_second_0:                 episode reward: -1.8500,                 loss: -0.0862
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 4101/30000 (13.6700%),                 avg. length: 1483.85,                last time consumption/overall running time: 186.3718s / 60178.9057 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2190
env0_second_0:                 episode reward: -1.6000,                 loss: -0.1338
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 4121/30000 (13.7367%),                 avg. length: 1629.45,                last time consumption/overall running time: 200.9427s / 60379.8484 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2221
env0_second_0:                 episode reward: -1.3000,                 loss: -0.0892
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4141/30000 (13.8033%),                 avg. length: 1519.8,                last time consumption/overall running time: 189.9206s / 60569.7690 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2184
env0_second_0:                 episode reward: -2.2500,                 loss: -0.1205
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4161/30000 (13.8700%),                 avg. length: 1880.0,                last time consumption/overall running time: 238.2944s / 60808.0634 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2347
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0949
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 4181/30000 (13.9367%),                 avg. length: 1691.95,                last time consumption/overall running time: 212.0179s / 61020.0813 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2306
env0_second_0:                 episode reward: -1.7500,                 loss: -0.0929
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 4201/30000 (14.0033%),                 avg. length: 1658.85,                last time consumption/overall running time: 208.6508s / 61228.7321 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2412
env0_second_0:                 episode reward: -2.0000,                 loss: -0.1306
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 4221/30000 (14.0700%),                 avg. length: 2368.9,                last time consumption/overall running time: 293.1677s / 61521.8999 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.2042
env0_second_0:                 episode reward: 2.6000,                 loss: -0.0983
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 4241/30000 (14.1367%),                 avg. length: 1541.85,                last time consumption/overall running time: 196.8131s / 61718.7129 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2288
env0_second_0:                 episode reward: -1.7500,                 loss: -0.0667
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 4261/30000 (14.2033%),                 avg. length: 1557.05,                last time consumption/overall running time: 176.5861s / 61895.2990 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2546
env0_second_0:                 episode reward: -2.0000,                 loss: -0.1184
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 4281/30000 (14.2700%),                 avg. length: 1596.0,                last time consumption/overall running time: 201.9248s / 62097.2238 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.2178
env0_second_0:                 episode reward: -2.7000,                 loss: -0.1062
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 4301/30000 (14.3367%),                 avg. length: 1634.75,                last time consumption/overall running time: 207.4931s / 62304.7168 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2549
env0_second_0:                 episode reward: -2.6500,                 loss: -0.1775
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 4321/30000 (14.4033%),                 avg. length: 1585.0,                last time consumption/overall running time: 200.2665s / 62504.9833 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2542
env0_second_0:                 episode reward: -2.6500,                 loss: -0.1650
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 4341/30000 (14.4700%),                 avg. length: 1536.6,                last time consumption/overall running time: 194.1681s / 62699.1514 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.1915
env0_second_0:                 episode reward: -2.7000,                 loss: -0.1161
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 4361/30000 (14.5367%),                 avg. length: 1475.1,                last time consumption/overall running time: 183.0131s / 62882.1645 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.2320
env0_second_0:                 episode reward: -2.5000,                 loss: -0.1368
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 4381/30000 (14.6033%),                 avg. length: 1504.55,                last time consumption/overall running time: 190.7277s / 63072.8923 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2101
env0_second_0:                 episode reward: -1.8500,                 loss: -0.1225
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 4401/30000 (14.6700%),                 avg. length: 1568.35,                last time consumption/overall running time: 194.2570s / 63267.1492 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.2208
env0_second_0:                 episode reward: -3.0500,                 loss: -0.1522
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 4421/30000 (14.7367%),                 avg. length: 1502.65,                last time consumption/overall running time: 191.8795s / 63459.0287 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2291
env0_second_0:                 episode reward: -0.8500,                 loss: -0.1621
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 4441/30000 (14.8033%),                 avg. length: 1524.45,                last time consumption/overall running time: 189.7757s / 63648.8044 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2183
env0_second_0:                 episode reward: -1.9000,                 loss: -0.1457
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 4461/30000 (14.8700%),                 avg. length: 1543.8,                last time consumption/overall running time: 186.1371s / 63834.9415 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2726
env0_second_0:                 episode reward: -1.7000,                 loss: -0.2090
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 4481/30000 (14.9367%),                 avg. length: 1607.75,                last time consumption/overall running time: 203.9422s / 64038.8837 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.2495
env0_second_0:                 episode reward: -2.7500,                 loss: -0.1910
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 4501/30000 (15.0033%),                 avg. length: 1522.9,                last time consumption/overall running time: 185.6229s / 64224.5065 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2176
env0_second_0:                 episode reward: -1.7000,                 loss: -0.1513
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 4521/30000 (15.0700%),                 avg. length: 1626.95,                last time consumption/overall running time: 197.5319s / 64422.0384 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2117
env0_second_0:                 episode reward: -1.8000,                 loss: -0.1409
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 4541/30000 (15.1367%),                 avg. length: 1572.4,                last time consumption/overall running time: 189.1520s / 64611.1904 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2413
env0_second_0:                 episode reward: -1.9000,                 loss: -0.1675
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 4561/30000 (15.2033%),                 avg. length: 1611.7,                last time consumption/overall running time: 194.0064s / 64805.1968 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2277
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0690
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 4581/30000 (15.2700%),                 avg. length: 1657.2,                last time consumption/overall running time: 207.3596s / 65012.5564 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2474
env0_second_0:                 episode reward: -1.8000,                 loss: -0.1643
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 4601/30000 (15.3367%),                 avg. length: 1615.4,                last time consumption/overall running time: 197.8015s / 65210.3579 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2371
env0_second_0:                 episode reward: -1.5000,                 loss: -0.1601
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 4621/30000 (15.4033%),                 avg. length: 1542.7,                last time consumption/overall running time: 196.4339s / 65406.7918 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2512
env0_second_0:                 episode reward: -1.9000,                 loss: -0.1180
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 4641/30000 (15.4700%),                 avg. length: 1573.35,                last time consumption/overall running time: 201.3915s / 65608.1834 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2639
env0_second_0:                 episode reward: -1.0500,                 loss: -0.1471
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 4661/30000 (15.5367%),                 avg. length: 1505.2,                last time consumption/overall running time: 190.9171s / 65799.1004 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.2526
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0343
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 4681/30000 (15.6033%),                 avg. length: 1520.7,                last time consumption/overall running time: 189.5755s / 65988.6759 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2248
env0_second_0:                 episode reward: -1.1500,                 loss: -0.0844
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 4701/30000 (15.6700%),                 avg. length: 1518.1,                last time consumption/overall running time: 184.9006s / 66173.5765 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2282
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0993
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 4721/30000 (15.7367%),                 avg. length: 1460.75,                last time consumption/overall running time: 186.2032s / 66359.7796 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.2296
env0_second_0:                 episode reward: -1.0000,                 loss: -0.1125
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 4741/30000 (15.8033%),                 avg. length: 1658.0,                last time consumption/overall running time: 204.3924s / 66564.1720 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.2029
env0_second_0:                 episode reward: -2.1000,                 loss: -0.0589
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 4761/30000 (15.8700%),                 avg. length: 1830.1,                last time consumption/overall running time: 227.8016s / 66791.9737 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1930
env0_second_0:                 episode reward: -2.5000,                 loss: -0.0997
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 4781/30000 (15.9367%),                 avg. length: 1544.8,                last time consumption/overall running time: 192.2069s / 66984.1806 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.2673
env0_second_0:                 episode reward: -2.5000,                 loss: -0.1567
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 4801/30000 (16.0033%),                 avg. length: 1536.4,                last time consumption/overall running time: 195.7619s / 67179.9424 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.2513
env0_second_0:                 episode reward: -2.2000,                 loss: 0.0533
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4821/30000 (16.0700%),                 avg. length: 1621.45,                last time consumption/overall running time: 202.3865s / 67382.3290 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2463
env0_second_0:                 episode reward: -2.3000,                 loss: -0.1176
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 4841/30000 (16.1367%),                 avg. length: 1692.15,                last time consumption/overall running time: 213.5197s / 67595.8487 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.2065
env0_second_0:                 episode reward: -1.4500,                 loss: -0.0849
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 4861/30000 (16.2033%),                 avg. length: 1825.1,                last time consumption/overall running time: 223.3195s / 67819.1681 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.2120
env0_second_0:                 episode reward: -3.3500,                 loss: -0.0903
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4881/30000 (16.2700%),                 avg. length: 1687.3,                last time consumption/overall running time: 211.3624s / 68030.5305 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2018
env0_second_0:                 episode reward: -1.8000,                 loss: -0.0585
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 4901/30000 (16.3367%),                 avg. length: 1590.3,                last time consumption/overall running time: 199.5617s / 68230.0922 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.2172
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0815
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 4921/30000 (16.4033%),                 avg. length: 1543.8,                last time consumption/overall running time: 194.1964s / 68424.2887 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1756
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0142
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4941/30000 (16.4700%),                 avg. length: 1635.65,                last time consumption/overall running time: 203.7323s / 68628.0210 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2022
env0_second_0:                 episode reward: -1.7500,                 loss: -0.0705
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 4961/30000 (16.5367%),                 avg. length: 1672.6,                last time consumption/overall running time: 211.8619s / 68839.8829 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2100
env0_second_0:                 episode reward: -1.8500,                 loss: -0.0980
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 4981/30000 (16.6033%),                 avg. length: 1599.2,                last time consumption/overall running time: 186.2846s / 69026.1675 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.2067
env0_second_0:                 episode reward: -0.9000,                 loss: -0.0895
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 5001/30000 (16.6700%),                 avg. length: 1744.65,                last time consumption/overall running time: 216.7369s / 69242.9044 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.2887
env0_second_0:                 episode reward: -5.0500,                 loss: 0.0321
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 5021/30000 (16.7367%),                 avg. length: 1790.45,                last time consumption/overall running time: 210.5148s / 69453.4193 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.1790
env0_second_0:                 episode reward: -9.7000,                 loss: 0.1142
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 5041/30000 (16.8033%),                 avg. length: 1902.15,                last time consumption/overall running time: 239.5525s / 69692.9717 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.1634
env0_second_0:                 episode reward: -6.8000,                 loss: 0.0552
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 5061/30000 (16.8700%),                 avg. length: 1638.05,                last time consumption/overall running time: 206.2285s / 69899.2002 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1969
env0_second_0:                 episode reward: -2.9500,                 loss: -0.0656
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 5081/30000 (16.9367%),                 avg. length: 1499.8,                last time consumption/overall running time: 189.8865s / 70089.0867 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2202
env0_second_0:                 episode reward: -2.3500,                 loss: -0.1269
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 5101/30000 (17.0033%),                 avg. length: 1603.1,                last time consumption/overall running time: 192.3577s / 70281.4444 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2320
env0_second_0:                 episode reward: -2.1500,                 loss: -0.1569
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 5121/30000 (17.0700%),                 avg. length: 1487.4,                last time consumption/overall running time: 185.3305s / 70466.7749 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.2546
env0_second_0:                 episode reward: -2.5000,                 loss: -0.1286
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 5141/30000 (17.1367%),                 avg. length: 1491.5,                last time consumption/overall running time: 180.8020s / 70647.5769 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.2437
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0286
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 5161/30000 (17.2033%),                 avg. length: 1469.6,                last time consumption/overall running time: 182.5297s / 70830.1066 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.2534
env0_second_0:                 episode reward: -2.7500,                 loss: -0.1129
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 5181/30000 (17.2700%),                 avg. length: 1445.95,                last time consumption/overall running time: 184.5206s / 71014.6272 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.2294
env0_second_0:                 episode reward: -1.2500,                 loss: -0.1482
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5201/30000 (17.3367%),                 avg. length: 1482.3,                last time consumption/overall running time: 186.7442s / 71201.3714 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.1681
env0_second_0:                 episode reward: -0.6500,                 loss: -0.0567
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 5221/30000 (17.4033%),                 avg. length: 1496.55,                last time consumption/overall running time: 190.5286s / 71391.9000 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1922
env0_second_0:                 episode reward: -2.4500,                 loss: -0.0947
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 5241/30000 (17.4700%),                 avg. length: 1474.65,                last time consumption/overall running time: 186.9001s / 71578.8001 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.2496
env0_second_0:                 episode reward: -2.6000,                 loss: 0.2280
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 5261/30000 (17.5367%),                 avg. length: 1479.6,                last time consumption/overall running time: 191.5765s / 71770.3766 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2646
env0_second_0:                 episode reward: -1.9000,                 loss: -0.1011
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 5281/30000 (17.6033%),                 avg. length: 1460.65,                last time consumption/overall running time: 188.7438s / 71959.1205 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2436
env0_second_0:                 episode reward: -1.7500,                 loss: -0.1278
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 5301/30000 (17.6700%),                 avg. length: 1467.95,                last time consumption/overall running time: 176.2828s / 72135.4032 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2376
env0_second_0:                 episode reward: -1.8500,                 loss: -0.1219
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 5321/30000 (17.7367%),                 avg. length: 1480.55,                last time consumption/overall running time: 191.4368s / 72326.8400 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2677
env0_second_0:                 episode reward: -2.3500,                 loss: -0.0634
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 5341/30000 (17.8033%),                 avg. length: 1441.75,                last time consumption/overall running time: 183.2384s / 72510.0784 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.2030
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0260
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 5361/30000 (17.8700%),                 avg. length: 1488.9,                last time consumption/overall running time: 190.2785s / 72700.3569 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2354
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0009
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 5381/30000 (17.9367%),                 avg. length: 1482.25,                last time consumption/overall running time: 181.4616s / 72881.8185 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2151
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0436
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 5401/30000 (18.0033%),                 avg. length: 1615.8,                last time consumption/overall running time: 208.7458s / 73090.5644 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.2306
env0_second_0:                 episode reward: -4.4000,                 loss: 0.3342
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 5421/30000 (18.0700%),                 avg. length: 1545.05,                last time consumption/overall running time: 199.5134s / 73290.0778 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2381
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0868
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5441/30000 (18.1367%),                 avg. length: 1492.3,                last time consumption/overall running time: 194.0273s / 73484.1050 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2085
env0_second_0:                 episode reward: -0.4500,                 loss: 0.3133
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 5461/30000 (18.2033%),                 avg. length: 1450.9,                last time consumption/overall running time: 182.2085s / 73666.3136 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.1561
env0_second_0:                 episode reward: 0.0500,                 loss: -0.0133
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 5481/30000 (18.2700%),                 avg. length: 1461.55,                last time consumption/overall running time: 189.9402s / 73856.2538 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1925
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0993
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 5501/30000 (18.3367%),                 avg. length: 1422.55,                last time consumption/overall running time: 176.4735s / 74032.7273 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.1490
env0_second_0:                 episode reward: 1.8000,                 loss: 0.1936
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 5521/30000 (18.4033%),                 avg. length: 1434.45,                last time consumption/overall running time: 182.1903s / 74214.9176 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1559
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0840
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 5541/30000 (18.4700%),                 avg. length: 1623.9,                last time consumption/overall running time: 204.1201s / 74419.0377 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1623
env0_second_0:                 episode reward: -4.6000,                 loss: 0.2003
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 5561/30000 (18.5367%),                 avg. length: 1476.65,                last time consumption/overall running time: 177.9589s / 74596.9966 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2411
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0332
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 5581/30000 (18.6033%),                 avg. length: 1434.45,                last time consumption/overall running time: 182.6200s / 74779.6166 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.2258
env0_second_0:                 episode reward: -2.8500,                 loss: 0.1065
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 5601/30000 (18.6700%),                 avg. length: 1433.3,                last time consumption/overall running time: 177.4987s / 74957.1154 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2204
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0406
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 5621/30000 (18.7367%),                 avg. length: 1451.6,                last time consumption/overall running time: 184.2410s / 75141.3564 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2775
env0_second_0:                 episode reward: -1.3500,                 loss: -0.1010
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 5641/30000 (18.8033%),                 avg. length: 1438.9,                last time consumption/overall running time: 182.8638s / 75324.2202 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.2071
env0_second_0:                 episode reward: -1.6500,                 loss: -0.0517
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 5661/30000 (18.8700%),                 avg. length: 1654.1,                last time consumption/overall running time: 200.4140s / 75524.6342 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1848
env0_second_0:                 episode reward: -2.2500,                 loss: 0.1091
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 5681/30000 (18.9367%),                 avg. length: 1614.1,                last time consumption/overall running time: 205.2802s / 75729.9144 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1690
env0_second_0:                 episode reward: -1.2000,                 loss: 0.1117
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 5701/30000 (19.0033%),                 avg. length: 1484.8,                last time consumption/overall running time: 190.3025s / 75920.2168 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2046
env0_second_0:                 episode reward: -0.6500,                 loss: 0.1057
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 5721/30000 (19.0700%),                 avg. length: 1503.25,                last time consumption/overall running time: 190.4388s / 76110.6556 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1983
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0032
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 5741/30000 (19.1367%),                 avg. length: 1505.6,                last time consumption/overall running time: 193.3237s / 76303.9793 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.2454
env0_second_0:                 episode reward: -2.5500,                 loss: -0.0106
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 5761/30000 (19.2033%),                 avg. length: 1474.55,                last time consumption/overall running time: 159.7097s / 76463.6889 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.2143
env0_second_0:                 episode reward: -1.4500,                 loss: -0.0703
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 5781/30000 (19.2700%),                 avg. length: 1463.3,                last time consumption/overall running time: 188.8627s / 76652.5516 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.2326
env0_second_0:                 episode reward: -2.4000,                 loss: -0.0991
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 5801/30000 (19.3367%),                 avg. length: 1433.05,                last time consumption/overall running time: 172.5575s / 76825.1091 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2425
env0_second_0:                 episode reward: -1.3500,                 loss: -0.1270
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 5821/30000 (19.4033%),                 avg. length: 1454.1,                last time consumption/overall running time: 186.0060s / 77011.1150 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2290
env0_second_0:                 episode reward: -1.7500,                 loss: -0.0017
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 5841/30000 (19.4700%),                 avg. length: 1433.8,                last time consumption/overall running time: 162.1639s / 77173.2789 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2398
env0_second_0:                 episode reward: -1.5500,                 loss: 0.3737
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 5861/30000 (19.5367%),                 avg. length: 1426.7,                last time consumption/overall running time: 168.9682s / 77342.2471 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2414
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0263
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 5881/30000 (19.6033%),                 avg. length: 1477.6,                last time consumption/overall running time: 178.1983s / 77520.4455 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2229
env0_second_0:                 episode reward: -1.3500,                 loss: -0.0179
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 5901/30000 (19.6700%),                 avg. length: 1426.65,                last time consumption/overall running time: 183.1036s / 77703.5491 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2409
env0_second_0:                 episode reward: -1.9500,                 loss: -0.0513
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5921/30000 (19.7367%),                 avg. length: 1515.25,                last time consumption/overall running time: 193.4773s / 77897.0263 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.1590
env0_second_0:                 episode reward: -0.8000,                 loss: 0.1256
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5941/30000 (19.8033%),                 avg. length: 1523.4,                last time consumption/overall running time: 189.5360s / 78086.5623 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1841
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0445
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 5961/30000 (19.8700%),                 avg. length: 1479.45,                last time consumption/overall running time: 187.7913s / 78274.3536 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.2493
env0_second_0:                 episode reward: -2.7500,                 loss: -0.0799
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 5981/30000 (19.9367%),                 avg. length: 1445.15,                last time consumption/overall running time: 183.3076s / 78457.6613 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2579
env0_second_0:                 episode reward: -2.3500,                 loss: 0.1216
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 6001/30000 (20.0033%),                 avg. length: 1426.35,                last time consumption/overall running time: 182.1273s / 78639.7886 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2249
env0_second_0:                 episode reward: -2.3000,                 loss: -0.0783
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 6021/30000 (20.0700%),                 avg. length: 1416.2,                last time consumption/overall running time: 180.2144s / 78820.0030 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2192
env0_second_0:                 episode reward: -1.9000,                 loss: -0.0759
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 6041/30000 (20.1367%),                 avg. length: 1425.7,                last time consumption/overall running time: 182.6049s / 79002.6078 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2291
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0258
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 6061/30000 (20.2033%),                 avg. length: 1460.9,                last time consumption/overall running time: 186.8151s / 79189.4229 s
env0_first_0:                 episode reward: 5.5500,                 loss: -0.1390
env0_second_0:                 episode reward: -5.5500,                 loss: 0.0532
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 6081/30000 (20.2700%),                 avg. length: 1423.2,                last time consumption/overall running time: 181.1458s / 79370.5688 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1931
env0_second_0:                 episode reward: -2.4500,                 loss: -0.0605
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 6101/30000 (20.3367%),                 avg. length: 1427.25,                last time consumption/overall running time: 180.6841s / 79551.2528 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2227
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0719
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 6121/30000 (20.4033%),                 avg. length: 1604.8,                last time consumption/overall running time: 178.0594s / 79729.3123 s
env0_first_0:                 episode reward: 7.6500,                 loss: -0.1485
env0_second_0:                 episode reward: -7.6500,                 loss: 0.0441
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 6141/30000 (20.4700%),                 avg. length: 1733.5,                last time consumption/overall running time: 217.2916s / 79946.6039 s
env0_first_0:                 episode reward: 12.6500,                 loss: -0.1905
env0_second_0:                 episode reward: -12.6500,                 loss: 0.1481
env1_first_0:                 episode reward: 12.2500,                 loss: nan
env1_second_0:                 episode reward: -12.2500,                 loss: nan
Episode: 6161/30000 (20.5367%),                 avg. length: 1503.7,                last time consumption/overall running time: 189.8982s / 80136.5021 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.2212
env0_second_0:                 episode reward: -5.1000,                 loss: 0.0070
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 6181/30000 (20.6033%),                 avg. length: 1474.4,                last time consumption/overall running time: 184.5078s / 80321.0099 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.2408
env0_second_0:                 episode reward: -2.8000,                 loss: -0.0004
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 6201/30000 (20.6700%),                 avg. length: 1450.0,                last time consumption/overall running time: 183.7106s / 80504.7204 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2524
env0_second_0:                 episode reward: -2.0000,                 loss: -0.1033
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 6221/30000 (20.7367%),                 avg. length: 1433.0,                last time consumption/overall running time: 177.2136s / 80681.9340 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.1860
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0845
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 6241/30000 (20.8033%),                 avg. length: 1491.45,                last time consumption/overall running time: 188.3333s / 80870.2673 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.2445
env0_second_0:                 episode reward: -3.1500,                 loss: 0.1924
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 6261/30000 (20.8700%),                 avg. length: 1487.3,                last time consumption/overall running time: 189.9432s / 81060.2105 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2938
env0_second_0:                 episode reward: -1.6000,                 loss: -0.1498
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 6281/30000 (20.9367%),                 avg. length: 1442.8,                last time consumption/overall running time: 184.1977s / 81244.4082 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.2481
env0_second_0:                 episode reward: -1.2500,                 loss: -0.1387
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 6301/30000 (21.0033%),                 avg. length: 1507.0,                last time consumption/overall running time: 185.8974s / 81430.3056 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1930
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0408
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 6321/30000 (21.0700%),                 avg. length: 1605.25,                last time consumption/overall running time: 191.6886s / 81621.9942 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.2239
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0398
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 6341/30000 (21.1367%),                 avg. length: 1480.3,                last time consumption/overall running time: 188.7697s / 81810.7639 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2470
env0_second_0:                 episode reward: -1.5000,                 loss: -0.0050
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 6361/30000 (21.2033%),                 avg. length: 1472.15,                last time consumption/overall running time: 186.1404s / 81996.9043 s
env0_first_0:                 episode reward: 10.1000,                 loss: -0.1116
env0_second_0:                 episode reward: -10.1000,                 loss: 0.1299
env1_first_0:                 episode reward: 10.2500,                 loss: nan
env1_second_0:                 episode reward: -10.2500,                 loss: nan
Episode: 6381/30000 (21.2700%),                 avg. length: 1443.2,                last time consumption/overall running time: 181.6683s / 82178.5726 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1990
env0_second_0:                 episode reward: -1.6000,                 loss: -0.0293
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 6401/30000 (21.3367%),                 avg. length: 1438.35,                last time consumption/overall running time: 179.9950s / 82358.5677 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1858
env0_second_0:                 episode reward: -3.9000,                 loss: -0.0133
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 6421/30000 (21.4033%),                 avg. length: 1444.1,                last time consumption/overall running time: 178.8347s / 82537.4023 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1673
env0_second_0:                 episode reward: -1.8000,                 loss: 0.1108
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 6441/30000 (21.4700%),                 avg. length: 1436.0,                last time consumption/overall running time: 185.9600s / 82723.3623 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2048
env0_second_0:                 episode reward: -1.3000,                 loss: 0.1176
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 6461/30000 (21.5367%),                 avg. length: 1486.05,                last time consumption/overall running time: 187.5308s / 82910.8931 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1532
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0874
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 6481/30000 (21.6033%),                 avg. length: 1460.55,                last time consumption/overall running time: 179.2420s / 83090.1351 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2000
env0_second_0:                 episode reward: -2.3500,                 loss: 0.1614
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 6501/30000 (21.6700%),                 avg. length: 1744.9,                last time consumption/overall running time: 216.9401s / 83307.0752 s
env0_first_0:                 episode reward: -4.3500,                 loss: -0.0093
env0_second_0:                 episode reward: 4.3500,                 loss: 0.3509
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 6521/30000 (21.7367%),                 avg. length: 1579.4,                last time consumption/overall running time: 201.0139s / 83508.0891 s
env0_first_0:                 episode reward: 7.3500,                 loss: -0.0750
env0_second_0:                 episode reward: -7.3500,                 loss: 0.2814
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 6541/30000 (21.8033%),                 avg. length: 1442.55,                last time consumption/overall running time: 181.5442s / 83689.6332 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.2373
env0_second_0:                 episode reward: -0.9500,                 loss: 0.1036
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 6561/30000 (21.8700%),                 avg. length: 1433.1,                last time consumption/overall running time: 182.6124s / 83872.2456 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2551
env0_second_0:                 episode reward: -2.6500,                 loss: 0.1261
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 6581/30000 (21.9367%),                 avg. length: 1444.15,                last time consumption/overall running time: 175.7957s / 84048.0413 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2504
env0_second_0:                 episode reward: -2.2500,                 loss: 0.1768
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 6601/30000 (22.0033%),                 avg. length: 1486.55,                last time consumption/overall running time: 186.1873s / 84234.2286 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2231
env0_second_0:                 episode reward: -0.4000,                 loss: 3.8248
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 6621/30000 (22.0700%),                 avg. length: 1438.25,                last time consumption/overall running time: 177.0234s / 84411.2520 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2579
env0_second_0:                 episode reward: -1.7500,                 loss: 0.2759
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 6641/30000 (22.1367%),                 avg. length: 1477.05,                last time consumption/overall running time: 182.5393s / 84593.7913 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1700
env0_second_0:                 episode reward: 1.8500,                 loss: 0.3626
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 6661/30000 (22.2033%),                 avg. length: 1455.95,                last time consumption/overall running time: 183.8985s / 84777.6899 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1766
env0_second_0:                 episode reward: 1.3000,                 loss: 0.1700
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 6681/30000 (22.2700%),                 avg. length: 1445.55,                last time consumption/overall running time: 181.3754s / 84959.0652 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.1152
env0_second_0:                 episode reward: 2.9000,                 loss: 0.5730
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 6701/30000 (22.3367%),                 avg. length: 1632.7,                last time consumption/overall running time: 208.8499s / 85167.9151 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.2233
env0_second_0:                 episode reward: 0.7000,                 loss: 0.1435
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 6721/30000 (22.4033%),                 avg. length: 1532.4,                last time consumption/overall running time: 193.2530s / 85361.1682 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2066
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0613
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 6741/30000 (22.4700%),                 avg. length: 1420.9,                last time consumption/overall running time: 183.0999s / 85544.2680 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.2470
env0_second_0:                 episode reward: -2.6000,                 loss: 0.2624
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 6761/30000 (22.5367%),                 avg. length: 1448.55,                last time consumption/overall running time: 183.3158s / 85727.5838 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.2413
env0_second_0:                 episode reward: -2.8500,                 loss: -0.0426
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 6781/30000 (22.6033%),                 avg. length: 1515.25,                last time consumption/overall running time: 192.2613s / 85919.8451 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.1582
env0_second_0:                 episode reward: -2.9000,                 loss: 0.1146
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 6801/30000 (22.6700%),                 avg. length: 1492.3,                last time consumption/overall running time: 179.0852s / 86098.9304 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1675
env0_second_0:                 episode reward: -2.7500,                 loss: 0.5114
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 6821/30000 (22.7367%),                 avg. length: 1448.95,                last time consumption/overall running time: 182.2499s / 86281.1802 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2518
env0_second_0:                 episode reward: -2.0500,                 loss: 0.1487
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 6841/30000 (22.8033%),                 avg. length: 1552.0,                last time consumption/overall running time: 198.3882s / 86479.5685 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.1929
env0_second_0:                 episode reward: -2.8000,                 loss: 0.2140
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 6861/30000 (22.8700%),                 avg. length: 1466.15,                last time consumption/overall running time: 186.4276s / 86665.9961 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.2130
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0554
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 6881/30000 (22.9367%),                 avg. length: 1472.7,                last time consumption/overall running time: 187.3530s / 86853.3491 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2287
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0282
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 6901/30000 (23.0033%),                 avg. length: 1485.8,                last time consumption/overall running time: 189.8486s / 87043.1976 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2270
env0_second_0:                 episode reward: -1.9500,                 loss: 0.1579
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 6921/30000 (23.0700%),                 avg. length: 1474.3,                last time consumption/overall running time: 187.6146s / 87230.8122 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2221
env0_second_0:                 episode reward: -1.9000,                 loss: 0.0273
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 6941/30000 (23.1367%),                 avg. length: 1630.65,                last time consumption/overall running time: 196.9178s / 87427.7300 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1721
env0_second_0:                 episode reward: -2.7500,                 loss: 0.1281
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 6961/30000 (23.2033%),                 avg. length: 1726.3,                last time consumption/overall running time: 198.9374s / 87626.6674 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1655
env0_second_0:                 episode reward: -0.0500,                 loss: 0.6323
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 6981/30000 (23.2700%),                 avg. length: 1826.3,                last time consumption/overall running time: 220.8128s / 87847.4803 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1120
env0_second_0:                 episode reward: -2.6500,                 loss: 0.4017
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 7001/30000 (23.3367%),                 avg. length: 1708.15,                last time consumption/overall running time: 216.9304s / 88064.4107 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.1591
env0_second_0:                 episode reward: -4.8500,                 loss: 0.5334
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 7021/30000 (23.4033%),                 avg. length: 1716.45,                last time consumption/overall running time: 217.6414s / 88282.0520 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1318
env0_second_0:                 episode reward: -2.4500,                 loss: 0.2530
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 7041/30000 (23.4700%),                 avg. length: 1554.3,                last time consumption/overall running time: 175.4885s / 88457.5405 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1634
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0948
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 7061/30000 (23.5367%),                 avg. length: 1488.05,                last time consumption/overall running time: 189.5882s / 88647.1287 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1869
env0_second_0:                 episode reward: -4.6000,                 loss: 0.0532
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 7081/30000 (23.6033%),                 avg. length: 1454.8,                last time consumption/overall running time: 175.0412s / 88822.1699 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2405
env0_second_0:                 episode reward: -2.0500,                 loss: -0.0540
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 7101/30000 (23.6700%),                 avg. length: 1437.4,                last time consumption/overall running time: 170.9524s / 88993.1223 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2665
env0_second_0:                 episode reward: -1.7500,                 loss: -0.1186
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 7121/30000 (23.7367%),                 avg. length: 1631.95,                last time consumption/overall running time: 198.0175s / 89191.1398 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1975
env0_second_0:                 episode reward: -1.3000,                 loss: -0.0261
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 7141/30000 (23.8033%),                 avg. length: 1657.75,                last time consumption/overall running time: 209.6978s / 89400.8376 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.1668
env0_second_0:                 episode reward: -4.5500,                 loss: 0.0317
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 7161/30000 (23.8700%),                 avg. length: 1824.95,                last time consumption/overall running time: 227.2928s / 89628.1304 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.1471
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0777
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 7181/30000 (23.9367%),                 avg. length: 1696.55,                last time consumption/overall running time: 214.0659s / 89842.1964 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.1563
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0454
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 7201/30000 (24.0033%),                 avg. length: 1666.35,                last time consumption/overall running time: 209.7184s / 90051.9148 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1808
env0_second_0:                 episode reward: -2.3500,                 loss: -0.0287
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 7221/30000 (24.0700%),                 avg. length: 1709.85,                last time consumption/overall running time: 207.5913s / 90259.5061 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.1256
env0_second_0:                 episode reward: -3.4500,                 loss: 0.1221
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 7241/30000 (24.1367%),                 avg. length: 1531.65,                last time consumption/overall running time: 189.8852s / 90449.3913 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1959
env0_second_0:                 episode reward: -1.2500,                 loss: 0.7207
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 7261/30000 (24.2033%),                 avg. length: 1525.65,                last time consumption/overall running time: 171.6475s / 90621.0388 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2600
env0_second_0:                 episode reward: -1.8000,                 loss: 0.4207
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 7281/30000 (24.2700%),                 avg. length: 1684.3,                last time consumption/overall running time: 213.9297s / 90834.9685 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.1891
env0_second_0:                 episode reward: -0.8000,                 loss: 0.4133
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 7301/30000 (24.3367%),                 avg. length: 1600.45,                last time consumption/overall running time: 198.4639s / 91033.4324 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.2157
env0_second_0:                 episode reward: -2.4000,                 loss: 0.2429
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 7321/30000 (24.4033%),                 avg. length: 1659.4,                last time consumption/overall running time: 208.8895s / 91242.3219 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2003
env0_second_0:                 episode reward: -2.0000,                 loss: 0.4258
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 7341/30000 (24.4700%),                 avg. length: 1624.05,                last time consumption/overall running time: 210.1684s / 91452.4903 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1996
env0_second_0:                 episode reward: -2.2000,                 loss: 0.3980
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 7361/30000 (24.5367%),                 avg. length: 1606.25,                last time consumption/overall running time: 202.4863s / 91654.9766 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.1632
env0_second_0:                 episode reward: 1.6500,                 loss: 0.2849
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 7381/30000 (24.6033%),                 avg. length: 1881.95,                last time consumption/overall running time: 234.6558s / 91889.6324 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.1719
env0_second_0:                 episode reward: 0.8500,                 loss: 0.5897
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 7401/30000 (24.6700%),                 avg. length: 1962.65,                last time consumption/overall running time: 247.1696s / 92136.8019 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2224
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0831
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 7421/30000 (24.7367%),                 avg. length: 1700.95,                last time consumption/overall running time: 212.9813s / 92349.7832 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.2208
env0_second_0:                 episode reward: -3.4500,                 loss: -0.0452
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 7441/30000 (24.8033%),                 avg. length: 1644.1,                last time consumption/overall running time: 211.1528s / 92560.9360 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.2008
env0_second_0:                 episode reward: -4.3000,                 loss: 0.0336
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 7461/30000 (24.8700%),                 avg. length: 1520.3,                last time consumption/overall running time: 193.0883s / 92754.0244 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.2259
env0_second_0:                 episode reward: -3.6000,                 loss: -0.0171
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 7481/30000 (24.9367%),                 avg. length: 1457.8,                last time consumption/overall running time: 182.1077s / 92936.1321 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2561
env0_second_0:                 episode reward: -1.1000,                 loss: -0.0755
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 7501/30000 (25.0033%),                 avg. length: 1482.4,                last time consumption/overall running time: 170.9464s / 93107.0785 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1833
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0518
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 7521/30000 (25.0700%),                 avg. length: 1488.5,                last time consumption/overall running time: 156.9946s / 93264.0731 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2056
env0_second_0:                 episode reward: -1.7000,                 loss: 0.1074
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 7541/30000 (25.1367%),                 avg. length: 1632.1,                last time consumption/overall running time: 206.2337s / 93470.3068 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1373
env0_second_0:                 episode reward: -3.0000,                 loss: 0.1221
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 7561/30000 (25.2033%),                 avg. length: 1766.8,                last time consumption/overall running time: 204.3169s / 93674.6237 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1825
env0_second_0:                 episode reward: -3.2500,                 loss: 0.2638
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 7581/30000 (25.2700%),                 avg. length: 1919.45,                last time consumption/overall running time: 228.1691s / 93902.7928 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.1473
env0_second_0:                 episode reward: -5.1500,                 loss: 0.1462
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 7601/30000 (25.3367%),                 avg. length: 1875.65,                last time consumption/overall running time: 237.2663s / 94140.0592 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.1319
env0_second_0:                 episode reward: -4.7000,                 loss: 0.1274
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 7621/30000 (25.4033%),                 avg. length: 1995.35,                last time consumption/overall running time: 248.7473s / 94388.8065 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.1433
env0_second_0:                 episode reward: -4.1500,                 loss: 0.1827
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 7641/30000 (25.4700%),                 avg. length: 1753.05,                last time consumption/overall running time: 217.8672s / 94606.6737 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.1282
env0_second_0:                 episode reward: -4.5500,                 loss: 0.1422
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 7661/30000 (25.5367%),                 avg. length: 2061.35,                last time consumption/overall running time: 256.5397s / 94863.2134 s
env0_first_0:                 episode reward: 5.5500,                 loss: -0.1389
env0_second_0:                 episode reward: -5.5500,                 loss: 0.7712
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 7681/30000 (25.6033%),                 avg. length: 2321.15,                last time consumption/overall running time: 291.8496s / 95155.0630 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1454
env0_second_0:                 episode reward: -2.3500,                 loss: 0.1851
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 7701/30000 (25.6700%),                 avg. length: 2105.75,                last time consumption/overall running time: 257.0438s / 95412.1068 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.1571
env0_second_0:                 episode reward: -3.0500,                 loss: 0.2356
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 7721/30000 (25.7367%),                 avg. length: 1940.7,                last time consumption/overall running time: 231.8162s / 95643.9230 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1900
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0446
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 7741/30000 (25.8033%),                 avg. length: 2090.95,                last time consumption/overall running time: 253.3635s / 95897.2865 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.1745
env0_second_0:                 episode reward: -4.7000,                 loss: -0.0140
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 7761/30000 (25.8700%),                 avg. length: 1849.95,                last time consumption/overall running time: 233.3030s / 96130.5895 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.2109
env0_second_0:                 episode reward: -2.9000,                 loss: -0.0470
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 7781/30000 (25.9367%),                 avg. length: 1898.2,                last time consumption/overall running time: 233.1147s / 96363.7042 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2054
env0_second_0:                 episode reward: -2.3000,                 loss: 0.1598
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 7801/30000 (26.0033%),                 avg. length: 1933.6,                last time consumption/overall running time: 239.2068s / 96602.9111 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1883
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0281
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 7821/30000 (26.0700%),                 avg. length: 1527.05,                last time consumption/overall running time: 192.1707s / 96795.0817 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2583
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0418
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 7841/30000 (26.1367%),                 avg. length: 1588.6,                last time consumption/overall running time: 199.0117s / 96994.0934 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2642
env0_second_0:                 episode reward: -2.0500,                 loss: 0.0116
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 7861/30000 (26.2033%),                 avg. length: 1964.45,                last time consumption/overall running time: 254.7849s / 97248.8783 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.1970
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0367
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 7881/30000 (26.2700%),                 avg. length: 1609.05,                last time consumption/overall running time: 221.9077s / 97470.7861 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.2310
env0_second_0:                 episode reward: -1.0000,                 loss: -0.0336
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 7901/30000 (26.3367%),                 avg. length: 1610.7,                last time consumption/overall running time: 199.2490s / 97670.0351 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2203
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0283
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 7921/30000 (26.4033%),                 avg. length: 1596.15,                last time consumption/overall running time: 198.5818s / 97868.6169 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1860
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0471
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 7941/30000 (26.4700%),                 avg. length: 1712.85,                last time consumption/overall running time: 216.6611s / 98085.2780 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1940
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0224
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 7961/30000 (26.5367%),                 avg. length: 1599.95,                last time consumption/overall running time: 199.8766s / 98285.1546 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.2596
env0_second_0:                 episode reward: -3.5000,                 loss: 0.1319
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 7981/30000 (26.6033%),                 avg. length: 1498.85,                last time consumption/overall running time: 188.7441s / 98473.8987 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2797
env0_second_0:                 episode reward: -1.0500,                 loss: 0.1286
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 8001/30000 (26.6700%),                 avg. length: 1502.95,                last time consumption/overall running time: 189.9544s / 98663.8530 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2568
env0_second_0:                 episode reward: -1.5500,                 loss: 0.1360
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 8021/30000 (26.7367%),                 avg. length: 1463.45,                last time consumption/overall running time: 182.1413s / 98845.9943 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2312
env0_second_0:                 episode reward: -0.6500,                 loss: 0.2968
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 8041/30000 (26.8033%),                 avg. length: 1490.85,                last time consumption/overall running time: 192.1744s / 99038.1687 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2512
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0389
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 8061/30000 (26.8700%),                 avg. length: 1620.25,                last time consumption/overall running time: 209.1591s / 99247.3277 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1277
env0_second_0:                 episode reward: 0.4500,                 loss: 0.2846
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 8081/30000 (26.9367%),                 avg. length: 1632.8,                last time consumption/overall running time: 242.7307s / 99490.0585 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1490
env0_second_0:                 episode reward: 0.0000,                 loss: 0.1629
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 8101/30000 (27.0033%),                 avg. length: 1544.6,                last time consumption/overall running time: 198.8661s / 99688.9246 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2063
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0638
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 8121/30000 (27.0700%),                 avg. length: 1519.5,                last time consumption/overall running time: 198.7207s / 99887.6453 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2321
env0_second_0:                 episode reward: -1.7000,                 loss: 0.1589
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 8141/30000 (27.1367%),                 avg. length: 1504.85,                last time consumption/overall running time: 197.5165s / 100085.1617 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.1208
env0_second_0:                 episode reward: 1.6500,                 loss: 0.3998
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 8161/30000 (27.2033%),                 avg. length: 1576.95,                last time consumption/overall running time: 206.8244s / 100291.9861 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.1116
env0_second_0:                 episode reward: 4.0000,                 loss: 0.2261
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 8181/30000 (27.2700%),                 avg. length: 1460.65,                last time consumption/overall running time: 193.4466s / 100485.4327 s
env0_first_0:                 episode reward: -3.0000,                 loss: -0.1214
env0_second_0:                 episode reward: 3.0000,                 loss: 0.1755
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 8201/30000 (27.3367%),                 avg. length: 1509.85,                last time consumption/overall running time: 196.1228s / 100681.5555 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2624
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0502
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 8221/30000 (27.4033%),                 avg. length: 1482.6,                last time consumption/overall running time: 209.3262s / 100890.8816 s
env0_first_0:                 episode reward: -2.9500,                 loss: -0.1462
env0_second_0:                 episode reward: 2.9500,                 loss: 0.2561
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 8241/30000 (27.4700%),                 avg. length: 1672.85,                last time consumption/overall running time: 216.8887s / 101107.7704 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.0798
env0_second_0:                 episode reward: 4.6000,                 loss: 0.3460
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 8261/30000 (27.5367%),                 avg. length: 1582.25,                last time consumption/overall running time: 214.4909s / 101322.2613 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.1354
env0_second_0:                 episode reward: 0.0500,                 loss: 0.1473
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 8281/30000 (27.6033%),                 avg. length: 1485.2,                last time consumption/overall running time: 200.8601s / 101523.1214 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2053
env0_second_0:                 episode reward: -0.5500,                 loss: 0.1425
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 8301/30000 (27.6700%),                 avg. length: 1626.3,                last time consumption/overall running time: 189.3524s / 101712.4738 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.1476
env0_second_0:                 episode reward: 0.8500,                 loss: 0.1725
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 8321/30000 (27.7367%),                 avg. length: 1500.2,                last time consumption/overall running time: 187.5668s / 101900.0406 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1326
env0_second_0:                 episode reward: 4.4000,                 loss: 0.1989
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 8341/30000 (27.8033%),                 avg. length: 1523.35,                last time consumption/overall running time: 189.1678s / 102089.2084 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.1339
env0_second_0:                 episode reward: 7.0000,                 loss: 0.1860
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 8361/30000 (27.8700%),                 avg. length: 1560.85,                last time consumption/overall running time: 185.6394s / 102274.8478 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.1601
env0_second_0:                 episode reward: 2.4000,                 loss: 0.3468
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 8381/30000 (27.9367%),                 avg. length: 1425.9,                last time consumption/overall running time: 181.7051s / 102456.5529 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.1464
env0_second_0:                 episode reward: 3.6500,                 loss: 0.3445
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 8401/30000 (28.0033%),                 avg. length: 1442.6,                last time consumption/overall running time: 185.4577s / 102642.0106 s
env0_first_0:                 episode reward: -5.1500,                 loss: -0.1064
env0_second_0:                 episode reward: 5.1500,                 loss: 0.3628
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 8421/30000 (28.0700%),                 avg. length: 1483.55,                last time consumption/overall running time: 189.2398s / 102831.2504 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.1236
env0_second_0:                 episode reward: 1.6500,                 loss: 0.1880
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 8441/30000 (28.1367%),                 avg. length: 1563.2,                last time consumption/overall running time: 192.7259s / 103023.9763 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.1522
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0967
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 8461/30000 (28.2033%),                 avg. length: 1660.7,                last time consumption/overall running time: 201.1041s / 103225.0804 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.1551
env0_second_0:                 episode reward: 1.2000,                 loss: 0.1486
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 8481/30000 (28.2700%),                 avg. length: 1562.45,                last time consumption/overall running time: 174.5846s / 103399.6650 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.0924
env0_second_0:                 episode reward: 1.6500,                 loss: 0.2954
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 8501/30000 (28.3367%),                 avg. length: 1476.2,                last time consumption/overall running time: 177.5880s / 103577.2530 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.1189
env0_second_0:                 episode reward: 1.2000,                 loss: 0.2027
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 8521/30000 (28.4033%),                 avg. length: 1567.9,                last time consumption/overall running time: 183.8746s / 103761.1276 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.1374
env0_second_0:                 episode reward: 1.1000,                 loss: 0.2034
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 8541/30000 (28.4700%),                 avg. length: 1548.4,                last time consumption/overall running time: 192.3725s / 103953.5001 s
env0_first_0:                 episode reward: -5.3500,                 loss: -0.0877
env0_second_0:                 episode reward: 5.3500,                 loss: 0.2817
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 8561/30000 (28.5367%),                 avg. length: 1495.1,                last time consumption/overall running time: 194.6886s / 104148.1887 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.1192
env0_second_0:                 episode reward: 3.8000,                 loss: 0.4607
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 8581/30000 (28.6033%),                 avg. length: 1464.6,                last time consumption/overall running time: 162.9633s / 104311.1520 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1680
env0_second_0:                 episode reward: -0.2000,                 loss: 0.2195
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 8601/30000 (28.6700%),                 avg. length: 1476.9,                last time consumption/overall running time: 186.9058s / 104498.0578 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1874
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0824
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 8621/30000 (28.7367%),                 avg. length: 1453.95,                last time consumption/overall running time: 183.1939s / 104681.2517 s
env0_first_0:                 episode reward: -3.0500,                 loss: -0.1511
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0809
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 8641/30000 (28.8033%),                 avg. length: 1544.95,                last time consumption/overall running time: 190.3733s / 104871.6250 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1905
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0750
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 8661/30000 (28.8700%),                 avg. length: 1497.65,                last time consumption/overall running time: 184.8100s / 105056.4350 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1556
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0546
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 8681/30000 (28.9367%),                 avg. length: 1536.15,                last time consumption/overall running time: 183.7358s / 105240.1709 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.1438
env0_second_0:                 episode reward: 1.2500,                 loss: 0.1243
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 8701/30000 (29.0033%),                 avg. length: 1469.7,                last time consumption/overall running time: 180.1011s / 105420.2720 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.1546
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0499
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 8721/30000 (29.0700%),                 avg. length: 1521.05,                last time consumption/overall running time: 180.1172s / 105600.3892 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.1912
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0305
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 8741/30000 (29.1367%),                 avg. length: 1479.2,                last time consumption/overall running time: 188.3289s / 105788.7181 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.1430
env0_second_0:                 episode reward: 2.5000,                 loss: 0.1892
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 8761/30000 (29.2033%),                 avg. length: 1484.35,                last time consumption/overall running time: 183.2759s / 105971.9940 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.1782
env0_second_0:                 episode reward: 0.3500,                 loss: 0.1196
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 8781/30000 (29.2700%),                 avg. length: 1456.0,                last time consumption/overall running time: 184.5156s / 106156.5096 s
env0_first_0:                 episode reward: -3.9000,                 loss: -0.0749
env0_second_0:                 episode reward: 3.9000,                 loss: 0.2810
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 8801/30000 (29.3367%),                 avg. length: 1503.9,                last time consumption/overall running time: 191.8677s / 106348.3773 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1668
env0_second_0:                 episode reward: -0.5000,                 loss: 0.1458
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 8821/30000 (29.4033%),                 avg. length: 1511.25,                last time consumption/overall running time: 192.6857s / 106541.0630 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1671
env0_second_0:                 episode reward: -1.3500,                 loss: 0.1860
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 8841/30000 (29.4700%),                 avg. length: 1611.95,                last time consumption/overall running time: 206.0143s / 106747.0773 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1537
env0_second_0:                 episode reward: -2.8500,                 loss: 0.2388
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 8861/30000 (29.5367%),                 avg. length: 1478.6,                last time consumption/overall running time: 189.4020s / 106936.4793 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1749
env0_second_0:                 episode reward: -0.5000,                 loss: 0.2167
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 8881/30000 (29.6033%),                 avg. length: 1478.35,                last time consumption/overall running time: 189.1907s / 107125.6700 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.1898
env0_second_0:                 episode reward: 1.6000,                 loss: 0.1564
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 8901/30000 (29.6700%),                 avg. length: 1525.45,                last time consumption/overall running time: 194.4339s / 107320.1039 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.1660
env0_second_0:                 episode reward: 1.4500,                 loss: 0.2266
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 8921/30000 (29.7367%),                 avg. length: 1447.7,                last time consumption/overall running time: 183.5174s / 107503.6213 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.1018
env0_second_0:                 episode reward: 6.1500,                 loss: 0.2412
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 8941/30000 (29.8033%),                 avg. length: 1440.8,                last time consumption/overall running time: 176.0637s / 107679.6850 s
env0_first_0:                 episode reward: -3.5000,                 loss: -0.1012
env0_second_0:                 episode reward: 3.5000,                 loss: 3.3274
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 8961/30000 (29.8700%),                 avg. length: 1514.15,                last time consumption/overall running time: 181.2666s / 107860.9516 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.1206
env0_second_0:                 episode reward: 1.6000,                 loss: 1.8688
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 8981/30000 (29.9367%),                 avg. length: 1517.05,                last time consumption/overall running time: 196.9400s / 108057.8916 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.1345
env0_second_0:                 episode reward: 2.0500,                 loss: 1.1807
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 9001/30000 (30.0033%),                 avg. length: 1695.75,                last time consumption/overall running time: 206.5849s / 108264.4765 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.1161
env0_second_0:                 episode reward: 1.2500,                 loss: 4.4287
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 9021/30000 (30.0700%),                 avg. length: 1622.85,                last time consumption/overall running time: 206.0211s / 108470.4976 s
env0_first_0:                 episode reward: -3.4500,                 loss: -0.0735
env0_second_0:                 episode reward: 3.4500,                 loss: 1.2632
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 9041/30000 (30.1367%),                 avg. length: 1537.85,                last time consumption/overall running time: 194.0817s / 108664.5793 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.1167
env0_second_0:                 episode reward: 1.8000,                 loss: 0.9761
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 9061/30000 (30.2033%),                 avg. length: 1516.4,                last time consumption/overall running time: 187.0769s / 108851.6562 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1940
env0_second_0:                 episode reward: -1.1500,                 loss: 0.8986
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 9081/30000 (30.2700%),                 avg. length: 1534.55,                last time consumption/overall running time: 188.0144s / 109039.6707 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1838
env0_second_0:                 episode reward: 0.4000,                 loss: 1.9183
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 9101/30000 (30.3367%),                 avg. length: 1445.15,                last time consumption/overall running time: 183.5086s / 109223.1793 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.1948
env0_second_0:                 episode reward: 0.9000,                 loss: 1.0637
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 9121/30000 (30.4033%),                 avg. length: 1475.25,                last time consumption/overall running time: 186.6435s / 109409.8228 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2371
env0_second_0:                 episode reward: -1.3500,                 loss: 0.6989
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 9141/30000 (30.4700%),                 avg. length: 1484.9,                last time consumption/overall running time: 185.6135s / 109595.4363 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.1907
env0_second_0:                 episode reward: 0.6000,                 loss: 0.6644
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 9161/30000 (30.5367%),                 avg. length: 1406.8,                last time consumption/overall running time: 176.2273s / 109771.6636 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.0996
env0_second_0:                 episode reward: 4.0000,                 loss: 0.7929
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 9181/30000 (30.6033%),                 avg. length: 1456.9,                last time consumption/overall running time: 174.3593s / 109946.0229 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.1182
env0_second_0:                 episode reward: 4.5000,                 loss: 0.5652
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 9201/30000 (30.6700%),                 avg. length: 1553.8,                last time consumption/overall running time: 195.9287s / 110141.9516 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.1918
env0_second_0:                 episode reward: 1.4000,                 loss: 0.3254
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 9221/30000 (30.7367%),                 avg. length: 1540.15,                last time consumption/overall running time: 197.7715s / 110339.7231 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1964
env0_second_0:                 episode reward: 0.1000,                 loss: 0.3931
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 9241/30000 (30.8033%),                 avg. length: 1460.55,                last time consumption/overall running time: 174.5816s / 110514.3047 s
env0_first_0:                 episode reward: -2.8000,                 loss: -0.1672
env0_second_0:                 episode reward: 2.8000,                 loss: 0.2852
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 9261/30000 (30.8700%),                 avg. length: 1406.25,                last time consumption/overall running time: 179.0945s / 110693.3992 s
env0_first_0:                 episode reward: -4.5500,                 loss: -0.1652
env0_second_0:                 episode reward: 4.5500,                 loss: 0.6352
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 9281/30000 (30.9367%),                 avg. length: 1445.6,                last time consumption/overall running time: 184.0375s / 110877.4367 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.0945
env0_second_0:                 episode reward: 5.8000,                 loss: 0.4506
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 9301/30000 (31.0033%),                 avg. length: 1315.05,                last time consumption/overall running time: 149.8199s / 111027.2566 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.0599
env0_second_0:                 episode reward: 7.5500,                 loss: 0.4593
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 9321/30000 (31.0700%),                 avg. length: 1513.35,                last time consumption/overall running time: 164.5873s / 111191.8439 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.1564
env0_second_0:                 episode reward: 0.3000,                 loss: 0.2672
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 9341/30000 (31.1367%),                 avg. length: 1484.35,                last time consumption/overall running time: 167.0056s / 111358.8495 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2145
env0_second_0:                 episode reward: 0.5000,                 loss: 0.1104
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 9361/30000 (31.2033%),                 avg. length: 1561.0,                last time consumption/overall running time: 174.5434s / 111533.3929 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2377
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0581
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9381/30000 (31.2700%),                 avg. length: 1636.5,                last time consumption/overall running time: 191.8210s / 111725.2139 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.2014
env0_second_0:                 episode reward: 0.7500,                 loss: 0.2910
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 9401/30000 (31.3367%),                 avg. length: 1533.5,                last time consumption/overall running time: 170.6381s / 111895.8520 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.1969
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0961
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 9421/30000 (31.4033%),                 avg. length: 1526.0,                last time consumption/overall running time: 167.3332s / 112063.1852 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2358
env0_second_0:                 episode reward: -0.5000,                 loss: 0.4938
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 9441/30000 (31.4700%),                 avg. length: 1658.75,                last time consumption/overall running time: 189.3238s / 112252.5090 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2158
env0_second_0:                 episode reward: -0.5000,                 loss: 0.4612
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 9461/30000 (31.5367%),                 avg. length: 1568.05,                last time consumption/overall running time: 190.2772s / 112442.7861 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2431
env0_second_0:                 episode reward: -2.6500,                 loss: 0.1643
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 9481/30000 (31.6033%),                 avg. length: 2183.4,                last time consumption/overall running time: 257.4047s / 112700.1908 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.1740
env0_second_0:                 episode reward: -2.5500,                 loss: 0.1536
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 9501/30000 (31.6700%),                 avg. length: 2148.65,                last time consumption/overall running time: 268.3690s / 112968.5599 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.1734
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0560
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 9521/30000 (31.7367%),                 avg. length: 2202.7,                last time consumption/overall running time: 276.6845s / 113245.2444 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1855
env0_second_0:                 episode reward: -2.7500,                 loss: 0.1078
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 9541/30000 (31.8033%),                 avg. length: 2342.4,                last time consumption/overall running time: 266.8935s / 113512.1379 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1841
env0_second_0:                 episode reward: -3.2000,                 loss: 0.2692
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 9561/30000 (31.8700%),                 avg. length: 2289.6,                last time consumption/overall running time: 273.7332s / 113785.8711 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.1913
env0_second_0:                 episode reward: -4.5500,                 loss: 0.0770
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 9581/30000 (31.9367%),                 avg. length: 2244.0,                last time consumption/overall running time: 272.7221s / 114058.5933 s
env0_first_0:                 episode reward: 7.7000,                 loss: -0.1890
env0_second_0:                 episode reward: -7.7000,                 loss: 0.0389
env1_first_0:                 episode reward: 8.1500,                 loss: nan
env1_second_0:                 episode reward: -8.1500,                 loss: nan
Episode: 9601/30000 (32.0033%),                 avg. length: 2213.6,                last time consumption/overall running time: 279.1090s / 114337.7023 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.2088
env0_second_0:                 episode reward: -5.8000,                 loss: -0.0250
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 9621/30000 (32.0700%),                 avg. length: 2166.55,                last time consumption/overall running time: 271.6805s / 114609.3829 s
env0_first_0:                 episode reward: 9.0000,                 loss: -0.1843
env0_second_0:                 episode reward: -9.0000,                 loss: 0.0986
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 9641/30000 (32.1367%),                 avg. length: 2009.3,                last time consumption/overall running time: 252.4259s / 114861.8088 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.1794
env0_second_0:                 episode reward: -5.3000,                 loss: 0.1003
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 9661/30000 (32.2033%),                 avg. length: 1613.0,                last time consumption/overall running time: 204.5588s / 115066.3675 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.2215
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0121
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 9681/30000 (32.2700%),                 avg. length: 1575.15,                last time consumption/overall running time: 200.9805s / 115267.3481 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1718
env0_second_0:                 episode reward: -1.3000,                 loss: 0.3701
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 9701/30000 (32.3367%),                 avg. length: 1472.55,                last time consumption/overall running time: 178.5963s / 115445.9444 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.1767
env0_second_0:                 episode reward: 1.1500,                 loss: 0.5577
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 9721/30000 (32.4033%),                 avg. length: 1438.95,                last time consumption/overall running time: 153.6428s / 115599.5871 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.1512
env0_second_0:                 episode reward: 2.2500,                 loss: 0.9320
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 9741/30000 (32.4700%),                 avg. length: 1417.95,                last time consumption/overall running time: 148.7543s / 115748.3415 s
env0_first_0:                 episode reward: -3.0500,                 loss: -0.1214
env0_second_0:                 episode reward: 3.0500,                 loss: 0.6299
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 9761/30000 (32.5367%),                 avg. length: 1487.4,                last time consumption/overall running time: 189.6108s / 115937.9522 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.1670
env0_second_0:                 episode reward: 2.3000,                 loss: 0.3242
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 9781/30000 (32.6033%),                 avg. length: 1545.95,                last time consumption/overall running time: 185.2159s / 116123.1681 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.1121
env0_second_0:                 episode reward: 2.4500,                 loss: 0.5185
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 9801/30000 (32.6700%),                 avg. length: 1519.25,                last time consumption/overall running time: 157.7869s / 116280.9550 s
env0_first_0:                 episode reward: -5.5500,                 loss: -0.0510
env0_second_0:                 episode reward: 5.5500,                 loss: 1.1823
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 9821/30000 (32.7367%),                 avg. length: 1826.85,                last time consumption/overall running time: 179.8140s / 116460.7690 s
env0_first_0:                 episode reward: -6.0000,                 loss: -0.0284
env0_second_0:                 episode reward: 6.0000,                 loss: 1.0650
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 9841/30000 (32.8033%),                 avg. length: 1521.15,                last time consumption/overall running time: 171.8994s / 116632.6684 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1043
env0_second_0:                 episode reward: -1.2500,                 loss: 0.3243
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 9861/30000 (32.8700%),                 avg. length: 1763.75,                last time consumption/overall running time: 196.5568s / 116829.2252 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1755
env0_second_0:                 episode reward: -1.5500,                 loss: 0.3699
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 9881/30000 (32.9367%),                 avg. length: 1717.45,                last time consumption/overall running time: 185.2570s / 117014.4821 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1420
env0_second_0:                 episode reward: -1.2000,                 loss: 0.4111
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 9901/30000 (33.0033%),                 avg. length: 1703.3,                last time consumption/overall running time: 204.2066s / 117218.6887 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1471
env0_second_0:                 episode reward: -2.9500,                 loss: 0.3171
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 9921/30000 (33.0700%),                 avg. length: 1529.4,                last time consumption/overall running time: 197.6283s / 117416.3170 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1890
env0_second_0:                 episode reward: -1.6000,                 loss: 0.2906
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 9941/30000 (33.1367%),                 avg. length: 1515.2,                last time consumption/overall running time: 191.5328s / 117607.8498 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.1950
env0_second_0:                 episode reward: -0.9500,                 loss: 0.2703
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 9961/30000 (33.2033%),                 avg. length: 1493.9,                last time consumption/overall running time: 185.6803s / 117793.5301 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1958
env0_second_0:                 episode reward: -2.3500,                 loss: 0.2557
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 9981/30000 (33.2700%),                 avg. length: 1505.35,                last time consumption/overall running time: 190.8373s / 117984.3674 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2052
env0_second_0:                 episode reward: -0.3500,                 loss: 0.3610
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 10001/30000 (33.3367%),                 avg. length: 1429.25,                last time consumption/overall running time: 182.0062s / 118166.3736 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2077
env0_second_0:                 episode reward: -0.8500,                 loss: 0.4673
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 10021/30000 (33.4033%),                 avg. length: 1489.75,                last time consumption/overall running time: 186.6400s / 118353.0136 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2019
env0_second_0:                 episode reward: -1.8500,                 loss: 0.4171
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 10041/30000 (33.4700%),                 avg. length: 1589.05,                last time consumption/overall running time: 200.7250s / 118553.7386 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1449
env0_second_0:                 episode reward: -0.7500,                 loss: 0.3397
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 10061/30000 (33.5367%),                 avg. length: 2086.85,                last time consumption/overall running time: 257.5167s / 118811.2554 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.1367
env0_second_0:                 episode reward: 1.7500,                 loss: 0.5648
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 10081/30000 (33.6033%),                 avg. length: 2132.9,                last time consumption/overall running time: 268.9761s / 119080.2314 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1246
env0_second_0:                 episode reward: 0.4500,                 loss: 0.6905
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 10101/30000 (33.6700%),                 avg. length: 1501.65,                last time consumption/overall running time: 191.9847s / 119272.2161 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.1102
env0_second_0:                 episode reward: -3.5500,                 loss: 0.3335
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 10121/30000 (33.7367%),                 avg. length: 1517.45,                last time consumption/overall running time: 194.2567s / 119466.4729 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.1100
env0_second_0:                 episode reward: -4.6500,                 loss: 0.5119
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 10141/30000 (33.8033%),                 avg. length: 1872.35,                last time consumption/overall running time: 237.9436s / 119704.4164 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1081
env0_second_0:                 episode reward: 0.2500,                 loss: 0.4729
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 10161/30000 (33.8700%),                 avg. length: 2396.85,                last time consumption/overall running time: 302.7774s / 120007.1938 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.1140
env0_second_0:                 episode reward: -0.4500,                 loss: 0.2626
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10181/30000 (33.9367%),                 avg. length: 2842.25,                last time consumption/overall running time: 352.1170s / 120359.3109 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1173
env0_second_0:                 episode reward: -2.6500,                 loss: 0.5907
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 10201/30000 (34.0033%),                 avg. length: 2625.7,                last time consumption/overall running time: 323.3126s / 120682.6235 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.1070
env0_second_0:                 episode reward: -4.0000,                 loss: 0.4445
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 10221/30000 (34.0700%),                 avg. length: 2857.85,                last time consumption/overall running time: 368.8985s / 121051.5220 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.1071
env0_second_0:                 episode reward: -3.5500,                 loss: 0.5779
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 10241/30000 (34.1367%),                 avg. length: 2431.35,                last time consumption/overall running time: 312.5043s / 121364.0263 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1065
env0_second_0:                 episode reward: -3.9000,                 loss: 0.8430
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 10261/30000 (34.2033%),                 avg. length: 2558.35,                last time consumption/overall running time: 318.2331s / 121682.2594 s
env0_first_0:                 episode reward: 8.8000,                 loss: -0.1229
env0_second_0:                 episode reward: -8.8000,                 loss: 0.7643
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 10281/30000 (34.2700%),                 avg. length: 2467.1,                last time consumption/overall running time: 304.9387s / 121987.1980 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.0860
env0_second_0:                 episode reward: -5.9000,                 loss: 0.5735
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 10301/30000 (34.3367%),                 avg. length: 2763.95,                last time consumption/overall running time: 349.1113s / 122336.3093 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.0610
env0_second_0:                 episode reward: 1.5000,                 loss: 0.4945
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 10321/30000 (34.4033%),                 avg. length: 2659.0,                last time consumption/overall running time: 338.9290s / 122675.2384 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.0778
env0_second_0:                 episode reward: -4.6500,                 loss: 0.4553
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 10341/30000 (34.4700%),                 avg. length: 2105.25,                last time consumption/overall running time: 266.1259s / 122941.3643 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.0732
env0_second_0:                 episode reward: -2.8500,                 loss: 0.7939
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 10361/30000 (34.5367%),                 avg. length: 2158.75,                last time consumption/overall running time: 273.5036s / 123214.8678 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.0351
env0_second_0:                 episode reward: -5.1500,                 loss: 0.9772
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 10381/30000 (34.6033%),                 avg. length: 2739.85,                last time consumption/overall running time: 341.6182s / 123556.4861 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1034
env0_second_0:                 episode reward: -4.6000,                 loss: 0.6911
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 10401/30000 (34.6700%),                 avg. length: 2346.65,                last time consumption/overall running time: 293.5828s / 123850.0688 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.0917
env0_second_0:                 episode reward: -5.3000,                 loss: 0.4285
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 10421/30000 (34.7367%),                 avg. length: 2668.6,                last time consumption/overall running time: 339.0787s / 124189.1475 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.0841
env0_second_0:                 episode reward: -2.0000,                 loss: 0.7433
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 10441/30000 (34.8033%),                 avg. length: 2798.9,                last time consumption/overall running time: 359.7084s / 124548.8559 s
env0_first_0:                 episode reward: 4.2500,                 loss: -0.0767
env0_second_0:                 episode reward: -4.2500,                 loss: 0.5041
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 10461/30000 (34.8700%),                 avg. length: 2407.2,                last time consumption/overall running time: 298.3748s / 124847.2307 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.0992
env0_second_0:                 episode reward: -4.8000,                 loss: 0.4893
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 10481/30000 (34.9367%),                 avg. length: 2131.1,                last time consumption/overall running time: 269.6963s / 125116.9270 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.0584
env0_second_0:                 episode reward: -0.9500,                 loss: 0.7571
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 10501/30000 (35.0033%),                 avg. length: 1876.25,                last time consumption/overall running time: 234.8776s / 125351.8046 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.0821
env0_second_0:                 episode reward: -4.7000,                 loss: 0.8793
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 10521/30000 (35.0700%),                 avg. length: 2004.55,                last time consumption/overall running time: 248.3188s / 125600.1234 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0932
env0_second_0:                 episode reward: -9.4500,                 loss: 0.6052
env1_first_0:                 episode reward: 11.4500,                 loss: nan
env1_second_0:                 episode reward: -11.4500,                 loss: nan
Episode: 10541/30000 (35.1367%),                 avg. length: 1816.55,                last time consumption/overall running time: 232.7711s / 125832.8945 s
env0_first_0:                 episode reward: 8.9500,                 loss: -0.0771
env0_second_0:                 episode reward: -8.9500,                 loss: 0.8063
env1_first_0:                 episode reward: 8.5500,                 loss: nan
env1_second_0:                 episode reward: -8.5500,                 loss: nan
Episode: 10561/30000 (35.2033%),                 avg. length: 1720.05,                last time consumption/overall running time: 217.7825s / 126050.6770 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.0807
env0_second_0:                 episode reward: -5.1000,                 loss: 0.7419
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 10581/30000 (35.2700%),                 avg. length: 1536.7,                last time consumption/overall running time: 196.0540s / 126246.7310 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1741
env0_second_0:                 episode reward: -4.8000,                 loss: 0.5226
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 10601/30000 (35.3367%),                 avg. length: 1680.9,                last time consumption/overall running time: 213.0563s / 126459.7873 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.0741
env0_second_0:                 episode reward: -4.0000,                 loss: 0.6886
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 10621/30000 (35.4033%),                 avg. length: 1568.9,                last time consumption/overall running time: 200.7824s / 126660.5697 s
env0_first_0:                 episode reward: 5.6500,                 loss: -0.0940
env0_second_0:                 episode reward: -5.6500,                 loss: 0.7338
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 10641/30000 (35.4700%),                 avg. length: 1610.75,                last time consumption/overall running time: 205.2724s / 126865.8421 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1011
env0_second_0:                 episode reward: -3.2500,                 loss: 0.8730
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 10661/30000 (35.5367%),                 avg. length: 1569.15,                last time consumption/overall running time: 195.5531s / 127061.3951 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.1533
env0_second_0:                 episode reward: -4.0500,                 loss: 0.9858
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 10681/30000 (35.6033%),                 avg. length: 1647.55,                last time consumption/overall running time: 206.1247s / 127267.5198 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1334
env0_second_0:                 episode reward: -4.8000,                 loss: 0.6871
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 10701/30000 (35.6700%),                 avg. length: 1449.05,                last time consumption/overall running time: 185.0595s / 127452.5793 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2299
env0_second_0:                 episode reward: -1.9500,                 loss: 0.6800
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 10721/30000 (35.7367%),                 avg. length: 1500.95,                last time consumption/overall running time: 190.4707s / 127643.0500 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.2125
env0_second_0:                 episode reward: -3.2000,                 loss: 0.7903
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 10741/30000 (35.8033%),                 avg. length: 1497.8,                last time consumption/overall running time: 190.5747s / 127833.6247 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.2273
env0_second_0:                 episode reward: -3.3500,                 loss: 1.0276
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 10761/30000 (35.8700%),                 avg. length: 1496.7,                last time consumption/overall running time: 192.8963s / 128026.5210 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2238
env0_second_0:                 episode reward: -2.3000,                 loss: 0.8551
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 10781/30000 (35.9367%),                 avg. length: 1461.55,                last time consumption/overall running time: 186.9503s / 128213.4714 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.2306
env0_second_0:                 episode reward: -2.4000,                 loss: 0.6104
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 10801/30000 (36.0033%),                 avg. length: 1468.15,                last time consumption/overall running time: 187.3327s / 128400.8040 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2264
env0_second_0:                 episode reward: -2.0000,                 loss: 1.0044
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 10821/30000 (36.0700%),                 avg. length: 1445.35,                last time consumption/overall running time: 187.8722s / 128588.6763 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.2484
env0_second_0:                 episode reward: -1.2500,                 loss: 0.5621
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 10841/30000 (36.1367%),                 avg. length: 1497.95,                last time consumption/overall running time: 194.2739s / 128782.9502 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.2262
env0_second_0:                 episode reward: -0.9000,                 loss: 0.6840
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 10861/30000 (36.2033%),                 avg. length: 1459.1,                last time consumption/overall running time: 187.9080s / 128970.8582 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.1758
env0_second_0:                 episode reward: -3.1000,                 loss: 0.9055
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 10881/30000 (36.2700%),                 avg. length: 1472.05,                last time consumption/overall running time: 183.3477s / 129154.2058 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2216
env0_second_0:                 episode reward: -2.3500,                 loss: 0.7209
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 10901/30000 (36.3367%),                 avg. length: 1482.4,                last time consumption/overall running time: 182.0019s / 129336.2077 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1539
env0_second_0:                 episode reward: -1.9500,                 loss: 1.1303
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 10921/30000 (36.4033%),                 avg. length: 1496.9,                last time consumption/overall running time: 155.9871s / 129492.1949 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1910
env0_second_0:                 episode reward: -1.6500,                 loss: 0.6006
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 10941/30000 (36.4700%),                 avg. length: 1592.95,                last time consumption/overall running time: 175.2508s / 129667.4457 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.1740
env0_second_0:                 episode reward: -1.1000,                 loss: 0.6931
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 10961/30000 (36.5367%),                 avg. length: 1618.55,                last time consumption/overall running time: 189.6735s / 129857.1192 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1944
env0_second_0:                 episode reward: 0.0000,                 loss: 0.5778
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 10981/30000 (36.6033%),                 avg. length: 1502.9,                last time consumption/overall running time: 166.7466s / 130023.8658 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1939
env0_second_0:                 episode reward: -1.6500,                 loss: 0.5887
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 11001/30000 (36.6700%),                 avg. length: 1727.65,                last time consumption/overall running time: 189.5513s / 130213.4171 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1764
env0_second_0:                 episode reward: -3.4000,                 loss: 0.9968
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 11021/30000 (36.7367%),                 avg. length: 2261.15,                last time consumption/overall running time: 245.2178s / 130458.6348 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.1008
env0_second_0:                 episode reward: -5.7000,                 loss: 1.3130
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 11041/30000 (36.8033%),                 avg. length: 2397.15,                last time consumption/overall running time: 249.7347s / 130708.3695 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1371
env0_second_0:                 episode reward: -3.2500,                 loss: 0.7444
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 11061/30000 (36.8700%),                 avg. length: 2412.7,                last time consumption/overall running time: 265.2044s / 130973.5740 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.0581
env0_second_0:                 episode reward: -5.4000,                 loss: 0.6393
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 11081/30000 (36.9367%),                 avg. length: 2029.1,                last time consumption/overall running time: 237.1071s / 131210.6811 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.0906
env0_second_0:                 episode reward: -3.0500,                 loss: 1.1135
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 11101/30000 (37.0033%),                 avg. length: 3694.1,                last time consumption/overall running time: 408.8987s / 131619.5798 s
env0_first_0:                 episode reward: -41.2000,                 loss: 0.1499
env0_second_0:                 episode reward: 41.2000,                 loss: 0.8709
env1_first_0:                 episode reward: -37.1000,                 loss: nan
env1_second_0:                 episode reward: 37.1000,                 loss: nan
Episode: 11121/30000 (37.0700%),                 avg. length: 3688.1,                last time consumption/overall running time: 420.3639s / 132039.9437 s
env0_first_0:                 episode reward: -5.5500,                 loss: -0.0624
env0_second_0:                 episode reward: 5.5500,                 loss: 0.5508
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 11141/30000 (37.1367%),                 avg. length: 2253.4,                last time consumption/overall running time: 230.5003s / 132270.4440 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.1448
env0_second_0:                 episode reward: -2.3000,                 loss: 0.6193
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 11161/30000 (37.2033%),                 avg. length: 1972.1,                last time consumption/overall running time: 203.0712s / 132473.5152 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1308
env0_second_0:                 episode reward: -1.1500,                 loss: 1.0730
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 11181/30000 (37.2700%),                 avg. length: 2396.2,                last time consumption/overall running time: 265.6255s / 132739.1407 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.1126
env0_second_0:                 episode reward: -3.7000,                 loss: 0.6143
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 11201/30000 (37.3367%),                 avg. length: 2104.6,                last time consumption/overall running time: 233.0001s / 132972.1408 s
env0_first_0:                 episode reward: 9.1500,                 loss: -0.0608
env0_second_0:                 episode reward: -9.1500,                 loss: 1.0792
env1_first_0:                 episode reward: 9.1000,                 loss: nan
env1_second_0:                 episode reward: -9.1000,                 loss: nan
Episode: 11221/30000 (37.4033%),                 avg. length: 1859.4,                last time consumption/overall running time: 190.2873s / 133162.4281 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.1325
env0_second_0:                 episode reward: 0.9500,                 loss: 0.8673
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 11241/30000 (37.4700%),                 avg. length: 1474.9,                last time consumption/overall running time: 156.7836s / 133319.2117 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.2386
env0_second_0:                 episode reward: -2.6000,                 loss: 0.7682
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 11261/30000 (37.5367%),                 avg. length: 1467.75,                last time consumption/overall running time: 169.6625s / 133488.8742 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.2590
env0_second_0:                 episode reward: -2.7000,                 loss: 0.9688
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 11281/30000 (37.6033%),                 avg. length: 1513.75,                last time consumption/overall running time: 184.5591s / 133673.4333 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2258
env0_second_0:                 episode reward: -1.6000,                 loss: 0.4898
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 11301/30000 (37.6700%),                 avg. length: 1684.1,                last time consumption/overall running time: 188.8821s / 133862.3154 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1373
env0_second_0:                 episode reward: 1.3000,                 loss: 1.2902
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 11321/30000 (37.7367%),                 avg. length: 1459.25,                last time consumption/overall running time: 170.0867s / 134032.4021 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.0944
env0_second_0:                 episode reward: 1.5000,                 loss: 1.1922
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 11341/30000 (37.8033%),                 avg. length: 1488.05,                last time consumption/overall running time: 168.8401s / 134201.2422 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1986
env0_second_0:                 episode reward: -1.9000,                 loss: 1.1447
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 11361/30000 (37.8700%),                 avg. length: 1462.0,                last time consumption/overall running time: 159.7290s / 134360.9712 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2008
env0_second_0:                 episode reward: -0.8500,                 loss: 0.6715
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 11381/30000 (37.9367%),                 avg. length: 1469.95,                last time consumption/overall running time: 156.4195s / 134517.3907 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2165
env0_second_0:                 episode reward: -1.3500,                 loss: 0.6377
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 11401/30000 (38.0033%),                 avg. length: 1476.5,                last time consumption/overall running time: 176.0185s / 134693.4092 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1797
env0_second_0:                 episode reward: -1.2500,                 loss: 0.7606
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 11421/30000 (38.0700%),                 avg. length: 1493.05,                last time consumption/overall running time: 180.4196s / 134873.8288 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2023
env0_second_0:                 episode reward: -1.3000,                 loss: 0.7984
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 11441/30000 (38.1367%),                 avg. length: 1457.7,                last time consumption/overall running time: 157.8266s / 135031.6553 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2037
env0_second_0:                 episode reward: -1.7500,                 loss: 0.5259
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 11461/30000 (38.2033%),                 avg. length: 1523.45,                last time consumption/overall running time: 166.7413s / 135198.3967 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2201
env0_second_0:                 episode reward: -1.6000,                 loss: 0.4331
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 11481/30000 (38.2700%),                 avg. length: 1498.35,                last time consumption/overall running time: 173.2352s / 135371.6319 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.2000
env0_second_0:                 episode reward: -1.4500,                 loss: 0.7830
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 11501/30000 (38.3367%),                 avg. length: 1493.5,                last time consumption/overall running time: 167.2958s / 135538.9277 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2124
env0_second_0:                 episode reward: -1.5000,                 loss: 0.4343
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 11521/30000 (38.4033%),                 avg. length: 1507.2,                last time consumption/overall running time: 166.3661s / 135705.2938 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1832
env0_second_0:                 episode reward: -0.3500,                 loss: 0.5273
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 11541/30000 (38.4700%),                 avg. length: 1504.5,                last time consumption/overall running time: 178.6594s / 135883.9532 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.2048
env0_second_0:                 episode reward: -1.4500,                 loss: 0.2840
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 11561/30000 (38.5367%),                 avg. length: 1511.7,                last time consumption/overall running time: 160.4271s / 136044.3803 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1845
env0_second_0:                 episode reward: -2.8500,                 loss: 0.3826
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 11581/30000 (38.6033%),                 avg. length: 1506.5,                last time consumption/overall running time: 183.1104s / 136227.4907 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2036
env0_second_0:                 episode reward: -0.1500,                 loss: 0.5168
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 11601/30000 (38.6700%),                 avg. length: 1456.65,                last time consumption/overall running time: 168.5991s / 136396.0899 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1872
env0_second_0:                 episode reward: 0.4000,                 loss: 0.4336
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 11621/30000 (38.7367%),                 avg. length: 1456.85,                last time consumption/overall running time: 177.0369s / 136573.1268 s
env0_first_0:                 episode reward: -1.9500,                 loss: -0.1669
env0_second_0:                 episode reward: 1.9500,                 loss: 0.3213
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 11641/30000 (38.8033%),                 avg. length: 1463.35,                last time consumption/overall running time: 181.6879s / 136754.8147 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1800
env0_second_0:                 episode reward: -1.4000,                 loss: 0.5837
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 11661/30000 (38.8700%),                 avg. length: 1457.35,                last time consumption/overall running time: 157.6028s / 136912.4175 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1935
env0_second_0:                 episode reward: -1.5500,                 loss: 0.6482
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 11681/30000 (38.9367%),                 avg. length: 1632.15,                last time consumption/overall running time: 173.9342s / 137086.3517 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.2087
env0_second_0:                 episode reward: -2.5500,                 loss: 0.5934
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 11701/30000 (39.0033%),                 avg. length: 1503.0,                last time consumption/overall running time: 157.1767s / 137243.5284 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2609
env0_second_0:                 episode reward: -1.7500,                 loss: 0.6816
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 11721/30000 (39.0700%),                 avg. length: 1524.25,                last time consumption/overall running time: 169.9860s / 137413.5144 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.2217
env0_second_0:                 episode reward: -0.6000,                 loss: 0.8319
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 11741/30000 (39.1367%),                 avg. length: 1725.1,                last time consumption/overall running time: 194.0317s / 137607.5461 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2056
env0_second_0:                 episode reward: -1.5000,                 loss: 0.8577
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 11761/30000 (39.2033%),                 avg. length: 1668.3,                last time consumption/overall running time: 181.4448s / 137788.9909 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.2273
env0_second_0:                 episode reward: -2.9500,                 loss: 0.7465
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 11781/30000 (39.2700%),                 avg. length: 1696.15,                last time consumption/overall running time: 199.9260s / 137988.9170 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.2251
env0_second_0:                 episode reward: -2.6000,                 loss: 0.5185
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 11801/30000 (39.3367%),                 avg. length: 2055.65,                last time consumption/overall running time: 234.6500s / 138223.5670 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.1283
env0_second_0:                 episode reward: -9.5000,                 loss: 0.7395
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 11821/30000 (39.4033%),                 avg. length: 1863.45,                last time consumption/overall running time: 221.7505s / 138445.3175 s
env0_first_0:                 episode reward: 12.2000,                 loss: -0.1576
env0_second_0:                 episode reward: -12.2000,                 loss: 0.4758
env1_first_0:                 episode reward: 12.4000,                 loss: nan
env1_second_0:                 episode reward: -12.4000,                 loss: nan
Episode: 11841/30000 (39.4700%),                 avg. length: 1598.9,                last time consumption/overall running time: 173.1797s / 138618.4972 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.2348
env0_second_0:                 episode reward: -4.8000,                 loss: 0.4376
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 11861/30000 (39.5367%),                 avg. length: 1537.1,                last time consumption/overall running time: 167.8791s / 138786.3764 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.2706
env0_second_0:                 episode reward: -1.6500,                 loss: 0.3527
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 11881/30000 (39.6033%),                 avg. length: 1571.45,                last time consumption/overall running time: 176.8565s / 138963.2329 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2239
env0_second_0:                 episode reward: -1.9000,                 loss: 0.2244
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 11901/30000 (39.6700%),                 avg. length: 1566.15,                last time consumption/overall running time: 169.2888s / 139132.5216 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.2637
env0_second_0:                 episode reward: -2.4000,                 loss: 0.2387
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 11921/30000 (39.7367%),                 avg. length: 1958.95,                last time consumption/overall running time: 202.0396s / 139334.5612 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.2027
env0_second_0:                 episode reward: -4.7000,                 loss: 0.6218
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 11941/30000 (39.8033%),                 avg. length: 1630.0,                last time consumption/overall running time: 186.6186s / 139521.1798 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.2625
env0_second_0:                 episode reward: -4.3500,                 loss: 0.3390
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 11961/30000 (39.8700%),                 avg. length: 1544.0,                last time consumption/overall running time: 168.8230s / 139690.0029 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.2460
env0_second_0:                 episode reward: -3.9000,                 loss: 0.1810
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 11981/30000 (39.9367%),                 avg. length: 1569.25,                last time consumption/overall running time: 172.7391s / 139862.7420 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.2475
env0_second_0:                 episode reward: -4.6000,                 loss: 0.1570
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 12001/30000 (40.0033%),                 avg. length: 1688.9,                last time consumption/overall running time: 189.7037s / 140052.4457 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.2249
env0_second_0:                 episode reward: -4.9000,                 loss: 0.3150
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 12021/30000 (40.0700%),                 avg. length: 1635.3,                last time consumption/overall running time: 177.1481s / 140229.5939 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.2402
env0_second_0:                 episode reward: -5.4500,                 loss: 0.3286
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 12041/30000 (40.1367%),                 avg. length: 1718.55,                last time consumption/overall running time: 177.4218s / 140407.0156 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2401
env0_second_0:                 episode reward: -2.0500,                 loss: 0.3597
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 12061/30000 (40.2033%),                 avg. length: 1802.7,                last time consumption/overall running time: 204.7986s / 140611.8142 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2496
env0_second_0:                 episode reward: -1.6000,                 loss: 0.2420
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 12081/30000 (40.2700%),                 avg. length: 1722.0,                last time consumption/overall running time: 194.7888s / 140806.6030 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2428
env0_second_0:                 episode reward: -2.0000,                 loss: 0.2772
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 12101/30000 (40.3367%),                 avg. length: 1709.6,                last time consumption/overall running time: 193.9308s / 141000.5338 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.1899
env0_second_0:                 episode reward: -3.8000,                 loss: 0.4012
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 12121/30000 (40.4033%),                 avg. length: 1588.45,                last time consumption/overall running time: 168.1212s / 141168.6550 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.2254
env0_second_0:                 episode reward: -4.3000,                 loss: 0.3313
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 12141/30000 (40.4700%),                 avg. length: 1473.45,                last time consumption/overall running time: 163.2260s / 141331.8809 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.2398
env0_second_0:                 episode reward: -3.3000,                 loss: 0.3383
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 12161/30000 (40.5367%),                 avg. length: 1548.5,                last time consumption/overall running time: 167.0549s / 141498.9359 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2486
env0_second_0:                 episode reward: -2.3500,                 loss: 0.4244
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 12181/30000 (40.6033%),                 avg. length: 1501.85,                last time consumption/overall running time: 182.3584s / 141681.2943 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2732
env0_second_0:                 episode reward: -2.3000,                 loss: 0.3210
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 12201/30000 (40.6700%),                 avg. length: 1552.0,                last time consumption/overall running time: 176.0609s / 141857.3552 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.2670
env0_second_0:                 episode reward: -2.1000,                 loss: 0.2214
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 12221/30000 (40.7367%),                 avg. length: 1524.65,                last time consumption/overall running time: 176.9527s / 142034.3079 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.2778
env0_second_0:                 episode reward: -2.4000,                 loss: 0.3445
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 12241/30000 (40.8033%),                 avg. length: 1613.1,                last time consumption/overall running time: 187.8202s / 142222.1281 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.1843
env0_second_0:                 episode reward: -0.5500,                 loss: 0.6791
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 12261/30000 (40.8700%),                 avg. length: 1580.05,                last time consumption/overall running time: 179.0886s / 142401.2167 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.2039
env0_second_0:                 episode reward: 2.6500,                 loss: 0.5196
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 12281/30000 (40.9367%),                 avg. length: 1780.5,                last time consumption/overall running time: 207.5827s / 142608.7993 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.1586
env0_second_0:                 episode reward: 4.1000,                 loss: 0.3895
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 12301/30000 (41.0033%),                 avg. length: 1751.1,                last time consumption/overall running time: 200.8468s / 142809.6462 s
env0_first_0:                 episode reward: -5.2000,                 loss: -0.1303
env0_second_0:                 episode reward: 5.2000,                 loss: 0.9841
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 12321/30000 (41.0700%),                 avg. length: 1588.6,                last time consumption/overall running time: 187.5103s / 142997.1565 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.1677
env0_second_0:                 episode reward: 2.6000,                 loss: 1.1834
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 12341/30000 (41.1367%),                 avg. length: 1482.7,                last time consumption/overall running time: 173.8344s / 143170.9909 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.1637
env0_second_0:                 episode reward: 1.7500,                 loss: 1.0290
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 12361/30000 (41.2033%),                 avg. length: 1607.75,                last time consumption/overall running time: 174.1494s / 143345.1403 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.1597
env0_second_0:                 episode reward: 1.4500,                 loss: 1.4013
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 12381/30000 (41.2700%),                 avg. length: 1727.2,                last time consumption/overall running time: 189.4299s / 143534.5703 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1107
env0_second_0:                 episode reward: -1.2500,                 loss: 1.9878
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 12401/30000 (41.3367%),                 avg. length: 1808.3,                last time consumption/overall running time: 204.0598s / 143738.6300 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.0804
env0_second_0:                 episode reward: 6.1500,                 loss: 1.3992
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 12421/30000 (41.4033%),                 avg. length: 1544.35,                last time consumption/overall running time: 179.5665s / 143918.1965 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1896
env0_second_0:                 episode reward: -1.3500,                 loss: 0.6742
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 12441/30000 (41.4700%),                 avg. length: 1506.55,                last time consumption/overall running time: 181.2392s / 144099.4357 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.1739
env0_second_0:                 episode reward: 0.0500,                 loss: 0.5586
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 12461/30000 (41.5367%),                 avg. length: 1487.2,                last time consumption/overall running time: 163.1293s / 144262.5650 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.1783
env0_second_0:                 episode reward: -0.5500,                 loss: 0.9294
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 12481/30000 (41.6033%),                 avg. length: 1701.35,                last time consumption/overall running time: 182.7422s / 144445.3073 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.0503
env0_second_0:                 episode reward: -0.2500,                 loss: 1.0225
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 12501/30000 (41.6700%),                 avg. length: 1561.8,                last time consumption/overall running time: 183.4079s / 144628.7152 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.1400
env0_second_0:                 episode reward: 2.9000,                 loss: 1.3053
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 12521/30000 (41.7367%),                 avg. length: 1745.65,                last time consumption/overall running time: 192.8968s / 144821.6119 s
env0_first_0:                 episode reward: -2.7500,                 loss: -0.0931
env0_second_0:                 episode reward: 2.7500,                 loss: 1.0739
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 12541/30000 (41.8033%),                 avg. length: 1614.95,                last time consumption/overall running time: 212.2774s / 145033.8893 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1453
env0_second_0:                 episode reward: -1.7500,                 loss: 0.6104
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 12561/30000 (41.8700%),                 avg. length: 1623.2,                last time consumption/overall running time: 189.6732s / 145223.5626 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1224
env0_second_0:                 episode reward: -0.7500,                 loss: 1.0068
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 12581/30000 (41.9367%),                 avg. length: 1931.05,                last time consumption/overall running time: 213.5568s / 145437.1194 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0248
env0_second_0:                 episode reward: 0.9500,                 loss: 0.7840
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 12601/30000 (42.0033%),                 avg. length: 1969.5,                last time consumption/overall running time: 210.1442s / 145647.2636 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0530
env0_second_0:                 episode reward: -0.1000,                 loss: 0.5053
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 12621/30000 (42.0700%),                 avg. length: 1655.25,                last time consumption/overall running time: 195.9293s / 145843.1929 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1400
env0_second_0:                 episode reward: -2.6500,                 loss: 0.5142
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 12641/30000 (42.1367%),                 avg. length: 1620.95,                last time consumption/overall running time: 172.5669s / 146015.7598 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.1343
env0_second_0:                 episode reward: -1.0000,                 loss: 0.7552
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 12661/30000 (42.2033%),                 avg. length: 1578.65,                last time consumption/overall running time: 169.6908s / 146185.4506 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1088
env0_second_0:                 episode reward: -1.7500,                 loss: 1.1238
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 12681/30000 (42.2700%),                 avg. length: 1504.4,                last time consumption/overall running time: 174.9035s / 146360.3540 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1557
env0_second_0:                 episode reward: -1.7000,                 loss: 0.9892
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 12701/30000 (42.3367%),                 avg. length: 1553.05,                last time consumption/overall running time: 182.9117s / 146543.2658 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1284
env0_second_0:                 episode reward: -1.8500,                 loss: 0.5466
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 12721/30000 (42.4033%),                 avg. length: 1524.75,                last time consumption/overall running time: 169.4881s / 146712.7539 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1270
env0_second_0:                 episode reward: -1.4500,                 loss: 0.5742
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 12741/30000 (42.4700%),                 avg. length: 1513.5,                last time consumption/overall running time: 157.3597s / 146870.1136 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.1568
env0_second_0:                 episode reward: -3.1000,                 loss: 0.6667
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 12761/30000 (42.5367%),                 avg. length: 1460.5,                last time consumption/overall running time: 158.3055s / 147028.4191 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1761
env0_second_0:                 episode reward: -1.6000,                 loss: 0.8180
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 12781/30000 (42.6033%),                 avg. length: 1465.35,                last time consumption/overall running time: 164.9122s / 147193.3314 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.0889
env0_second_0:                 episode reward: 1.2000,                 loss: 0.5165
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 12801/30000 (42.6700%),                 avg. length: 1406.95,                last time consumption/overall running time: 165.8234s / 147359.1548 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.1278
env0_second_0:                 episode reward: 4.2000,                 loss: 2.1082
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 12821/30000 (42.7367%),                 avg. length: 1490.65,                last time consumption/overall running time: 166.1255s / 147525.2804 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.1215
env0_second_0:                 episode reward: 1.6000,                 loss: 1.1407
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 12841/30000 (42.8033%),                 avg. length: 1516.35,                last time consumption/overall running time: 168.5190s / 147693.7993 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1785
env0_second_0:                 episode reward: -1.2000,                 loss: 1.1697
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 12861/30000 (42.8700%),                 avg. length: 1460.95,                last time consumption/overall running time: 155.2802s / 147849.0795 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2005
env0_second_0:                 episode reward: 0.5000,                 loss: 0.7691
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 12881/30000 (42.9367%),                 avg. length: 1555.9,                last time consumption/overall running time: 158.4551s / 148007.5346 s
env0_first_0:                 episode reward: -3.5500,                 loss: -0.1313
env0_second_0:                 episode reward: 3.5500,                 loss: 1.0272
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 12901/30000 (43.0033%),                 avg. length: 1538.2,                last time consumption/overall running time: 166.7544s / 148174.2890 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.1950
env0_second_0:                 episode reward: 1.2500,                 loss: 0.7947
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 12921/30000 (43.0700%),                 avg. length: 1502.3,                last time consumption/overall running time: 171.3023s / 148345.5914 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.2044
env0_second_0:                 episode reward: -0.7000,                 loss: 2.4086
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 12941/30000 (43.1367%),                 avg. length: 1484.15,                last time consumption/overall running time: 165.1113s / 148510.7027 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2022
env0_second_0:                 episode reward: -0.6500,                 loss: 0.8544
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 12961/30000 (43.2033%),                 avg. length: 1535.7,                last time consumption/overall running time: 172.6764s / 148683.3791 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2344
env0_second_0:                 episode reward: -2.0500,                 loss: 0.8133
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 12981/30000 (43.2700%),                 avg. length: 1532.25,                last time consumption/overall running time: 173.0649s / 148856.4440 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2011
env0_second_0:                 episode reward: -1.9500,                 loss: 1.2688
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 13001/30000 (43.3367%),                 avg. length: 1598.0,                last time consumption/overall running time: 167.5193s / 149023.9633 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1785
env0_second_0:                 episode reward: -1.6000,                 loss: 1.2919
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 13021/30000 (43.4033%),                 avg. length: 1865.8,                last time consumption/overall running time: 196.1251s / 149220.0884 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1610
env0_second_0:                 episode reward: -0.0500,                 loss: 1.1369
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 13041/30000 (43.4700%),                 avg. length: 1674.9,                last time consumption/overall running time: 206.2276s / 149426.3160 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.1407
env0_second_0:                 episode reward: -4.7000,                 loss: 1.3417
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 13061/30000 (43.5367%),                 avg. length: 1580.3,                last time consumption/overall running time: 169.3329s / 149595.6488 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.1130
env0_second_0:                 episode reward: -5.4000,                 loss: 0.9451
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 13081/30000 (43.6033%),                 avg. length: 2062.05,                last time consumption/overall running time: 207.2537s / 149802.9026 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1280
env0_second_0:                 episode reward: -1.6000,                 loss: 0.7335
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 13101/30000 (43.6700%),                 avg. length: 2374.0,                last time consumption/overall running time: 250.3793s / 150053.2818 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1396
env0_second_0:                 episode reward: -1.2000,                 loss: 0.6079
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 13121/30000 (43.7367%),                 avg. length: 1941.2,                last time consumption/overall running time: 213.1071s / 150266.3890 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.1203
env0_second_0:                 episode reward: 0.2000,                 loss: 0.5491
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 13141/30000 (43.8033%),                 avg. length: 1790.35,                last time consumption/overall running time: 198.0904s / 150464.4794 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1488
env0_second_0:                 episode reward: 0.0000,                 loss: 0.5017
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 13161/30000 (43.8700%),                 avg. length: 2273.6,                last time consumption/overall running time: 256.6402s / 150721.1196 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1509
env0_second_0:                 episode reward: -0.5000,                 loss: 0.6767
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 13181/30000 (43.9367%),                 avg. length: 1938.9,                last time consumption/overall running time: 218.7715s / 150939.8911 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1522
env0_second_0:                 episode reward: -1.2000,                 loss: 0.6656
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 13201/30000 (44.0033%),                 avg. length: 1747.5,                last time consumption/overall running time: 198.7664s / 151138.6575 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1533
env0_second_0:                 episode reward: -1.0500,                 loss: 0.8417
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 13221/30000 (44.0700%),                 avg. length: 2226.7,                last time consumption/overall running time: 248.7592s / 151387.4167 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.1721
env0_second_0:                 episode reward: -0.6000,                 loss: 0.4391
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 13241/30000 (44.1367%),                 avg. length: 2317.25,                last time consumption/overall running time: 254.4975s / 151641.9143 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1480
env0_second_0:                 episode reward: -2.8500,                 loss: 0.5453
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 13261/30000 (44.2033%),                 avg. length: 2258.8,                last time consumption/overall running time: 253.4640s / 151895.3782 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1684
env0_second_0:                 episode reward: -3.0000,                 loss: 0.4190
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 13281/30000 (44.2700%),                 avg. length: 1892.5,                last time consumption/overall running time: 212.1113s / 152107.4895 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.1294
env0_second_0:                 episode reward: -0.6000,                 loss: 1.4990
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 13301/30000 (44.3367%),                 avg. length: 1940.75,                last time consumption/overall running time: 217.6618s / 152325.1513 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1378
env0_second_0:                 episode reward: -1.3500,                 loss: 1.4616
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 13321/30000 (44.4033%),                 avg. length: 2035.9,                last time consumption/overall running time: 228.3618s / 152553.5131 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.1292
env0_second_0:                 episode reward: 2.1500,                 loss: 1.3412
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 13341/30000 (44.4700%),                 avg. length: 1867.05,                last time consumption/overall running time: 195.9770s / 152749.4902 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1577
env0_second_0:                 episode reward: -1.1500,                 loss: 1.0682
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 13361/30000 (44.5367%),                 avg. length: 1913.85,                last time consumption/overall running time: 203.3364s / 152952.8266 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1089
env0_second_0:                 episode reward: -4.6000,                 loss: 0.9082
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 13381/30000 (44.6033%),                 avg. length: 2023.95,                last time consumption/overall running time: 222.3011s / 153175.1277 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.1583
env0_second_0:                 episode reward: -3.6500,                 loss: 0.7273
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 13401/30000 (44.6700%),                 avg. length: 1642.65,                last time consumption/overall running time: 183.6970s / 153358.8248 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1172
env0_second_0:                 episode reward: -2.7500,                 loss: 1.3321
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 13421/30000 (44.7367%),                 avg. length: 1623.9,                last time consumption/overall running time: 176.3445s / 153535.1693 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.1100
env0_second_0:                 episode reward: -7.0000,                 loss: 0.9581
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 13441/30000 (44.8033%),                 avg. length: 1679.0,                last time consumption/overall running time: 186.9371s / 153722.1064 s
env0_first_0:                 episode reward: 8.5000,                 loss: -0.1128
env0_second_0:                 episode reward: -8.5000,                 loss: 0.7610
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 13461/30000 (44.8700%),                 avg. length: 1648.65,                last time consumption/overall running time: 176.1642s / 153898.2705 s
env0_first_0:                 episode reward: 8.4500,                 loss: -0.1142
env0_second_0:                 episode reward: -8.4500,                 loss: 1.0644
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 13481/30000 (44.9367%),                 avg. length: 1627.2,                last time consumption/overall running time: 179.5303s / 154077.8008 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0798
env0_second_0:                 episode reward: -9.7000,                 loss: 1.3205
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 13501/30000 (45.0033%),                 avg. length: 1720.7,                last time consumption/overall running time: 200.6003s / 154278.4011 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.0835
env0_second_0:                 episode reward: -7.1500,                 loss: 1.3807
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 13521/30000 (45.0700%),                 avg. length: 1926.65,                last time consumption/overall running time: 202.6344s / 154481.0355 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.0569
env0_second_0:                 episode reward: -6.4000,                 loss: 1.7448
env1_first_0:                 episode reward: 9.1500,                 loss: nan
env1_second_0:                 episode reward: -9.1500,                 loss: nan
Episode: 13541/30000 (45.1367%),                 avg. length: 2202.0,                last time consumption/overall running time: 233.5140s / 154714.5494 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.0805
env0_second_0:                 episode reward: -5.7000,                 loss: 1.3599
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 13561/30000 (45.2033%),                 avg. length: 2206.35,                last time consumption/overall running time: 260.2656s / 154974.8150 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.0748
env0_second_0:                 episode reward: -4.8500,                 loss: 1.1047
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 13581/30000 (45.2700%),                 avg. length: 2044.65,                last time consumption/overall running time: 232.5376s / 155207.3526 s
env0_first_0:                 episode reward: 7.3500,                 loss: -0.0400
env0_second_0:                 episode reward: -7.3500,                 loss: 2.1130
env1_first_0:                 episode reward: 7.9500,                 loss: nan
env1_second_0:                 episode reward: -7.9500,                 loss: nan
Episode: 13601/30000 (45.3367%),                 avg. length: 2055.3,                last time consumption/overall running time: 222.3390s / 155429.6915 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1106
env0_second_0:                 episode reward: -4.8000,                 loss: 1.3178
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 13621/30000 (45.4033%),                 avg. length: 2040.4,                last time consumption/overall running time: 235.0147s / 155664.7062 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.0483
env0_second_0:                 episode reward: -0.4500,                 loss: 1.2313
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 13641/30000 (45.4700%),                 avg. length: 2447.4,                last time consumption/overall running time: 263.5839s / 155928.2901 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.0895
env0_second_0:                 episode reward: -3.3000,                 loss: 1.1972
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 13661/30000 (45.5367%),                 avg. length: 1889.85,                last time consumption/overall running time: 199.9385s / 156128.2286 s
env0_first_0:                 episode reward: 8.1500,                 loss: -0.0774
env0_second_0:                 episode reward: -8.1500,                 loss: 1.0890
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 13681/30000 (45.6033%),                 avg. length: 1949.5,                last time consumption/overall running time: 209.6623s / 156337.8909 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.0051
env0_second_0:                 episode reward: -4.4000,                 loss: 1.0508
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 13701/30000 (45.6700%),                 avg. length: 1746.65,                last time consumption/overall running time: 201.5833s / 156539.4742 s
env0_first_0:                 episode reward: 8.7000,                 loss: -0.0183
env0_second_0:                 episode reward: -8.7000,                 loss: 1.3421
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 13721/30000 (45.7367%),                 avg. length: 1811.4,                last time consumption/overall running time: 215.7595s / 156755.2338 s
env0_first_0:                 episode reward: 4.2500,                 loss: -0.0296
env0_second_0:                 episode reward: -4.2500,                 loss: 2.0030
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 13741/30000 (45.8033%),                 avg. length: 1660.3,                last time consumption/overall running time: 189.5815s / 156944.8153 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.0581
env0_second_0:                 episode reward: -9.5500,                 loss: 0.9907
env1_first_0:                 episode reward: 8.8000,                 loss: nan
env1_second_0:                 episode reward: -8.8000,                 loss: nan
Episode: 13761/30000 (45.8700%),                 avg. length: 1711.6,                last time consumption/overall running time: 205.6801s / 157150.4954 s
env0_first_0:                 episode reward: 10.9500,                 loss: -0.0954
env0_second_0:                 episode reward: -10.9500,                 loss: 0.8721
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 13781/30000 (45.9367%),                 avg. length: 1773.4,                last time consumption/overall running time: 197.3035s / 157347.7989 s
env0_first_0:                 episode reward: 8.4000,                 loss: -0.0471
env0_second_0:                 episode reward: -8.4000,                 loss: 0.8242
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 13801/30000 (46.0033%),                 avg. length: 2053.1,                last time consumption/overall running time: 235.9555s / 157583.7544 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.0311
env0_second_0:                 episode reward: -3.9000,                 loss: 0.9564
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 13821/30000 (46.0700%),                 avg. length: 2286.15,                last time consumption/overall running time: 259.0573s / 157842.8117 s
env0_first_0:                 episode reward: 7.0500,                 loss: 0.0087
env0_second_0:                 episode reward: -7.0500,                 loss: 0.9655
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 13841/30000 (46.1367%),                 avg. length: 2345.7,                last time consumption/overall running time: 253.1306s / 158095.9424 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.0177
env0_second_0:                 episode reward: 1.5000,                 loss: 0.7503
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 13861/30000 (46.2033%),                 avg. length: 2222.85,                last time consumption/overall running time: 237.4474s / 158333.3898 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0399
env0_second_0:                 episode reward: 2.3000,                 loss: 0.8631
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 13881/30000 (46.2700%),                 avg. length: 2131.5,                last time consumption/overall running time: 239.6971s / 158573.0869 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.0642
env0_second_0:                 episode reward: 1.3500,                 loss: 0.7138
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 13901/30000 (46.3367%),                 avg. length: 1780.7,                last time consumption/overall running time: 207.7529s / 158780.8398 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.0753
env0_second_0:                 episode reward: -3.9500,                 loss: 1.0284
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 13921/30000 (46.4033%),                 avg. length: 2011.25,                last time consumption/overall running time: 205.0344s / 158985.8742 s
env0_first_0:                 episode reward: 6.7500,                 loss: -0.0116
env0_second_0:                 episode reward: -6.7500,                 loss: 1.0426
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 13941/30000 (46.4700%),                 avg. length: 1666.55,                last time consumption/overall running time: 197.7064s / 159183.5806 s
env0_first_0:                 episode reward: 8.7000,                 loss: -0.0315
env0_second_0:                 episode reward: -8.7000,                 loss: 0.9318
env1_first_0:                 episode reward: 8.7000,                 loss: nan
env1_second_0:                 episode reward: -8.7000,                 loss: nan
Episode: 13961/30000 (46.5367%),                 avg. length: 1562.35,                last time consumption/overall running time: 180.8647s / 159364.4453 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0748
env0_second_0:                 episode reward: -9.7000,                 loss: 1.1940
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 13981/30000 (46.6033%),                 avg. length: 1538.45,                last time consumption/overall running time: 177.1687s / 159541.6140 s
env0_first_0:                 episode reward: 7.8500,                 loss: -0.1018
env0_second_0:                 episode reward: -7.8500,                 loss: 1.0617
env1_first_0:                 episode reward: 8.7000,                 loss: nan
env1_second_0:                 episode reward: -8.7000,                 loss: nan
Episode: 14001/30000 (46.6700%),                 avg. length: 2031.3,                last time consumption/overall running time: 218.4660s / 159760.0800 s
env0_first_0:                 episode reward: 10.4500,                 loss: -0.0686
env0_second_0:                 episode reward: -10.4500,                 loss: 1.6317
env1_first_0:                 episode reward: 9.2000,                 loss: nan
env1_second_0:                 episode reward: -9.2000,                 loss: nan
Episode: 14021/30000 (46.7367%),                 avg. length: 1864.75,                last time consumption/overall running time: 213.2200s / 159973.3000 s
env0_first_0:                 episode reward: 5.3500,                 loss: -0.1298
env0_second_0:                 episode reward: -5.3500,                 loss: 0.9576
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 14041/30000 (46.8033%),                 avg. length: 1934.65,                last time consumption/overall running time: 215.5083s / 160188.8084 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.1493
env0_second_0:                 episode reward: -4.9500,                 loss: 0.9619
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 14061/30000 (46.8700%),                 avg. length: 1744.35,                last time consumption/overall running time: 192.0393s / 160380.8476 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.1444
env0_second_0:                 episode reward: -6.6000,                 loss: 0.8280
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 14081/30000 (46.9367%),                 avg. length: 1545.2,                last time consumption/overall running time: 176.8442s / 160557.6919 s
env0_first_0:                 episode reward: 7.0500,                 loss: -0.1456
env0_second_0:                 episode reward: -7.0500,                 loss: 0.7294
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 14101/30000 (47.0033%),                 avg. length: 1754.7,                last time consumption/overall running time: 192.8147s / 160750.5066 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.1567
env0_second_0:                 episode reward: -4.8500,                 loss: 0.8144
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 14121/30000 (47.0700%),                 avg. length: 2217.95,                last time consumption/overall running time: 252.5089s / 161003.0155 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.1197
env0_second_0:                 episode reward: -1.1000,                 loss: 0.9882
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 14141/30000 (47.1367%),                 avg. length: 1901.55,                last time consumption/overall running time: 217.7339s / 161220.7494 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1321
env0_second_0:                 episode reward: -2.7500,                 loss: 0.7894
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 14161/30000 (47.2033%),                 avg. length: 1852.8,                last time consumption/overall running time: 208.8293s / 161429.5787 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.1256
env0_second_0:                 episode reward: -5.7500,                 loss: 1.3418
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 14181/30000 (47.2700%),                 avg. length: 2135.5,                last time consumption/overall running time: 233.8492s / 161663.4279 s
env0_first_0:                 episode reward: 8.5500,                 loss: -0.1010
env0_second_0:                 episode reward: -8.5500,                 loss: 1.0614
env1_first_0:                 episode reward: 7.7500,                 loss: nan
env1_second_0:                 episode reward: -7.7500,                 loss: nan
Episode: 14201/30000 (47.3367%),                 avg. length: 1916.2,                last time consumption/overall running time: 209.1427s / 161872.5706 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1327
env0_second_0:                 episode reward: -3.2500,                 loss: 0.7816
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 14221/30000 (47.4033%),                 avg. length: 1630.85,                last time consumption/overall running time: 188.0690s / 162060.6396 s
env0_first_0:                 episode reward: 8.7000,                 loss: -0.1216
env0_second_0:                 episode reward: -8.7000,                 loss: 0.9130
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 14241/30000 (47.4700%),                 avg. length: 3610.2,                last time consumption/overall running time: 387.9995s / 162448.6391 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.1830
env0_second_0:                 episode reward: 0.5500,                 loss: 1.1425
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 14261/30000 (47.5367%),                 avg. length: 2556.8,                last time consumption/overall running time: 279.8441s / 162728.4832 s
env0_first_0:                 episode reward: 6.0000,                 loss: -0.1515
env0_second_0:                 episode reward: -6.0000,                 loss: 1.0160
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 14281/30000 (47.6033%),                 avg. length: 2690.95,                last time consumption/overall running time: 311.2661s / 163039.7493 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1308
env0_second_0:                 episode reward: -3.2500,                 loss: 0.7059
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 14301/30000 (47.6700%),                 avg. length: 4777.75,                last time consumption/overall running time: 518.2339s / 163557.9832 s
env0_first_0:                 episode reward: 6.4500,                 loss: -0.1837
env0_second_0:                 episode reward: -6.4500,                 loss: 0.6799
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 14321/30000 (47.7367%),                 avg. length: 6723.75,                last time consumption/overall running time: 712.4477s / 164270.4308 s
env0_first_0:                 episode reward: 14.6000,                 loss: -0.1633
env0_second_0:                 episode reward: -14.6000,                 loss: 0.6177
env1_first_0:                 episode reward: 14.6000,                 loss: nan
env1_second_0:                 episode reward: -14.6000,                 loss: nan
Episode: 14341/30000 (47.8033%),                 avg. length: 3222.95,                last time consumption/overall running time: 354.2806s / 164624.7114 s
env0_first_0:                 episode reward: 7.2000,                 loss: -0.1598
env0_second_0:                 episode reward: -7.2000,                 loss: 0.6927
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 14361/30000 (47.8700%),                 avg. length: 2820.45,                last time consumption/overall running time: 312.4271s / 164937.1385 s
env0_first_0:                 episode reward: 6.1000,                 loss: -0.1665
env0_second_0:                 episode reward: -6.1000,                 loss: 0.6239
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 14381/30000 (47.9367%),                 avg. length: 1847.25,                last time consumption/overall running time: 232.7036s / 165169.8421 s
env0_first_0:                 episode reward: 7.8000,                 loss: -0.1581
env0_second_0:                 episode reward: -7.8000,                 loss: 0.9049
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 14401/30000 (48.0033%),                 avg. length: 3275.5,                last time consumption/overall running time: 348.2241s / 165518.0662 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.1666
env0_second_0:                 episode reward: -5.2500,                 loss: 0.6996
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 14421/30000 (48.0700%),                 avg. length: 2846.65,                last time consumption/overall running time: 309.6881s / 165827.7542 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.1357
env0_second_0:                 episode reward: -7.1500,                 loss: 0.6424
env1_first_0:                 episode reward: 7.8500,                 loss: nan
env1_second_0:                 episode reward: -7.8500,                 loss: nan
Episode: 14441/30000 (48.1367%),                 avg. length: 2291.85,                last time consumption/overall running time: 268.8426s / 166096.5969 s
env0_first_0:                 episode reward: 5.2000,                 loss: -0.1392
env0_second_0:                 episode reward: -5.2000,                 loss: 1.2235
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 14461/30000 (48.2033%),                 avg. length: 2090.95,                last time consumption/overall running time: 228.5885s / 166325.1854 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.1484
env0_second_0:                 episode reward: -4.4000,                 loss: 0.7061
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 14481/30000 (48.2700%),                 avg. length: 1837.25,                last time consumption/overall running time: 204.2160s / 166529.4014 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1079
env0_second_0:                 episode reward: -4.6000,                 loss: 1.7032
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 14501/30000 (48.3367%),                 avg. length: 1988.45,                last time consumption/overall running time: 237.7577s / 166767.1590 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.0737
env0_second_0:                 episode reward: -2.2000,                 loss: 1.4990
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 14521/30000 (48.4033%),                 avg. length: 2150.25,                last time consumption/overall running time: 239.8014s / 167006.9604 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.0657
env0_second_0:                 episode reward: 4.6000,                 loss: 1.3326
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 14541/30000 (48.4700%),                 avg. length: 2073.0,                last time consumption/overall running time: 240.7600s / 167247.7204 s