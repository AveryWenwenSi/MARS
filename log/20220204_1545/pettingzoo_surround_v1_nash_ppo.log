pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220204_1545/pettingzoo_surround_v1_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220204_1545/pettingzoo_surround_v1_nash_ppo.
Episode: 1/30000 (0.0033%),                 avg. length: 1427.0,                last time consumption/overall running time: 8.7746s / 8.7746 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.0058
env0_second_0:                 episode reward: 4.0000,                 loss: -0.0106
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 1436.25,                last time consumption/overall running time: 156.6050s / 165.3796 s
env0_first_0:                 episode reward: -3.2500,                 loss: -0.0370
env0_second_0:                 episode reward: 3.2500,                 loss: -0.0350
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 1519.25,                last time consumption/overall running time: 165.1424s / 330.5221 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.0432
env0_second_0:                 episode reward: 2.2000,                 loss: -0.0411
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 1761.6,                last time consumption/overall running time: 199.1127s / 529.6348 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.0613
env0_second_0:                 episode reward: 2.0500,                 loss: -0.0636
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2352.3,                last time consumption/overall running time: 231.6802s / 761.3150 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.0539
env0_second_0:                 episode reward: 2.0000,                 loss: -0.0520
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2415.65,                last time consumption/overall running time: 242.9395s / 1004.2545 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.0607
env0_second_0:                 episode reward: 1.2000,                 loss: -0.0575
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2554.8,                last time consumption/overall running time: 254.5425s / 1258.7969 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0701
env0_second_0:                 episode reward: 0.9500,                 loss: -0.0666
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2489.75,                last time consumption/overall running time: 268.7165s / 1527.5135 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0778
env0_second_0:                 episode reward: 0.7500,                 loss: -0.0745
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2057.65,                last time consumption/overall running time: 218.8398s / 1746.3533 s
env0_first_0:                 episode reward: -5.6000,                 loss: -0.0788
env0_second_0:                 episode reward: 5.6000,                 loss: -0.0775
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2238.1,                last time consumption/overall running time: 231.6657s / 1978.0191 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.0619
env0_second_0:                 episode reward: 4.6500,                 loss: -0.0557
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2093.4,                last time consumption/overall running time: 238.5950s / 2216.6141 s
env0_first_0:                 episode reward: -4.3500,                 loss: -0.0534
env0_second_0:                 episode reward: 4.3500,                 loss: -0.0514
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2145.65,                last time consumption/overall running time: 219.4582s / 2436.0724 s
env0_first_0:                 episode reward: -5.5500,                 loss: -0.0687
env0_second_0:                 episode reward: 5.5500,                 loss: -0.0570
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 241/30000 (0.8033%),                 avg. length: 2261.4,                last time consumption/overall running time: 236.4322s / 2672.5046 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.0629
env0_second_0:                 episode reward: 3.9500,                 loss: -0.0562
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 261/30000 (0.8700%),                 avg. length: 2278.25,                last time consumption/overall running time: 248.4544s / 2920.9590 s
env0_first_0:                 episode reward: -4.9500,                 loss: -0.0779
env0_second_0:                 episode reward: 4.9500,                 loss: -0.0721
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 281/30000 (0.9367%),                 avg. length: 2146.7,                last time consumption/overall running time: 227.6992s / 3148.6582 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.0556
env0_second_0:                 episode reward: 5.7000,                 loss: -0.0504
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 301/30000 (1.0033%),                 avg. length: 2013.45,                last time consumption/overall running time: 221.7491s / 3370.4074 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.0508
env0_second_0:                 episode reward: 5.8000,                 loss: -0.0393
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 321/30000 (1.0700%),                 avg. length: 2257.6,                last time consumption/overall running time: 251.0139s / 3621.4212 s
env0_first_0:                 episode reward: -4.8000,                 loss: -0.0420
env0_second_0:                 episode reward: 4.8000,                 loss: -0.0295
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 341/30000 (1.1367%),                 avg. length: 2119.7,                last time consumption/overall running time: 248.5855s / 3870.0067 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.0539
env0_second_0:                 episode reward: 5.4000,                 loss: -0.0365
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 361/30000 (1.2033%),                 avg. length: 2146.95,                last time consumption/overall running time: 250.2449s / 4120.2517 s
env0_first_0:                 episode reward: -5.3500,                 loss: -0.0494
env0_second_0:                 episode reward: 5.3500,                 loss: -0.0397
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 381/30000 (1.2700%),                 avg. length: 1957.4,                last time consumption/overall running time: 201.9777s / 4322.2293 s
env0_first_0:                 episode reward: -6.6500,                 loss: -0.0725
env0_second_0:                 episode reward: 6.6500,                 loss: -0.0617
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 401/30000 (1.3367%),                 avg. length: 1909.85,                last time consumption/overall running time: 194.0063s / 4516.2356 s
env0_first_0:                 episode reward: -8.2500,                 loss: -0.0632
env0_second_0:                 episode reward: 8.2500,                 loss: -0.0502
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 421/30000 (1.4033%),                 avg. length: 1813.65,                last time consumption/overall running time: 202.6120s / 4718.8476 s
env0_first_0:                 episode reward: -8.5000,                 loss: -0.0600
env0_second_0:                 episode reward: 8.5000,                 loss: -0.0416
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 441/30000 (1.4700%),                 avg. length: 1981.2,                last time consumption/overall running time: 209.9889s / 4928.8366 s
env0_first_0:                 episode reward: -6.6000,                 loss: -0.0742
env0_second_0:                 episode reward: 6.6000,                 loss: -0.0524
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 461/30000 (1.5367%),                 avg. length: 2002.15,                last time consumption/overall running time: 192.1616s / 5120.9982 s
env0_first_0:                 episode reward: -6.2000,                 loss: -0.0815
env0_second_0:                 episode reward: 6.2000,                 loss: -0.0597
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 481/30000 (1.6033%),                 avg. length: 1825.15,                last time consumption/overall running time: 191.7736s / 5312.7718 s
env0_first_0:                 episode reward: -8.3000,                 loss: -0.0909
env0_second_0:                 episode reward: 8.3000,                 loss: -0.0702
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 501/30000 (1.6700%),                 avg. length: 2057.9,                last time consumption/overall running time: 233.3792s / 5546.1510 s
env0_first_0:                 episode reward: -6.5000,                 loss: -0.0772
env0_second_0:                 episode reward: 6.5000,                 loss: -0.0573
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 521/30000 (1.7367%),                 avg. length: 2052.85,                last time consumption/overall running time: 224.2684s / 5770.4195 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.0591
env0_second_0:                 episode reward: 4.4000,                 loss: -0.0417
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 541/30000 (1.8033%),                 avg. length: 1942.15,                last time consumption/overall running time: 194.3047s / 5964.7242 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.1059
env0_second_0:                 episode reward: 7.5500,                 loss: -0.0856
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 561/30000 (1.8700%),                 avg. length: 2044.15,                last time consumption/overall running time: 219.5146s / 6184.2388 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.1283
env0_second_0:                 episode reward: 6.0500,                 loss: -0.1045
env1_first_0:                 episode reward: -8.0500,                 loss: nan
env1_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 581/30000 (1.9367%),                 avg. length: 1989.3,                last time consumption/overall running time: 216.4906s / 6400.7294 s
env0_first_0:                 episode reward: -4.9500,                 loss: -0.1167
env0_second_0:                 episode reward: 4.9500,                 loss: -0.0828
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 601/30000 (2.0033%),                 avg. length: 2221.9,                last time consumption/overall running time: 222.4364s / 6623.1659 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.0878
env0_second_0:                 episode reward: -1.0500,                 loss: -0.0597
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 621/30000 (2.0700%),                 avg. length: 2301.9,                last time consumption/overall running time: 236.9394s / 6860.1053 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.1001
env0_second_0:                 episode reward: 2.1000,                 loss: -0.0639
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 641/30000 (2.1367%),                 avg. length: 2265.8,                last time consumption/overall running time: 237.4089s / 7097.5142 s
env0_first_0:                 episode reward: -4.1500,                 loss: -0.1135
env0_second_0:                 episode reward: 4.1500,                 loss: -0.0760
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 661/30000 (2.2033%),                 avg. length: 2543.55,                last time consumption/overall running time: 253.8406s / 7351.3548 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.0849
env0_second_0:                 episode reward: 5.1000,                 loss: -0.0452
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 681/30000 (2.2700%),                 avg. length: 2909.6,                last time consumption/overall running time: 296.9828s / 7648.3376 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.0667
env0_second_0:                 episode reward: 2.3000,                 loss: -0.0201
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 701/30000 (2.3367%),                 avg. length: 2805.1,                last time consumption/overall running time: 310.0068s / 7958.3444 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.0814
env0_second_0:                 episode reward: 1.1500,                 loss: -0.0466
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 721/30000 (2.4033%),                 avg. length: 2799.05,                last time consumption/overall running time: 306.2358s / 8264.5803 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.0803
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0338
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 741/30000 (2.4700%),                 avg. length: 2828.15,                last time consumption/overall running time: 291.6415s / 8556.2218 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.0707
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0423
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 761/30000 (2.5367%),                 avg. length: 2894.6,                last time consumption/overall running time: 330.5446s / 8886.7665 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.0565
env0_second_0:                 episode reward: -3.2500,                 loss: -0.0107
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 781/30000 (2.6033%),                 avg. length: 2891.8,                last time consumption/overall running time: 311.6189s / 9198.3854 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.0586
env0_second_0:                 episode reward: -1.8000,                 loss: -0.0216
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 801/30000 (2.6700%),                 avg. length: 2579.55,                last time consumption/overall running time: 294.7230s / 9493.1084 s
env0_first_0:                 episode reward: 5.2000,                 loss: -0.0534
env0_second_0:                 episode reward: -5.2000,                 loss: -0.0165
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 821/30000 (2.7367%),                 avg. length: 1884.25,                last time consumption/overall running time: 207.9546s / 9701.0629 s
env0_first_0:                 episode reward: 6.7000,                 loss: -0.0671
env0_second_0:                 episode reward: -6.7000,                 loss: -0.0240
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 841/30000 (2.8033%),                 avg. length: 2456.05,                last time consumption/overall running time: 265.4120s / 9966.4749 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.0427
env0_second_0:                 episode reward: 0.1000,                 loss: -0.0187
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 861/30000 (2.8700%),                 avg. length: 1951.35,                last time consumption/overall running time: 225.5523s / 10192.0272 s
env0_first_0:                 episode reward: -8.2000,                 loss: -0.0814
env0_second_0:                 episode reward: 8.2000,                 loss: -0.0496
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 881/30000 (2.9367%),                 avg. length: 2095.4,                last time consumption/overall running time: 218.6508s / 10410.6780 s
env0_first_0:                 episode reward: -6.2000,                 loss: -0.0574
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0029
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 901/30000 (3.0033%),                 avg. length: 2380.0,                last time consumption/overall running time: 275.0591s / 10685.7371 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.0568
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0306
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 921/30000 (3.0700%),                 avg. length: 2049.3,                last time consumption/overall running time: 232.6959s / 10918.4331 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.0639
env0_second_0:                 episode reward: 5.1000,                 loss: 0.2036
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 941/30000 (3.1367%),                 avg. length: 2536.3,                last time consumption/overall running time: 291.9838s / 11210.4169 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.0808
env0_second_0:                 episode reward: 2.5000,                 loss: 0.0329
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 961/30000 (3.2033%),                 avg. length: 2825.75,                last time consumption/overall running time: 305.8879s / 11516.3048 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0686
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0130
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 981/30000 (3.2700%),                 avg. length: 2486.45,                last time consumption/overall running time: 272.8706s / 11789.1754 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.0499
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0208
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 1001/30000 (3.3367%),                 avg. length: 2659.3,                last time consumption/overall running time: 278.4727s / 12067.6481 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.0588
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0407
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 1021/30000 (3.4033%),                 avg. length: 2583.25,                last time consumption/overall running time: 271.2408s / 12338.8888 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.0650
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0099
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 1041/30000 (3.4700%),                 avg. length: 2897.9,                last time consumption/overall running time: 322.4699s / 12661.3587 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.0562
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0291
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 1061/30000 (3.5367%),                 avg. length: 2721.0,                last time consumption/overall running time: 282.9606s / 12944.3193 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.0902
env0_second_0:                 episode reward: -3.9500,                 loss: -0.0240
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 1081/30000 (3.6033%),                 avg. length: 2711.75,                last time consumption/overall running time: 294.2285s / 13238.5477 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.0614
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0138
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1101/30000 (3.6700%),                 avg. length: 2434.65,                last time consumption/overall running time: 247.5712s / 13486.1189 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.0700
env0_second_0:                 episode reward: -5.1500,                 loss: -0.0068
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 1121/30000 (3.7367%),                 avg. length: 2906.55,                last time consumption/overall running time: 326.6421s / 13812.7610 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.0611
env0_second_0:                 episode reward: -3.9500,                 loss: 0.0038
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 1141/30000 (3.8033%),                 avg. length: 2873.15,                last time consumption/overall running time: 304.1860s / 14116.9470 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.0565
env0_second_0:                 episode reward: -3.2000,                 loss: -0.0192
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 1161/30000 (3.8700%),                 avg. length: 2504.55,                last time consumption/overall running time: 259.3677s / 14376.3146 s
env0_first_0:                 episode reward: 5.0000,                 loss: -0.0641
env0_second_0:                 episode reward: -5.0000,                 loss: -0.0055
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 1181/30000 (3.9367%),                 avg. length: 2393.4,                last time consumption/overall running time: 237.3581s / 14613.6727 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.0837
env0_second_0:                 episode reward: -5.4500,                 loss: -0.0312
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 1201/30000 (4.0033%),                 avg. length: 2190.45,                last time consumption/overall running time: 244.2208s / 14857.8935 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.0780
env0_second_0:                 episode reward: -6.6500,                 loss: -0.0314
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 1221/30000 (4.0700%),                 avg. length: 2030.15,                last time consumption/overall running time: 198.2485s / 15056.1421 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.0717
env0_second_0:                 episode reward: -6.8000,                 loss: -0.0160
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 1241/30000 (4.1367%),                 avg. length: 1938.15,                last time consumption/overall running time: 193.4863s / 15249.6284 s
env0_first_0:                 episode reward: 7.9000,                 loss: -0.0496
env0_second_0:                 episode reward: -7.9000,                 loss: 0.4182
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 1261/30000 (4.2033%),                 avg. length: 1993.55,                last time consumption/overall running time: 203.9969s / 15453.6253 s
env0_first_0:                 episode reward: 8.1500,                 loss: -0.0885
env0_second_0:                 episode reward: -8.1500,                 loss: -0.0178
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 1281/30000 (4.2700%),                 avg. length: 2304.3,                last time consumption/overall running time: 236.5748s / 15690.2001 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.0671
env0_second_0:                 episode reward: -5.2500,                 loss: 0.0002
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 1301/30000 (4.3367%),                 avg. length: 2785.75,                last time consumption/overall running time: 291.9722s / 15982.1723 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.0606
env0_second_0:                 episode reward: -4.9000,                 loss: 0.0064
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 1321/30000 (4.4033%),                 avg. length: 2533.3,                last time consumption/overall running time: 278.2633s / 16260.4357 s
env0_first_0:                 episode reward: 5.6500,                 loss: -0.0599
env0_second_0:                 episode reward: -5.6500,                 loss: 0.0511
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 1341/30000 (4.4700%),                 avg. length: 2561.85,                last time consumption/overall running time: 269.2545s / 16529.6901 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.0481
env0_second_0:                 episode reward: -5.7500,                 loss: 0.0541
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 1361/30000 (4.5367%),                 avg. length: 2850.3,                last time consumption/overall running time: 285.8422s / 16815.5324 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.0201
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0804
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1381/30000 (4.6033%),                 avg. length: 2548.45,                last time consumption/overall running time: 260.8573s / 17076.3897 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.0096
env0_second_0:                 episode reward: -4.7000,                 loss: 0.2243
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 1401/30000 (4.6700%),                 avg. length: 3057.8,                last time consumption/overall running time: 310.3086s / 17386.6983 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.0095
env0_second_0:                 episode reward: -2.2500,                 loss: 0.0788
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 1421/30000 (4.7367%),                 avg. length: 2944.35,                last time consumption/overall running time: 305.3718s / 17692.0701 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.0283
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0531
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 1441/30000 (4.8033%),                 avg. length: 2508.95,                last time consumption/overall running time: 282.5552s / 17974.6253 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0380
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0232
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 1461/30000 (4.8700%),                 avg. length: 2745.7,                last time consumption/overall running time: 326.6533s / 18301.2786 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.0414
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0197
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 1481/30000 (4.9367%),                 avg. length: 2872.65,                last time consumption/overall running time: 342.2568s / 18643.5354 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0311
env0_second_0:                 episode reward: -0.6000,                 loss: 0.1434
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 1501/30000 (5.0033%),                 avg. length: 2698.8,                last time consumption/overall running time: 307.9801s / 18951.5155 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.0458
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0444
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 1521/30000 (5.0700%),                 avg. length: 2809.0,                last time consumption/overall running time: 297.5159s / 19249.0314 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.0518
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0502
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 1541/30000 (5.1367%),                 avg. length: 2563.25,                last time consumption/overall running time: 277.2144s / 19526.2458 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.0447
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0671
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1561/30000 (5.2033%),                 avg. length: 2697.4,                last time consumption/overall running time: 294.5739s / 19820.8197 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0047
env0_second_0:                 episode reward: 2.0000,                 loss: 0.8756
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 1581/30000 (5.2700%),                 avg. length: 2445.25,                last time consumption/overall running time: 253.6324s / 20074.4522 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.0180
env0_second_0:                 episode reward: -4.6500,                 loss: 0.2898
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 1601/30000 (5.3367%),                 avg. length: 2212.25,                last time consumption/overall running time: 218.3060s / 20292.7581 s
env0_first_0:                 episode reward: 6.0000,                 loss: -0.0597
env0_second_0:                 episode reward: -6.0000,                 loss: 0.3504
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 1621/30000 (5.4033%),                 avg. length: 2660.9,                last time consumption/overall running time: 297.9954s / 20590.7535 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.0243
env0_second_0:                 episode reward: -2.8000,                 loss: 0.2249
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 1641/30000 (5.4700%),                 avg. length: 3042.75,                last time consumption/overall running time: 365.6457s / 20956.3992 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.0514
env0_second_0:                 episode reward: -3.2000,                 loss: 0.2637
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 1661/30000 (5.5367%),                 avg. length: 2795.55,                last time consumption/overall running time: 308.3553s / 21264.7545 s
env0_first_0:                 episode reward: 4.2500,                 loss: -0.0507
env0_second_0:                 episode reward: -4.2500,                 loss: 0.1316
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 1681/30000 (5.6033%),                 avg. length: 2821.45,                last time consumption/overall running time: 294.0373s / 21558.7919 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.0756
env0_second_0:                 episode reward: -4.6000,                 loss: 0.0228
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 1701/30000 (5.6700%),                 avg. length: 2662.5,                last time consumption/overall running time: 314.7248s / 21873.5167 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.0651
env0_second_0:                 episode reward: -5.7500,                 loss: 0.2950
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 1721/30000 (5.7367%),                 avg. length: 2535.25,                last time consumption/overall running time: 284.5868s / 22158.1035 s
env0_first_0:                 episode reward: 6.3000,                 loss: -0.0665
env0_second_0:                 episode reward: -6.3000,                 loss: 0.0513
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 1741/30000 (5.8033%),                 avg. length: 2844.7,                last time consumption/overall running time: 303.9477s / 22462.0512 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.0255
env0_second_0:                 episode reward: -3.7000,                 loss: 0.1677
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 1761/30000 (5.8700%),                 avg. length: 2736.9,                last time consumption/overall running time: 280.7952s / 22742.8464 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.0147
env0_second_0:                 episode reward: -5.4000,                 loss: 0.7013
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1781/30000 (5.9367%),                 avg. length: 2498.15,                last time consumption/overall running time: 257.7890s / 23000.6355 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.0630
env0_second_0:                 episode reward: -4.9000,                 loss: 0.6164
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 1801/30000 (6.0033%),                 avg. length: 2808.15,                last time consumption/overall running time: 300.1103s / 23300.7457 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.0588
env0_second_0:                 episode reward: -4.0500,                 loss: 0.6493
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1821/30000 (6.0700%),                 avg. length: 2811.9,                last time consumption/overall running time: 342.0217s / 23642.7675 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.0223
env0_second_0:                 episode reward: -4.1000,                 loss: 0.2698
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 1841/30000 (6.1367%),                 avg. length: 2561.05,                last time consumption/overall running time: 291.6329s / 23934.4004 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.0504
env0_second_0:                 episode reward: -4.9500,                 loss: 0.2398
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 1861/30000 (6.2033%),                 avg. length: 2500.95,                last time consumption/overall running time: 275.1304s / 24209.5307 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.0756
env0_second_0:                 episode reward: -6.0500,                 loss: 0.0642
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 1881/30000 (6.2700%),                 avg. length: 2557.6,                last time consumption/overall running time: 311.2407s / 24520.7715 s
env0_first_0:                 episode reward: 6.2500,                 loss: -0.0431
env0_second_0:                 episode reward: -6.2500,                 loss: 0.1400
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 1901/30000 (6.3367%),                 avg. length: 2358.0,                last time consumption/overall running time: 288.6811s / 24809.4526 s
env0_first_0:                 episode reward: 7.1000,                 loss: -0.0278
env0_second_0:                 episode reward: -7.1000,                 loss: 0.2279
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 1921/30000 (6.4033%),                 avg. length: 2706.7,                last time consumption/overall running time: 323.7522s / 25133.2047 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0139
env0_second_0:                 episode reward: -3.6500,                 loss: 0.1700
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 1941/30000 (6.4700%),                 avg. length: 2474.0,                last time consumption/overall running time: 285.4548s / 25418.6595 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.0598
env0_second_0:                 episode reward: -5.8500,                 loss: 0.1465
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1961/30000 (6.5367%),                 avg. length: 2649.3,                last time consumption/overall running time: 316.3534s / 25735.0129 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.0487
env0_second_0:                 episode reward: -5.8500,                 loss: 0.0871
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 1981/30000 (6.6033%),                 avg. length: 1359.35,                last time consumption/overall running time: 165.0618s / 25900.0747 s
env0_first_0:                 episode reward: 3.8000,                 loss: 0.0467
env0_second_0:                 episode reward: -3.8000,                 loss: 0.2002
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 2001/30000 (6.6700%),                 avg. length: 1266.9,                last time consumption/overall running time: 154.5050s / 26054.5798 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.0526
env0_second_0:                 episode reward: -5.6000,                 loss: 0.0421
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 2021/30000 (6.7367%),                 avg. length: 1417.95,                last time consumption/overall running time: 158.6782s / 26213.2580 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.0275
env0_second_0:                 episode reward: -5.4000,                 loss: 0.0528
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 2041/30000 (6.8033%),                 avg. length: 1546.1,                last time consumption/overall running time: 173.7425s / 26387.0005 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.0095
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0181
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 2061/30000 (6.8700%),                 avg. length: 1538.1,                last time consumption/overall running time: 172.1002s / 26559.1007 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.0154
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0094
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 2081/30000 (6.9367%),                 avg. length: 1356.6,                last time consumption/overall running time: 154.2214s / 26713.3221 s
env0_first_0:                 episode reward: 6.2000,                 loss: -0.0531
env0_second_0:                 episode reward: -6.2000,                 loss: -0.0204
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 2101/30000 (7.0033%),                 avg. length: 1366.15,                last time consumption/overall running time: 171.3140s / 26884.6361 s
env0_first_0:                 episode reward: 7.2500,                 loss: -0.0567
env0_second_0:                 episode reward: -7.2500,                 loss: 0.0210
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 2121/30000 (7.0700%),                 avg. length: 1367.25,                last time consumption/overall running time: 151.2372s / 27035.8733 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.0577
env0_second_0:                 episode reward: -7.7500,                 loss: 0.0343
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 2141/30000 (7.1367%),                 avg. length: 1459.5,                last time consumption/overall running time: 166.4016s / 27202.2750 s
env0_first_0:                 episode reward: 7.2000,                 loss: -0.0503
env0_second_0:                 episode reward: -7.2000,                 loss: 0.0514
env1_first_0:                 episode reward: 7.9500,                 loss: nan
env1_second_0:                 episode reward: -7.9500,                 loss: nan
Episode: 2161/30000 (7.2033%),                 avg. length: 1936.7,                last time consumption/overall running time: 232.0842s / 27434.3591 s
env0_first_0:                 episode reward: 6.1000,                 loss: -0.0290
env0_second_0:                 episode reward: -6.1000,                 loss: 0.1415
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 2181/30000 (7.2700%),                 avg. length: 2804.55,                last time consumption/overall running time: 286.5653s / 27720.9245 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.0366
env0_second_0:                 episode reward: -1.9500,                 loss: 0.1211
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2201/30000 (7.3367%),                 avg. length: 2828.65,                last time consumption/overall running time: 309.2054s / 28030.1299 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.0494
env0_second_0:                 episode reward: -3.8500,                 loss: 0.1694
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 2221/30000 (7.4033%),                 avg. length: 2308.25,                last time consumption/overall running time: 237.0565s / 28267.1865 s
env0_first_0:                 episode reward: 6.1500,                 loss: -0.0476
env0_second_0:                 episode reward: -6.1500,                 loss: 0.0779
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 2241/30000 (7.4700%),                 avg. length: 2437.9,                last time consumption/overall running time: 259.2475s / 28526.4340 s
env0_first_0:                 episode reward: 6.1500,                 loss: -0.0366
env0_second_0:                 episode reward: -6.1500,                 loss: 0.1016
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 2261/30000 (7.5367%),                 avg. length: 2561.0,                last time consumption/overall running time: 268.7086s / 28795.1425 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.0389
env0_second_0:                 episode reward: -5.4500,                 loss: 0.0466
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 2281/30000 (7.6033%),                 avg. length: 2359.95,                last time consumption/overall running time: 257.5445s / 29052.6870 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.0360
env0_second_0:                 episode reward: -6.4000,                 loss: 0.0609
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 2301/30000 (7.6700%),                 avg. length: 2383.95,                last time consumption/overall running time: 258.3478s / 29311.0348 s
env0_first_0:                 episode reward: 5.6500,                 loss: -0.0512
env0_second_0:                 episode reward: -5.6500,                 loss: 0.0535
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 2321/30000 (7.7367%),                 avg. length: 1796.9,                last time consumption/overall running time: 190.7934s / 29501.8283 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.0570
env0_second_0:                 episode reward: -7.7500,                 loss: 0.0748
env1_first_0:                 episode reward: 7.3500,                 loss: nan
env1_second_0:                 episode reward: -7.3500,                 loss: nan
Episode: 2341/30000 (7.8033%),                 avg. length: 2369.65,                last time consumption/overall running time: 262.6839s / 29764.5122 s
env0_first_0:                 episode reward: 5.5500,                 loss: -0.0445
env0_second_0:                 episode reward: -5.5500,                 loss: 0.1098
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 2361/30000 (7.8700%),                 avg. length: 2701.35,                last time consumption/overall running time: 316.1965s / 30080.7087 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.0433
env0_second_0:                 episode reward: -4.7500,                 loss: 0.0803
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 2381/30000 (7.9367%),                 avg. length: 2887.35,                last time consumption/overall running time: 323.3406s / 30404.0493 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.0348
env0_second_0:                 episode reward: -4.5000,                 loss: 0.0410
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 2401/30000 (8.0033%),                 avg. length: 2822.05,                last time consumption/overall running time: 301.3909s / 30705.4401 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.0339
env0_second_0:                 episode reward: -4.8000,                 loss: 0.0749
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 2421/30000 (8.0700%),                 avg. length: 2918.3,                last time consumption/overall running time: 326.5175s / 31031.9577 s
env0_first_0:                 episode reward: 4.2500,                 loss: -0.0335
env0_second_0:                 episode reward: -4.2500,                 loss: 0.0364
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 2441/30000 (8.1367%),                 avg. length: 2650.75,                last time consumption/overall running time: 305.8059s / 31337.7636 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.0138
env0_second_0:                 episode reward: -5.6000,                 loss: 0.1138
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 2461/30000 (8.2033%),                 avg. length: 2205.8,                last time consumption/overall running time: 265.1473s / 31602.9108 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.0572
env0_second_0:                 episode reward: -7.3000,                 loss: 0.1802
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 2481/30000 (8.2700%),                 avg. length: 2188.1,                last time consumption/overall running time: 257.7534s / 31860.6642 s
env0_first_0:                 episode reward: 6.9500,                 loss: -0.0429
env0_second_0:                 episode reward: -6.9500,                 loss: 0.0481
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 2501/30000 (8.3367%),                 avg. length: 2848.3,                last time consumption/overall running time: 311.6141s / 32172.2782 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.0473
env0_second_0:                 episode reward: -4.7000,                 loss: 0.1522
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 2521/30000 (8.4033%),                 avg. length: 2683.95,                last time consumption/overall running time: 321.6443s / 32493.9226 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.0581
env0_second_0:                 episode reward: -4.3500,                 loss: 0.0562
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 2541/30000 (8.4700%),                 avg. length: 2500.15,                last time consumption/overall running time: 261.0733s / 32754.9959 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.0477
env0_second_0:                 episode reward: -5.6000,                 loss: 0.0546
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 2561/30000 (8.5367%),                 avg. length: 2848.7,                last time consumption/overall running time: 303.5283s / 33058.5241 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.0614
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0745
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 2581/30000 (8.6033%),                 avg. length: 2743.3,                last time consumption/overall running time: 308.8486s / 33367.3728 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.0696
env0_second_0:                 episode reward: -5.0500,                 loss: 0.0365
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 2601/30000 (8.6700%),                 avg. length: 2848.95,                last time consumption/overall running time: 346.5675s / 33713.9402 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.0840
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0061
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 2621/30000 (8.7367%),                 avg. length: 2919.7,                last time consumption/overall running time: 321.9523s / 34035.8925 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.0746
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0425
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 2641/30000 (8.8033%),                 avg. length: 3122.75,                last time consumption/overall running time: 356.2581s / 34392.1506 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.0601
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0580
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2661/30000 (8.8700%),                 avg. length: 2916.7,                last time consumption/overall running time: 313.6642s / 34705.8148 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.0509
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0578
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 2681/30000 (8.9367%),                 avg. length: 2553.05,                last time consumption/overall running time: 291.3100s / 34997.1248 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.0415
env0_second_0:                 episode reward: -5.4500,                 loss: 0.0985
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 2701/30000 (9.0033%),                 avg. length: 3176.05,                last time consumption/overall running time: 351.3206s / 35348.4454 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.0562
env0_second_0:                 episode reward: -2.1000,                 loss: 0.0361
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 2721/30000 (9.0700%),                 avg. length: 2884.05,                last time consumption/overall running time: 331.0448s / 35679.4903 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.0590
env0_second_0:                 episode reward: -4.1000,                 loss: 0.0234
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 2741/30000 (9.1367%),                 avg. length: 2843.4,                last time consumption/overall running time: 306.3600s / 35985.8503 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.0614
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0151
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 2761/30000 (9.2033%),                 avg. length: 2822.8,                last time consumption/overall running time: 290.0507s / 36275.9010 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.0327
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0334
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 2781/30000 (9.2700%),                 avg. length: 2748.25,                last time consumption/overall running time: 309.7873s / 36585.6883 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.0420
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0142
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 2801/30000 (9.3367%),                 avg. length: 2516.85,                last time consumption/overall running time: 273.1508s / 36858.8390 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.0453
env0_second_0:                 episode reward: -4.4000,                 loss: 0.0340
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 2821/30000 (9.4033%),                 avg. length: 2796.75,                last time consumption/overall running time: 315.0715s / 37173.9105 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.0602
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0147
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 2841/30000 (9.4700%),                 avg. length: 2714.1,                last time consumption/overall running time: 304.7351s / 37478.6456 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0065
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0956
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2861/30000 (9.5367%),                 avg. length: 2896.9,                last time consumption/overall running time: 303.2465s / 37781.8921 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0389
env0_second_0:                 episode reward: 0.5000,                 loss: 0.1418
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 2881/30000 (9.6033%),                 avg. length: 1571.35,                last time consumption/overall running time: 161.4793s / 37943.3714 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0206
env0_second_0:                 episode reward: 0.8500,                 loss: 0.1377
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2901/30000 (9.6700%),                 avg. length: 1442.65,                last time consumption/overall running time: 167.2347s / 38110.6061 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.0683
env0_second_0:                 episode reward: -6.5000,                 loss: 0.0147
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 2921/30000 (9.7367%),                 avg. length: 1580.85,                last time consumption/overall running time: 175.4405s / 38286.0466 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.0449
env0_second_0:                 episode reward: -5.1000,                 loss: 0.0103
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 2941/30000 (9.8033%),                 avg. length: 1882.5,                last time consumption/overall running time: 215.5829s / 38501.6295 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0178
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0554
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2961/30000 (9.8700%),                 avg. length: 2119.25,                last time consumption/overall running time: 252.5659s / 38754.1954 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0057
env0_second_0:                 episode reward: -0.8500,                 loss: 0.1018
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 2981/30000 (9.9367%),                 avg. length: 2157.55,                last time consumption/overall running time: 254.6236s / 39008.8190 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0066
env0_second_0:                 episode reward: -0.6000,                 loss: 0.1162
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 3001/30000 (10.0033%),                 avg. length: 2088.6,                last time consumption/overall running time: 255.7448s / 39264.5639 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.0132
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0985
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 3021/30000 (10.0700%),                 avg. length: 2050.7,                last time consumption/overall running time: 225.4507s / 39490.0146 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.0378
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0962
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 3041/30000 (10.1367%),                 avg. length: 1906.35,                last time consumption/overall running time: 204.9440s / 39694.9587 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.0406
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0305
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 3061/30000 (10.2033%),                 avg. length: 1938.05,                last time consumption/overall running time: 225.5511s / 39920.5097 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.0549
env0_second_0:                 episode reward: -1.3000,                 loss: -0.0014
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 3081/30000 (10.2700%),                 avg. length: 2198.95,                last time consumption/overall running time: 258.4217s / 40178.9314 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.0435
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0417
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 3101/30000 (10.3367%),                 avg. length: 2329.9,                last time consumption/overall running time: 271.7438s / 40450.6752 s
env0_first_0:                 episode reward: -1.9500,                 loss: -0.0405
env0_second_0:                 episode reward: 1.9500,                 loss: 0.0469
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 3121/30000 (10.4033%),                 avg. length: 1897.3,                last time consumption/overall running time: 217.6100s / 40668.2852 s
env0_first_0:                 episode reward: -4.2500,                 loss: -0.0412
env0_second_0:                 episode reward: 4.2500,                 loss: 0.0187
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 3141/30000 (10.4700%),                 avg. length: 2355.5,                last time consumption/overall running time: 262.3878s / 40930.6730 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.0069
env0_second_0:                 episode reward: 2.4000,                 loss: 0.1142
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 3161/30000 (10.5367%),                 avg. length: 2174.25,                last time consumption/overall running time: 259.6251s / 41190.2981 s
env0_first_0:                 episode reward: -3.9000,                 loss: -0.0228
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1387
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 3181/30000 (10.6033%),                 avg. length: 2233.3,                last time consumption/overall running time: 236.5359s / 41426.8340 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.0192
env0_second_0:                 episode reward: 2.1000,                 loss: -0.3809
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 3201/30000 (10.6700%),                 avg. length: 2150.3,                last time consumption/overall running time: 226.7032s / 41653.5372 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0350
env0_second_0:                 episode reward: 0.0000,                 loss: 1.4514
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3221/30000 (10.7367%),                 avg. length: 2191.4,                last time consumption/overall running time: 216.2129s / 41869.7501 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.0271
env0_second_0:                 episode reward: -2.8500,                 loss: 0.5156
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 3241/30000 (10.8033%),                 avg. length: 1938.85,                last time consumption/overall running time: 222.8374s / 42092.5875 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0211
env0_second_0:                 episode reward: -1.3500,                 loss: 0.5887
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3261/30000 (10.8700%),                 avg. length: 1914.2,                last time consumption/overall running time: 193.1429s / 42285.7304 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0309
env0_second_0:                 episode reward: 0.4000,                 loss: 0.3363
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3281/30000 (10.9367%),                 avg. length: 1938.45,                last time consumption/overall running time: 192.8651s / 42478.5955 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.0638
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0733
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 3301/30000 (11.0033%),                 avg. length: 2288.15,                last time consumption/overall running time: 248.0530s / 42726.6485 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.0635
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0800
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3321/30000 (11.0700%),                 avg. length: 2175.0,                last time consumption/overall running time: 239.5153s / 42966.1638 s
env0_first_0:                 episode reward: 5.5000,                 loss: -0.0662
env0_second_0:                 episode reward: -5.5000,                 loss: 0.0400
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 3341/30000 (11.1367%),                 avg. length: 2158.7,                last time consumption/overall running time: 231.8517s / 43198.0155 s
env0_first_0:                 episode reward: 5.9500,                 loss: -0.0883
env0_second_0:                 episode reward: -5.9500,                 loss: 0.0295
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 3361/30000 (11.2033%),                 avg. length: 2202.65,                last time consumption/overall running time: 244.0664s / 43442.0820 s
env0_first_0:                 episode reward: 5.6500,                 loss: -0.0950
env0_second_0:                 episode reward: -5.6500,                 loss: -0.0205
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 3381/30000 (11.2700%),                 avg. length: 2183.5,                last time consumption/overall running time: 256.2404s / 43698.3224 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.0630
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0542
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3401/30000 (11.3367%),                 avg. length: 2422.4,                last time consumption/overall running time: 280.3973s / 43978.7196 s
env0_first_0:                 episode reward: -2.9500,                 loss: -0.0510
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0464
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 3421/30000 (11.4033%),                 avg. length: 2361.9,                last time consumption/overall running time: 242.5200s / 44221.2397 s
env0_first_0:                 episode reward: -2.8000,                 loss: -0.0426
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0422
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3441/30000 (11.4700%),                 avg. length: 2721.5,                last time consumption/overall running time: 277.3369s / 44498.5766 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.0251
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0729
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 3461/30000 (11.5367%),                 avg. length: 2657.1,                last time consumption/overall running time: 284.0443s / 44782.6209 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.0373
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0228
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 3481/30000 (11.6033%),                 avg. length: 2712.3,                last time consumption/overall running time: 299.1387s / 45081.7596 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.0470
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0180
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 3501/30000 (11.6700%),                 avg. length: 2618.6,                last time consumption/overall running time: 273.2779s / 45355.0375 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.0722
env0_second_0:                 episode reward: -4.6500,                 loss: -0.0171
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 3521/30000 (11.7367%),                 avg. length: 2894.25,                last time consumption/overall running time: 317.2089s / 45672.2464 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0468
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0116
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 3541/30000 (11.8033%),                 avg. length: 2896.8,                last time consumption/overall running time: 341.4109s / 46013.6573 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0474
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0128
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 3561/30000 (11.8700%),                 avg. length: 2788.5,                last time consumption/overall running time: 332.4313s / 46346.0886 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.0700
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0269
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 3581/30000 (11.9367%),                 avg. length: 2430.5,                last time consumption/overall running time: 258.0160s / 46604.1046 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.0680
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0239
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 3601/30000 (12.0033%),                 avg. length: 2759.1,                last time consumption/overall running time: 320.8766s / 46924.9812 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0693
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0216
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 3621/30000 (12.0700%),                 avg. length: 2794.1,                last time consumption/overall running time: 324.3584s / 47249.3396 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.0481
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0077
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 3641/30000 (12.1367%),                 avg. length: 2818.25,                last time consumption/overall running time: 319.0775s / 47568.4171 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0532
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0114
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3661/30000 (12.2033%),                 avg. length: 2930.8,                last time consumption/overall running time: 345.1236s / 47913.5407 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.0394
env0_second_0:                 episode reward: 0.1500,                 loss: 0.1172
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 3681/30000 (12.2700%),                 avg. length: 2872.8,                last time consumption/overall running time: 323.3959s / 48236.9366 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0464
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0985
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3701/30000 (12.3367%),                 avg. length: 2815.95,                last time consumption/overall running time: 331.3961s / 48568.3326 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.0647
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0080
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 3721/30000 (12.4033%),                 avg. length: 2715.35,                last time consumption/overall running time: 331.3134s / 48899.6460 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.0649
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0202
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 3741/30000 (12.4700%),                 avg. length: 2947.3,                last time consumption/overall running time: 355.8865s / 49255.5325 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.0626
env0_second_0:                 episode reward: -1.2000,                 loss: -0.0040
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3761/30000 (12.5367%),                 avg. length: 2749.4,                last time consumption/overall running time: 331.3957s / 49586.9282 s
env0_first_0:                 episode reward: -3.0000,                 loss: -0.0481
env0_second_0:                 episode reward: 3.0000,                 loss: 0.0183
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 3781/30000 (12.6033%),                 avg. length: 3035.3,                last time consumption/overall running time: 365.8836s / 49952.8118 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0523
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0187
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 3801/30000 (12.6700%),                 avg. length: 2943.25,                last time consumption/overall running time: 358.9145s / 50311.7263 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0270
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0454
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 3821/30000 (12.7367%),                 avg. length: 3047.65,                last time consumption/overall running time: 324.8232s / 50636.5496 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.0405
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0541
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 3841/30000 (12.8033%),                 avg. length: 2983.9,                last time consumption/overall running time: 292.8073s / 50929.3568 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0489
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0468
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 3861/30000 (12.8700%),                 avg. length: 3052.2,                last time consumption/overall running time: 341.5370s / 51270.8938 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.0512
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0375
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 3881/30000 (12.9367%),                 avg. length: 3270.5,                last time consumption/overall running time: 376.6099s / 51647.5038 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0632
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0435
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 3901/30000 (13.0033%),                 avg. length: 3018.5,                last time consumption/overall running time: 334.7052s / 51982.2089 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.0608
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0555
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 3921/30000 (13.0700%),                 avg. length: 3307.25,                last time consumption/overall running time: 394.3472s / 52376.5561 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.0566
env0_second_0:                 episode reward: 1.7000,                 loss: 0.1006
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 3941/30000 (13.1367%),                 avg. length: 2597.65,                last time consumption/overall running time: 288.2980s / 52664.8541 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0098
env0_second_0:                 episode reward: -0.2000,                 loss: 0.2445
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3961/30000 (13.2033%),                 avg. length: 2392.15,                last time consumption/overall running time: 283.1804s / 52948.0345 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.0328
env0_second_0:                 episode reward: -5.7500,                 loss: 0.1142
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 3981/30000 (13.2700%),                 avg. length: 2467.9,                last time consumption/overall running time: 274.3729s / 53222.4075 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.0069
env0_second_0:                 episode reward: 2.9000,                 loss: 0.2774
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 4001/30000 (13.3367%),                 avg. length: 2995.05,                last time consumption/overall running time: 341.1858s / 53563.5932 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.0297
env0_second_0:                 episode reward: -0.8000,                 loss: 0.1386
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 4021/30000 (13.4033%),                 avg. length: 1489.55,                last time consumption/overall running time: 159.1041s / 53722.6974 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.0206
env0_second_0:                 episode reward: -4.2000,                 loss: 0.1462
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 4041/30000 (13.4700%),                 avg. length: 1621.2,                last time consumption/overall running time: 175.0455s / 53897.7428 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.0510
env0_second_0:                 episode reward: -4.8500,                 loss: 0.0398
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 4061/30000 (13.5367%),                 avg. length: 2253.8,                last time consumption/overall running time: 264.7784s / 54162.5212 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.0402
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0557
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 4081/30000 (13.6033%),                 avg. length: 1821.35,                last time consumption/overall running time: 203.7444s / 54366.2657 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.0266
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0762
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 4101/30000 (13.6700%),                 avg. length: 1937.7,                last time consumption/overall running time: 208.4756s / 54574.7413 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.0269
env0_second_0:                 episode reward: -1.7500,                 loss: 0.0807
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 4121/30000 (13.7367%),                 avg. length: 2164.9,                last time consumption/overall running time: 224.5187s / 54799.2600 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.0606
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0113
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 4141/30000 (13.8033%),                 avg. length: 1954.35,                last time consumption/overall running time: 228.6687s / 55027.9287 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.0778
env0_second_0:                 episode reward: 5.9500,                 loss: -0.0186
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 4161/30000 (13.8700%),                 avg. length: 1938.5,                last time consumption/overall running time: 214.0968s / 55242.0255 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.0460
env0_second_0:                 episode reward: 2.8500,                 loss: 0.1068
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 4181/30000 (13.9367%),                 avg. length: 2061.7,                last time consumption/overall running time: 249.7791s / 55491.8047 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.0738
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0243
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 4201/30000 (14.0033%),                 avg. length: 2145.2,                last time consumption/overall running time: 255.1970s / 55747.0017 s
env0_first_0:                 episode reward: -3.5000,                 loss: -0.0735
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0145
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 4221/30000 (14.0700%),                 avg. length: 2282.05,                last time consumption/overall running time: 266.7110s / 56013.7126 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.0697
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0700
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 4241/30000 (14.1367%),                 avg. length: 1971.05,                last time consumption/overall running time: 235.1207s / 56248.8333 s
env0_first_0:                 episode reward: -6.1000,                 loss: -0.0977
env0_second_0:                 episode reward: 6.1000,                 loss: -0.0230
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 4261/30000 (14.2033%),                 avg. length: 1813.25,                last time consumption/overall running time: 222.6073s / 56471.4406 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.0549
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0546
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 4281/30000 (14.2700%),                 avg. length: 1766.85,                last time consumption/overall running time: 209.1598s / 56680.6004 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.0776
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0412
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 4301/30000 (14.3367%),                 avg. length: 2217.15,                last time consumption/overall running time: 245.2540s / 56925.8545 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.0660
env0_second_0:                 episode reward: 2.1000,                 loss: 0.0571
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 4321/30000 (14.4033%),                 avg. length: 2497.05,                last time consumption/overall running time: 278.3845s / 57204.2389 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.0813
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0056
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 4341/30000 (14.4700%),                 avg. length: 2225.25,                last time consumption/overall running time: 273.5981s / 57477.8370 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.0936
env0_second_0:                 episode reward: 3.8000,                 loss: -0.0388
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 4361/30000 (14.5367%),                 avg. length: 2328.9,                last time consumption/overall running time: 284.4358s / 57762.2729 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.0854
env0_second_0:                 episode reward: 5.1000,                 loss: -0.0277
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 4381/30000 (14.6033%),                 avg. length: 1755.4,                last time consumption/overall running time: 218.3568s / 57980.6296 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.0835
env0_second_0:                 episode reward: -0.3500,                 loss: -0.0179
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 4401/30000 (14.6700%),                 avg. length: 2181.95,                last time consumption/overall running time: 266.5556s / 58247.1853 s
env0_first_0:                 episode reward: -2.8000,                 loss: -0.0643
env0_second_0:                 episode reward: 2.8000,                 loss: -0.0033
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 4421/30000 (14.7367%),                 avg. length: 2274.95,                last time consumption/overall running time: 232.7199s / 58479.9052 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.1027
env0_second_0:                 episode reward: 5.7000,                 loss: -0.0251
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 4441/30000 (14.8033%),                 avg. length: 1946.15,                last time consumption/overall running time: 216.3593s / 58696.2645 s
env0_first_0:                 episode reward: -8.1000,                 loss: -0.1058
env0_second_0:                 episode reward: 8.1000,                 loss: -0.0329
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 4461/30000 (14.8700%),                 avg. length: 2097.65,                last time consumption/overall running time: 224.6090s / 58920.8735 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.0899
env0_second_0:                 episode reward: 7.2500,                 loss: -0.0101
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 4481/30000 (14.9367%),                 avg. length: 2089.3,                last time consumption/overall running time: 236.8399s / 59157.7134 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.0767
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0186
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 4501/30000 (15.0033%),                 avg. length: 2597.9,                last time consumption/overall running time: 306.1267s / 59463.8401 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.0670
env0_second_0:                 episode reward: 4.7000,                 loss: -0.0125
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 4521/30000 (15.0700%),                 avg. length: 2558.55,                last time consumption/overall running time: 298.6377s / 59762.4778 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.0846
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0167
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 4541/30000 (15.1367%),                 avg. length: 2728.4,                last time consumption/overall running time: 317.3605s / 60079.8383 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.0723
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0442
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 4561/30000 (15.2033%),                 avg. length: 2413.65,                last time consumption/overall running time: 290.9717s / 60370.8099 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.0901
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0343
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 4581/30000 (15.2700%),                 avg. length: 2695.4,                last time consumption/overall running time: 283.0312s / 60653.8411 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.0842
env0_second_0:                 episode reward: 3.1000,                 loss: 0.3576
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 4601/30000 (15.3367%),                 avg. length: 2679.3,                last time consumption/overall running time: 274.9912s / 60928.8323 s
env0_first_0:                 episode reward: -5.0000,                 loss: -0.0869
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0184
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 4621/30000 (15.4033%),                 avg. length: 2489.6,                last time consumption/overall running time: 265.8039s / 61194.6362 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.1010
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0194
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 4641/30000 (15.4700%),                 avg. length: 2475.45,                last time consumption/overall running time: 245.4384s / 61440.0745 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.0974
env0_second_0:                 episode reward: 2.5500,                 loss: -0.0181
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 4661/30000 (15.5367%),                 avg. length: 2425.25,                last time consumption/overall running time: 291.0514s / 61731.1260 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0762
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0320
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 4681/30000 (15.6033%),                 avg. length: 2395.9,                last time consumption/overall running time: 243.4780s / 61974.6039 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.0882
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0800
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 4701/30000 (15.6700%),                 avg. length: 2116.1,                last time consumption/overall running time: 218.0935s / 62192.6974 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.0723
env0_second_0:                 episode reward: -4.0500,                 loss: 0.0417
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 4721/30000 (15.7367%),                 avg. length: 2707.35,                last time consumption/overall running time: 311.2392s / 62503.9367 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.0898
env0_second_0:                 episode reward: -2.2500,                 loss: -0.0360
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 4741/30000 (15.8033%),                 avg. length: 3082.25,                last time consumption/overall running time: 344.0812s / 62848.0179 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0815
env0_second_0:                 episode reward: -0.8500,                 loss: -0.0095
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 4761/30000 (15.8700%),                 avg. length: 2964.5,                last time consumption/overall running time: 338.4610s / 63186.4789 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.0858
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0082
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 4781/30000 (15.9367%),                 avg. length: 2914.95,                last time consumption/overall running time: 354.1382s / 63540.6171 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0996
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0181
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4801/30000 (16.0033%),                 avg. length: 2811.75,                last time consumption/overall running time: 321.0496s / 63861.6667 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.0995
env0_second_0:                 episode reward: 2.0000,                 loss: -0.0225
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 4821/30000 (16.0700%),                 avg. length: 2495.15,                last time consumption/overall running time: 281.3631s / 64143.0298 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.1196
env0_second_0:                 episode reward: 4.4500,                 loss: -0.0389
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 4841/30000 (16.1367%),                 avg. length: 3168.9,                last time consumption/overall running time: 358.4641s / 64501.4939 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0965
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0307
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 4861/30000 (16.2033%),                 avg. length: 3103.3,                last time consumption/overall running time: 350.0081s / 64851.5020 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.0877
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0141
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 4881/30000 (16.2700%),                 avg. length: 3170.6,                last time consumption/overall running time: 381.5672s / 65233.0692 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0859
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0119
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 4901/30000 (16.3367%),                 avg. length: 2826.4,                last time consumption/overall running time: 325.4152s / 65558.4844 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.0769
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0358
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 4921/30000 (16.4033%),                 avg. length: 2881.05,                last time consumption/overall running time: 346.2297s / 65904.7141 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0904
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0080
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 4941/30000 (16.4700%),                 avg. length: 2733.8,                last time consumption/overall running time: 316.2986s / 66221.0127 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.0722
env0_second_0:                 episode reward: 2.4500,                 loss: 0.0514
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 4961/30000 (16.5367%),                 avg. length: 3101.15,                last time consumption/overall running time: 368.1823s / 66589.1950 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.0940
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0162
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 4981/30000 (16.6033%),                 avg. length: 3238.65,                last time consumption/overall running time: 358.1217s / 66947.3167 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0993
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0034
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 5001/30000 (16.6700%),                 avg. length: 2974.5,                last time consumption/overall running time: 349.4905s / 67296.8071 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.0957
env0_second_0:                 episode reward: -0.7000,                 loss: -0.0118
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5021/30000 (16.7367%),                 avg. length: 2517.5,                last time consumption/overall running time: 291.5511s / 67588.3582 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.0635
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0559
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 5041/30000 (16.8033%),                 avg. length: 2999.45,                last time consumption/overall running time: 293.5072s / 67881.8654 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.0891
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0097
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 5061/30000 (16.8700%),                 avg. length: 3115.8,                last time consumption/overall running time: 309.5839s / 68191.4493 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.0829
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0295
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 5081/30000 (16.9367%),                 avg. length: 3210.9,                last time consumption/overall running time: 353.0601s / 68544.5093 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.0837
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0067
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 5101/30000 (17.0033%),                 avg. length: 3003.4,                last time consumption/overall running time: 350.7495s / 68895.2589 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0806
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0047
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 5121/30000 (17.0700%),                 avg. length: 2503.4,                last time consumption/overall running time: 285.7409s / 69180.9998 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.0643
env0_second_0:                 episode reward: -1.9000,                 loss: 0.0204
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 5141/30000 (17.1367%),                 avg. length: 2460.15,                last time consumption/overall running time: 295.1549s / 69476.1548 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.0435
env0_second_0:                 episode reward: -4.0500,                 loss: 0.0304
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 5161/30000 (17.2033%),                 avg. length: 2747.55,                last time consumption/overall running time: 278.8473s / 69755.0021 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.0835
env0_second_0:                 episode reward: -4.4000,                 loss: 0.0245
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 5181/30000 (17.2700%),                 avg. length: 3051.05,                last time consumption/overall running time: 335.5383s / 70090.5404 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0798
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0549
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 5201/30000 (17.3367%),                 avg. length: 3074.6,                last time consumption/overall running time: 305.4382s / 70395.9786 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.0685
env0_second_0:                 episode reward: 2.2500,                 loss: 0.0422
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 5221/30000 (17.4033%),                 avg. length: 3241.95,                last time consumption/overall running time: 392.2785s / 70788.2571 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0770
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0692
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 5241/30000 (17.4700%),                 avg. length: 2881.65,                last time consumption/overall running time: 308.0254s / 71096.2825 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.0625
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0485
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 5261/30000 (17.5367%),                 avg. length: 2859.8,                last time consumption/overall running time: 316.6577s / 71412.9401 s
env0_first_0:                 episode reward: -5.2500,                 loss: -0.0828
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0439
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 5281/30000 (17.6033%),                 avg. length: 2762.75,                last time consumption/overall running time: 315.7350s / 71728.6751 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0216
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0949
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 5301/30000 (17.6700%),                 avg. length: 3026.1,                last time consumption/overall running time: 331.5301s / 72060.2052 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0877
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0098
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 5321/30000 (17.7367%),                 avg. length: 2931.25,                last time consumption/overall running time: 279.7410s / 72339.9462 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.0868
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0399
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 5341/30000 (17.8033%),                 avg. length: 2967.3,                last time consumption/overall running time: 338.2213s / 72678.1675 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.0930
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0207
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5361/30000 (17.8700%),                 avg. length: 2784.25,                last time consumption/overall running time: 300.6819s / 72978.8494 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.0930
env0_second_0:                 episode reward: 5.4500,                 loss: 0.4316
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 5381/30000 (17.9367%),                 avg. length: 2942.3,                last time consumption/overall running time: 354.3981s / 73333.2475 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.0891
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0453
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 5401/30000 (18.0033%),                 avg. length: 3010.0,                last time consumption/overall running time: 335.9147s / 73669.1622 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.0851
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0260
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 5421/30000 (18.0700%),                 avg. length: 3164.2,                last time consumption/overall running time: 360.6524s / 74029.8146 s
env0_first_0:                 episode reward: -4.9500,                 loss: -0.0726
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0402
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 5441/30000 (18.1367%),                 avg. length: 2878.9,                last time consumption/overall running time: 318.8974s / 74348.7120 s
env0_first_0:                 episode reward: -5.6500,                 loss: -0.0683
env0_second_0:                 episode reward: 5.6500,                 loss: 0.1459
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 5461/30000 (18.2033%),                 avg. length: 3141.25,                last time consumption/overall running time: 366.8225s / 74715.5344 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0701
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0963
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 5481/30000 (18.2700%),                 avg. length: 2924.75,                last time consumption/overall running time: 331.3455s / 75046.8799 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.0880
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0677
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 5501/30000 (18.3367%),                 avg. length: 2471.5,                last time consumption/overall running time: 303.4614s / 75350.3414 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0737
env0_second_0:                 episode reward: -0.1500,                 loss: 0.1024
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 5521/30000 (18.4033%),                 avg. length: 3195.25,                last time consumption/overall running time: 328.0527s / 75678.3941 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0811
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0317
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 5541/30000 (18.4700%),                 avg. length: 3054.65,                last time consumption/overall running time: 323.6317s / 76002.0257 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.0726
env0_second_0:                 episode reward: -5.8500,                 loss: 0.0701
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 5561/30000 (18.5367%),                 avg. length: 3018.25,                last time consumption/overall running time: 326.5627s / 76328.5884 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.0820
env0_second_0:                 episode reward: -5.4000,                 loss: 0.0470
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 5581/30000 (18.6033%),                 avg. length: 3257.5,                last time consumption/overall running time: 341.3319s / 76669.9202 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.0659
env0_second_0:                 episode reward: -5.1500,                 loss: 0.0658
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 5601/30000 (18.6700%),                 avg. length: 3571.55,                last time consumption/overall running time: 365.3189s / 77035.2391 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.0616
env0_second_0:                 episode reward: -2.1000,                 loss: 0.2449
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5621/30000 (18.7367%),                 avg. length: 3589.35,                last time consumption/overall running time: 407.8831s / 77443.1222 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.0853
env0_second_0:                 episode reward: -3.0500,                 loss: 0.1670
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 5641/30000 (18.8033%),                 avg. length: 3502.1,                last time consumption/overall running time: 367.5945s / 77810.7167 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.0611
env0_second_0:                 episode reward: -2.6500,                 loss: 0.2064
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5661/30000 (18.8700%),                 avg. length: 3721.15,                last time consumption/overall running time: 430.9119s / 78241.6286 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.0519
env0_second_0:                 episode reward: -3.7500,                 loss: 0.6000
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 5681/30000 (18.9367%),                 avg. length: 3027.1,                last time consumption/overall running time: 340.0373s / 78581.6659 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.0409
env0_second_0:                 episode reward: -1.3000,                 loss: 0.3172
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 5701/30000 (19.0033%),                 avg. length: 3141.4,                last time consumption/overall running time: 348.6486s / 78930.3145 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.0776
env0_second_0:                 episode reward: -4.6000,                 loss: 0.3537
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 5721/30000 (19.0700%),                 avg. length: 3404.7,                last time consumption/overall running time: 393.2645s / 79323.5790 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.0851
env0_second_0:                 episode reward: -4.3000,                 loss: 0.3759
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 5741/30000 (19.1367%),                 avg. length: 3868.6,                last time consumption/overall running time: 447.1965s / 79770.7754 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.0764
env0_second_0:                 episode reward: -1.7500,                 loss: 0.3347
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 5761/30000 (19.2033%),                 avg. length: 3368.65,                last time consumption/overall running time: 356.3406s / 80127.1160 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.0784
env0_second_0:                 episode reward: -2.6000,                 loss: 0.4758
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 5781/30000 (19.2700%),                 avg. length: 3712.5,                last time consumption/overall running time: 434.7275s / 80561.8436 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0465
env0_second_0:                 episode reward: 1.8500,                 loss: 0.1988
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 5801/30000 (19.3367%),                 avg. length: 3812.1,                last time consumption/overall running time: 455.5607s / 81017.4043 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.0513
env0_second_0:                 episode reward: 0.4500,                 loss: 0.1700
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 5821/30000 (19.4033%),                 avg. length: 4031.95,                last time consumption/overall running time: 452.3693s / 81469.7736 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0798
env0_second_0:                 episode reward: 1.8500,                 loss: 0.1285
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 5841/30000 (19.4700%),                 avg. length: 3294.2,                last time consumption/overall running time: 378.5839s / 81848.3576 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.0511
env0_second_0:                 episode reward: -1.4000,                 loss: 0.1906
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 5861/30000 (19.5367%),                 avg. length: 1819.2,                last time consumption/overall running time: 187.7238s / 82036.0813 s
env0_first_0:                 episode reward: 6.9000,                 loss: -0.0065
env0_second_0:                 episode reward: -6.9000,                 loss: 0.6143
env1_first_0:                 episode reward: 7.1500,                 loss: nan
env1_second_0:                 episode reward: -7.1500,                 loss: nan
Episode: 5881/30000 (19.6033%),                 avg. length: 2109.55,                last time consumption/overall running time: 249.5361s / 82285.6174 s
env0_first_0:                 episode reward: 5.2500,                 loss: 0.0009
env0_second_0:                 episode reward: -5.2500,                 loss: 0.4099
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 5901/30000 (19.6700%),                 avg. length: 2470.5,                last time consumption/overall running time: 258.4103s / 82544.0277 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.0061
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0951
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 5921/30000 (19.7367%),                 avg. length: 2234.75,                last time consumption/overall running time: 225.4809s / 82769.5086 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.0239
env0_second_0:                 episode reward: 4.1000,                 loss: 0.2912
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 5941/30000 (19.8033%),                 avg. length: 2012.45,                last time consumption/overall running time: 214.9499s / 82984.4585 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0373
env0_second_0:                 episode reward: 2.6500,                 loss: 0.7640
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 5961/30000 (19.8700%),                 avg. length: 1174.25,                last time consumption/overall running time: 140.8025s / 83125.2610 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.0705
env0_second_0:                 episode reward: 7.8500,                 loss: 0.5061
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 5981/30000 (19.9367%),                 avg. length: 1114.25,                last time consumption/overall running time: 123.1131s / 83248.3741 s
env0_first_0:                 episode reward: -8.3500,                 loss: -0.1113
env0_second_0:                 episode reward: 8.3500,                 loss: 0.1702
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 6001/30000 (20.0033%),                 avg. length: 1103.1,                last time consumption/overall running time: 139.0371s / 83387.4113 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.1182
env0_second_0:                 episode reward: 8.6500,                 loss: -0.0236
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 6021/30000 (20.0700%),                 avg. length: 1095.15,                last time consumption/overall running time: 137.5721s / 83524.9834 s
env0_first_0:                 episode reward: -8.3000,                 loss: -0.1197
env0_second_0:                 episode reward: 8.3000,                 loss: -0.0455
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 6041/30000 (20.1367%),                 avg. length: 963.85,                last time consumption/overall running time: 119.6243s / 83644.6077 s
env0_first_0:                 episode reward: -8.9000,                 loss: -0.1251
env0_second_0:                 episode reward: 8.9000,                 loss: -0.0080
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
Episode: 6061/30000 (20.2033%),                 avg. length: 937.15,                last time consumption/overall running time: 99.7440s / 83744.3517 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.0945
env0_second_0:                 episode reward: 9.3500,                 loss: 0.2102
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 6081/30000 (20.2700%),                 avg. length: 988.2,                last time consumption/overall running time: 99.9082s / 83844.2599 s
env0_first_0:                 episode reward: -8.8500,                 loss: -0.0533
env0_second_0:                 episode reward: 8.8500,                 loss: 0.3117
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 6101/30000 (20.3367%),                 avg. length: 925.4,                last time consumption/overall running time: 119.4383s / 83963.6981 s
env0_first_0:                 episode reward: -8.8500,                 loss: -0.1093
env0_second_0:                 episode reward: 8.8500,                 loss: -0.0087
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
Episode: 6121/30000 (20.4033%),                 avg. length: 912.7,                last time consumption/overall running time: 112.3145s / 84076.0126 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0985
env0_second_0:                 episode reward: 9.5000,                 loss: -0.0381
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 6141/30000 (20.4700%),                 avg. length: 918.45,                last time consumption/overall running time: 121.0001s / 84197.0127 s
env0_first_0:                 episode reward: -9.6500,                 loss: -0.0903
env0_second_0:                 episode reward: 9.6500,                 loss: -0.0481
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 6161/30000 (20.5367%),                 avg. length: 1044.75,                last time consumption/overall running time: 117.0620s / 84314.0747 s
env0_first_0:                 episode reward: -8.0000,                 loss: 0.0282
env0_second_0:                 episode reward: 8.0000,                 loss: 0.2518
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 6181/30000 (20.6033%),                 avg. length: 1722.75,                last time consumption/overall running time: 185.6063s / 84499.6810 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0221
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1314
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 6201/30000 (20.6700%),                 avg. length: 1435.05,                last time consumption/overall running time: 142.4527s / 84642.1337 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0126
env0_second_0:                 episode reward: 6.6500,                 loss: 0.1272
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 6221/30000 (20.7367%),                 avg. length: 1348.65,                last time consumption/overall running time: 145.0877s / 84787.2215 s
env0_first_0:                 episode reward: -7.7500,                 loss: -0.0244
env0_second_0:                 episode reward: 7.7500,                 loss: 0.3780
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 6241/30000 (20.8033%),                 avg. length: 1374.7,                last time consumption/overall running time: 142.9657s / 84930.1872 s
env0_first_0:                 episode reward: -8.2500,                 loss: 0.0141
env0_second_0:                 episode reward: 8.2500,                 loss: 0.4566
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 6261/30000 (20.8700%),                 avg. length: 1544.9,                last time consumption/overall running time: 191.4647s / 85121.6519 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0178
env0_second_0:                 episode reward: 4.8000,                 loss: 1.5195
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 6281/30000 (20.9367%),                 avg. length: 1540.6,                last time consumption/overall running time: 167.7322s / 85289.3841 s
env0_first_0:                 episode reward: -7.0500,                 loss: -0.0178
env0_second_0:                 episode reward: 7.0500,                 loss: 1.5528
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 6301/30000 (21.0033%),                 avg. length: 1575.9,                last time consumption/overall running time: 187.3164s / 85476.7006 s
env0_first_0:                 episode reward: -7.7000,                 loss: -0.0366
env0_second_0:                 episode reward: 7.7000,                 loss: 1.2312
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 6321/30000 (21.0700%),                 avg. length: 1545.0,                last time consumption/overall running time: 172.6577s / 85649.3582 s
env0_first_0:                 episode reward: -8.8000,                 loss: -0.0552
env0_second_0:                 episode reward: 8.8000,                 loss: 0.8088
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 6341/30000 (21.1367%),                 avg. length: 1562.65,                last time consumption/overall running time: 172.4501s / 85821.8084 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.0811
env0_second_0:                 episode reward: 7.8500,                 loss: 0.8201
env1_first_0:                 episode reward: -7.9500,                 loss: nan
env1_second_0:                 episode reward: 7.9500,                 loss: nan
Episode: 6361/30000 (21.2033%),                 avg. length: 1689.7,                last time consumption/overall running time: 190.0981s / 86011.9065 s
env0_first_0:                 episode reward: -5.2500,                 loss: -0.0249
env0_second_0:                 episode reward: 5.2500,                 loss: 0.7590
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 6381/30000 (21.2700%),                 avg. length: 1654.7,                last time consumption/overall running time: 203.6652s / 86215.5717 s
env0_first_0:                 episode reward: -5.9000,                 loss: -0.0519
env0_second_0:                 episode reward: 5.9000,                 loss: 0.2704
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 6401/30000 (21.3367%),                 avg. length: 1655.1,                last time consumption/overall running time: 186.1763s / 86401.7480 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.0710
env0_second_0:                 episode reward: 8.7500,                 loss: 0.2743
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 6421/30000 (21.4033%),                 avg. length: 1836.25,                last time consumption/overall running time: 229.0515s / 86630.7995 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.0931
env0_second_0:                 episode reward: 7.5500,                 loss: 0.1890
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 6441/30000 (21.4700%),                 avg. length: 1840.6,                last time consumption/overall running time: 214.6568s / 86845.4563 s
env0_first_0:                 episode reward: -8.3500,                 loss: -0.0839
env0_second_0:                 episode reward: 8.3500,                 loss: 0.6126
env1_first_0:                 episode reward: -8.3500,                 loss: nan
env1_second_0:                 episode reward: 8.3500,                 loss: nan
Episode: 6461/30000 (21.5367%),                 avg. length: 1782.2,                last time consumption/overall running time: 191.9054s / 87037.3618 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.0736
env0_second_0:                 episode reward: 7.8500,                 loss: 0.2907
env1_first_0:                 episode reward: -8.7000,                 loss: nan
env1_second_0:                 episode reward: 8.7000,                 loss: nan
Episode: 6481/30000 (21.6033%),                 avg. length: 1913.55,                last time consumption/overall running time: 208.3833s / 87245.7451 s
env0_first_0:                 episode reward: -8.9000,                 loss: -0.1134
env0_second_0:                 episode reward: 8.9000,                 loss: 0.3039
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 6501/30000 (21.6700%),                 avg. length: 1957.95,                last time consumption/overall running time: 230.3009s / 87476.0459 s
env0_first_0:                 episode reward: -8.6000,                 loss: -0.1057
env0_second_0:                 episode reward: 8.6000,                 loss: 0.3090
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 6521/30000 (21.7367%),                 avg. length: 2396.15,                last time consumption/overall running time: 259.2542s / 87735.3001 s
env0_first_0:                 episode reward: -5.5000,                 loss: -0.1157
env0_second_0:                 episode reward: 5.5000,                 loss: 0.2321
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 6541/30000 (21.8033%),                 avg. length: 2268.35,                last time consumption/overall running time: 258.4365s / 87993.7366 s
env0_first_0:                 episode reward: -5.7500,                 loss: -0.1124
env0_second_0:                 episode reward: 5.7500,                 loss: 0.1814
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 6561/30000 (21.8700%),                 avg. length: 2540.65,                last time consumption/overall running time: 310.5294s / 88304.2660 s
env0_first_0:                 episode reward: -5.2500,                 loss: -0.1151
env0_second_0:                 episode reward: 5.2500,                 loss: 0.1660
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 6581/30000 (21.9367%),                 avg. length: 2910.9,                last time consumption/overall running time: 312.7417s / 88617.0078 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.1160
env0_second_0:                 episode reward: 4.2000,                 loss: 0.1633
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 6601/30000 (22.0033%),                 avg. length: 3011.7,                last time consumption/overall running time: 301.0121s / 88918.0199 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.1083
env0_second_0:                 episode reward: 1.0500,                 loss: 0.1220
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 6621/30000 (22.0700%),                 avg. length: 3258.05,                last time consumption/overall running time: 348.1372s / 89266.1570 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.1062
env0_second_0:                 episode reward: -0.6500,                 loss: 0.1277
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 6641/30000 (22.1367%),                 avg. length: 3158.4,                last time consumption/overall running time: 330.6248s / 89596.7819 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.0858
env0_second_0:                 episode reward: -2.3000,                 loss: 0.1448
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 6661/30000 (22.2033%),                 avg. length: 3193.5,                last time consumption/overall running time: 343.5065s / 89940.2884 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.0552
env0_second_0:                 episode reward: -3.4500,                 loss: 0.3500
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 6681/30000 (22.2700%),                 avg. length: 3086.3,                last time consumption/overall running time: 302.2029s / 90242.4913 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.0543
env0_second_0:                 episode reward: -4.6500,                 loss: 0.3608
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 6701/30000 (22.3367%),                 avg. length: 3553.75,                last time consumption/overall running time: 391.5911s / 90634.0824 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0693
env0_second_0:                 episode reward: -2.5000,                 loss: 0.3492
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 6721/30000 (22.4033%),                 avg. length: 3468.8,                last time consumption/overall running time: 390.9301s / 91025.0125 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.0599
env0_second_0:                 episode reward: -2.8000,                 loss: 0.2382
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 6741/30000 (22.4700%),                 avg. length: 4131.4,                last time consumption/overall running time: 464.1690s / 91489.1815 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.0619
env0_second_0:                 episode reward: 2.1000,                 loss: 0.2174
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 6761/30000 (22.5367%),                 avg. length: 3604.9,                last time consumption/overall running time: 359.5445s / 91848.7260 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.0838
env0_second_0:                 episode reward: 5.4500,                 loss: 0.2779
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 6781/30000 (22.6033%),                 avg. length: 3966.75,                last time consumption/overall running time: 459.6349s / 92308.3609 s
env0_first_0:                 episode reward: -3.4500,                 loss: -0.0736
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1860
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 6801/30000 (22.6700%),                 avg. length: 4004.95,                last time consumption/overall running time: 479.2156s / 92787.5765 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.0811
env0_second_0:                 episode reward: 0.8500,                 loss: 0.3208
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 6821/30000 (22.7367%),                 avg. length: 2940.7,                last time consumption/overall running time: 331.4072s / 93118.9837 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0348
env0_second_0:                 episode reward: -3.6500,                 loss: 0.3636
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 6841/30000 (22.8033%),                 avg. length: 2893.9,                last time consumption/overall running time: 308.7124s / 93427.6961 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.0693
env0_second_0:                 episode reward: -6.8000,                 loss: 0.3887
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 6861/30000 (22.8700%),                 avg. length: 3865.8,                last time consumption/overall running time: 422.9108s / 93850.6068 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.0713
env0_second_0:                 episode reward: -1.9500,                 loss: 0.1959
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 6881/30000 (22.9367%),                 avg. length: 3992.4,                last time consumption/overall running time: 422.1852s / 94272.7920 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.0795
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0656
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 6901/30000 (23.0033%),                 avg. length: 4302.3,                last time consumption/overall running time: 473.8968s / 94746.6888 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0815
env0_second_0:                 episode reward: 0.4000,                 loss: 0.1869
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 6921/30000 (23.0700%),                 avg. length: 3917.4,                last time consumption/overall running time: 460.7001s / 95207.3890 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.0794
env0_second_0:                 episode reward: 0.0500,                 loss: 0.2167
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 6941/30000 (23.1367%),                 avg. length: 3861.25,                last time consumption/overall running time: 443.2794s / 95650.6683 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.0765
env0_second_0:                 episode reward: 1.2500,                 loss: 0.4719
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 6961/30000 (23.2033%),                 avg. length: 3658.95,                last time consumption/overall running time: 422.2899s / 96072.9583 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.0843
env0_second_0:                 episode reward: -2.5500,                 loss: 0.2940
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 6981/30000 (23.2700%),                 avg. length: 3532.45,                last time consumption/overall running time: 416.9263s / 96489.8846 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.0834
env0_second_0:                 episode reward: 2.2000,                 loss: 0.3589
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 7001/30000 (23.3367%),                 avg. length: 3027.05,                last time consumption/overall running time: 363.4067s / 96853.2912 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.0768
env0_second_0:                 episode reward: 2.7000,                 loss: 0.3720
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 7021/30000 (23.4033%),                 avg. length: 3768.35,                last time consumption/overall running time: 413.9975s / 97267.2887 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.0736
env0_second_0:                 episode reward: -0.3500,                 loss: 0.1904
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 7041/30000 (23.4700%),                 avg. length: 3208.5,                last time consumption/overall running time: 343.6873s / 97610.9761 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.0660
env0_second_0:                 episode reward: -4.9000,                 loss: 0.2602
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 7061/30000 (23.5367%),                 avg. length: 2593.4,                last time consumption/overall running time: 310.6033s / 97921.5794 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.0683
env0_second_0:                 episode reward: -6.4000,                 loss: 0.2702
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 7081/30000 (23.6033%),                 avg. length: 2546.55,                last time consumption/overall running time: 276.0862s / 98197.6656 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.0620
env0_second_0:                 episode reward: -5.8500,                 loss: 0.4555
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 7101/30000 (23.6700%),                 avg. length: 3811.1,                last time consumption/overall running time: 416.4035s / 98614.0691 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.0461
env0_second_0:                 episode reward: -1.6500,                 loss: 0.3990
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 7121/30000 (23.7367%),                 avg. length: 3440.4,                last time consumption/overall running time: 407.2827s / 99021.3518 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0359
env0_second_0:                 episode reward: 0.6500,                 loss: 0.5651
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 7141/30000 (23.8033%),                 avg. length: 3771.4,                last time consumption/overall running time: 416.5208s / 99437.8726 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.0507
env0_second_0:                 episode reward: -2.2500,                 loss: 0.3353
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 7161/30000 (23.8700%),                 avg. length: 4277.2,                last time consumption/overall running time: 456.1985s / 99894.0712 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.0786
env0_second_0:                 episode reward: -2.6500,                 loss: 0.1532
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 7181/30000 (23.9367%),                 avg. length: 3691.6,                last time consumption/overall running time: 413.0427s / 100307.1139 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.0839
env0_second_0:                 episode reward: -3.3000,                 loss: 0.2972
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 7201/30000 (24.0033%),                 avg. length: 4065.65,                last time consumption/overall running time: 429.0261s / 100736.1400 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.0760
env0_second_0:                 episode reward: -2.7500,                 loss: 0.2791
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 7221/30000 (24.0700%),                 avg. length: 3429.15,                last time consumption/overall running time: 337.0251s / 101073.1651 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.0339
env0_second_0:                 episode reward: -2.5500,                 loss: 0.3666
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 7241/30000 (24.1367%),                 avg. length: 2234.8,                last time consumption/overall running time: 267.5874s / 101340.7525 s
env0_first_0:                 episode reward: 5.9500,                 loss: -0.0332
env0_second_0:                 episode reward: -5.9500,                 loss: 0.4602
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 7261/30000 (24.2033%),                 avg. length: 2757.65,                last time consumption/overall running time: 321.4479s / 101662.2004 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.0696
env0_second_0:                 episode reward: -2.0000,                 loss: 0.3374
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 7281/30000 (24.2700%),                 avg. length: 2769.2,                last time consumption/overall running time: 310.0035s / 101972.2039 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.0645
env0_second_0:                 episode reward: -4.9000,                 loss: 0.4263
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 7301/30000 (24.3367%),                 avg. length: 2702.4,                last time consumption/overall running time: 319.7347s / 102291.9386 s
env0_first_0:                 episode reward: 6.3500,                 loss: -0.0855
env0_second_0:                 episode reward: -6.3500,                 loss: 0.3305
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 7321/30000 (24.4033%),                 avg. length: 2666.0,                last time consumption/overall running time: 325.9578s / 102617.8964 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.0951
env0_second_0:                 episode reward: -6.6500,                 loss: 0.3697
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 7341/30000 (24.4700%),                 avg. length: 2919.3,                last time consumption/overall running time: 333.2700s / 102951.1664 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.0715
env0_second_0:                 episode reward: -6.6000,                 loss: 0.3282
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 7361/30000 (24.5367%),                 avg. length: 3090.7,                last time consumption/overall running time: 313.6959s / 103264.8622 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.0702
env0_second_0:                 episode reward: -4.8000,                 loss: 0.4217
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 7381/30000 (24.6033%),                 avg. length: 3261.05,                last time consumption/overall running time: 307.5081s / 103572.3703 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.0759
env0_second_0:                 episode reward: -6.6000,                 loss: 0.2253
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 7401/30000 (24.6700%),                 avg. length: 2913.0,                last time consumption/overall running time: 354.0633s / 103926.4336 s
env0_first_0:                 episode reward: 6.7000,                 loss: -0.0637
env0_second_0:                 episode reward: -6.7000,                 loss: 0.3887
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 7421/30000 (24.7367%),                 avg. length: 3030.1,                last time consumption/overall running time: 323.0890s / 104249.5226 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0846
env0_second_0:                 episode reward: -3.6500,                 loss: 0.4406
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 7441/30000 (24.8033%),                 avg. length: 3632.2,                last time consumption/overall running time: 424.6467s / 104674.1694 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.0783
env0_second_0:                 episode reward: -2.4000,                 loss: 0.1730
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 7461/30000 (24.8700%),                 avg. length: 3045.1,                last time consumption/overall running time: 338.3101s / 105012.4795 s
env0_first_0:                 episode reward: 6.0000,                 loss: -0.0597
env0_second_0:                 episode reward: -6.0000,                 loss: 0.1310
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 7481/30000 (24.9367%),                 avg. length: 2881.95,                last time consumption/overall running time: 323.4327s / 105335.9122 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.0135
env0_second_0:                 episode reward: -1.8500,                 loss: 0.5090
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 7501/30000 (25.0033%),                 avg. length: 3194.85,                last time consumption/overall running time: 342.6428s / 105678.5550 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.0550
env0_second_0:                 episode reward: -1.9000,                 loss: 0.8380
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 7521/30000 (25.0700%),                 avg. length: 3906.1,                last time consumption/overall running time: 467.2240s / 106145.7790 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.0575
env0_second_0:                 episode reward: -3.8500,                 loss: 0.2706
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 7541/30000 (25.1367%),                 avg. length: 3305.6,                last time consumption/overall running time: 340.0261s / 106485.8051 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.0808
env0_second_0:                 episode reward: -5.2500,                 loss: 0.2108
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 7561/30000 (25.2033%),                 avg. length: 3778.7,                last time consumption/overall running time: 401.3881s / 106887.1931 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.0815
env0_second_0:                 episode reward: -3.4500,                 loss: 0.3343
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 7581/30000 (25.2700%),                 avg. length: 3596.75,                last time consumption/overall running time: 370.3783s / 107257.5714 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0712
env0_second_0:                 episode reward: -0.6000,                 loss: 0.4569
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 7601/30000 (25.3367%),                 avg. length: 2916.4,                last time consumption/overall running time: 308.4669s / 107566.0383 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.0731
env0_second_0:                 episode reward: -2.4000,                 loss: 0.3206
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 7621/30000 (25.4033%),                 avg. length: 2830.1,                last time consumption/overall running time: 335.3902s / 107901.4285 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.0341
env0_second_0:                 episode reward: -4.9500,                 loss: 0.3760
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 7641/30000 (25.4700%),                 avg. length: 3462.5,                last time consumption/overall running time: 410.6057s / 108312.0342 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.0513
env0_second_0:                 episode reward: -3.0000,                 loss: 0.3264
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 7661/30000 (25.5367%),                 avg. length: 2941.5,                last time consumption/overall running time: 318.7220s / 108630.7562 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.0599
env0_second_0:                 episode reward: -6.0500,                 loss: 0.2272
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 7681/30000 (25.6033%),                 avg. length: 2878.1,                last time consumption/overall running time: 318.0379s / 108948.7941 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.0511
env0_second_0:                 episode reward: -4.0500,                 loss: 0.3546
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 7701/30000 (25.6700%),                 avg. length: 3358.55,                last time consumption/overall running time: 372.2857s / 109321.0798 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.0521
env0_second_0:                 episode reward: -0.4500,                 loss: 0.4056
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 7721/30000 (25.7367%),                 avg. length: 3472.35,                last time consumption/overall running time: 406.2387s / 109727.3185 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0325
env0_second_0:                 episode reward: -3.6500,                 loss: 0.6691
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 7741/30000 (25.8033%),                 avg. length: 2973.85,                last time consumption/overall running time: 355.3020s / 110082.6205 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.0658
env0_second_0:                 episode reward: -4.5500,                 loss: 0.5052
env1_first_0:                 episode reward: 7.3500,                 loss: nan
env1_second_0:                 episode reward: -7.3500,                 loss: nan
Episode: 7761/30000 (25.8700%),                 avg. length: 2392.5,                last time consumption/overall running time: 281.2752s / 110363.8957 s
env0_first_0:                 episode reward: 5.5500,                 loss: -0.0283
env0_second_0:                 episode reward: -5.5500,                 loss: 0.5546
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 7781/30000 (25.9367%),                 avg. length: 1610.1,                last time consumption/overall running time: 199.4872s / 110563.3829 s
env0_first_0:                 episode reward: 3.4000,                 loss: 0.0003
env0_second_0:                 episode reward: -3.4000,                 loss: 0.5520
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 7801/30000 (26.0033%),                 avg. length: 1713.3,                last time consumption/overall running time: 203.4122s / 110766.7950 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.0197
env0_second_0:                 episode reward: -3.8500,                 loss: 0.3610
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 7821/30000 (26.0700%),                 avg. length: 1875.9,                last time consumption/overall running time: 226.3513s / 110993.1463 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0219
env0_second_0:                 episode reward: 0.6500,                 loss: 0.3889
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 7841/30000 (26.1367%),                 avg. length: 1976.5,                last time consumption/overall running time: 232.2990s / 111225.4453 s
env0_first_0:                 episode reward: 1.9000,                 loss: 0.0056
env0_second_0:                 episode reward: -1.9000,                 loss: 1.2159
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 7861/30000 (26.2033%),                 avg. length: 2321.25,                last time consumption/overall running time: 266.7114s / 111492.1567 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.0435
env0_second_0:                 episode reward: 3.2000,                 loss: 0.6323
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 7881/30000 (26.2700%),                 avg. length: 2193.35,                last time consumption/overall running time: 211.0386s / 111703.1954 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.0452
env0_second_0:                 episode reward: 5.7000,                 loss: 0.6723
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 7901/30000 (26.3367%),                 avg. length: 1148.15,                last time consumption/overall running time: 135.6850s / 111838.8803 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.1049
env0_second_0:                 episode reward: 8.7500,                 loss: 0.5832
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 7921/30000 (26.4033%),                 avg. length: 1074.4,                last time consumption/overall running time: 125.0827s / 111963.9630 s
env0_first_0:                 episode reward: -8.9500,                 loss: -0.1185
env0_second_0:                 episode reward: 8.9500,                 loss: -0.0004
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
Episode: 7941/30000 (26.4700%),                 avg. length: 933.35,                last time consumption/overall running time: 95.0968s / 112059.0598 s
env0_first_0:                 episode reward: -9.6500,                 loss: -0.0872
env0_second_0:                 episode reward: 9.6500,                 loss: -0.0391
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 7961/30000 (26.5367%),                 avg. length: 947.3,                last time consumption/overall running time: 100.4826s / 112159.5424 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0939
env0_second_0:                 episode reward: 9.5000,                 loss: -0.0302
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 7981/30000 (26.6033%),                 avg. length: 919.1,                last time consumption/overall running time: 109.7045s / 112269.2469 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.0732
env0_second_0:                 episode reward: 9.2000,                 loss: -0.0236
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 8001/30000 (26.6700%),                 avg. length: 932.3,                last time consumption/overall running time: 99.9334s / 112369.1803 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.0595
env0_second_0:                 episode reward: 9.4000,                 loss: 0.0148
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 8021/30000 (26.7367%),                 avg. length: 948.45,                last time consumption/overall running time: 113.2025s / 112482.3827 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.0405
env0_second_0:                 episode reward: 8.7500,                 loss: 0.0739
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 8041/30000 (26.8033%),                 avg. length: 927.1,                last time consumption/overall running time: 115.9305s / 112598.3132 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.0636
env0_second_0:                 episode reward: 9.3000,                 loss: -0.0109
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 8061/30000 (26.8700%),                 avg. length: 913.15,                last time consumption/overall running time: 101.4538s / 112699.7670 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.0494
env0_second_0:                 episode reward: 9.5500,                 loss: -0.0063
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 8081/30000 (26.9367%),                 avg. length: 947.05,                last time consumption/overall running time: 109.4719s / 112809.2389 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.0447
env0_second_0:                 episode reward: 9.3000,                 loss: 0.0104
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 8101/30000 (27.0033%),                 avg. length: 910.25,                last time consumption/overall running time: 115.3901s / 112924.6290 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.0404
env0_second_0:                 episode reward: 9.3000,                 loss: 0.0233
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 8121/30000 (27.0700%),                 avg. length: 917.55,                last time consumption/overall running time: 92.1830s / 113016.8120 s
env0_first_0:                 episode reward: -9.1500,                 loss: -0.0570
env0_second_0:                 episode reward: 9.1500,                 loss: -0.0116
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 8141/30000 (27.1367%),                 avg. length: 939.7,                last time consumption/overall running time: 99.9608s / 113116.7728 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0525
env0_second_0:                 episode reward: 9.5000,                 loss: -0.0146
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 8161/30000 (27.2033%),                 avg. length: 917.05,                last time consumption/overall running time: 99.2372s / 113216.0101 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0719
env0_second_0:                 episode reward: 9.5000,                 loss: -0.0384
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 8181/30000 (27.2700%),                 avg. length: 925.75,                last time consumption/overall running time: 113.9466s / 113329.9567 s
env0_first_0:                 episode reward: -9.2500,                 loss: -0.0289
env0_second_0:                 episode reward: 9.2500,                 loss: 0.1962
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 8201/30000 (27.3367%),                 avg. length: 925.2,                last time consumption/overall running time: 120.3315s / 113450.2881 s
env0_first_0:                 episode reward: -9.0500,                 loss: -0.0483
env0_second_0:                 episode reward: 9.0500,                 loss: 0.0842
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 8221/30000 (27.4033%),                 avg. length: 909.4,                last time consumption/overall running time: 101.9763s / 113552.2644 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.0538
env0_second_0:                 episode reward: 9.7500,                 loss: 0.0072
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 8241/30000 (27.4700%),                 avg. length: 919.75,                last time consumption/overall running time: 101.4999s / 113653.7643 s
env0_first_0:                 episode reward: -9.6500,                 loss: -0.0969
env0_second_0:                 episode reward: 9.6500,                 loss: -0.0616
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 8261/30000 (27.5367%),                 avg. length: 916.35,                last time consumption/overall running time: 113.9042s / 113767.6685 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0806
env0_second_0:                 episode reward: 9.5000,                 loss: -0.0231
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 8281/30000 (27.6033%),                 avg. length: 909.2,                last time consumption/overall running time: 115.2261s / 113882.8946 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.0517
env0_second_0:                 episode reward: 9.5500,                 loss: 0.0637
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 8301/30000 (27.6700%),                 avg. length: 916.8,                last time consumption/overall running time: 97.7370s / 113980.6316 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.0692
env0_second_0:                 episode reward: 9.4000,                 loss: 0.0080
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 8321/30000 (27.7367%),                 avg. length: 923.75,                last time consumption/overall running time: 93.6603s / 114074.2919 s
env0_first_0:                 episode reward: -9.4500,                 loss: -0.1075
env0_second_0:                 episode reward: 9.4500,                 loss: -0.0584
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 8341/30000 (27.8033%),                 avg. length: 915.4,                last time consumption/overall running time: 106.9249s / 114181.2168 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.0698
env0_second_0:                 episode reward: 9.6000,                 loss: 0.2043
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 8361/30000 (27.8700%),                 avg. length: 917.8,                last time consumption/overall running time: 112.9202s / 114294.1370 s
env0_first_0:                 episode reward: -8.8000,                 loss: -0.0754
env0_second_0:                 episode reward: 8.8000,                 loss: 0.1519
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 8381/30000 (27.9367%),                 avg. length: 912.2,                last time consumption/overall running time: 120.5657s / 114414.7027 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.0982
env0_second_0:                 episode reward: 9.7500,                 loss: 0.0389
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 8401/30000 (28.0033%),                 avg. length: 917.6,                last time consumption/overall running time: 110.1421s / 114524.8448 s
env0_first_0:                 episode reward: -9.3500,                 loss: 0.0996
env0_second_0:                 episode reward: 9.3500,                 loss: -0.0293
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 8421/30000 (28.0700%),                 avg. length: 918.4,                last time consumption/overall running time: 92.5767s / 114617.4215 s
env0_first_0:                 episode reward: -9.0000,                 loss: -0.1345
env0_second_0:                 episode reward: 9.0000,                 loss: -0.0483
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 8441/30000 (28.1367%),                 avg. length: 932.9,                last time consumption/overall running time: 110.7154s / 114728.1369 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1238
env0_second_0:                 episode reward: 9.3500,                 loss: -0.0011
env1_first_0:                 episode reward: -8.7000,                 loss: nan
env1_second_0:                 episode reward: 8.7000,                 loss: nan
Episode: 8461/30000 (28.2033%),                 avg. length: 1045.05,                last time consumption/overall running time: 117.3553s / 114845.4922 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.0420
env0_second_0:                 episode reward: 6.7500,                 loss: 0.1251
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 8481/30000 (28.2700%),                 avg. length: 956.45,                last time consumption/overall running time: 104.9169s / 114950.4091 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.0697
env0_second_0:                 episode reward: 9.3500,                 loss: 0.0298
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 8501/30000 (28.3367%),                 avg. length: 924.8,                last time consumption/overall running time: 115.7661s / 115066.1752 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.0929
env0_second_0:                 episode reward: 9.4000,                 loss: 0.0805
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 8521/30000 (28.4033%),                 avg. length: 915.45,                last time consumption/overall running time: 118.0829s / 115184.2580 s
env0_first_0:                 episode reward: -9.8000,                 loss: -0.1227
env0_second_0:                 episode reward: 9.8000,                 loss: 0.2323
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 8541/30000 (28.4700%),                 avg. length: 912.3,                last time consumption/overall running time: 115.6567s / 115299.9148 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.1133
env0_second_0:                 episode reward: 9.4000,                 loss: 0.0972
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 8561/30000 (28.5367%),                 avg. length: 964.25,                last time consumption/overall running time: 125.6869s / 115425.6017 s
env0_first_0:                 episode reward: -8.1500,                 loss: -0.0828
env0_second_0:                 episode reward: 8.1500,                 loss: 0.3055
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 8581/30000 (28.6033%),                 avg. length: 926.15,                last time consumption/overall running time: 105.5881s / 115531.1898 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.0997
env0_second_0:                 episode reward: 9.4000,                 loss: 0.3036
env1_first_0:                 episode reward: -9.6500,                 loss: nan
env1_second_0:                 episode reward: 9.6500,                 loss: nan
Episode: 8601/30000 (28.6700%),                 avg. length: 907.2,                last time consumption/overall running time: 109.9919s / 115641.1816 s
env0_first_0:                 episode reward: -9.8500,                 loss: -0.1130
env0_second_0:                 episode reward: 9.8500,                 loss: 0.0470
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 8621/30000 (28.7367%),                 avg. length: 924.9,                last time consumption/overall running time: 108.8887s / 115750.0703 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.1139
env0_second_0:                 episode reward: 9.6000,                 loss: 0.0518
env1_first_0:                 episode reward: -8.5000,                 loss: nan
env1_second_0:                 episode reward: 8.5000,                 loss: nan
Episode: 8641/30000 (28.8033%),                 avg. length: 915.75,                last time consumption/overall running time: 101.2405s / 115851.3108 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.1214
env0_second_0:                 episode reward: 9.6000,                 loss: 0.2519
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 8661/30000 (28.8700%),                 avg. length: 921.9,                last time consumption/overall running time: 97.4803s / 115948.7911 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1282
env0_second_0:                 episode reward: 9.3500,                 loss: 0.1891
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 8681/30000 (28.9367%),                 avg. length: 908.35,                last time consumption/overall running time: 104.2922s / 116053.0834 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.1180
env0_second_0:                 episode reward: 9.3000,                 loss: 0.1270
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 8701/30000 (29.0033%),                 avg. length: 907.8,                last time consumption/overall running time: 98.4848s / 116151.5682 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.1191
env0_second_0:                 episode reward: 9.5000,                 loss: 0.1933
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 8721/30000 (29.0700%),                 avg. length: 908.95,                last time consumption/overall running time: 92.5083s / 116244.0765 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.1179
env0_second_0:                 episode reward: 9.6000,                 loss: 0.1702
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 8741/30000 (29.1367%),                 avg. length: 916.0,                last time consumption/overall running time: 95.2950s / 116339.3714 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.1187
env0_second_0:                 episode reward: 8.7500,                 loss: 0.1606
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 8761/30000 (29.2033%),                 avg. length: 918.7,                last time consumption/overall running time: 103.5394s / 116442.9109 s
env0_first_0:                 episode reward: -8.2500,                 loss: -0.0882
env0_second_0:                 episode reward: 8.2500,                 loss: 8.4531
env1_first_0:                 episode reward: -8.8500,                 loss: nan
env1_second_0:                 episode reward: 8.8500,                 loss: nan
Episode: 8781/30000 (29.2700%),                 avg. length: 916.3,                last time consumption/overall running time: 110.0364s / 116552.9473 s
env0_first_0:                 episode reward: -9.2500,                 loss: -0.0959
env0_second_0:                 episode reward: 9.2500,                 loss: 0.1845
env1_first_0:                 episode reward: -8.8000,                 loss: nan
env1_second_0:                 episode reward: 8.8000,                 loss: nan
Episode: 8801/30000 (29.3367%),                 avg. length: 908.8,                last time consumption/overall running time: 114.8261s / 116667.7733 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.1203
env0_second_0:                 episode reward: 9.4000,                 loss: 0.1303
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 8821/30000 (29.4033%),                 avg. length: 908.2,                last time consumption/overall running time: 117.7563s / 116785.5297 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1288
env0_second_0:                 episode reward: 9.3500,                 loss: 0.0672
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 8841/30000 (29.4700%),                 avg. length: 907.95,                last time consumption/overall running time: 106.9588s / 116892.4885 s
env0_first_0:                 episode reward: -9.8500,                 loss: -0.1627
env0_second_0:                 episode reward: 9.8500,                 loss: -0.0555
env1_first_0:                 episode reward: -9.6500,                 loss: nan
env1_second_0:                 episode reward: 9.6500,                 loss: nan
Episode: 8861/30000 (29.5367%),                 avg. length: 907.2,                last time consumption/overall running time: 98.7050s / 116991.1935 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.1631
env0_second_0:                 episode reward: 9.5500,                 loss: -0.0475
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 8881/30000 (29.6033%),                 avg. length: 908.8,                last time consumption/overall running time: 95.5546s / 117086.7482 s
env0_first_0:                 episode reward: -9.6500,                 loss: -0.1673
env0_second_0:                 episode reward: 9.6500,                 loss: -0.0639
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 8901/30000 (29.6700%),                 avg. length: 981.05,                last time consumption/overall running time: 117.0822s / 117203.8303 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.0968
env0_second_0:                 episode reward: 5.4000,                 loss: 0.3302
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 8921/30000 (29.7367%),                 avg. length: 932.05,                last time consumption/overall running time: 120.2130s / 117324.0433 s
env0_first_0:                 episode reward: -8.6000,                 loss: -0.1112
env0_second_0:                 episode reward: 8.6000,                 loss: 0.0931
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 8941/30000 (29.8033%),                 avg. length: 937.95,                last time consumption/overall running time: 115.3775s / 117439.4208 s
env0_first_0:                 episode reward: -9.1000,                 loss: -0.1247
env0_second_0:                 episode reward: 9.1000,                 loss: 0.0140
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 8961/30000 (29.8700%),                 avg. length: 914.4,                last time consumption/overall running time: 116.4720s / 117555.8928 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.1332
env0_second_0:                 episode reward: 9.4000,                 loss: -0.0454
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 8981/30000 (29.9367%),                 avg. length: 909.75,                last time consumption/overall running time: 114.8088s / 117670.7017 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.1438
env0_second_0:                 episode reward: 9.6000,                 loss: 0.0306
env1_first_0:                 episode reward: -8.5000,                 loss: nan
env1_second_0:                 episode reward: 8.5000,                 loss: nan
Episode: 9001/30000 (30.0033%),                 avg. length: 919.5,                last time consumption/overall running time: 110.4994s / 117781.2011 s
env0_first_0:                 episode reward: -8.8500,                 loss: -0.1268
env0_second_0:                 episode reward: 8.8500,                 loss: -0.0302
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 9021/30000 (30.0700%),                 avg. length: 907.75,                last time consumption/overall running time: 116.7298s / 117897.9309 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.1645
env0_second_0:                 episode reward: 9.7500,                 loss: 0.0276
env1_first_0:                 episode reward: -9.6500,                 loss: nan
env1_second_0:                 episode reward: 9.6500,                 loss: nan
Episode: 9041/30000 (30.1367%),                 avg. length: 907.8,                last time consumption/overall running time: 107.7562s / 118005.6871 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1426
env0_second_0:                 episode reward: 9.7000,                 loss: -0.0225
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 9061/30000 (30.2033%),                 avg. length: 916.75,                last time consumption/overall running time: 117.9780s / 118123.6651 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1529
env0_second_0:                 episode reward: 9.3500,                 loss: -0.0414
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 9081/30000 (30.2700%),                 avg. length: 1264.9,                last time consumption/overall running time: 152.6142s / 118276.2793 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.0368
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1953
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 9101/30000 (30.3367%),                 avg. length: 2299.05,                last time consumption/overall running time: 264.8905s / 118541.1697 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.0194
env0_second_0:                 episode reward: -1.4000,                 loss: 0.1761
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 9121/30000 (30.4033%),                 avg. length: 2448.65,                last time consumption/overall running time: 285.1526s / 118826.3224 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.0212
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0988
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 9141/30000 (30.4700%),                 avg. length: 2579.05,                last time consumption/overall running time: 271.1385s / 119097.4608 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.0392
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0480
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 9161/30000 (30.5367%),                 avg. length: 2766.15,                last time consumption/overall running time: 297.1654s / 119394.6262 s
env0_first_0:                 episode reward: -3.1500,                 loss: -0.0427
env0_second_0:                 episode reward: 3.1500,                 loss: 0.0420
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 9181/30000 (30.6033%),                 avg. length: 2335.25,                last time consumption/overall running time: 286.5350s / 119681.1612 s
env0_first_0:                 episode reward: -4.3000,                 loss: -0.0476
env0_second_0:                 episode reward: 4.3000,                 loss: 0.0987
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 9201/30000 (30.6700%),                 avg. length: 2638.8,                last time consumption/overall running time: 320.2810s / 120001.4422 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.0741
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0632
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 9221/30000 (30.7367%),                 avg. length: 2662.4,                last time consumption/overall running time: 324.4212s / 120325.8634 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0509
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0956
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 9241/30000 (30.8033%),                 avg. length: 2609.25,                last time consumption/overall running time: 318.6576s / 120644.5209 s
env0_first_0:                 episode reward: -4.7500,                 loss: -0.0513
env0_second_0:                 episode reward: 4.7500,                 loss: 0.1150
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 9261/30000 (30.8700%),                 avg. length: 2512.0,                last time consumption/overall running time: 309.2598s / 120953.7808 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.0535
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0738
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 9281/30000 (30.9367%),                 avg. length: 2843.8,                last time consumption/overall running time: 347.7690s / 121301.5498 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0648
env0_second_0:                 episode reward: 0.2500,                 loss: 0.1298
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 9301/30000 (31.0033%),                 avg. length: 2665.9,                last time consumption/overall running time: 326.6790s / 121628.2288 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0620
env0_second_0:                 episode reward: -0.3000,                 loss: 0.1369
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 9321/30000 (31.0700%),                 avg. length: 2362.85,                last time consumption/overall running time: 291.1253s / 121919.3541 s
env0_first_0:                 episode reward: -3.7000,                 loss: -0.0772
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1070
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 9341/30000 (31.1367%),                 avg. length: 2060.55,                last time consumption/overall running time: 256.1797s / 122175.5338 s
env0_first_0:                 episode reward: -7.1500,                 loss: -0.0824
env0_second_0:                 episode reward: 7.1500,                 loss: 0.1227
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 9361/30000 (31.2033%),                 avg. length: 2525.8,                last time consumption/overall running time: 310.8267s / 122486.3606 s
env0_first_0:                 episode reward: -5.6000,                 loss: -0.0802
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0551
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 9381/30000 (31.2700%),                 avg. length: 2896.1,                last time consumption/overall running time: 354.9051s / 122841.2657 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.0685
env0_second_0:                 episode reward: 2.4500,                 loss: 0.0475
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 9401/30000 (31.3367%),                 avg. length: 3068.2,                last time consumption/overall running time: 373.2375s / 123214.5032 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.0697
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0482
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 9421/30000 (31.4033%),                 avg. length: 2791.2,                last time consumption/overall running time: 341.4826s / 123555.9857 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.0798
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0167
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 9441/30000 (31.4700%),                 avg. length: 2938.6,                last time consumption/overall running time: 360.0593s / 123916.0450 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.0737
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0217
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 9461/30000 (31.5367%),                 avg. length: 2909.7,                last time consumption/overall running time: 355.4307s / 124271.4757 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.0925
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0004
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 9481/30000 (31.6033%),                 avg. length: 2992.65,                last time consumption/overall running time: 365.1967s / 124636.6724 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.0834
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0286
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 9501/30000 (31.6700%),                 avg. length: 2844.05,                last time consumption/overall running time: 347.7593s / 124984.4317 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.0836
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0979
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 9521/30000 (31.7367%),                 avg. length: 3074.1,                last time consumption/overall running time: 374.0387s / 125358.4704 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.0821
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0707
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 9541/30000 (31.8033%),                 avg. length: 3021.85,                last time consumption/overall running time: 366.3122s / 125724.7826 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0900
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0949
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 9561/30000 (31.8700%),                 avg. length: 3026.8,                last time consumption/overall running time: 367.7843s / 126092.5669 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.0986
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0610
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 9581/30000 (31.9367%),                 avg. length: 3045.35,                last time consumption/overall running time: 370.8054s / 126463.3723 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0879
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0725
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9601/30000 (32.0033%),                 avg. length: 2844.1,                last time consumption/overall running time: 346.2575s / 126809.6298 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.0887
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0966
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 9621/30000 (32.0700%),                 avg. length: 2803.6,                last time consumption/overall running time: 341.0315s / 127150.6613 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0688
env0_second_0:                 episode reward: 1.8500,                 loss: 0.2347
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 9641/30000 (32.1367%),                 avg. length: 2813.9,                last time consumption/overall running time: 343.8251s / 127494.4864 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.0660
env0_second_0:                 episode reward: -1.1000,                 loss: 0.2496
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 9661/30000 (32.2033%),                 avg. length: 3207.05,                last time consumption/overall running time: 390.9809s / 127885.4674 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.0771
env0_second_0:                 episode reward: -1.5000,                 loss: 0.2150
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 9681/30000 (32.2700%),                 avg. length: 3023.45,                last time consumption/overall running time: 368.9572s / 128254.4245 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.0884
env0_second_0:                 episode reward: -4.1500,                 loss: 0.1572
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 9701/30000 (32.3367%),                 avg. length: 2852.65,                last time consumption/overall running time: 346.6285s / 128601.0530 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.0577
env0_second_0:                 episode reward: -3.8000,                 loss: 0.2191
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 9721/30000 (32.4033%),                 avg. length: 2425.2,                last time consumption/overall running time: 297.2948s / 128898.3478 s
env0_first_0:                 episode reward: 6.7000,                 loss: -0.0638
env0_second_0:                 episode reward: -6.7000,                 loss: 0.1459
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 9741/30000 (32.4700%),                 avg. length: 2736.4,                last time consumption/overall running time: 330.3214s / 129228.6692 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.0600
env0_second_0:                 episode reward: -5.9000,                 loss: 0.0627
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 9761/30000 (32.5367%),                 avg. length: 2571.6,                last time consumption/overall running time: 313.3270s / 129541.9962 s
env0_first_0:                 episode reward: 6.5500,                 loss: -0.0850
env0_second_0:                 episode reward: -6.5500,                 loss: 0.0852
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 9781/30000 (32.6033%),                 avg. length: 2581.4,                last time consumption/overall running time: 298.0074s / 129840.0036 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.0855
env0_second_0:                 episode reward: -7.1500,                 loss: 0.0540
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 9801/30000 (32.6700%),                 avg. length: 2547.9,                last time consumption/overall running time: 271.4850s / 130111.4886 s
env0_first_0:                 episode reward: 7.1000,                 loss: -0.0760
env0_second_0:                 episode reward: -7.1000,                 loss: 0.0313
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 9821/30000 (32.7367%),                 avg. length: 2608.5,                last time consumption/overall running time: 280.1092s / 130391.5978 s
env0_first_0:                 episode reward: 7.1000,                 loss: -0.0704
env0_second_0:                 episode reward: -7.1000,                 loss: 0.0793
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 9841/30000 (32.8033%),                 avg. length: 2844.9,                last time consumption/overall running time: 326.6562s / 130718.2541 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.0730
env0_second_0:                 episode reward: -5.7500,                 loss: 0.0675
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 9861/30000 (32.8700%),                 avg. length: 2809.1,                last time consumption/overall running time: 308.6525s / 131026.9065 s
env0_first_0:                 episode reward: 6.3000,                 loss: -0.0610
env0_second_0:                 episode reward: -6.3000,                 loss: 0.0560
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 9881/30000 (32.9367%),                 avg. length: 2624.1,                last time consumption/overall running time: 290.9092s / 131317.8157 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.0628
env0_second_0:                 episode reward: -6.5000,                 loss: 0.1169
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 9901/30000 (33.0033%),                 avg. length: 2393.5,                last time consumption/overall running time: 246.0471s / 131563.8628 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.0868
env0_second_0:                 episode reward: -7.1500,                 loss: 0.0683
env1_first_0:                 episode reward: 8.1500,                 loss: nan
env1_second_0:                 episode reward: -8.1500,                 loss: nan
Episode: 9921/30000 (33.0700%),                 avg. length: 2438.2,                last time consumption/overall running time: 278.1579s / 131842.0207 s
env0_first_0:                 episode reward: 7.8000,                 loss: -0.0726
env0_second_0:                 episode reward: -7.8000,                 loss: 0.1087
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 9941/30000 (33.1367%),                 avg. length: 2575.9,                last time consumption/overall running time: 287.4380s / 132129.4587 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.0560
env0_second_0:                 episode reward: -5.8000,                 loss: 0.1622
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 9961/30000 (33.2033%),                 avg. length: 2550.05,                last time consumption/overall running time: 264.0198s / 132393.4785 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.0341
env0_second_0:                 episode reward: -3.0500,                 loss: 0.1046
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 9981/30000 (33.2700%),                 avg. length: 2089.95,                last time consumption/overall running time: 220.9488s / 132614.4273 s
env0_first_0:                 episode reward: -4.3000,                 loss: -0.0145
env0_second_0:                 episode reward: 4.3000,                 loss: 0.3264
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 10001/30000 (33.3367%),                 avg. length: 1658.05,                last time consumption/overall running time: 174.8605s / 132789.2877 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.0184
env0_second_0:                 episode reward: 5.9500,                 loss: 0.3435
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 10021/30000 (33.4033%),                 avg. length: 2514.6,                last time consumption/overall running time: 260.3291s / 133049.6168 s
env0_first_0:                 episode reward: -3.6000,                 loss: -0.0153
env0_second_0:                 episode reward: 3.6000,                 loss: 0.2420
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 10041/30000 (33.4700%),                 avg. length: 2728.2,                last time consumption/overall running time: 284.2595s / 133333.8763 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.0162
env0_second_0:                 episode reward: -3.2000,                 loss: 0.2304
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 10061/30000 (33.5367%),                 avg. length: 2733.45,                last time consumption/overall running time: 311.1969s / 133645.0732 s
env0_first_0:                 episode reward: 5.5000,                 loss: -0.0395
env0_second_0:                 episode reward: -5.5000,                 loss: 0.1694
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 10081/30000 (33.6033%),                 avg. length: 2650.45,                last time consumption/overall running time: 283.6712s / 133928.7444 s
env0_first_0:                 episode reward: 6.7500,                 loss: -0.0459
env0_second_0:                 episode reward: -6.7500,                 loss: 0.5156
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 10101/30000 (33.6700%),                 avg. length: 2714.65,                last time consumption/overall running time: 286.1654s / 134214.9098 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.0560
env0_second_0:                 episode reward: -5.3000,                 loss: 0.6819
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 10121/30000 (33.7367%),                 avg. length: 2037.65,                last time consumption/overall running time: 219.1624s / 134434.0722 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.0196
env0_second_0:                 episode reward: -4.4000,                 loss: 0.6155
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 10141/30000 (33.8033%),                 avg. length: 863.0,                last time consumption/overall running time: 102.3843s / 134536.4565 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0597
env0_second_0:                 episode reward: -10.0000,                 loss: 0.1753
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10161/30000 (33.8700%),                 avg. length: 863.0,                last time consumption/overall running time: 101.9695s / 134638.4260 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0386
env0_second_0:                 episode reward: -10.0000,                 loss: 0.6407
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10181/30000 (33.9367%),                 avg. length: 863.0,                last time consumption/overall running time: 98.6700s / 134737.0960 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0685
env0_second_0:                 episode reward: -10.0000,                 loss: 0.3995
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 10201/30000 (34.0033%),                 avg. length: 863.0,                last time consumption/overall running time: 97.7421s / 134834.8381 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0445
env0_second_0:                 episode reward: -9.9500,                 loss: 0.7459
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10221/30000 (34.0700%),                 avg. length: 863.0,                last time consumption/overall running time: 97.9960s / 134932.8341 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0502
env0_second_0:                 episode reward: -10.0000,                 loss: 1.8254
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10241/30000 (34.1367%),                 avg. length: 863.0,                last time consumption/overall running time: 100.0652s / 135032.8993 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0256
env0_second_0:                 episode reward: -10.0000,                 loss: 2.4532
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10261/30000 (34.2033%),                 avg. length: 863.0,                last time consumption/overall running time: 107.0170s / 135139.9162 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0023
env0_second_0:                 episode reward: -10.0000,                 loss: 2.3626
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10281/30000 (34.2700%),                 avg. length: 863.0,                last time consumption/overall running time: 98.9821s / 135238.8983 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0206
env0_second_0:                 episode reward: -10.0000,                 loss: 1.6913
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10301/30000 (34.3367%),                 avg. length: 863.0,                last time consumption/overall running time: 98.3234s / 135337.2218 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0366
env0_second_0:                 episode reward: -10.0000,                 loss: 4.3287
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10321/30000 (34.4033%),                 avg. length: 863.0,                last time consumption/overall running time: 102.9788s / 135440.2005 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0118
env0_second_0:                 episode reward: -10.0000,                 loss: 1.6219
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10341/30000 (34.4700%),                 avg. length: 863.0,                last time consumption/overall running time: 96.7944s / 135536.9950 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0435
env0_second_0:                 episode reward: -10.0000,                 loss: 1.8709
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10361/30000 (34.5367%),                 avg. length: 1178.4,                last time consumption/overall running time: 128.8622s / 135665.8571 s
env0_first_0:                 episode reward: 6.9000,                 loss: -0.0372
env0_second_0:                 episode reward: -6.9000,                 loss: 3.0669
env1_first_0:                 episode reward: 8.1500,                 loss: nan
env1_second_0:                 episode reward: -8.1500,                 loss: nan
Episode: 10381/30000 (34.6033%),                 avg. length: 1679.4,                last time consumption/overall running time: 191.2872s / 135857.1444 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.0230
env0_second_0:                 episode reward: -6.5000,                 loss: 1.0608
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 10401/30000 (34.6700%),                 avg. length: 2360.15,                last time consumption/overall running time: 265.1344s / 136122.2788 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.0474
env0_second_0:                 episode reward: -4.3000,                 loss: 0.7936
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 10421/30000 (34.7367%),                 avg. length: 2190.05,                last time consumption/overall running time: 226.3982s / 136348.6770 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.0689
env0_second_0:                 episode reward: -6.0500,                 loss: 0.9890
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 10441/30000 (34.8033%),                 avg. length: 2373.15,                last time consumption/overall running time: 264.7532s / 136613.4302 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.0241
env0_second_0:                 episode reward: -3.8000,                 loss: 0.9283
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 10461/30000 (34.8700%),                 avg. length: 2225.35,                last time consumption/overall running time: 243.0808s / 136856.5110 s
env0_first_0:                 episode reward: 6.0000,                 loss: -0.0589
env0_second_0:                 episode reward: -6.0000,                 loss: 0.5255
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 10481/30000 (34.9367%),                 avg. length: 2107.45,                last time consumption/overall running time: 236.9871s / 137093.4981 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.0641
env0_second_0:                 episode reward: -5.3000,                 loss: 0.5757
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 10501/30000 (35.0033%),                 avg. length: 1613.9,                last time consumption/overall running time: 181.6735s / 137275.1716 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.0327
env0_second_0:                 episode reward: -5.7000,                 loss: 0.6876
env1_first_0:                 episode reward: 8.1500,                 loss: nan
env1_second_0:                 episode reward: -8.1500,                 loss: nan
Episode: 10521/30000 (35.0700%),                 avg. length: 2391.55,                last time consumption/overall running time: 262.1163s / 137537.2879 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.0560
env0_second_0:                 episode reward: -6.5000,                 loss: 0.9737
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 10541/30000 (35.1367%),                 avg. length: 2361.95,                last time consumption/overall running time: 245.9875s / 137783.2753 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.0572
env0_second_0:                 episode reward: -6.0500,                 loss: 0.7144
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 10561/30000 (35.2033%),                 avg. length: 2373.65,                last time consumption/overall running time: 265.5355s / 138048.8108 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.0328
env0_second_0:                 episode reward: -4.8500,                 loss: 0.9149
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 10581/30000 (35.2700%),                 avg. length: 2075.85,                last time consumption/overall running time: 215.6920s / 138264.5028 s
env0_first_0:                 episode reward: 7.4500,                 loss: -0.0965
env0_second_0:                 episode reward: -7.4500,                 loss: 0.5137
env1_first_0:                 episode reward: 8.0500,                 loss: nan
env1_second_0:                 episode reward: -8.0500,                 loss: nan
Episode: 10601/30000 (35.3367%),                 avg. length: 2326.65,                last time consumption/overall running time: 242.9572s / 138507.4600 s
env0_first_0:                 episode reward: 6.7000,                 loss: -0.0779
env0_second_0:                 episode reward: -6.7000,                 loss: 0.5936
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 10621/30000 (35.4033%),                 avg. length: 2676.8,                last time consumption/overall running time: 290.1159s / 138797.5760 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.0195
env0_second_0:                 episode reward: -3.3500,                 loss: 0.5729
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 10641/30000 (35.4700%),                 avg. length: 2274.4,                last time consumption/overall running time: 262.6177s / 139060.1937 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.0774
env0_second_0:                 episode reward: -7.1500,                 loss: 0.5089
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 10661/30000 (35.5367%),                 avg. length: 2040.85,                last time consumption/overall running time: 232.2904s / 139292.4840 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.0976
env0_second_0:                 episode reward: -7.7500,                 loss: 0.4121
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 10681/30000 (35.6033%),                 avg. length: 2331.85,                last time consumption/overall running time: 247.2597s / 139539.7437 s
env0_first_0:                 episode reward: 8.3000,                 loss: -0.1131
env0_second_0:                 episode reward: -8.3000,                 loss: 0.4145
env1_first_0:                 episode reward: 8.5500,                 loss: nan
env1_second_0:                 episode reward: -8.5500,                 loss: nan
Episode: 10701/30000 (35.6700%),                 avg. length: 2371.4,                last time consumption/overall running time: 260.4706s / 139800.2142 s
env0_first_0:                 episode reward: 9.0500,                 loss: -0.1219
env0_second_0:                 episode reward: -9.0500,                 loss: 0.4423
env1_first_0:                 episode reward: 8.6500,                 loss: nan
env1_second_0:                 episode reward: -8.6500,                 loss: nan
Episode: 10721/30000 (35.7367%),                 avg. length: 2428.1,                last time consumption/overall running time: 263.8567s / 140064.0709 s
env0_first_0:                 episode reward: 8.5500,                 loss: -0.0862
env0_second_0:                 episode reward: -8.5500,                 loss: 0.2579
env1_first_0:                 episode reward: 8.6000,                 loss: nan
env1_second_0:                 episode reward: -8.6000,                 loss: nan
Episode: 10741/30000 (35.8033%),                 avg. length: 2704.85,                last time consumption/overall running time: 301.9635s / 140366.0345 s
env0_first_0:                 episode reward: 8.4000,                 loss: -0.0635
env0_second_0:                 episode reward: -8.4000,                 loss: 0.1961
env1_first_0:                 episode reward: 7.1500,                 loss: nan
env1_second_0:                 episode reward: -7.1500,                 loss: nan
Episode: 10761/30000 (35.8700%),                 avg. length: 2741.1,                last time consumption/overall running time: 289.2944s / 140655.3289 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0074
env0_second_0:                 episode reward: -5.8500,                 loss: 0.6540
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 10781/30000 (35.9367%),                 avg. length: 2649.3,                last time consumption/overall running time: 294.4755s / 140949.8044 s
env0_first_0:                 episode reward: 6.3000,                 loss: -0.0358
env0_second_0:                 episode reward: -6.3000,                 loss: 1.4608
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 10801/30000 (36.0033%),                 avg. length: 2466.55,                last time consumption/overall running time: 277.7109s / 141227.5153 s
env0_first_0:                 episode reward: 8.6000,                 loss: -0.0920
env0_second_0:                 episode reward: -8.6000,                 loss: 0.2358
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 10821/30000 (36.0700%),                 avg. length: 2585.6,                last time consumption/overall running time: 296.2326s / 141523.7479 s
env0_first_0:                 episode reward: 7.6500,                 loss: -0.0646
env0_second_0:                 episode reward: -7.6500,                 loss: 0.2548
env1_first_0:                 episode reward: 7.4000,                 loss: nan
env1_second_0:                 episode reward: -7.4000,                 loss: nan
Episode: 10841/30000 (36.1367%),                 avg. length: 2845.4,                last time consumption/overall running time: 321.1964s / 141844.9444 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.0437
env0_second_0:                 episode reward: -4.4000,                 loss: 1.0219
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 10861/30000 (36.2033%),                 avg. length: 2928.7,                last time consumption/overall running time: 319.3140s / 142164.2584 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.0642
env0_second_0:                 episode reward: -4.9000,                 loss: 0.9388
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 10881/30000 (36.2700%),                 avg. length: 3333.45,                last time consumption/overall running time: 372.1462s / 142536.4046 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.0335
env0_second_0:                 episode reward: -3.2500,                 loss: 0.7322
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 10901/30000 (36.3367%),                 avg. length: 3834.05,                last time consumption/overall running time: 433.9144s / 142970.3190 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.0241
env0_second_0:                 episode reward: -2.9500,                 loss: 0.5255
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 10921/30000 (36.4033%),                 avg. length: 980.8,                last time consumption/overall running time: 109.8489s / 143080.1680 s
env0_first_0:                 episode reward: -5.7500,                 loss: -0.0138
env0_second_0:                 episode reward: 5.7500,                 loss: 0.4146
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 10941/30000 (36.4700%),                 avg. length: 909.8,                last time consumption/overall running time: 113.6823s / 143193.8503 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.1065
env0_second_0:                 episode reward: 9.4000,                 loss: 0.0361
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 10961/30000 (36.5367%),                 avg. length: 908.55,                last time consumption/overall running time: 110.6826s / 143304.5329 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0841
env0_second_0:                 episode reward: 9.5000,                 loss: 0.0403
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 10981/30000 (36.6033%),                 avg. length: 927.55,                last time consumption/overall running time: 105.1518s / 143409.6847 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.0910
env0_second_0:                 episode reward: 9.2000,                 loss: 0.0271
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 11001/30000 (36.6700%),                 avg. length: 922.6,                last time consumption/overall running time: 103.3026s / 143512.9872 s
env0_first_0:                 episode reward: -9.0000,                 loss: -0.1129
env0_second_0:                 episode reward: 9.0000,                 loss: 0.0844
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 11021/30000 (36.7367%),                 avg. length: 909.2,                last time consumption/overall running time: 109.9087s / 143622.8960 s
env0_first_0:                 episode reward: -9.4500,                 loss: -0.0897
env0_second_0:                 episode reward: 9.4500,                 loss: -0.0613
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 11041/30000 (36.8033%),                 avg. length: 909.75,                last time consumption/overall running time: 102.0566s / 143724.9526 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.0864
env0_second_0:                 episode reward: 9.5500,                 loss: -0.1272
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
Episode: 11061/30000 (36.8700%),                 avg. length: 920.2,                last time consumption/overall running time: 102.5204s / 143827.4730 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0978
env0_second_0:                 episode reward: 9.5000,                 loss: 0.1891
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 11081/30000 (36.9367%),                 avg. length: 935.25,                last time consumption/overall running time: 107.2420s / 143934.7150 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.0967
env0_second_0:                 episode reward: 9.2000,                 loss: 0.3180
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 11101/30000 (37.0033%),                 avg. length: 928.9,                last time consumption/overall running time: 100.9051s / 144035.6201 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.0978
env0_second_0:                 episode reward: 9.5500,                 loss: -0.0211
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 11121/30000 (37.0700%),                 avg. length: 925.15,                last time consumption/overall running time: 113.0921s / 144148.7122 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.1080
env0_second_0:                 episode reward: 9.5500,                 loss: -0.0359
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 11141/30000 (37.1367%),                 avg. length: 915.0,                last time consumption/overall running time: 106.7792s / 144255.4915 s
env0_first_0:                 episode reward: -9.8000,                 loss: -0.1206
env0_second_0:                 episode reward: 9.8000,                 loss: -0.0060
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 11161/30000 (37.2033%),                 avg. length: 908.4,                last time consumption/overall running time: 108.7327s / 144364.2242 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.1191
env0_second_0:                 episode reward: 9.5000,                 loss: 1.7330
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
Episode: 11181/30000 (37.2700%),                 avg. length: 907.2,                last time consumption/overall running time: 101.2454s / 144465.4696 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1225
env0_second_0:                 episode reward: 9.7000,                 loss: 0.2380
env1_first_0:                 episode reward: -9.6500,                 loss: nan
env1_second_0:                 episode reward: 9.6500,                 loss: nan
Episode: 11201/30000 (37.3367%),                 avg. length: 908.4,                last time consumption/overall running time: 107.1691s / 144572.6387 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.0814
env0_second_0:                 episode reward: 9.3000,                 loss: 0.2239
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 11221/30000 (37.4033%),                 avg. length: 909.75,                last time consumption/overall running time: 105.7081s / 144678.3468 s
env0_first_0:                 episode reward: -9.2500,                 loss: -0.1485
env0_second_0:                 episode reward: 9.2500,                 loss: 0.0132
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 11241/30000 (37.4700%),                 avg. length: 911.7,                last time consumption/overall running time: 104.0584s / 144782.4052 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.1587
env0_second_0:                 episode reward: 9.5500,                 loss: -0.0380
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
Episode: 11261/30000 (37.5367%),                 avg. length: 909.2,                last time consumption/overall running time: 118.1842s / 144900.5894 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1413
env0_second_0:                 episode reward: 9.7000,                 loss: -0.0780
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 11281/30000 (37.6033%),                 avg. length: 907.0,                last time consumption/overall running time: 100.2135s / 145000.8029 s
env0_first_0:                 episode reward: -9.8000,                 loss: -0.1541
env0_second_0:                 episode reward: 9.8000,                 loss: -0.1020
env1_first_0:                 episode reward: -9.8500,                 loss: nan
env1_second_0:                 episode reward: 9.8500,                 loss: nan
Episode: 11301/30000 (37.6700%),                 avg. length: 1520.55,                last time consumption/overall running time: 167.0747s / 145167.8776 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.0191
env0_second_0:                 episode reward: 1.4000,                 loss: 0.1690
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 11321/30000 (37.7367%),                 avg. length: 1643.15,                last time consumption/overall running time: 170.7977s / 145338.6753 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0051
env0_second_0:                 episode reward: -4.2000,                 loss: 0.3033
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 11341/30000 (37.8033%),                 avg. length: 1855.55,                last time consumption/overall running time: 201.8683s / 145540.5436 s
env0_first_0:                 episode reward: -3.2500,                 loss: -0.0040
env0_second_0:                 episode reward: 3.2500,                 loss: 0.8182
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 11361/30000 (37.8700%),                 avg. length: 2224.55,                last time consumption/overall running time: 239.5363s / 145780.0799 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.0219
env0_second_0:                 episode reward: 2.2500,                 loss: 0.4352
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 11381/30000 (37.9367%),                 avg. length: 2320.65,                last time consumption/overall running time: 246.8459s / 146026.9257 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.0408
env0_second_0:                 episode reward: -1.5000,                 loss: 0.1839
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 11401/30000 (38.0033%),                 avg. length: 2330.1,                last time consumption/overall running time: 246.3807s / 146273.3064 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.0087
env0_second_0:                 episode reward: 2.2500,                 loss: 0.2338
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 11421/30000 (38.0700%),                 avg. length: 2059.45,                last time consumption/overall running time: 235.3927s / 146508.6992 s
env0_first_0:                 episode reward: -6.4000,                 loss: -0.0595
env0_second_0:                 episode reward: 6.4000,                 loss: 0.2259
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 11441/30000 (38.1367%),                 avg. length: 1718.6,                last time consumption/overall running time: 193.3278s / 146702.0270 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.0411
env0_second_0:                 episode reward: 8.6500,                 loss: 0.2867
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 11461/30000 (38.2033%),                 avg. length: 2017.6,                last time consumption/overall running time: 222.0917s / 146924.1187 s
env0_first_0:                 episode reward: -7.6000,                 loss: -0.0596
env0_second_0:                 episode reward: 7.6000,                 loss: 0.1725
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 11481/30000 (38.2700%),                 avg. length: 1782.8,                last time consumption/overall running time: 195.7718s / 147119.8906 s
env0_first_0:                 episode reward: -8.2500,                 loss: -0.0597
env0_second_0:                 episode reward: 8.2500,                 loss: 0.2579
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 11501/30000 (38.3367%),                 avg. length: 2357.9,                last time consumption/overall running time: 248.3999s / 147368.2905 s
env0_first_0:                 episode reward: -2.9500,                 loss: -0.0174
env0_second_0:                 episode reward: 2.9500,                 loss: 0.3054
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 11521/30000 (38.4033%),                 avg. length: 2178.1,                last time consumption/overall running time: 226.8094s / 147595.0999 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.0345
env0_second_0:                 episode reward: 4.1000,                 loss: 0.1879
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 11541/30000 (38.4700%),                 avg. length: 2005.25,                last time consumption/overall running time: 216.1201s / 147811.2200 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.0297
env0_second_0:                 episode reward: 4.4500,                 loss: 0.2022
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 11561/30000 (38.5367%),                 avg. length: 1981.5,                last time consumption/overall running time: 215.7126s / 148026.9325 s
env0_first_0:                 episode reward: -4.8500,                 loss: -0.0498
env0_second_0:                 episode reward: 4.8500,                 loss: 0.1713
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 11581/30000 (38.6033%),                 avg. length: 1713.8,                last time consumption/overall running time: 186.2610s / 148213.1936 s
env0_first_0:                 episode reward: -8.0000,                 loss: -0.0787
env0_second_0:                 episode reward: 8.0000,                 loss: 0.1566
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 11601/30000 (38.6700%),                 avg. length: 1876.05,                last time consumption/overall running time: 208.3609s / 148421.5544 s
env0_first_0:                 episode reward: -7.4000,                 loss: -0.0811
env0_second_0:                 episode reward: 7.4000,                 loss: 0.1784
env1_first_0:                 episode reward: -7.9000,                 loss: nan
env1_second_0:                 episode reward: 7.9000,                 loss: nan
Episode: 11621/30000 (38.7367%),                 avg. length: 1930.35,                last time consumption/overall running time: 207.8922s / 148629.4466 s
env0_first_0:                 episode reward: -8.1000,                 loss: -0.0861
env0_second_0:                 episode reward: 8.1000,                 loss: 0.1411
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 11641/30000 (38.8033%),                 avg. length: 2330.35,                last time consumption/overall running time: 249.7168s / 148879.1634 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0009
env0_second_0:                 episode reward: 0.3500,                 loss: 0.2558
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 11661/30000 (38.8700%),                 avg. length: 2479.1,                last time consumption/overall running time: 276.5717s / 149155.7352 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.0374
env0_second_0:                 episode reward: -2.4500,                 loss: 0.2863
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 11681/30000 (38.9367%),                 avg. length: 2216.75,                last time consumption/overall running time: 235.8788s / 149391.6139 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0124
env0_second_0:                 episode reward: 4.1500,                 loss: 0.7997
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 11701/30000 (39.0033%),                 avg. length: 1922.55,                last time consumption/overall running time: 212.1388s / 149603.7527 s
env0_first_0:                 episode reward: -5.5000,                 loss: -0.0503
env0_second_0:                 episode reward: 5.5000,                 loss: 0.5641
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 11721/30000 (39.0700%),                 avg. length: 2722.95,                last time consumption/overall running time: 311.4741s / 149915.2268 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0197
env0_second_0:                 episode reward: -0.8500,                 loss: 0.4618
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 11741/30000 (39.1367%),                 avg. length: 1996.7,                last time consumption/overall running time: 218.8342s / 150134.0610 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.0460
env0_second_0:                 episode reward: -6.4000,                 loss: 0.3028
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 11761/30000 (39.2033%),                 avg. length: 2092.3,                last time consumption/overall running time: 219.9240s / 150353.9851 s
env0_first_0:                 episode reward: 6.2500,                 loss: -0.0347
env0_second_0:                 episode reward: -6.2500,                 loss: 0.4267
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 11781/30000 (39.2700%),                 avg. length: 2361.85,                last time consumption/overall running time: 261.7622s / 150615.7473 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.0141
env0_second_0:                 episode reward: -5.8000,                 loss: 0.6918
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 11801/30000 (39.3367%),                 avg. length: 2630.05,                last time consumption/overall running time: 286.7871s / 150902.5344 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.0090
env0_second_0:                 episode reward: 2.2500,                 loss: 0.5638
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 11821/30000 (39.4033%),                 avg. length: 2438.45,                last time consumption/overall running time: 271.1562s / 151173.6906 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0072
env0_second_0:                 episode reward: 2.1000,                 loss: 0.5903
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 11841/30000 (39.4700%),                 avg. length: 1720.1,                last time consumption/overall running time: 187.3857s / 151361.0763 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0091
env0_second_0:                 episode reward: 3.2000,                 loss: 0.8355
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 11861/30000 (39.5367%),                 avg. length: 1742.45,                last time consumption/overall running time: 191.3299s / 151552.4061 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.0699
env0_second_0:                 episode reward: 7.8500,                 loss: 0.7552
env1_first_0:                 episode reward: -8.0500,                 loss: nan
env1_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 11881/30000 (39.6033%),                 avg. length: 2715.25,                last time consumption/overall running time: 307.5184s / 151859.9245 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.0318
env0_second_0:                 episode reward: -0.5000,                 loss: 0.5176
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 11901/30000 (39.6700%),                 avg. length: 2502.85,                last time consumption/overall running time: 284.2651s / 152144.1896 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.0393
env0_second_0:                 episode reward: 5.9500,                 loss: 0.6094
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 11921/30000 (39.7367%),                 avg. length: 2785.75,                last time consumption/overall running time: 305.7654s / 152449.9550 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.0506
env0_second_0:                 episode reward: 5.7000,                 loss: 0.6318
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 11941/30000 (39.8033%),                 avg. length: 2610.1,                last time consumption/overall running time: 289.2022s / 152739.1572 s
env0_first_0:                 episode reward: -6.2000,                 loss: -0.0392
env0_second_0:                 episode reward: 6.2000,                 loss: 0.9094
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 11961/30000 (39.8700%),                 avg. length: 2252.5,                last time consumption/overall running time: 252.1420s / 152991.2992 s
env0_first_0:                 episode reward: -6.1000,                 loss: -0.0369
env0_second_0:                 episode reward: 6.1000,                 loss: 1.4917
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 11981/30000 (39.9367%),                 avg. length: 2551.25,                last time consumption/overall running time: 277.2756s / 153268.5747 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.0378
env0_second_0:                 episode reward: 2.0500,                 loss: 0.7369
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 12001/30000 (40.0033%),                 avg. length: 2922.0,                last time consumption/overall running time: 309.4685s / 153578.0433 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.0550
env0_second_0:                 episode reward: -2.5500,                 loss: 0.7901
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 12021/30000 (40.0700%),                 avg. length: 2927.6,                last time consumption/overall running time: 325.9376s / 153903.9809 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.0770
env0_second_0:                 episode reward: 4.6000,                 loss: 0.4188
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 12041/30000 (40.1367%),                 avg. length: 2797.5,                last time consumption/overall running time: 297.8290s / 154201.8099 s
env0_first_0:                 episode reward: -5.9000,                 loss: -0.0546
env0_second_0:                 episode reward: 5.9000,                 loss: 0.7848
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 12061/30000 (40.2033%),                 avg. length: 2761.4,                last time consumption/overall running time: 291.5818s / 154493.3917 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.0693
env0_second_0:                 episode reward: 7.0000,                 loss: 0.6057
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 12081/30000 (40.2700%),                 avg. length: 2741.65,                last time consumption/overall running time: 294.7812s / 154788.1728 s
env0_first_0:                 episode reward: -5.8500,                 loss: -0.0581
env0_second_0:                 episode reward: 5.8500,                 loss: 0.6172
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 12101/30000 (40.3367%),                 avg. length: 2480.75,                last time consumption/overall running time: 274.7560s / 155062.9289 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.0521
env0_second_0:                 episode reward: 5.9500,                 loss: 0.5485
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 12121/30000 (40.4033%),                 avg. length: 2688.4,                last time consumption/overall running time: 285.6463s / 155348.5752 s
env0_first_0:                 episode reward: -5.5000,                 loss: -0.0540
env0_second_0:                 episode reward: 5.5000,                 loss: 0.6069
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 12141/30000 (40.4700%),                 avg. length: 2404.6,                last time consumption/overall running time: 266.5852s / 155615.1604 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.0691
env0_second_0:                 episode reward: 7.2500,                 loss: 0.8155
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 12161/30000 (40.5367%),                 avg. length: 2152.25,                last time consumption/overall running time: 226.5887s / 155841.7490 s
env0_first_0:                 episode reward: -8.2000,                 loss: -0.0359
env0_second_0:                 episode reward: 8.2000,                 loss: 0.8955
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 12181/30000 (40.6033%),                 avg. length: 2420.45,                last time consumption/overall running time: 257.6532s / 156099.4023 s
env0_first_0:                 episode reward: -7.6000,                 loss: -0.0765
env0_second_0:                 episode reward: 7.6000,                 loss: 0.4624
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 12201/30000 (40.6700%),                 avg. length: 3010.85,                last time consumption/overall running time: 322.9268s / 156422.3290 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.0517
env0_second_0:                 episode reward: 5.8000,                 loss: 0.6496
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 12221/30000 (40.7367%),                 avg. length: 2706.4,                last time consumption/overall running time: 309.6215s / 156731.9505 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.0755
env0_second_0:                 episode reward: 6.8000,                 loss: 0.7088
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 12241/30000 (40.8033%),                 avg. length: 2503.0,                last time consumption/overall running time: 272.9315s / 157004.8821 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.0722
env0_second_0:                 episode reward: 6.7500,                 loss: 0.8950
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 12261/30000 (40.8700%),                 avg. length: 2459.05,                last time consumption/overall running time: 279.4609s / 157284.3430 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.0109
env0_second_0:                 episode reward: 3.3000,                 loss: 0.9467
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 12281/30000 (40.9367%),                 avg. length: 2093.8,                last time consumption/overall running time: 226.3524s / 157510.6954 s
env0_first_0:                 episode reward: -7.6500,                 loss: -0.0788
env0_second_0:                 episode reward: 7.6500,                 loss: 0.6587
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 12301/30000 (41.0033%),                 avg. length: 2358.0,                last time consumption/overall running time: 247.9108s / 157758.6062 s
env0_first_0:                 episode reward: -6.4000,                 loss: -0.0642
env0_second_0:                 episode reward: 6.4000,                 loss: 0.6282
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 12321/30000 (41.0700%),                 avg. length: 1559.2,                last time consumption/overall running time: 169.7449s / 157928.3510 s
env0_first_0:                 episode reward: -8.6000,                 loss: -0.0576
env0_second_0:                 episode reward: 8.6000,                 loss: 0.6042
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 12341/30000 (41.1367%),                 avg. length: 2223.45,                last time consumption/overall running time: 238.4760s / 158166.8270 s
env0_first_0:                 episode reward: -5.3000,                 loss: -0.0186
env0_second_0:                 episode reward: 5.3000,                 loss: 0.5781
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 12361/30000 (41.2033%),                 avg. length: 2309.4,                last time consumption/overall running time: 245.0429s / 158411.8699 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0520
env0_second_0:                 episode reward: 0.6500,                 loss: 0.7011
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 12381/30000 (41.2700%),                 avg. length: 2872.8,                last time consumption/overall running time: 311.5286s / 158723.3985 s
env0_first_0:                 episode reward: -6.0000,                 loss: -0.0745
env0_second_0:                 episode reward: 6.0000,                 loss: 1.0806
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 12401/30000 (41.3367%),                 avg. length: 2091.15,                last time consumption/overall running time: 231.1178s / 158954.5163 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.0560
env0_second_0:                 episode reward: 2.3500,                 loss: 0.5366
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 12421/30000 (41.4033%),                 avg. length: 1080.45,                last time consumption/overall running time: 123.3583s / 159077.8746 s
env0_first_0:                 episode reward: 8.8000,                 loss: -0.0589
env0_second_0:                 episode reward: -8.8000,                 loss: 0.4225
env1_first_0:                 episode reward: 8.4500,                 loss: nan
env1_second_0:                 episode reward: -8.4500,                 loss: nan
Episode: 12441/30000 (41.4700%),                 avg. length: 1029.65,                last time consumption/overall running time: 118.8893s / 159196.7639 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.0944
env0_second_0:                 episode reward: -9.6000,                 loss: 0.0608
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 12461/30000 (41.5367%),                 avg. length: 1219.75,                last time consumption/overall running time: 151.1598s / 159347.9237 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.0623
env0_second_0:                 episode reward: -7.7500,                 loss: 0.2454
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 12481/30000 (41.6033%),                 avg. length: 2348.1,                last time consumption/overall running time: 256.1514s / 159604.0751 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.0481
env0_second_0:                 episode reward: 5.4000,                 loss: 0.4213
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 12501/30000 (41.6700%),                 avg. length: 2559.15,                last time consumption/overall running time: 261.7753s / 159865.8504 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.0396
env0_second_0:                 episode reward: 4.6500,                 loss: 0.4709
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 12521/30000 (41.7367%),                 avg. length: 3098.75,                last time consumption/overall running time: 348.4248s / 160214.2752 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0476
env0_second_0:                 episode reward: -0.3000,                 loss: 0.5207
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 12541/30000 (41.8033%),                 avg. length: 2726.0,                last time consumption/overall running time: 289.1081s / 160503.3834 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.0405
env0_second_0:                 episode reward: -4.5500,                 loss: 0.6514
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 12561/30000 (41.8700%),                 avg. length: 2302.05,                last time consumption/overall running time: 253.4131s / 160756.7965 s
env0_first_0:                 episode reward: 7.5500,                 loss: -0.0647
env0_second_0:                 episode reward: -7.5500,                 loss: 0.3225
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 12581/30000 (41.9367%),                 avg. length: 2482.4,                last time consumption/overall running time: 275.3086s / 161032.1051 s
env0_first_0:                 episode reward: 7.0500,                 loss: -0.0713
env0_second_0:                 episode reward: -7.0500,                 loss: 0.2608
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 12601/30000 (42.0033%),                 avg. length: 2592.6,                last time consumption/overall running time: 297.1665s / 161329.2717 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.0773
env0_second_0:                 episode reward: -6.4000,                 loss: 0.4303
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 12621/30000 (42.0700%),                 avg. length: 2254.9,                last time consumption/overall running time: 253.0328s / 161582.3045 s
env0_first_0:                 episode reward: 7.6500,                 loss: -0.0821
env0_second_0:                 episode reward: -7.6500,                 loss: 0.6152
env1_first_0:                 episode reward: 7.8500,                 loss: nan
env1_second_0:                 episode reward: -7.8500,                 loss: nan
Episode: 12641/30000 (42.1367%),                 avg. length: 2220.9,                last time consumption/overall running time: 229.2678s / 161811.5723 s
env0_first_0:                 episode reward: 8.6000,                 loss: -0.0835
env0_second_0:                 episode reward: -8.6000,                 loss: 0.6726
env1_first_0:                 episode reward: 8.1000,                 loss: nan
env1_second_0:                 episode reward: -8.1000,                 loss: nan
Episode: 12661/30000 (42.2033%),                 avg. length: 2494.5,                last time consumption/overall running time: 265.4636s / 162077.0359 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.0813
env0_second_0:                 episode reward: -7.7500,                 loss: 0.3831
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 12681/30000 (42.2700%),                 avg. length: 2862.6,                last time consumption/overall running time: 300.6383s / 162377.6742 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.0777
env0_second_0:                 episode reward: -6.6000,                 loss: 0.3126
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 12701/30000 (42.3367%),                 avg. length: 2945.45,                last time consumption/overall running time: 311.8819s / 162689.5562 s
env0_first_0:                 episode reward: 5.0000,                 loss: -0.0772
env0_second_0:                 episode reward: -5.0000,                 loss: 0.3808
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 12721/30000 (42.4033%),                 avg. length: 2717.25,                last time consumption/overall running time: 296.5430s / 162986.0992 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.0597
env0_second_0:                 episode reward: -1.5000,                 loss: 0.5808
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 12741/30000 (42.4700%),                 avg. length: 2940.1,                last time consumption/overall running time: 301.5444s / 163287.6436 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.0651
env0_second_0:                 episode reward: 1.2500,                 loss: 0.6859
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 12761/30000 (42.5367%),                 avg. length: 3300.35,                last time consumption/overall running time: 364.5305s / 163652.1742 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.0716
env0_second_0:                 episode reward: -4.6000,                 loss: 0.3581
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 12781/30000 (42.6033%),                 avg. length: 3252.5,                last time consumption/overall running time: 350.9770s / 164003.1511 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.0704
env0_second_0:                 episode reward: -4.3500,                 loss: 0.3351
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 12801/30000 (42.6700%),                 avg. length: 2815.3,                last time consumption/overall running time: 303.7574s / 164306.9085 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.0762
env0_second_0:                 episode reward: -5.8000,                 loss: 0.3720
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 12821/30000 (42.7367%),                 avg. length: 2947.85,                last time consumption/overall running time: 310.2191s / 164617.1276 s
env0_first_0:                 episode reward: 6.5500,                 loss: -0.0600
env0_second_0:                 episode reward: -6.5500,                 loss: 0.5466
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 12841/30000 (42.8033%),                 avg. length: 2858.9,                last time consumption/overall running time: 307.1909s / 164924.3185 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.0524
env0_second_0:                 episode reward: -1.3500,                 loss: 0.5618