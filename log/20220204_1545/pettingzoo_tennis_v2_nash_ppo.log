pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220204_1545/pettingzoo_tennis_v2_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220204_1545/pettingzoo_tennis_v2_nash_ppo.
Episode: 1/30000 (0.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 50.6952s / 50.6952 s
env0_first_0:                 episode reward: -8.0000,                 loss: -0.0147
env0_second_0:                 episode reward: 8.0000,                 loss: -0.0101
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 7068.05,                last time consumption/overall running time: 709.5889s / 760.2841 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0414
env0_second_0:                 episode reward: -3.6500,                 loss: -0.0399
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 4803.15,                last time consumption/overall running time: 442.0271s / 1202.3111 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.0434
env0_second_0:                 episode reward: -2.8500,                 loss: -0.0289
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 3948.2,                last time consumption/overall running time: 372.8307s / 1575.1419 s
env0_first_0:                 episode reward: -3.4000,                 loss: -0.0689
env0_second_0:                 episode reward: 3.4000,                 loss: -0.0585
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 3621.95,                last time consumption/overall running time: 354.9613s / 1930.1032 s
env0_first_0:                 episode reward: -2.7500,                 loss: -0.1009
env0_second_0:                 episode reward: 2.7500,                 loss: -0.0881
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 5241.3,                last time consumption/overall running time: 508.3430s / 2438.4462 s
env0_first_0:                 episode reward: 9.1500,                 loss: -0.0993
env0_second_0:                 episode reward: -9.1500,                 loss: -0.0972
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 5577.15,                last time consumption/overall running time: 561.0975s / 2999.5437 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.1417
env0_second_0:                 episode reward: -5.1000,                 loss: -0.1228
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 6984.55,                last time consumption/overall running time: 687.8726s / 3687.4163 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.1913
env0_second_0:                 episode reward: -3.8500,                 loss: -0.1764
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 8537.3,                last time consumption/overall running time: 826.8851s / 4514.3014 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.1871
env0_second_0:                 episode reward: -4.9000,                 loss: -0.1648
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 9412.4,                last time consumption/overall running time: 915.0379s / 5429.3394 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2217
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2102
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 9578.0,                last time consumption/overall running time: 950.0032s / 6379.3425 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.2051
env0_second_0:                 episode reward: 5.4000,                 loss: -0.1880
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 9712.5,                last time consumption/overall running time: 975.9982s / 7355.3407 s
env0_first_0:                 episode reward: 19.5000,                 loss: -0.1939
env0_second_0:                 episode reward: -19.5000,                 loss: -0.1813
env1_first_0:                 episode reward: 17.1000,                 loss: nan
env1_second_0:                 episode reward: -17.1000,                 loss: nan
Episode: 241/30000 (0.8033%),                 avg. length: 9991.5,                last time consumption/overall running time: 996.9783s / 8352.3190 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.2332
env0_second_0:                 episode reward: -2.9500,                 loss: -0.2192
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 261/30000 (0.8700%),                 avg. length: 9189.25,                last time consumption/overall running time: 1049.3294s / 9401.6484 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.2251
env0_second_0:                 episode reward: -7.3000,                 loss: -0.2076
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 281/30000 (0.9367%),                 avg. length: 9698.95,                last time consumption/overall running time: 1099.5975s / 10501.2459 s
env0_first_0:                 episode reward: 8.3500,                 loss: -0.2372
env0_second_0:                 episode reward: -8.3500,                 loss: -0.2265
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 301/30000 (1.0033%),                 avg. length: 9597.7,                last time consumption/overall running time: 1079.9706s / 11581.2165 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.2302
env0_second_0:                 episode reward: 3.8500,                 loss: -0.2207
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 321/30000 (1.0700%),                 avg. length: 9992.05,                last time consumption/overall running time: 1044.8826s / 12626.0991 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.2410
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2311
env1_first_0:                 episode reward: -10.4000,                 loss: nan
env1_second_0:                 episode reward: 10.4000,                 loss: nan
Episode: 341/30000 (1.1367%),                 avg. length: 9998.25,                last time consumption/overall running time: 1187.6799s / 13813.7790 s
env0_first_0:                 episode reward: -5.2500,                 loss: -0.2231
env0_second_0:                 episode reward: 5.2500,                 loss: -0.2151
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 361/30000 (1.2033%),                 avg. length: 9479.2,                last time consumption/overall running time: 1129.4693s / 14943.2483 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.2023
env0_second_0:                 episode reward: -7.0000,                 loss: -0.1930
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 381/30000 (1.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1192.8364s / 16136.0847 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2181
env0_second_0:                 episode reward: -0.6500,                 loss: -0.2131
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 401/30000 (1.3367%),                 avg. length: 9742.3,                last time consumption/overall running time: 1169.0972s / 17305.1819 s
env0_first_0:                 episode reward: -17.1000,                 loss: -0.1482
env0_second_0:                 episode reward: 17.1000,                 loss: -0.1158
env1_first_0:                 episode reward: -25.6000,                 loss: nan
env1_second_0:                 episode reward: 25.6000,                 loss: nan
Episode: 421/30000 (1.4033%),                 avg. length: 9720.35,                last time consumption/overall running time: 1149.4440s / 18454.6259 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.2119
env0_second_0:                 episode reward: 6.7500,                 loss: -0.2056
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 441/30000 (1.4700%),                 avg. length: 9643.25,                last time consumption/overall running time: 1105.0028s / 19559.6287 s
env0_first_0:                 episode reward: 22.7000,                 loss: -0.1627
env0_second_0:                 episode reward: -22.7000,                 loss: -0.1481
env1_first_0:                 episode reward: 10.3500,                 loss: nan
env1_second_0:                 episode reward: -10.3500,                 loss: nan
Episode: 461/30000 (1.5367%),                 avg. length: 9865.5,                last time consumption/overall running time: 1128.1361s / 20687.7648 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.2279
env0_second_0:                 episode reward: -4.0500,                 loss: -0.2214
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 481/30000 (1.6033%),                 avg. length: 9978.65,                last time consumption/overall running time: 1090.4899s / 21778.2547 s
env0_first_0:                 episode reward: -13.9000,                 loss: -0.2097
env0_second_0:                 episode reward: 13.9000,                 loss: -0.1982
env1_first_0:                 episode reward: -25.4000,                 loss: nan
env1_second_0:                 episode reward: 25.4000,                 loss: nan
Episode: 501/30000 (1.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 992.7450s / 22770.9997 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.1680
env0_second_0:                 episode reward: -5.2500,                 loss: -0.1446
env1_first_0:                 episode reward: 14.5500,                 loss: nan
env1_second_0:                 episode reward: -14.5500,                 loss: nan
Episode: 521/30000 (1.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1033.0872s / 23804.0869 s
env0_first_0:                 episode reward: 16.7500,                 loss: -0.1644
env0_second_0:                 episode reward: -16.7500,                 loss: -0.1650
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 541/30000 (1.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1107.2825s / 24911.3694 s
env0_first_0:                 episode reward: -3.6000,                 loss: -0.2190
env0_second_0:                 episode reward: 3.6000,                 loss: -0.2093
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 561/30000 (1.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1183.9711s / 26095.3405 s
env0_first_0:                 episode reward: 8.6000,                 loss: -0.2278
env0_second_0:                 episode reward: -8.6000,                 loss: -0.2175
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 581/30000 (1.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1154.7694s / 27250.1100 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.1978
env0_second_0:                 episode reward: 2.6500,                 loss: -0.1906
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 601/30000 (2.0033%),                 avg. length: 9925.4,                last time consumption/overall running time: 1130.7861s / 28380.8961 s
env0_first_0:                 episode reward: 14.8000,                 loss: -0.2177
env0_second_0:                 episode reward: -14.8000,                 loss: -0.2079
env1_first_0:                 episode reward: 8.5000,                 loss: nan
env1_second_0:                 episode reward: -8.5000,                 loss: nan
Episode: 621/30000 (2.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1118.8024s / 29499.6984 s
env0_first_0:                 episode reward: 8.0000,                 loss: -0.2545
env0_second_0:                 episode reward: -8.0000,                 loss: -0.2457
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 641/30000 (2.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1165.4414s / 30665.1399 s
env0_first_0:                 episode reward: 8.9000,                 loss: -0.2331
env0_second_0:                 episode reward: -8.9000,                 loss: -0.2169
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 661/30000 (2.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1166.4520s / 31831.5918 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.2309
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2262
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 681/30000 (2.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1200.2904s / 33031.8823 s
env0_first_0:                 episode reward: 23.7500,                 loss: -0.1853
env0_second_0:                 episode reward: -23.7500,                 loss: -0.1817
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 701/30000 (2.3367%),                 avg. length: 9864.5,                last time consumption/overall running time: 1124.8249s / 34156.7072 s
env0_first_0:                 episode reward: 6.4500,                 loss: -0.2365
env0_second_0:                 episode reward: -6.4500,                 loss: -0.2302
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 721/30000 (2.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1107.4385s / 35264.1457 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.2133
env0_second_0:                 episode reward: -7.7500,                 loss: -0.2050
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 741/30000 (2.4700%),                 avg. length: 9843.6,                last time consumption/overall running time: 1138.7224s / 36402.8680 s
env0_first_0:                 episode reward: 10.6000,                 loss: -0.2034
env0_second_0:                 episode reward: -10.6000,                 loss: -0.1951
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 761/30000 (2.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1187.7087s / 37590.5768 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.2381
env0_second_0:                 episode reward: -4.1500,                 loss: -0.2259
env1_first_0:                 episode reward: -10.7500,                 loss: nan
env1_second_0:                 episode reward: 10.7500,                 loss: nan
Episode: 781/30000 (2.6033%),                 avg. length: 9879.4,                last time consumption/overall running time: 1164.8529s / 38755.4296 s
env0_first_0:                 episode reward: -18.8000,                 loss: -0.2281
env0_second_0:                 episode reward: 18.8000,                 loss: -0.2145
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 801/30000 (2.6700%),                 avg. length: 9843.15,                last time consumption/overall running time: 1145.0861s / 39900.5157 s
env0_first_0:                 episode reward: -13.3500,                 loss: -0.1768
env0_second_0:                 episode reward: 13.3500,                 loss: -0.1614
env1_first_0:                 episode reward: -29.3500,                 loss: nan
env1_second_0:                 episode reward: 29.3500,                 loss: nan
Episode: 821/30000 (2.7367%),                 avg. length: 9785.35,                last time consumption/overall running time: 1153.6222s / 41054.1379 s
env0_first_0:                 episode reward: -23.6500,                 loss: -0.2044
env0_second_0:                 episode reward: 23.6500,                 loss: -0.1887
env1_first_0:                 episode reward: -20.9000,                 loss: nan
env1_second_0:                 episode reward: 20.9000,                 loss: nan
Episode: 841/30000 (2.8033%),                 avg. length: 9699.8,                last time consumption/overall running time: 1148.4483s / 42202.5861 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.2206
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2053
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 861/30000 (2.8700%),                 avg. length: 9965.2,                last time consumption/overall running time: 1105.2799s / 43307.8660 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.2232
env0_second_0:                 episode reward: 7.5500,                 loss: -0.2112
env1_first_0:                 episode reward: -14.5000,                 loss: nan
env1_second_0:                 episode reward: 14.5000,                 loss: nan
Episode: 881/30000 (2.9367%),                 avg. length: 9918.8,                last time consumption/overall running time: 1063.3327s / 44371.1987 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.2100
env0_second_0:                 episode reward: 3.3000,                 loss: -0.1906
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 901/30000 (3.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1054.0268s / 45425.2255 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.2421
env0_second_0:                 episode reward: 8.7500,                 loss: -0.2253
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 921/30000 (3.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1053.0907s / 46478.3162 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.2397
env0_second_0:                 episode reward: -4.4000,                 loss: -0.2173
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 941/30000 (3.1367%),                 avg. length: 9814.9,                last time consumption/overall running time: 1159.4273s / 47637.7435 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2087
env0_second_0:                 episode reward: -1.7500,                 loss: -0.1981
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 961/30000 (3.2033%),                 avg. length: 9800.8,                last time consumption/overall running time: 1175.5117s / 48813.2553 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1818
env0_second_0:                 episode reward: -2.2500,                 loss: -0.1708
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 981/30000 (3.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1223.3230s / 50036.5783 s
env0_first_0:                 episode reward: 6.2500,                 loss: -0.2164
env0_second_0:                 episode reward: -6.2500,                 loss: -0.2060
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 1001/30000 (3.3367%),                 avg. length: 9990.35,                last time consumption/overall running time: 1209.9587s / 51246.5370 s
env0_first_0:                 episode reward: -12.7000,                 loss: -0.2119
env0_second_0:                 episode reward: 12.7000,                 loss: -0.2045
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 1021/30000 (3.4033%),                 avg. length: 9899.45,                last time consumption/overall running time: 1200.8395s / 52447.3765 s
env0_first_0:                 episode reward: -6.5000,                 loss: -0.2337
env0_second_0:                 episode reward: 6.5000,                 loss: -0.2226
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 1041/30000 (3.4700%),                 avg. length: 9962.2,                last time consumption/overall running time: 1250.6632s / 53698.0397 s
env0_first_0:                 episode reward: -18.0000,                 loss: -0.1703
env0_second_0:                 episode reward: 18.0000,                 loss: -0.1538
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 1061/30000 (3.5367%),                 avg. length: 9602.85,                last time consumption/overall running time: 1188.0668s / 54886.1064 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.2396
env0_second_0:                 episode reward: 2.5000,                 loss: -0.2319
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1081/30000 (3.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1254.1320s / 56140.2384 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2281
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2210
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 1101/30000 (3.6700%),                 avg. length: 9858.3,                last time consumption/overall running time: 1246.9509s / 57387.1893 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.2069
env0_second_0:                 episode reward: -3.8000,                 loss: -0.1932
env1_first_0:                 episode reward: -20.5500,                 loss: nan
env1_second_0:                 episode reward: 20.5500,                 loss: nan
Episode: 1121/30000 (3.7367%),                 avg. length: 9973.6,                last time consumption/overall running time: 1256.0898s / 58643.2791 s
env0_first_0:                 episode reward: -3.6000,                 loss: -0.2409
env0_second_0:                 episode reward: 3.6000,                 loss: -0.2307
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 1141/30000 (3.8033%),                 avg. length: 9890.85,                last time consumption/overall running time: 1232.4963s / 59875.7755 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2231
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2106
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 1161/30000 (3.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1214.9391s / 61090.7145 s
env0_first_0:                 episode reward: -3.2500,                 loss: -0.2254
env0_second_0:                 episode reward: 3.2500,                 loss: -0.2170
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 1181/30000 (3.9367%),                 avg. length: 9546.15,                last time consumption/overall running time: 1184.5633s / 62275.2778 s
env0_first_0:                 episode reward: -11.2500,                 loss: -0.2000
env0_second_0:                 episode reward: 11.2500,                 loss: -0.1898
env1_first_0:                 episode reward: -19.5500,                 loss: nan
env1_second_0:                 episode reward: 19.5500,                 loss: nan
Episode: 1201/30000 (4.0033%),                 avg. length: 9940.9,                last time consumption/overall running time: 1200.7389s / 63476.0167 s
env0_first_0:                 episode reward: -5.3000,                 loss: -0.2549
env0_second_0:                 episode reward: 5.3000,                 loss: -0.2435
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 1221/30000 (4.0700%),                 avg. length: 9986.3,                last time consumption/overall running time: 1224.8913s / 64700.9080 s
env0_first_0:                 episode reward: -7.1500,                 loss: -0.2591
env0_second_0:                 episode reward: 7.1500,                 loss: -0.2476
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 1241/30000 (4.1367%),                 avg. length: 9975.4,                last time consumption/overall running time: 1248.0092s / 65948.9172 s
env0_first_0:                 episode reward: -10.4500,                 loss: -0.2414
env0_second_0:                 episode reward: 10.4500,                 loss: -0.2314
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 1261/30000 (4.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1237.9014s / 67186.8186 s
env0_first_0:                 episode reward: -8.5500,                 loss: -0.2808
env0_second_0:                 episode reward: 8.5500,                 loss: -0.2710
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 1281/30000 (4.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1218.2842s / 68405.1028 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2752
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2656
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 1301/30000 (4.3367%),                 avg. length: 9901.0,                last time consumption/overall running time: 1205.6882s / 69610.7910 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2511
env0_second_0:                 episode reward: -1.0500,                 loss: -0.2354
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 1321/30000 (4.4033%),                 avg. length: 9848.0,                last time consumption/overall running time: 1215.1922s / 70825.9832 s
env0_first_0:                 episode reward: 15.1500,                 loss: -0.2415
env0_second_0:                 episode reward: -15.1500,                 loss: -0.2293
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 1341/30000 (4.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1252.0149s / 72077.9981 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.2799
env0_second_0:                 episode reward: 0.6000,                 loss: -0.2653
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 1361/30000 (4.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1252.1970s / 73330.1951 s
env0_first_0:                 episode reward: 8.1500,                 loss: -0.2770
env0_second_0:                 episode reward: -8.1500,                 loss: -0.2615
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 1381/30000 (4.6033%),                 avg. length: 9951.25,                last time consumption/overall running time: 1237.9864s / 74568.1815 s
env0_first_0:                 episode reward: -7.2000,                 loss: -0.2578
env0_second_0:                 episode reward: 7.2000,                 loss: -0.2443
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1401/30000 (4.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1214.5068s / 75782.6883 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.2454
env0_second_0:                 episode reward: -4.7500,                 loss: -0.2300
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 1421/30000 (4.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1254.5478s / 77037.2361 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.2595
env0_second_0:                 episode reward: -3.6500,                 loss: -0.2433
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 1441/30000 (4.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1262.8686s / 78300.1047 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.2513
env0_second_0:                 episode reward: -6.6500,                 loss: -0.2348
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 1461/30000 (4.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1252.8612s / 79552.9658 s
env0_first_0:                 episode reward: 13.9000,                 loss: -0.2264
env0_second_0:                 episode reward: -13.9000,                 loss: -0.2047
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 1481/30000 (4.9367%),                 avg. length: 9936.0,                last time consumption/overall running time: 1237.7004s / 80790.6662 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.2367
env0_second_0:                 episode reward: -6.6500,                 loss: -0.2220
env1_first_0:                 episode reward: 10.9000,                 loss: nan
env1_second_0:                 episode reward: -10.9000,                 loss: nan
Episode: 1501/30000 (5.0033%),                 avg. length: 9922.7,                last time consumption/overall running time: 1196.9077s / 81987.5739 s
env0_first_0:                 episode reward: 7.2500,                 loss: -0.2370
env0_second_0:                 episode reward: -7.2500,                 loss: -0.2203
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 1521/30000 (5.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1235.4201s / 83222.9940 s
env0_first_0:                 episode reward: 6.5500,                 loss: -0.2321
env0_second_0:                 episode reward: -6.5500,                 loss: -0.2133
env1_first_0:                 episode reward: 18.1500,                 loss: nan
env1_second_0:                 episode reward: -18.1500,                 loss: nan
Episode: 1541/30000 (5.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1259.8791s / 84482.8731 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.1937
env0_second_0:                 episode reward: -2.3000,                 loss: -0.1757
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 1561/30000 (5.2033%),                 avg. length: 9891.25,                last time consumption/overall running time: 1201.3576s / 85684.2307 s
env0_first_0:                 episode reward: 11.7500,                 loss: -0.2073
env0_second_0:                 episode reward: -11.7500,                 loss: -0.1916
env1_first_0:                 episode reward: 12.9500,                 loss: nan
env1_second_0:                 episode reward: -12.9500,                 loss: nan
Episode: 1581/30000 (5.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1233.1406s / 86917.3712 s
env0_first_0:                 episode reward: 7.6500,                 loss: -0.2407
env0_second_0:                 episode reward: -7.6500,                 loss: -0.2244
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 1601/30000 (5.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1238.0197s / 88155.3909 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.2287
env0_second_0:                 episode reward: -10.0000,                 loss: -0.2170
env1_first_0:                 episode reward: 11.5000,                 loss: nan
env1_second_0:                 episode reward: -11.5000,                 loss: nan
Episode: 1621/30000 (5.4033%),                 avg. length: 9965.8,                last time consumption/overall running time: 1230.0212s / 89385.4121 s
env0_first_0:                 episode reward: 14.0500,                 loss: -0.2261
env0_second_0:                 episode reward: -14.0500,                 loss: -0.2130
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 1641/30000 (5.4700%),                 avg. length: 9750.7,                last time consumption/overall running time: 1233.2730s / 90618.6851 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.2263
env0_second_0:                 episode reward: -5.7000,                 loss: -0.2182
env1_first_0:                 episode reward: 12.6000,                 loss: nan
env1_second_0:                 episode reward: -12.6000,                 loss: nan
Episode: 1661/30000 (5.5367%),                 avg. length: 9993.5,                last time consumption/overall running time: 1232.8377s / 91851.5229 s
env0_first_0:                 episode reward: 8.7000,                 loss: -0.2477
env0_second_0:                 episode reward: -8.7000,                 loss: -0.2293
env1_first_0:                 episode reward: 11.2500,                 loss: nan
env1_second_0:                 episode reward: -11.2500,                 loss: nan
Episode: 1681/30000 (5.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1252.1746s / 93103.6975 s
env0_first_0:                 episode reward: 11.6500,                 loss: -0.2596
env0_second_0:                 episode reward: -11.6500,                 loss: -0.2438
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 1701/30000 (5.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1236.5532s / 94340.2507 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.2262
env0_second_0:                 episode reward: -5.8500,                 loss: -0.2131
env1_first_0:                 episode reward: 10.2500,                 loss: nan
env1_second_0:                 episode reward: -10.2500,                 loss: nan
Episode: 1721/30000 (5.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1249.4639s / 95589.7146 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.2291
env0_second_0:                 episode reward: -0.7500,                 loss: -0.2169
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 1741/30000 (5.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1243.9071s / 96833.6217 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.2912
env0_second_0:                 episode reward: -5.3000,                 loss: -0.2794
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 1761/30000 (5.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1296.2737s / 98129.8955 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.2465
env0_second_0:                 episode reward: -5.9000,                 loss: -0.2254
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 1781/30000 (5.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1280.5842s / 99410.4796 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.2936
env0_second_0:                 episode reward: -4.2000,                 loss: -0.2847
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 1801/30000 (6.0033%),                 avg. length: 9685.05,                last time consumption/overall running time: 1267.6881s / 100678.1678 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.3033
env0_second_0:                 episode reward: -3.7500,                 loss: -0.2915
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 1821/30000 (6.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1315.4338s / 101993.6015 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.3106
env0_second_0:                 episode reward: -4.3000,                 loss: -0.2964
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 1841/30000 (6.1367%),                 avg. length: 9994.75,                last time consumption/overall running time: 1241.4387s / 103235.0402 s
env0_first_0:                 episode reward: 8.8500,                 loss: -0.2824
env0_second_0:                 episode reward: -8.8500,                 loss: -0.2750
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 1861/30000 (6.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1196.9412s / 104431.9814 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.2759
env0_second_0:                 episode reward: -4.4000,                 loss: -0.2636
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 1881/30000 (6.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1227.8979s / 105659.8793 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.2890
env0_second_0:                 episode reward: -3.8000,                 loss: -0.2759
env1_first_0:                 episode reward: 10.1500,                 loss: nan
env1_second_0:                 episode reward: -10.1500,                 loss: nan
Episode: 1901/30000 (6.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1243.8467s / 106903.7261 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.2691
env0_second_0:                 episode reward: -5.4000,                 loss: -0.2549
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 1921/30000 (6.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1258.2760s / 108162.0021 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3147
env0_second_0:                 episode reward: -1.1500,                 loss: -0.2967
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 1941/30000 (6.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1238.2019s / 109400.2039 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.2908
env0_second_0:                 episode reward: -3.3500,                 loss: -0.2733
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1961/30000 (6.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1238.8508s / 110639.0548 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.3162
env0_second_0:                 episode reward: 0.2500,                 loss: -0.3006
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1981/30000 (6.6033%),                 avg. length: 9975.75,                last time consumption/overall running time: 1249.1988s / 111888.2535 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.2849
env0_second_0:                 episode reward: -3.2500,                 loss: -0.2633
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 2001/30000 (6.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1209.4351s / 113097.6886 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.3046
env0_second_0:                 episode reward: -3.8000,                 loss: -0.2852
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 2021/30000 (6.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1235.4299s / 114333.1186 s
env0_first_0:                 episode reward: 9.1000,                 loss: -0.2865
env0_second_0:                 episode reward: -9.1000,                 loss: -0.2731
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 2041/30000 (6.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1253.6498s / 115586.7684 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.3093
env0_second_0:                 episode reward: -4.0000,                 loss: -0.2918
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 2061/30000 (6.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1232.3432s / 116819.1115 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.3074
env0_second_0:                 episode reward: -4.3000,                 loss: -0.2875
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 2081/30000 (6.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1244.4261s / 118063.5376 s
env0_first_0:                 episode reward: 7.7000,                 loss: -0.3128
env0_second_0:                 episode reward: -7.7000,                 loss: -0.2984
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 2101/30000 (7.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1266.3696s / 119329.9072 s
env0_first_0:                 episode reward: 6.1000,                 loss: -0.3131
env0_second_0:                 episode reward: -6.1000,                 loss: -0.2955
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 2121/30000 (7.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1266.6419s / 120596.5491 s
env0_first_0:                 episode reward: 6.1500,                 loss: -0.2956
env0_second_0:                 episode reward: -6.1500,                 loss: -0.2745
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 2141/30000 (7.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1282.6727s / 121879.2218 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3068
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2919
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 2161/30000 (7.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1278.4794s / 123157.7012 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3052
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2919
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 2181/30000 (7.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1282.2454s / 124439.9466 s
env0_first_0:                 episode reward: -12.3000,                 loss: -0.2771
env0_second_0:                 episode reward: 12.3000,                 loss: -0.2560
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 2201/30000 (7.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1274.7873s / 125714.7339 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2901
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2734
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 2221/30000 (7.4033%),                 avg. length: 9989.7,                last time consumption/overall running time: 1265.1716s / 126979.9055 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.2755
env0_second_0:                 episode reward: 3.2000,                 loss: -0.2582
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2241/30000 (7.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1266.4146s / 128246.3201 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.2771
env0_second_0:                 episode reward: 0.9500,                 loss: -0.2600
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 2261/30000 (7.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1240.7818s / 129487.1018 s
env0_first_0:                 episode reward: -2.7500,                 loss: -0.2843
env0_second_0:                 episode reward: 2.7500,                 loss: -0.2659
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 2281/30000 (7.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1024.6608s / 130511.7627 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3050
env0_second_0:                 episode reward: -1.7000,                 loss: -0.2918
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 2301/30000 (7.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1058.7506s / 131570.5133 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2987
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2795
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2321/30000 (7.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 999.8943s / 132570.4076 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.3029
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2825
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 2341/30000 (7.8033%),                 avg. length: 9998.05,                last time consumption/overall running time: 1025.1849s / 133595.5925 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2927
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2723
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 2361/30000 (7.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1013.6455s / 134609.2380 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.3069
env0_second_0:                 episode reward: 0.6500,                 loss: -0.2870
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 2381/30000 (7.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1027.2348s / 135636.4728 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3134
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2912
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2401/30000 (8.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.8972s / 136659.3700 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.3005
env0_second_0:                 episode reward: 6.8000,                 loss: -0.2759
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 2421/30000 (8.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1016.8002s / 137676.1702 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3164
env0_second_0:                 episode reward: -2.2500,                 loss: -0.2927
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 2441/30000 (8.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1028.9923s / 138705.1625 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.3084
env0_second_0:                 episode reward: 1.1000,                 loss: -0.2855
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 2461/30000 (8.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1024.0687s / 139729.2312 s
env0_first_0:                 episode reward: -8.3500,                 loss: -0.2964
env0_second_0:                 episode reward: 8.3500,                 loss: -0.2637
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 2481/30000 (8.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 999.8995s / 140729.1307 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3150
env0_second_0:                 episode reward: -1.8000,                 loss: -0.2890
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 2501/30000 (8.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1014.4140s / 141743.5447 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.3199
env0_second_0:                 episode reward: -4.4500,                 loss: -0.2988
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 2521/30000 (8.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1009.6620s / 142753.2067 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3243
env0_second_0:                 episode reward: -2.0000,                 loss: -0.2955
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 2541/30000 (8.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1025.3609s / 143778.5676 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.3180
env0_second_0:                 episode reward: -6.4000,                 loss: -0.2751
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 2561/30000 (8.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1048.4429s / 144827.0105 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3229
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2975
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2581/30000 (8.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1004.4152s / 145831.4257 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3284
env0_second_0:                 episode reward: -2.0000,                 loss: -0.3002
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 2601/30000 (8.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1067.9387s / 146899.3644 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.3262
env0_second_0:                 episode reward: 0.3500,                 loss: -0.2994
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 2621/30000 (8.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1037.8847s / 147937.2490 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2919
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2526
env1_first_0:                 episode reward: -14.6000,                 loss: nan
env1_second_0:                 episode reward: 14.6000,                 loss: nan
Episode: 2641/30000 (8.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1062.9003s / 149000.1493 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.3197
env0_second_0:                 episode reward: -2.5500,                 loss: -0.2862
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2661/30000 (8.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1073.8172s / 150073.9666 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.2863
env0_second_0:                 episode reward: -1.4500,                 loss: -0.2161
env1_first_0:                 episode reward: -10.0500,                 loss: nan
env1_second_0:                 episode reward: 10.0500,                 loss: nan
Episode: 2681/30000 (8.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1077.9289s / 151151.8955 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3201
env0_second_0:                 episode reward: -1.0500,                 loss: -0.2807
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 2701/30000 (9.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1077.3771s / 152229.2727 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.3364
env0_second_0:                 episode reward: -4.4500,                 loss: -0.2955
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 2721/30000 (9.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1084.7054s / 153313.9780 s
env0_first_0:                 episode reward: -5.0500,                 loss: -0.3088
env0_second_0:                 episode reward: 5.0500,                 loss: -0.2660
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 2741/30000 (9.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1078.7969s / 154392.7749 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3440
env0_second_0:                 episode reward: 0.8500,                 loss: -0.3195
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 2761/30000 (9.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1084.1018s / 155476.8767 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.3279
env0_second_0:                 episode reward: -3.6000,                 loss: -0.2937
env1_first_0:                 episode reward: -10.5000,                 loss: nan
env1_second_0:                 episode reward: 10.5000,                 loss: nan
Episode: 2781/30000 (9.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1123.8894s / 156600.7662 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.3378
env0_second_0:                 episode reward: -4.0000,                 loss: -0.3056
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 2801/30000 (9.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1111.5092s / 157712.2754 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3418
env0_second_0:                 episode reward: -1.5000,                 loss: -0.3147
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 2821/30000 (9.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1082.4775s / 158794.7528 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.3356
env0_second_0:                 episode reward: -3.5500,                 loss: -0.3088
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 2841/30000 (9.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1138.4820s / 159933.2349 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3304
env0_second_0:                 episode reward: -1.9000,                 loss: -0.3010
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 2861/30000 (9.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1126.7232s / 161059.9580 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.3313
env0_second_0:                 episode reward: -3.7500,                 loss: -0.2943
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2881/30000 (9.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1106.9721s / 162166.9302 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.3055
env0_second_0:                 episode reward: 6.8000,                 loss: -0.2585
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 2901/30000 (9.6700%),                 avg. length: 9955.35,                last time consumption/overall running time: 1084.9809s / 163251.9111 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.3272
env0_second_0:                 episode reward: -4.7000,                 loss: -0.2925
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 2921/30000 (9.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1119.3047s / 164371.2158 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.3382
env0_second_0:                 episode reward: -3.5000,                 loss: -0.3017
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 2941/30000 (9.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1145.0442s / 165516.2599 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3194
env0_second_0:                 episode reward: -1.8500,                 loss: -0.2834
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 2961/30000 (9.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1122.3187s / 166638.5786 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3310
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2884
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 2981/30000 (9.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1101.9045s / 167740.4831 s
env0_first_0:                 episode reward: -2.7500,                 loss: -0.3257
env0_second_0:                 episode reward: 2.7500,                 loss: -0.2881
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 3001/30000 (10.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1151.1418s / 168891.6248 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.3263
env0_second_0:                 episode reward: -4.5000,                 loss: -0.2918
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 3021/30000 (10.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1166.3237s / 170057.9485 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3411
env0_second_0:                 episode reward: 0.1000,                 loss: -0.3070
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 3041/30000 (10.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1148.4561s / 171206.4047 s
env0_first_0:                 episode reward: -27.9000,                 loss: -0.2370
env0_second_0:                 episode reward: 27.9000,                 loss: -0.1813
env1_first_0:                 episode reward: -31.9000,                 loss: nan
env1_second_0:                 episode reward: 31.9000,                 loss: nan
Episode: 3061/30000 (10.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1155.4859s / 172361.8906 s
env0_first_0:                 episode reward: -19.5500,                 loss: -0.2969
env0_second_0:                 episode reward: 19.5500,                 loss: -0.2484
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
Episode: 3081/30000 (10.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1136.1495s / 173498.0401 s
env0_first_0:                 episode reward: -36.7000,                 loss: -0.2991
env0_second_0:                 episode reward: 36.7000,                 loss: -0.2599
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 3101/30000 (10.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1126.5483s / 174624.5884 s
env0_first_0:                 episode reward: -13.8000,                 loss: -0.2635
env0_second_0:                 episode reward: 13.8000,                 loss: -0.2102
env1_first_0:                 episode reward: -35.4500,                 loss: nan
env1_second_0:                 episode reward: 35.4500,                 loss: nan
Episode: 3121/30000 (10.4033%),                 avg. length: 9936.55,                last time consumption/overall running time: 1127.7196s / 175752.3079 s
env0_first_0:                 episode reward: -12.4500,                 loss: -0.2906
env0_second_0:                 episode reward: 12.4500,                 loss: -0.2426
env1_first_0:                 episode reward: -19.4000,                 loss: nan
env1_second_0:                 episode reward: 19.4000,                 loss: nan
Episode: 3141/30000 (10.4700%),                 avg. length: 9949.1,                last time consumption/overall running time: 1132.1458s / 176884.4537 s
env0_first_0:                 episode reward: -29.3000,                 loss: -0.2605
env0_second_0:                 episode reward: 29.3000,                 loss: -0.2051
env1_first_0:                 episode reward: -13.7500,                 loss: nan
env1_second_0:                 episode reward: 13.7500,                 loss: nan
Episode: 3161/30000 (10.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1116.6502s / 178001.1039 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3278
env0_second_0:                 episode reward: 0.6000,                 loss: -0.2925
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3181/30000 (10.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1110.9368s / 179112.0408 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.3385
env0_second_0:                 episode reward: -0.4000,                 loss: -0.3026
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3201/30000 (10.6700%),                 avg. length: 9877.35,                last time consumption/overall running time: 1105.6796s / 180217.7203 s
env0_first_0:                 episode reward: -7.7500,                 loss: -0.3161
env0_second_0:                 episode reward: 7.7500,                 loss: -0.2807
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3221/30000 (10.7367%),                 avg. length: 9130.1,                last time consumption/overall running time: 1017.2860s / 181235.0063 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.2895
env0_second_0:                 episode reward: -7.3000,                 loss: -0.2227
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 3241/30000 (10.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1131.1903s / 182366.1966 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.3006
env0_second_0:                 episode reward: -2.1000,                 loss: -0.2649
env1_first_0:                 episode reward: -17.9500,                 loss: nan
env1_second_0:                 episode reward: 17.9500,                 loss: nan
Episode: 3261/30000 (10.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1161.8434s / 183528.0400 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3175
env0_second_0:                 episode reward: -2.7500,                 loss: -0.2719
env1_first_0:                 episode reward: -17.0000,                 loss: nan
env1_second_0:                 episode reward: 17.0000,                 loss: nan
Episode: 3281/30000 (10.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1132.8135s / 184660.8534 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.3332
env0_second_0:                 episode reward: 5.7000,                 loss: -0.2931
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 3301/30000 (11.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1151.0730s / 185811.9264 s
env0_first_0:                 episode reward: -18.7000,                 loss: -0.3096
env0_second_0:                 episode reward: 18.7000,                 loss: -0.2460
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 3321/30000 (11.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1142.7585s / 186954.6849 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3525
env0_second_0:                 episode reward: -0.6000,                 loss: -0.3173
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 3341/30000 (11.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1121.5905s / 188076.2754 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.3397
env0_second_0:                 episode reward: -0.6500,                 loss: -0.2833
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3361/30000 (11.2033%),                 avg. length: 9925.4,                last time consumption/overall running time: 1123.0952s / 189199.3706 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3283
env0_second_0:                 episode reward: -1.5500,                 loss: -0.2782
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 3381/30000 (11.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1129.1762s / 190328.5468 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.3417
env0_second_0:                 episode reward: -3.0000,                 loss: -0.2997
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 3401/30000 (11.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1150.3376s / 191478.8844 s
env0_first_0:                 episode reward: -10.6500,                 loss: -0.3078
env0_second_0:                 episode reward: 10.6500,                 loss: -0.2574
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 3421/30000 (11.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1130.3412s / 192609.2256 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.3204
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2787
env1_first_0:                 episode reward: -11.3000,                 loss: nan
env1_second_0:                 episode reward: 11.3000,                 loss: nan
Episode: 3441/30000 (11.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1124.4287s / 193733.6543 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.3228
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2802
env1_first_0:                 episode reward: -11.5000,                 loss: nan
env1_second_0:                 episode reward: 11.5000,                 loss: nan
Episode: 3461/30000 (11.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1119.4770s / 194853.1313 s
env0_first_0:                 episode reward: -11.3000,                 loss: -0.3165
env0_second_0:                 episode reward: 11.3000,                 loss: -0.2765
env1_first_0:                 episode reward: -9.4500,                 loss: nan
env1_second_0:                 episode reward: 9.4500,                 loss: nan
Episode: 3481/30000 (11.6033%),                 avg. length: 9950.55,                last time consumption/overall running time: 1113.9512s / 195967.0825 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.3135
env0_second_0:                 episode reward: 1.3000,                 loss: -0.2761
env1_first_0:                 episode reward: -21.4500,                 loss: nan
env1_second_0:                 episode reward: 21.4500,                 loss: nan
Episode: 3501/30000 (11.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1136.4712s / 197103.5536 s
env0_first_0:                 episode reward: -18.2000,                 loss: -0.3034
env0_second_0:                 episode reward: 18.2000,                 loss: -0.2534
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3521/30000 (11.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1140.3296s / 198243.8832 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.3245
env0_second_0:                 episode reward: 9.2000,                 loss: -0.2683
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 3541/30000 (11.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1120.9390s / 199364.8222 s
env0_first_0:                 episode reward: -20.5500,                 loss: -0.2979
env0_second_0:                 episode reward: 20.5500,                 loss: -0.2398
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 3561/30000 (11.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1131.7713s / 200496.5936 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.3167
env0_second_0:                 episode reward: -2.2000,                 loss: -0.2720
env1_first_0:                 episode reward: -15.1500,                 loss: nan
env1_second_0:                 episode reward: 15.1500,                 loss: nan
Episode: 3581/30000 (11.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1106.1808s / 201602.7743 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3467
env0_second_0:                 episode reward: -0.5500,                 loss: -0.3049
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 3601/30000 (12.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1140.8318s / 202743.6062 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3534
env0_second_0:                 episode reward: -1.8000,                 loss: -0.3134
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 3621/30000 (12.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1120.7160s / 203864.3221 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2934
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2239
env1_first_0:                 episode reward: -15.7000,                 loss: nan
env1_second_0:                 episode reward: 15.7000,                 loss: nan
Episode: 3641/30000 (12.1367%),                 avg. length: 9950.9,                last time consumption/overall running time: 1113.1306s / 204977.4527 s
env0_first_0:                 episode reward: 6.7500,                 loss: -0.3108
env0_second_0:                 episode reward: -6.7500,                 loss: -0.2650
env1_first_0:                 episode reward: -14.4000,                 loss: nan
env1_second_0:                 episode reward: 14.4000,                 loss: nan
Episode: 3661/30000 (12.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1131.3073s / 206108.7600 s
env0_first_0:                 episode reward: -6.3500,                 loss: -0.3363
env0_second_0:                 episode reward: 6.3500,                 loss: -0.2962
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 3681/30000 (12.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1126.3846s / 207235.1445 s
env0_first_0:                 episode reward: -15.3500,                 loss: -0.3185
env0_second_0:                 episode reward: 15.3500,                 loss: -0.2600
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 3701/30000 (12.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1115.6735s / 208350.8180 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3485
env0_second_0:                 episode reward: -2.7500,                 loss: -0.3068
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 3721/30000 (12.4033%),                 avg. length: 9950.6,                last time consumption/overall running time: 1127.1327s / 209477.9507 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.3489
env0_second_0:                 episode reward: -2.1500,                 loss: -0.3062
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 3741/30000 (12.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1119.5098s / 210597.4605 s
env0_first_0:                 episode reward: -8.0000,                 loss: -0.3311
env0_second_0:                 episode reward: 8.0000,                 loss: -0.2769
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 3761/30000 (12.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1122.1780s / 211719.6385 s
env0_first_0:                 episode reward: -10.9000,                 loss: -0.3330
env0_second_0:                 episode reward: 10.9000,                 loss: -0.2867
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 3781/30000 (12.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1099.1486s / 212818.7871 s
env0_first_0:                 episode reward: -25.3000,                 loss: -0.2838
env0_second_0:                 episode reward: 25.3000,                 loss: -0.2105
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 3801/30000 (12.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1110.4783s / 213929.2654 s
env0_first_0:                 episode reward: -30.5500,                 loss: -0.3084
env0_second_0:                 episode reward: 30.5500,                 loss: -0.2619
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 3821/30000 (12.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1102.8653s / 215032.1307 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3179
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2653
env1_first_0:                 episode reward: 9.3500,                 loss: nan
env1_second_0:                 episode reward: -9.3500,                 loss: nan
Episode: 3841/30000 (12.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1124.6069s / 216156.7376 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.3223
env0_second_0:                 episode reward: -3.8500,                 loss: -0.2665
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 3861/30000 (12.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1105.3143s / 217262.0519 s
env0_first_0:                 episode reward: -10.0500,                 loss: -0.3391
env0_second_0:                 episode reward: 10.0500,                 loss: -0.3013
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 3881/30000 (12.9367%),                 avg. length: 9884.75,                last time consumption/overall running time: 1095.7625s / 218357.8144 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3497
env0_second_0:                 episode reward: -0.2500,                 loss: -0.3098
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3901/30000 (13.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1127.1931s / 219485.0075 s
env0_first_0:                 episode reward: -4.3000,                 loss: -0.3481
env0_second_0:                 episode reward: 4.3000,                 loss: -0.3069
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 3921/30000 (13.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1122.3883s / 220607.3958 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.3487
env0_second_0:                 episode reward: 0.8000,                 loss: -0.2994
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 3941/30000 (13.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1126.5096s / 221733.9053 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3415
env0_second_0:                 episode reward: -1.7000,                 loss: -0.2924
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 3961/30000 (13.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1156.9291s / 222890.8344 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3237
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2653
env1_first_0:                 episode reward: -19.0500,                 loss: nan
env1_second_0:                 episode reward: 19.0500,                 loss: nan
Episode: 3981/30000 (13.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1130.9601s / 224021.7946 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.3519
env0_second_0:                 episode reward: -3.5000,                 loss: -0.2936
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 4001/30000 (13.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1115.7820s / 225137.5765 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3559
env0_second_0:                 episode reward: -1.8000,                 loss: -0.2993
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 4021/30000 (13.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1130.3208s / 226267.8973 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.3421
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2925
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 4041/30000 (13.4700%),                 avg. length: 9939.1,                last time consumption/overall running time: 1134.3772s / 227402.2746 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.3496
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2992
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 4061/30000 (13.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1133.7573s / 228536.0318 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3620
env0_second_0:                 episode reward: -1.1000,                 loss: -0.3059
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4081/30000 (13.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1096.0866s / 229632.1184 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3495
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2988
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 4101/30000 (13.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1118.0347s / 230750.1531 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3478
env0_second_0:                 episode reward: -1.0000,                 loss: -0.2944
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 4121/30000 (13.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1124.4014s / 231874.5545 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3468
env0_second_0:                 episode reward: -1.5500,                 loss: -0.2965
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 4141/30000 (13.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1111.5486s / 232986.1031 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3456
env0_second_0:                 episode reward: 0.0000,                 loss: -0.3001
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 4161/30000 (13.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1121.0944s / 234107.1975 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.3471
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2934
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 4181/30000 (13.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1117.1986s / 235224.3961 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3497
env0_second_0:                 episode reward: -1.4500,                 loss: -0.2925
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4201/30000 (14.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1100.8439s / 236325.2401 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3441
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2881
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4221/30000 (14.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1112.3104s / 237437.5504 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3484
env0_second_0:                 episode reward: -1.5500,                 loss: -0.2950
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 4241/30000 (14.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1124.7806s / 238562.3310 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.3559
env0_second_0:                 episode reward: -1.3000,                 loss: -0.2986
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 4261/30000 (14.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1123.9135s / 239686.2445 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3500
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2833
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 4281/30000 (14.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1115.1526s / 240801.3970 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.3528
env0_second_0:                 episode reward: -2.0500,                 loss: -0.2954
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 4301/30000 (14.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1123.9032s / 241925.3002 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3435
env0_second_0:                 episode reward: -0.9500,                 loss: -0.2867
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 4321/30000 (14.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1134.4029s / 243059.7031 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3396
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2856
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 4341/30000 (14.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1126.2971s / 244186.0002 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3464
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2909
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 4361/30000 (14.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1126.3793s / 245312.3795 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.3487
env0_second_0:                 episode reward: -0.8000,                 loss: -0.2836
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 4381/30000 (14.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1092.5563s / 246404.9358 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3483
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2941
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 4401/30000 (14.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1122.9160s / 247527.8518 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3446
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2795
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 4421/30000 (14.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1099.7415s / 248627.5933 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3471
env0_second_0:                 episode reward: -0.6000,                 loss: -0.2713
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 4441/30000 (14.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1105.1930s / 249732.7863 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.3378
env0_second_0:                 episode reward: 0.6500,                 loss: -0.2631
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 4461/30000 (14.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1088.4542s / 250821.2405 s
env0_first_0:                 episode reward: 9.1500,                 loss: -0.3337
env0_second_0:                 episode reward: -9.1500,                 loss: -0.2692
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 4481/30000 (14.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1103.4619s / 251924.7024 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3415
env0_second_0:                 episode reward: -0.6000,                 loss: -0.2588
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 4501/30000 (15.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1125.6681s / 253050.3704 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.3499
env0_second_0:                 episode reward: -2.2000,                 loss: -0.2683
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 4521/30000 (15.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1167.8910s / 254218.2615 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.3461
env0_second_0:                 episode reward: 1.7500,                 loss: -0.2866
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 4541/30000 (15.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1152.7103s / 255370.9717 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.3444
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2843
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 4561/30000 (15.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1124.9203s / 256495.8921 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3405
env0_second_0:                 episode reward: -1.1000,                 loss: -0.0867
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 4581/30000 (15.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1123.2532s / 257619.1453 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3290
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2254
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 4601/30000 (15.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1125.5324s / 258744.6777 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.3310
env0_second_0:                 episode reward: 0.5500,                 loss: -0.2636
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 4621/30000 (15.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1174.7849s / 259919.4626 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3147
env0_second_0:                 episode reward: 1.3500,                 loss: -0.2278
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 4641/30000 (15.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1142.3010s / 261061.7637 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.3382
env0_second_0:                 episode reward: 1.4500,                 loss: -0.2590
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 4661/30000 (15.5367%),                 avg. length: 9867.25,                last time consumption/overall running time: 1123.4297s / 262185.1934 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3231
env0_second_0:                 episode reward: -0.9000,                 loss: -0.2391
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4681/30000 (15.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1130.5237s / 263315.7171 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.3534
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2963
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 4701/30000 (15.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1124.1686s / 264439.8857 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3311
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2701
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 4721/30000 (15.7367%),                 avg. length: 9919.9,                last time consumption/overall running time: 1095.9173s / 265535.8030 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3360
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2620
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 4741/30000 (15.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1125.0705s / 266660.8735 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3383
env0_second_0:                 episode reward: -1.4500,                 loss: -0.2805
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4761/30000 (15.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1152.1510s / 267813.0245 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3554
env0_second_0:                 episode reward: -0.9000,                 loss: -0.3001
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 4781/30000 (15.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1150.3540s / 268963.3785 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.3498
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2877
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 4801/30000 (16.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1141.3549s / 270104.7334 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3260
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2649
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 4821/30000 (16.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1153.3146s / 271258.0480 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3418
env0_second_0:                 episode reward: -1.2500,                 loss: -0.2685
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 4841/30000 (16.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1145.7935s / 272403.8415 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.3273
env0_second_0:                 episode reward: 0.5500,                 loss: -0.2550
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 4861/30000 (16.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1128.4084s / 273532.2499 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3432
env0_second_0:                 episode reward: -1.5500,                 loss: -0.2780
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 4881/30000 (16.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1125.4296s / 274657.6795 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3219
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2511
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 4901/30000 (16.3367%),                 avg. length: 9996.1,                last time consumption/overall running time: 1139.5349s / 275797.2144 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.3305
env0_second_0:                 episode reward: -9.7500,                 loss: -0.1114
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 4921/30000 (16.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1121.1294s / 276918.3438 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.2934
env0_second_0:                 episode reward: 0.9000,                 loss: -0.1442
env1_first_0:                 episode reward: -27.5500,                 loss: nan
env1_second_0:                 episode reward: 27.5500,                 loss: nan
Episode: 4941/30000 (16.4700%),                 avg. length: 9842.85,                last time consumption/overall running time: 1100.1539s / 278018.4978 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3396
env0_second_0:                 episode reward: -1.1500,                 loss: -0.2676
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 4961/30000 (16.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1114.8213s / 279133.3191 s
env0_first_0:                 episode reward: -10.5000,                 loss: -0.3413
env0_second_0:                 episode reward: 10.5000,                 loss: -0.2728
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 4981/30000 (16.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1127.6113s / 280260.9304 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.3509
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2689
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 5001/30000 (16.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1165.8845s / 281426.8148 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.3391
env0_second_0:                 episode reward: 1.3000,                 loss: -0.2608
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 5021/30000 (16.7367%),                 avg. length: 9995.05,                last time consumption/overall running time: 1114.5148s / 282541.3297 s