pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220204_1545/pettingzoo_tennis_v2_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220204_1545/pettingzoo_tennis_v2_nash_ppo.
Episode: 1/30000 (0.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 50.6952s / 50.6952 s
env0_first_0:                 episode reward: -8.0000,                 loss: -0.0147
env0_second_0:                 episode reward: 8.0000,                 loss: -0.0101
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 7068.05,                last time consumption/overall running time: 709.5889s / 760.2841 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0414
env0_second_0:                 episode reward: -3.6500,                 loss: -0.0399
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 4803.15,                last time consumption/overall running time: 442.0271s / 1202.3111 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.0434
env0_second_0:                 episode reward: -2.8500,                 loss: -0.0289
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 3948.2,                last time consumption/overall running time: 372.8307s / 1575.1419 s
env0_first_0:                 episode reward: -3.4000,                 loss: -0.0689
env0_second_0:                 episode reward: 3.4000,                 loss: -0.0585
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 3621.95,                last time consumption/overall running time: 354.9613s / 1930.1032 s
env0_first_0:                 episode reward: -2.7500,                 loss: -0.1009
env0_second_0:                 episode reward: 2.7500,                 loss: -0.0881
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 5241.3,                last time consumption/overall running time: 508.3430s / 2438.4462 s
env0_first_0:                 episode reward: 9.1500,                 loss: -0.0993
env0_second_0:                 episode reward: -9.1500,                 loss: -0.0972
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 5577.15,                last time consumption/overall running time: 561.0975s / 2999.5437 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.1417
env0_second_0:                 episode reward: -5.1000,                 loss: -0.1228
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 6984.55,                last time consumption/overall running time: 687.8726s / 3687.4163 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.1913
env0_second_0:                 episode reward: -3.8500,                 loss: -0.1764
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 8537.3,                last time consumption/overall running time: 826.8851s / 4514.3014 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.1871
env0_second_0:                 episode reward: -4.9000,                 loss: -0.1648
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 9412.4,                last time consumption/overall running time: 915.0379s / 5429.3394 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2217
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2102
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 9578.0,                last time consumption/overall running time: 950.0032s / 6379.3425 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.2051
env0_second_0:                 episode reward: 5.4000,                 loss: -0.1880
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 9712.5,                last time consumption/overall running time: 975.9982s / 7355.3407 s
env0_first_0:                 episode reward: 19.5000,                 loss: -0.1939
env0_second_0:                 episode reward: -19.5000,                 loss: -0.1813
env1_first_0:                 episode reward: 17.1000,                 loss: nan
env1_second_0:                 episode reward: -17.1000,                 loss: nan
Episode: 241/30000 (0.8033%),                 avg. length: 9991.5,                last time consumption/overall running time: 996.9783s / 8352.3190 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.2332
env0_second_0:                 episode reward: -2.9500,                 loss: -0.2192
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 261/30000 (0.8700%),                 avg. length: 9189.25,                last time consumption/overall running time: 1049.3294s / 9401.6484 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.2251
env0_second_0:                 episode reward: -7.3000,                 loss: -0.2076
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 281/30000 (0.9367%),                 avg. length: 9698.95,                last time consumption/overall running time: 1099.5975s / 10501.2459 s
env0_first_0:                 episode reward: 8.3500,                 loss: -0.2372
env0_second_0:                 episode reward: -8.3500,                 loss: -0.2265
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 301/30000 (1.0033%),                 avg. length: 9597.7,                last time consumption/overall running time: 1079.9706s / 11581.2165 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.2302
env0_second_0:                 episode reward: 3.8500,                 loss: -0.2207
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 321/30000 (1.0700%),                 avg. length: 9992.05,                last time consumption/overall running time: 1044.8826s / 12626.0991 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.2410
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2311
env1_first_0:                 episode reward: -10.4000,                 loss: nan
env1_second_0:                 episode reward: 10.4000,                 loss: nan
Episode: 341/30000 (1.1367%),                 avg. length: 9998.25,                last time consumption/overall running time: 1187.6799s / 13813.7790 s
env0_first_0:                 episode reward: -5.2500,                 loss: -0.2231
env0_second_0:                 episode reward: 5.2500,                 loss: -0.2151
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 361/30000 (1.2033%),                 avg. length: 9479.2,                last time consumption/overall running time: 1129.4693s / 14943.2483 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.2023
env0_second_0:                 episode reward: -7.0000,                 loss: -0.1930
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 381/30000 (1.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1192.8364s / 16136.0847 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2181
env0_second_0:                 episode reward: -0.6500,                 loss: -0.2131
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 401/30000 (1.3367%),                 avg. length: 9742.3,                last time consumption/overall running time: 1169.0972s / 17305.1819 s
env0_first_0:                 episode reward: -17.1000,                 loss: -0.1482
env0_second_0:                 episode reward: 17.1000,                 loss: -0.1158
env1_first_0:                 episode reward: -25.6000,                 loss: nan
env1_second_0:                 episode reward: 25.6000,                 loss: nan
Episode: 421/30000 (1.4033%),                 avg. length: 9720.35,                last time consumption/overall running time: 1149.4440s / 18454.6259 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.2119
env0_second_0:                 episode reward: 6.7500,                 loss: -0.2056
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 441/30000 (1.4700%),                 avg. length: 9643.25,                last time consumption/overall running time: 1105.0028s / 19559.6287 s
env0_first_0:                 episode reward: 22.7000,                 loss: -0.1627
env0_second_0:                 episode reward: -22.7000,                 loss: -0.1481
env1_first_0:                 episode reward: 10.3500,                 loss: nan
env1_second_0:                 episode reward: -10.3500,                 loss: nan
Episode: 461/30000 (1.5367%),                 avg. length: 9865.5,                last time consumption/overall running time: 1128.1361s / 20687.7648 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.2279
env0_second_0:                 episode reward: -4.0500,                 loss: -0.2214
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 481/30000 (1.6033%),                 avg. length: 9978.65,                last time consumption/overall running time: 1090.4899s / 21778.2547 s
env0_first_0:                 episode reward: -13.9000,                 loss: -0.2097
env0_second_0:                 episode reward: 13.9000,                 loss: -0.1982
env1_first_0:                 episode reward: -25.4000,                 loss: nan
env1_second_0:                 episode reward: 25.4000,                 loss: nan
Episode: 501/30000 (1.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 992.7450s / 22770.9997 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.1680
env0_second_0:                 episode reward: -5.2500,                 loss: -0.1446
env1_first_0:                 episode reward: 14.5500,                 loss: nan
env1_second_0:                 episode reward: -14.5500,                 loss: nan
Episode: 521/30000 (1.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1033.0872s / 23804.0869 s
env0_first_0:                 episode reward: 16.7500,                 loss: -0.1644
env0_second_0:                 episode reward: -16.7500,                 loss: -0.1650
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 541/30000 (1.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1107.2825s / 24911.3694 s
env0_first_0:                 episode reward: -3.6000,                 loss: -0.2190
env0_second_0:                 episode reward: 3.6000,                 loss: -0.2093
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 561/30000 (1.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1183.9711s / 26095.3405 s
env0_first_0:                 episode reward: 8.6000,                 loss: -0.2278
env0_second_0:                 episode reward: -8.6000,                 loss: -0.2175
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 581/30000 (1.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1154.7694s / 27250.1100 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.1978
env0_second_0:                 episode reward: 2.6500,                 loss: -0.1906
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 601/30000 (2.0033%),                 avg. length: 9925.4,                last time consumption/overall running time: 1130.7861s / 28380.8961 s
env0_first_0:                 episode reward: 14.8000,                 loss: -0.2177
env0_second_0:                 episode reward: -14.8000,                 loss: -0.2079
env1_first_0:                 episode reward: 8.5000,                 loss: nan
env1_second_0:                 episode reward: -8.5000,                 loss: nan
Episode: 621/30000 (2.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1118.8024s / 29499.6984 s
env0_first_0:                 episode reward: 8.0000,                 loss: -0.2545
env0_second_0:                 episode reward: -8.0000,                 loss: -0.2457
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 641/30000 (2.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1165.4414s / 30665.1399 s
env0_first_0:                 episode reward: 8.9000,                 loss: -0.2331
env0_second_0:                 episode reward: -8.9000,                 loss: -0.2169
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 661/30000 (2.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1166.4520s / 31831.5918 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.2309
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2262
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 681/30000 (2.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1200.2904s / 33031.8823 s
env0_first_0:                 episode reward: 23.7500,                 loss: -0.1853
env0_second_0:                 episode reward: -23.7500,                 loss: -0.1817
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 701/30000 (2.3367%),                 avg. length: 9864.5,                last time consumption/overall running time: 1124.8249s / 34156.7072 s
env0_first_0:                 episode reward: 6.4500,                 loss: -0.2365
env0_second_0:                 episode reward: -6.4500,                 loss: -0.2302
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 721/30000 (2.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1107.4385s / 35264.1457 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.2133
env0_second_0:                 episode reward: -7.7500,                 loss: -0.2050
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 741/30000 (2.4700%),                 avg. length: 9843.6,                last time consumption/overall running time: 1138.7224s / 36402.8680 s
env0_first_0:                 episode reward: 10.6000,                 loss: -0.2034
env0_second_0:                 episode reward: -10.6000,                 loss: -0.1951
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 761/30000 (2.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1187.7087s / 37590.5768 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.2381
env0_second_0:                 episode reward: -4.1500,                 loss: -0.2259
env1_first_0:                 episode reward: -10.7500,                 loss: nan
env1_second_0:                 episode reward: 10.7500,                 loss: nan
Episode: 781/30000 (2.6033%),                 avg. length: 9879.4,                last time consumption/overall running time: 1164.8529s / 38755.4296 s
env0_first_0:                 episode reward: -18.8000,                 loss: -0.2281
env0_second_0:                 episode reward: 18.8000,                 loss: -0.2145
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 801/30000 (2.6700%),                 avg. length: 9843.15,                last time consumption/overall running time: 1145.0861s / 39900.5157 s
env0_first_0:                 episode reward: -13.3500,                 loss: -0.1768
env0_second_0:                 episode reward: 13.3500,                 loss: -0.1614
env1_first_0:                 episode reward: -29.3500,                 loss: nan
env1_second_0:                 episode reward: 29.3500,                 loss: nan
Episode: 821/30000 (2.7367%),                 avg. length: 9785.35,                last time consumption/overall running time: 1153.6222s / 41054.1379 s
env0_first_0:                 episode reward: -23.6500,                 loss: -0.2044
env0_second_0:                 episode reward: 23.6500,                 loss: -0.1887
env1_first_0:                 episode reward: -20.9000,                 loss: nan
env1_second_0:                 episode reward: 20.9000,                 loss: nan
Episode: 841/30000 (2.8033%),                 avg. length: 9699.8,                last time consumption/overall running time: 1148.4483s / 42202.5861 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.2206
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2053
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 861/30000 (2.8700%),                 avg. length: 9965.2,                last time consumption/overall running time: 1105.2799s / 43307.8660 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.2232
env0_second_0:                 episode reward: 7.5500,                 loss: -0.2112
env1_first_0:                 episode reward: -14.5000,                 loss: nan
env1_second_0:                 episode reward: 14.5000,                 loss: nan
Episode: 881/30000 (2.9367%),                 avg. length: 9918.8,                last time consumption/overall running time: 1063.3327s / 44371.1987 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.2100
env0_second_0:                 episode reward: 3.3000,                 loss: -0.1906
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 901/30000 (3.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1054.0268s / 45425.2255 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.2421
env0_second_0:                 episode reward: 8.7500,                 loss: -0.2253
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 921/30000 (3.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1053.0907s / 46478.3162 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.2397
env0_second_0:                 episode reward: -4.4000,                 loss: -0.2173
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 941/30000 (3.1367%),                 avg. length: 9814.9,                last time consumption/overall running time: 1159.4273s / 47637.7435 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2087
env0_second_0:                 episode reward: -1.7500,                 loss: -0.1981
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 961/30000 (3.2033%),                 avg. length: 9800.8,                last time consumption/overall running time: 1175.5117s / 48813.2553 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1818
env0_second_0:                 episode reward: -2.2500,                 loss: -0.1708
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 981/30000 (3.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1223.3230s / 50036.5783 s
env0_first_0:                 episode reward: 6.2500,                 loss: -0.2164
env0_second_0:                 episode reward: -6.2500,                 loss: -0.2060
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 1001/30000 (3.3367%),                 avg. length: 9990.35,                last time consumption/overall running time: 1209.9587s / 51246.5370 s
env0_first_0:                 episode reward: -12.7000,                 loss: -0.2119
env0_second_0:                 episode reward: 12.7000,                 loss: -0.2045
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 1021/30000 (3.4033%),                 avg. length: 9899.45,                last time consumption/overall running time: 1200.8395s / 52447.3765 s
env0_first_0:                 episode reward: -6.5000,                 loss: -0.2337
env0_second_0:                 episode reward: 6.5000,                 loss: -0.2226
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 1041/30000 (3.4700%),                 avg. length: 9962.2,                last time consumption/overall running time: 1250.6632s / 53698.0397 s
env0_first_0:                 episode reward: -18.0000,                 loss: -0.1703
env0_second_0:                 episode reward: 18.0000,                 loss: -0.1538
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 1061/30000 (3.5367%),                 avg. length: 9602.85,                last time consumption/overall running time: 1188.0668s / 54886.1064 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.2396
env0_second_0:                 episode reward: 2.5000,                 loss: -0.2319
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1081/30000 (3.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1254.1320s / 56140.2384 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2281
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2210
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 1101/30000 (3.6700%),                 avg. length: 9858.3,                last time consumption/overall running time: 1246.9509s / 57387.1893 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.2069
env0_second_0:                 episode reward: -3.8000,                 loss: -0.1932
env1_first_0:                 episode reward: -20.5500,                 loss: nan
env1_second_0:                 episode reward: 20.5500,                 loss: nan
Episode: 1121/30000 (3.7367%),                 avg. length: 9973.6,                last time consumption/overall running time: 1256.0898s / 58643.2791 s
env0_first_0:                 episode reward: -3.6000,                 loss: -0.2409
env0_second_0:                 episode reward: 3.6000,                 loss: -0.2307
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 1141/30000 (3.8033%),                 avg. length: 9890.85,                last time consumption/overall running time: 1232.4963s / 59875.7755 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2231
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2106
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 1161/30000 (3.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1214.9391s / 61090.7145 s
env0_first_0:                 episode reward: -3.2500,                 loss: -0.2254
env0_second_0:                 episode reward: 3.2500,                 loss: -0.2170
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 1181/30000 (3.9367%),                 avg. length: 9546.15,                last time consumption/overall running time: 1184.5633s / 62275.2778 s
env0_first_0:                 episode reward: -11.2500,                 loss: -0.2000
env0_second_0:                 episode reward: 11.2500,                 loss: -0.1898
env1_first_0:                 episode reward: -19.5500,                 loss: nan
env1_second_0:                 episode reward: 19.5500,                 loss: nan
Episode: 1201/30000 (4.0033%),                 avg. length: 9940.9,                last time consumption/overall running time: 1200.7389s / 63476.0167 s
env0_first_0:                 episode reward: -5.3000,                 loss: -0.2549
env0_second_0:                 episode reward: 5.3000,                 loss: -0.2435
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 1221/30000 (4.0700%),                 avg. length: 9986.3,                last time consumption/overall running time: 1224.8913s / 64700.9080 s
env0_first_0:                 episode reward: -7.1500,                 loss: -0.2591
env0_second_0:                 episode reward: 7.1500,                 loss: -0.2476
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 1241/30000 (4.1367%),                 avg. length: 9975.4,                last time consumption/overall running time: 1248.0092s / 65948.9172 s
env0_first_0:                 episode reward: -10.4500,                 loss: -0.2414
env0_second_0:                 episode reward: 10.4500,                 loss: -0.2314
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 1261/30000 (4.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1237.9014s / 67186.8186 s
env0_first_0:                 episode reward: -8.5500,                 loss: -0.2808
env0_second_0:                 episode reward: 8.5500,                 loss: -0.2710
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 1281/30000 (4.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1218.2842s / 68405.1028 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2752
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2656
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 1301/30000 (4.3367%),                 avg. length: 9901.0,                last time consumption/overall running time: 1205.6882s / 69610.7910 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2511
env0_second_0:                 episode reward: -1.0500,                 loss: -0.2354
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 1321/30000 (4.4033%),                 avg. length: 9848.0,                last time consumption/overall running time: 1215.1922s / 70825.9832 s
env0_first_0:                 episode reward: 15.1500,                 loss: -0.2415
env0_second_0:                 episode reward: -15.1500,                 loss: -0.2293
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 1341/30000 (4.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1252.0149s / 72077.9981 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.2799
env0_second_0:                 episode reward: 0.6000,                 loss: -0.2653
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 1361/30000 (4.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1252.1970s / 73330.1951 s
env0_first_0:                 episode reward: 8.1500,                 loss: -0.2770
env0_second_0:                 episode reward: -8.1500,                 loss: -0.2615
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 1381/30000 (4.6033%),                 avg. length: 9951.25,                last time consumption/overall running time: 1237.9864s / 74568.1815 s
env0_first_0:                 episode reward: -7.2000,                 loss: -0.2578
env0_second_0:                 episode reward: 7.2000,                 loss: -0.2443
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1401/30000 (4.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1214.5068s / 75782.6883 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.2454
env0_second_0:                 episode reward: -4.7500,                 loss: -0.2300
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 1421/30000 (4.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1254.5478s / 77037.2361 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.2595
env0_second_0:                 episode reward: -3.6500,                 loss: -0.2433
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 1441/30000 (4.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1262.8686s / 78300.1047 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.2513
env0_second_0:                 episode reward: -6.6500,                 loss: -0.2348
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 1461/30000 (4.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1252.8612s / 79552.9658 s
env0_first_0:                 episode reward: 13.9000,                 loss: -0.2264
env0_second_0:                 episode reward: -13.9000,                 loss: -0.2047
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 1481/30000 (4.9367%),                 avg. length: 9936.0,                last time consumption/overall running time: 1237.7004s / 80790.6662 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.2367
env0_second_0:                 episode reward: -6.6500,                 loss: -0.2220
env1_first_0:                 episode reward: 10.9000,                 loss: nan
env1_second_0:                 episode reward: -10.9000,                 loss: nan
Episode: 1501/30000 (5.0033%),                 avg. length: 9922.7,                last time consumption/overall running time: 1196.9077s / 81987.5739 s
env0_first_0:                 episode reward: 7.2500,                 loss: -0.2370
env0_second_0:                 episode reward: -7.2500,                 loss: -0.2203
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 1521/30000 (5.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1235.4201s / 83222.9940 s
env0_first_0:                 episode reward: 6.5500,                 loss: -0.2321
env0_second_0:                 episode reward: -6.5500,                 loss: -0.2133
env1_first_0:                 episode reward: 18.1500,                 loss: nan
env1_second_0:                 episode reward: -18.1500,                 loss: nan
Episode: 1541/30000 (5.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1259.8791s / 84482.8731 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.1937
env0_second_0:                 episode reward: -2.3000,                 loss: -0.1757
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 1561/30000 (5.2033%),                 avg. length: 9891.25,                last time consumption/overall running time: 1201.3576s / 85684.2307 s
env0_first_0:                 episode reward: 11.7500,                 loss: -0.2073
env0_second_0:                 episode reward: -11.7500,                 loss: -0.1916
env1_first_0:                 episode reward: 12.9500,                 loss: nan
env1_second_0:                 episode reward: -12.9500,                 loss: nan
Episode: 1581/30000 (5.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1233.1406s / 86917.3712 s
env0_first_0:                 episode reward: 7.6500,                 loss: -0.2407
env0_second_0:                 episode reward: -7.6500,                 loss: -0.2244
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 1601/30000 (5.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1238.0197s / 88155.3909 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.2287
env0_second_0:                 episode reward: -10.0000,                 loss: -0.2170
env1_first_0:                 episode reward: 11.5000,                 loss: nan
env1_second_0:                 episode reward: -11.5000,                 loss: nan
Episode: 1621/30000 (5.4033%),                 avg. length: 9965.8,                last time consumption/overall running time: 1230.0212s / 89385.4121 s
env0_first_0:                 episode reward: 14.0500,                 loss: -0.2261
env0_second_0:                 episode reward: -14.0500,                 loss: -0.2130
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 1641/30000 (5.4700%),                 avg. length: 9750.7,                last time consumption/overall running time: 1233.2730s / 90618.6851 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.2263
env0_second_0:                 episode reward: -5.7000,                 loss: -0.2182
env1_first_0:                 episode reward: 12.6000,                 loss: nan
env1_second_0:                 episode reward: -12.6000,                 loss: nan
Episode: 1661/30000 (5.5367%),                 avg. length: 9993.5,                last time consumption/overall running time: 1232.8377s / 91851.5229 s
env0_first_0:                 episode reward: 8.7000,                 loss: -0.2477
env0_second_0:                 episode reward: -8.7000,                 loss: -0.2293
env1_first_0:                 episode reward: 11.2500,                 loss: nan
env1_second_0:                 episode reward: -11.2500,                 loss: nan
Episode: 1681/30000 (5.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1252.1746s / 93103.6975 s
env0_first_0:                 episode reward: 11.6500,                 loss: -0.2596
env0_second_0:                 episode reward: -11.6500,                 loss: -0.2438
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 1701/30000 (5.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1236.5532s / 94340.2507 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.2262
env0_second_0:                 episode reward: -5.8500,                 loss: -0.2131
env1_first_0:                 episode reward: 10.2500,                 loss: nan
env1_second_0:                 episode reward: -10.2500,                 loss: nan
Episode: 1721/30000 (5.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1249.4639s / 95589.7146 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.2291
env0_second_0:                 episode reward: -0.7500,                 loss: -0.2169
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 1741/30000 (5.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1243.9071s / 96833.6217 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.2912
env0_second_0:                 episode reward: -5.3000,                 loss: -0.2794
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 1761/30000 (5.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1296.2737s / 98129.8955 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.2465
env0_second_0:                 episode reward: -5.9000,                 loss: -0.2254
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 1781/30000 (5.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1280.5842s / 99410.4796 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.2936
env0_second_0:                 episode reward: -4.2000,                 loss: -0.2847
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 1801/30000 (6.0033%),                 avg. length: 9685.05,                last time consumption/overall running time: 1267.6881s / 100678.1678 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.3033
env0_second_0:                 episode reward: -3.7500,                 loss: -0.2915
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 1821/30000 (6.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1315.4338s / 101993.6015 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.3106
env0_second_0:                 episode reward: -4.3000,                 loss: -0.2964
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 1841/30000 (6.1367%),                 avg. length: 9994.75,                last time consumption/overall running time: 1241.4387s / 103235.0402 s
env0_first_0:                 episode reward: 8.8500,                 loss: -0.2824
env0_second_0:                 episode reward: -8.8500,                 loss: -0.2750
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 1861/30000 (6.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1196.9412s / 104431.9814 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.2759
env0_second_0:                 episode reward: -4.4000,                 loss: -0.2636
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 1881/30000 (6.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1227.8979s / 105659.8793 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.2890
env0_second_0:                 episode reward: -3.8000,                 loss: -0.2759
env1_first_0:                 episode reward: 10.1500,                 loss: nan
env1_second_0:                 episode reward: -10.1500,                 loss: nan
Episode: 1901/30000 (6.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1243.8467s / 106903.7261 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.2691
env0_second_0:                 episode reward: -5.4000,                 loss: -0.2549
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 1921/30000 (6.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1258.2760s / 108162.0021 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3147
env0_second_0:                 episode reward: -1.1500,                 loss: -0.2967
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 1941/30000 (6.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1238.2019s / 109400.2039 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.2908
env0_second_0:                 episode reward: -3.3500,                 loss: -0.2733
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1961/30000 (6.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1238.8508s / 110639.0548 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.3162
env0_second_0:                 episode reward: 0.2500,                 loss: -0.3006
env1_first_0:                 episode reward: 0.4500,                 loss: nan