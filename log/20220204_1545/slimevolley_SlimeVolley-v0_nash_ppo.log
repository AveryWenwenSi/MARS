pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 2, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'num_process': 1, 'batch_size': 32, 'max_episodes': 50000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'Tanh', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'Tanh', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220204_1545/slimevolley_SlimeVolley-v0_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220204_1545/slimevolley_SlimeVolley-v0_nash_ppo.
Episode: 1/50000 (0.0020%),                 avg. length: 458.0,                last time consumption/overall running time: 2.1008s / 2.1008 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.0043
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0591
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 575.8,                last time consumption/overall running time: 35.1962s / 37.2971 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0830
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0963
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 570.4,                last time consumption/overall running time: 35.1232s / 72.4203 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.1947
env0_second_0:                 episode reward: 0.5000,                 loss: 0.1783
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 548.9,                last time consumption/overall running time: 33.6276s / 106.0479 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.2443
env0_second_0:                 episode reward: -0.6000,                 loss: 0.2487
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 600.5,                last time consumption/overall running time: 36.4684s / 142.5162 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.2359
env0_second_0:                 episode reward: 0.7000,                 loss: 0.2448
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 586.9,                last time consumption/overall running time: 36.2298s / 178.7460 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.2161
env0_second_0:                 episode reward: -1.0500,                 loss: 0.2231
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 583.85,                last time consumption/overall running time: 35.4146s / 214.1606 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.2212
env0_second_0:                 episode reward: -0.6000,                 loss: 0.2212
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 567.95,                last time consumption/overall running time: 34.9104s / 249.0710 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.2240
env0_second_0:                 episode reward: -0.3500,                 loss: 0.2241
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 590.35,                last time consumption/overall running time: 35.6130s / 284.6840 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.2379
env0_second_0:                 episode reward: 0.2000,                 loss: 0.2338
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 562.05,                last time consumption/overall running time: 33.4087s / 318.0927 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.2504
env0_second_0:                 episode reward: 0.5500,                 loss: 0.2540
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 584.4,                last time consumption/overall running time: 35.1558s / 353.2485 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.2508
env0_second_0:                 episode reward: 0.2500,                 loss: 0.2492
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 577.4,                last time consumption/overall running time: 34.1593s / 387.4079 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.2359
env0_second_0:                 episode reward: -0.3000,                 loss: 0.2374
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 522.05,                last time consumption/overall running time: 31.9287s / 419.3366 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.2661
env0_second_0:                 episode reward: 0.8000,                 loss: 0.2730
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 556.25,                last time consumption/overall running time: 33.3156s / 452.6522 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.2770
env0_second_0:                 episode reward: -0.5500,                 loss: 0.2952
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 580.05,                last time consumption/overall running time: 34.3714s / 487.0236 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.2502
env0_second_0:                 episode reward: -0.4500,                 loss: 0.2546
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 555.9,                last time consumption/overall running time: 33.1657s / 520.1893 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.2678
env0_second_0:                 episode reward: 0.4500,                 loss: 0.2818
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 576.4,                last time consumption/overall running time: 34.5295s / 554.7188 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2412
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2652
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 571.2,                last time consumption/overall running time: 34.5720s / 589.2908 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.2595
env0_second_0:                 episode reward: 0.4000,                 loss: 0.2500
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 563.85,                last time consumption/overall running time: 33.6870s / 622.9778 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.2328
env0_second_0:                 episode reward: 0.0500,                 loss: 0.2233
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 555.0,                last time consumption/overall running time: 33.1767s / 656.1545 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.2481
env0_second_0:                 episode reward: -0.7500,                 loss: 0.2467
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 553.95,                last time consumption/overall running time: 34.1976s / 690.3521 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.2549
env0_second_0:                 episode reward: 0.7000,                 loss: 0.2549
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 620.8,                last time consumption/overall running time: 36.8685s / 727.2207 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.2339
env0_second_0:                 episode reward: -0.9000,                 loss: 0.2282
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 593.85,                last time consumption/overall running time: 35.2997s / 762.5204 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.2546
env0_second_0:                 episode reward: -0.3000,                 loss: 0.2549
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 587.65,                last time consumption/overall running time: 35.9210s / 798.4414 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.2598
env0_second_0:                 episode reward: -0.1000,                 loss: 0.2642
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 519.5,                last time consumption/overall running time: 31.8135s / 830.2550 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.2759
env0_second_0:                 episode reward: -0.1500,                 loss: 0.2695
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 573.15,                last time consumption/overall running time: 34.3562s / 864.6111 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2591
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2530
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 574.35,                last time consumption/overall running time: 34.8381s / 899.4492 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.2356
env0_second_0:                 episode reward: 0.2500,                 loss: 0.2397
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 569.0,                last time consumption/overall running time: 34.6025s / 934.0518 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.2611
env0_second_0:                 episode reward: 0.2000,                 loss: 0.2478
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 589.05,                last time consumption/overall running time: 35.4606s / 969.5123 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.2395
env0_second_0:                 episode reward: 0.3000,                 loss: 0.2364
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 574.7,                last time consumption/overall running time: 34.6392s / 1004.1515 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.2424
env0_second_0:                 episode reward: -0.4500,                 loss: 0.2449
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 553.95,                last time consumption/overall running time: 33.3248s / 1037.4763 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2339
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2442
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 595.85,                last time consumption/overall running time: 35.4537s / 1072.9300 s