pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0524/pettingzoo_boxing_v1_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0524/pettingzoo_boxing_v1_nash_dqn.
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 51.0769s / 51.0769 s
env0_first_0:                 episode reward: 3.0000,                 loss: 0.0129
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0102
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 1618.1775s / 1669.2544 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0192
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0191
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 1948.8611s / 3618.1155 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0192
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0186
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2045.9198s / 5664.0353 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0160
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0161
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2081.0349s / 7745.0702 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0157
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0158
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2089.6972s / 9834.7673 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0164
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0173
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2098.2096s / 11932.9770 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0159
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0160
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2101.3132s / 14034.2902 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0156
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0156
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2092.5524s / 16126.8426 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0171
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0175
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2102.3057s / 18229.1483 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0172
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0171
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2105.4842s / 20334.6325 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0164
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0160
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2092.7034s / 22427.3359 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0175
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0175
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2108.0052s / 24535.3410 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0157
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0161
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2081.1818s / 26616.5229 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0143
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0148
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2089.2366s / 28705.7595 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0157
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0153
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2094.2766s / 30800.0361 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0153
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0155
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2096.2028s / 32896.2389 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0161
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0166
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2090.5553s / 34986.7942 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0167
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0166
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2092.3025s / 37079.0967 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0167
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0166
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2430.1962s / 39509.2928 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0175
env0_second_0:                 episode reward: 2.9000,                 loss: 0.0174
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2463.0806s / 41972.3734 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0162
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0169
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2450.7330s / 44423.1064 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0171
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0175
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2439.4172s / 46862.5236 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0169
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0171
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2435.6535s / 49298.1772 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0164
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0163
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2442.8004s / 51740.9776 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0188
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0176
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2441.4123s / 54182.3899 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0190
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0179
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2439.8822s / 56622.2722 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0167
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0172
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2434.9065s / 59057.1787 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0173
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0176
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2431.0755s / 61488.2542 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0174
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0181
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2449.6703s / 63937.9245 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0187
env0_second_0:                 episode reward: 4.4500,                 loss: 0.0193
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2444.8610s / 66382.7855 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0212
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0214
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2446.1136s / 68828.8991 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0211
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0218
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2432.8460s / 71261.7451 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0278
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0252
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2439.6616s / 73701.4067 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0332
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0296
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2436.5827s / 76137.9894 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0342
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0319
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2442.3700s / 78580.3594 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0384
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0341
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2443.2181s / 81023.5775 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0393
env0_second_0:                 episode reward: 2.2500,                 loss: 0.0400
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2438.3471s / 83461.9246 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0328
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0393
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2435.6656s / 85897.5902 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0335
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0304
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2443.4273s / 88341.0175 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0382
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0385
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2433.2983s / 90774.3158 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0427
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0460
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2447.0824s / 93221.3982 s
env0_first_0:                 episode reward: -7.6000,                 loss: 0.0540
env0_second_0:                 episode reward: 7.6000,                 loss: 0.0552
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2428.3094s / 95649.7076 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0591
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0613
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2405.5053s / 98055.2128 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.0453
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0432
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2392.3614s / 100447.5743 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0396
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0364
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2395.3503s / 102842.9245 s
env0_first_0:                 episode reward: -6.8000,                 loss: 0.0517
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0346
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2388.2471s / 105231.1716 s
env0_first_0:                 episode reward: -10.3500,                 loss: 0.0782
env0_second_0:                 episode reward: 10.3500,                 loss: 0.0627
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2394.9266s / 107626.0982 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0679
env0_second_0:                 episode reward: 2.5500,                 loss: 0.0697
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2402.6099s / 110028.7081 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0522
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0562
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2394.3947s / 112423.1028 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0485
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0549
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2395.7516s / 114818.8544 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0484
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0542
env1_first_0:                 episode reward: -10.2500,                 loss: nan
env1_second_0:                 episode reward: 10.2500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2403.9066s / 117222.7611 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0545
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0531
env1_first_0:                 episode reward: -7.9500,                 loss: nan
env1_second_0:                 episode reward: 7.9500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2390.8400s / 119613.6010 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0510
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0538
env1_first_0:                 episode reward: -9.8500,                 loss: nan
env1_second_0:                 episode reward: 9.8500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2405.8659s / 122019.4669 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0605
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0598
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2402.6202s / 124422.0871 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0551
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0545
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2391.4869s / 126813.5740 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0571
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0555
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2395.6871s / 129209.2611 s
env0_first_0:                 episode reward: -11.9500,                 loss: 0.0588
env0_second_0:                 episode reward: 11.9500,                 loss: 0.0501
env1_first_0:                 episode reward: -11.2500,                 loss: nan
env1_second_0:                 episode reward: 11.2500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2400.0550s / 131609.3161 s
env0_first_0:                 episode reward: -6.7000,                 loss: 0.0636
env0_second_0:                 episode reward: 6.7000,                 loss: 0.0591
env1_first_0:                 episode reward: -13.7000,                 loss: nan
env1_second_0:                 episode reward: 13.7000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1774.4,                last time consumption/overall running time: 2376.1499s / 133985.4661 s
env0_first_0:                 episode reward: -16.8000,                 loss: 0.0656
env0_second_0:                 episode reward: 16.8000,                 loss: 0.0679
env1_first_0:                 episode reward: -14.8000,                 loss: nan
env1_second_0:                 episode reward: 14.8000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2398.7876s / 136384.2536 s
env0_first_0:                 episode reward: -8.4000,                 loss: 0.0727
env0_second_0:                 episode reward: 8.4000,                 loss: 0.0726
env1_first_0:                 episode reward: -11.7500,                 loss: nan
env1_second_0:                 episode reward: 11.7500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2398.3699s / 138782.6235 s
env0_first_0:                 episode reward: -13.3500,                 loss: 0.0621
env0_second_0:                 episode reward: 13.3500,                 loss: 0.0625
env1_first_0:                 episode reward: -11.9000,                 loss: nan
env1_second_0:                 episode reward: 11.9000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2398.5402s / 141181.1637 s
env0_first_0:                 episode reward: -12.6500,                 loss: 0.0715
env0_second_0:                 episode reward: 12.6500,                 loss: 0.0634
env1_first_0:                 episode reward: -14.8500,                 loss: nan
env1_second_0:                 episode reward: 14.8500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1771.6,                last time consumption/overall running time: 2384.5939s / 143565.7576 s
env0_first_0:                 episode reward: -20.8000,                 loss: 0.0925
env0_second_0:                 episode reward: 20.8000,                 loss: 0.0764
env1_first_0:                 episode reward: -16.8000,                 loss: nan
env1_second_0:                 episode reward: 16.8000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1757.7,                last time consumption/overall running time: 2335.9519s / 145901.7094 s
env0_first_0:                 episode reward: -21.7500,                 loss: 0.0986
env0_second_0:                 episode reward: 21.7500,                 loss: 0.0861
env1_first_0:                 episode reward: -18.4000,                 loss: nan
env1_second_0:                 episode reward: 18.4000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2343.1356s / 148244.8450 s
env0_first_0:                 episode reward: -5.3000,                 loss: 0.0942
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0760
env1_first_0:                 episode reward: -14.2500,                 loss: nan
env1_second_0:                 episode reward: 14.2500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2353.2249s / 150598.0699 s
env0_first_0:                 episode reward: -12.9000,                 loss: 0.0684
env0_second_0:                 episode reward: 12.9000,                 loss: 0.0712
env1_first_0:                 episode reward: -14.8000,                 loss: nan
env1_second_0:                 episode reward: 14.8000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1773.45,                last time consumption/overall running time: 2319.7203s / 152917.7902 s
env0_first_0:                 episode reward: -17.6500,                 loss: 0.0785
env0_second_0:                 episode reward: 17.6500,                 loss: 0.0806
env1_first_0:                 episode reward: -21.0000,                 loss: nan
env1_second_0:                 episode reward: 21.0000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1723.7,                last time consumption/overall running time: 2253.8819s / 155171.6721 s
env0_first_0:                 episode reward: -25.5000,                 loss: 0.1100
env0_second_0:                 episode reward: 25.5000,                 loss: 0.0863
env1_first_0:                 episode reward: -25.5500,                 loss: nan
env1_second_0:                 episode reward: 25.5500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1732.8,                last time consumption/overall running time: 2253.7982s / 157425.4703 s
env0_first_0:                 episode reward: -31.1000,                 loss: 0.1393
env0_second_0:                 episode reward: 31.1000,                 loss: 0.1119
env1_first_0:                 episode reward: -23.7500,                 loss: nan
env1_second_0:                 episode reward: 23.7500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1761.2,                last time consumption/overall running time: 2297.0492s / 159722.5195 s
env0_first_0:                 episode reward: -20.6500,                 loss: 0.1359
env0_second_0:                 episode reward: 20.6500,                 loss: 0.1337
env1_first_0:                 episode reward: -22.3000,                 loss: nan
env1_second_0:                 episode reward: 22.3000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1759.05,                last time consumption/overall running time: 2284.2894s / 162006.8089 s
env0_first_0:                 episode reward: -22.9500,                 loss: 0.1008
env0_second_0:                 episode reward: 22.9500,                 loss: 0.1268
env1_first_0:                 episode reward: -21.2500,                 loss: nan
env1_second_0:                 episode reward: 21.2500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1688.75,                last time consumption/overall running time: 2195.0514s / 164201.8603 s
env0_first_0:                 episode reward: -25.1500,                 loss: 0.1036
env0_second_0:                 episode reward: 25.1500,                 loss: 0.1245
env1_first_0:                 episode reward: -26.8500,                 loss: nan
env1_second_0:                 episode reward: 26.8500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 1715.15,                last time consumption/overall running time: 2204.3536s / 166406.2138 s
env0_first_0:                 episode reward: -30.8000,                 loss: 0.1336
env0_second_0:                 episode reward: 30.8000,                 loss: 0.1339
env1_first_0:                 episode reward: -27.6000,                 loss: nan
env1_second_0:                 episode reward: 27.6000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1664.85,                last time consumption/overall running time: 2139.6171s / 168545.8309 s
env0_first_0:                 episode reward: -20.8000,                 loss: 0.1890
env0_second_0:                 episode reward: 20.8000,                 loss: 0.1535
env1_first_0:                 episode reward: -27.9500,                 loss: nan
env1_second_0:                 episode reward: 27.9500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1489.3,                last time consumption/overall running time: 1915.7661s / 170461.5971 s
env0_first_0:                 episode reward: -35.0000,                 loss: 0.2090
env0_second_0:                 episode reward: 35.0000,                 loss: 0.1756
env1_first_0:                 episode reward: -41.3500,                 loss: nan
env1_second_0:                 episode reward: 41.3500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1541.9,                last time consumption/overall running time: 1986.0406s / 172447.6377 s
env0_first_0:                 episode reward: -41.1500,                 loss: 0.2448
env0_second_0:                 episode reward: 41.1500,                 loss: 0.2378
env1_first_0:                 episode reward: -38.1000,                 loss: nan
env1_second_0:                 episode reward: 38.1000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1381.7,                last time consumption/overall running time: 1788.1704s / 174235.8081 s
env0_first_0:                 episode reward: -42.6000,                 loss: 0.2730
env0_second_0:                 episode reward: 42.6000,                 loss: 0.2578
env1_first_0:                 episode reward: -34.0500,                 loss: nan
env1_second_0:                 episode reward: 34.0500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1485.25,                last time consumption/overall running time: 1926.2359s / 176162.0440 s
env0_first_0:                 episode reward: -36.5500,                 loss: 0.2736
env0_second_0:                 episode reward: 36.5500,                 loss: 0.2579
env1_first_0:                 episode reward: -34.2500,                 loss: nan
env1_second_0:                 episode reward: 34.2500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1378.5,                last time consumption/overall running time: 1776.7782s / 177938.8222 s
env0_first_0:                 episode reward: -39.9000,                 loss: 0.3113
env0_second_0:                 episode reward: 39.9000,                 loss: 0.2340
env1_first_0:                 episode reward: -29.3000,                 loss: nan
env1_second_0:                 episode reward: 29.3000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1169.1,                last time consumption/overall running time: 1499.8282s / 179438.6505 s
env0_first_0:                 episode reward: -40.8500,                 loss: 0.3105
env0_second_0:                 episode reward: 40.8500,                 loss: 0.2554
env1_first_0:                 episode reward: -45.8000,                 loss: nan
env1_second_0:                 episode reward: 45.8000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1265.2,                last time consumption/overall running time: 1619.3118s / 181057.9623 s
env0_first_0:                 episode reward: -31.9000,                 loss: 0.2956
env0_second_0:                 episode reward: 31.9000,                 loss: 0.2458
env1_first_0:                 episode reward: -43.6500,                 loss: nan
env1_second_0:                 episode reward: 43.6500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 966.5,                last time consumption/overall running time: 1246.2546s / 182304.2169 s
env0_first_0:                 episode reward: -47.3500,                 loss: 0.2998
env0_second_0:                 episode reward: 47.3500,                 loss: 0.2671
env1_first_0:                 episode reward: -45.7000,                 loss: nan
env1_second_0:                 episode reward: 45.7000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1073.65,                last time consumption/overall running time: 1376.5357s / 183680.7527 s
env0_first_0:                 episode reward: -41.4000,                 loss: 0.2967
env0_second_0:                 episode reward: 41.4000,                 loss: 0.3031
env1_first_0:                 episode reward: -47.3000,                 loss: nan
env1_second_0:                 episode reward: 47.3000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 886.65,                last time consumption/overall running time: 1135.5705s / 184816.3232 s
env0_first_0:                 episode reward: -44.8500,                 loss: 0.3465
env0_second_0:                 episode reward: 44.8500,                 loss: 0.3252
env1_first_0:                 episode reward: -65.3500,                 loss: nan
env1_second_0:                 episode reward: 65.3500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 961.1,                last time consumption/overall running time: 1232.9911s / 186049.3143 s
env0_first_0:                 episode reward: -49.6500,                 loss: 0.3972
env0_second_0:                 episode reward: 49.6500,                 loss: 0.3621
env1_first_0:                 episode reward: -47.0000,                 loss: nan
env1_second_0:                 episode reward: 47.0000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1020.5,                last time consumption/overall running time: 1299.4828s / 187348.7971 s
env0_first_0:                 episode reward: -47.9000,                 loss: 0.4747
env0_second_0:                 episode reward: 47.9000,                 loss: 0.3715
env1_first_0:                 episode reward: -47.7500,                 loss: nan
env1_second_0:                 episode reward: 47.7500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 768.2,                last time consumption/overall running time: 976.6485s / 188325.4456 s
env0_first_0:                 episode reward: -45.5000,                 loss: 0.4902
env0_second_0:                 episode reward: 45.5000,                 loss: 0.3505
env1_first_0:                 episode reward: -52.3000,                 loss: nan
env1_second_0:                 episode reward: 52.3000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 816.6,                last time consumption/overall running time: 1042.4295s / 189367.8751 s
env0_first_0:                 episode reward: -53.0000,                 loss: 0.5291
env0_second_0:                 episode reward: 53.0000,                 loss: 0.3713
env1_first_0:                 episode reward: -48.0000,                 loss: nan
env1_second_0:                 episode reward: 48.0000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 697.4,                last time consumption/overall running time: 881.5019s / 190249.3770 s
env0_first_0:                 episode reward: -46.1500,                 loss: 0.5374
env0_second_0:                 episode reward: 46.1500,                 loss: 0.4030
env1_first_0:                 episode reward: -65.3000,                 loss: nan
env1_second_0:                 episode reward: 65.3000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 652.5,                last time consumption/overall running time: 825.8903s / 191075.2673 s
env0_first_0:                 episode reward: -48.5500,                 loss: 0.5548
env0_second_0:                 episode reward: 48.5500,                 loss: 0.4355
env1_first_0:                 episode reward: -66.0500,                 loss: nan
env1_second_0:                 episode reward: 66.0500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 612.55,                last time consumption/overall running time: 767.5097s / 191842.7770 s
env0_first_0:                 episode reward: -67.7500,                 loss: 0.5896
env0_second_0:                 episode reward: 67.7500,                 loss: 0.4676
env1_first_0:                 episode reward: -58.9000,                 loss: nan
env1_second_0:                 episode reward: 58.9000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 644.55,                last time consumption/overall running time: 803.7281s / 192646.5051 s
env0_first_0:                 episode reward: -44.3500,                 loss: 0.5836
env0_second_0:                 episode reward: 44.3500,                 loss: 0.5021
env1_first_0:                 episode reward: -63.7000,                 loss: nan
env1_second_0:                 episode reward: 63.7000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 621.8,                last time consumption/overall running time: 778.0387s / 193424.5439 s
env0_first_0:                 episode reward: -52.8000,                 loss: 0.5948
env0_second_0:                 episode reward: 52.8000,                 loss: 0.5197
env1_first_0:                 episode reward: -60.3500,                 loss: nan
env1_second_0:                 episode reward: 60.3500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 624.2,                last time consumption/overall running time: 781.3169s / 194205.8607 s
env0_first_0:                 episode reward: -49.2000,                 loss: 0.6487
env0_second_0:                 episode reward: 49.2000,                 loss: 0.5347
env1_first_0:                 episode reward: -55.5000,                 loss: nan
env1_second_0:                 episode reward: 55.5000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 596.55,                last time consumption/overall running time: 746.2595s / 194952.1202 s
env0_first_0:                 episode reward: -68.3500,                 loss: 0.6293
env0_second_0:                 episode reward: 68.3500,                 loss: 0.6278
env1_first_0:                 episode reward: -55.5500,                 loss: nan
env1_second_0:                 episode reward: 55.5500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 559.5,                last time consumption/overall running time: 698.2200s / 195650.3403 s
env0_first_0:                 episode reward: -51.4000,                 loss: 0.6646
env0_second_0:                 episode reward: 51.4000,                 loss: 0.6375
env1_first_0:                 episode reward: -62.6000,                 loss: nan
env1_second_0:                 episode reward: 62.6000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 600.0,                last time consumption/overall running time: 753.5515s / 196403.8918 s
env0_first_0:                 episode reward: -51.2500,                 loss: 0.6744
env0_second_0:                 episode reward: 51.2500,                 loss: 0.6050
env1_first_0:                 episode reward: -69.5000,                 loss: nan
env1_second_0:                 episode reward: 69.5000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 615.1,                last time consumption/overall running time: 760.7196s / 197164.6114 s
env0_first_0:                 episode reward: -61.2000,                 loss: 0.6463
env0_second_0:                 episode reward: 61.2000,                 loss: 0.6795
env1_first_0:                 episode reward: -67.3000,                 loss: nan
env1_second_0:                 episode reward: 67.3000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 508.45,                last time consumption/overall running time: 630.6209s / 197795.2324 s
env0_first_0:                 episode reward: -65.2500,                 loss: 0.6715
env0_second_0:                 episode reward: 65.2500,                 loss: 0.7326
env1_first_0:                 episode reward: -69.2500,                 loss: nan
env1_second_0:                 episode reward: 69.2500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 606.55,                last time consumption/overall running time: 757.3104s / 198552.5427 s
env0_first_0:                 episode reward: -51.2500,                 loss: 0.7380
env0_second_0:                 episode reward: 51.2500,                 loss: 0.7498
env1_first_0:                 episode reward: -66.3500,                 loss: nan
env1_second_0:                 episode reward: 66.3500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 488.6,                last time consumption/overall running time: 606.2633s / 199158.8061 s
env0_first_0:                 episode reward: -61.0500,                 loss: 0.8215
env0_second_0:                 episode reward: 61.0500,                 loss: 0.7433
env1_first_0:                 episode reward: -57.2500,                 loss: nan
env1_second_0:                 episode reward: 57.2500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 453.75,                last time consumption/overall running time: 558.8122s / 199717.6183 s
env0_first_0:                 episode reward: -62.8500,                 loss: 0.7834
env0_second_0:                 episode reward: 62.8500,                 loss: 0.6804
env1_first_0:                 episode reward: -64.0000,                 loss: nan
env1_second_0:                 episode reward: 64.0000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 574.05,                last time consumption/overall running time: 703.5556s / 200421.1739 s
env0_first_0:                 episode reward: -60.0500,                 loss: 0.7822
env0_second_0:                 episode reward: 60.0500,                 loss: 0.6638
env1_first_0:                 episode reward: -55.5000,                 loss: nan
env1_second_0:                 episode reward: 55.5000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 475.55,                last time consumption/overall running time: 579.3124s / 201000.4863 s
env0_first_0:                 episode reward: -55.3500,                 loss: 0.9002
env0_second_0:                 episode reward: 55.3500,                 loss: 0.6198
env1_first_0:                 episode reward: -71.7500,                 loss: nan
env1_second_0:                 episode reward: 71.7500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 482.8,                last time consumption/overall running time: 583.6238s / 201584.1102 s
env0_first_0:                 episode reward: -62.4500,                 loss: 0.9661
env0_second_0:                 episode reward: 62.4500,                 loss: 0.6554
env1_first_0:                 episode reward: -60.9500,                 loss: nan
env1_second_0:                 episode reward: 60.9500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 424.75,                last time consumption/overall running time: 514.0031s / 202098.1133 s
env0_first_0:                 episode reward: -50.1000,                 loss: 0.9938
env0_second_0:                 episode reward: 50.1000,                 loss: 0.6613
env1_first_0:                 episode reward: -67.3500,                 loss: nan
env1_second_0:                 episode reward: 67.3500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 455.95,                last time consumption/overall running time: 549.3807s / 202647.4940 s
env0_first_0:                 episode reward: -66.0500,                 loss: 1.0327
env0_second_0:                 episode reward: 66.0500,                 loss: 0.6793
env1_first_0:                 episode reward: -54.9500,                 loss: nan
env1_second_0:                 episode reward: 54.9500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 600.55,                last time consumption/overall running time: 734.8109s / 203382.3049 s
env0_first_0:                 episode reward: -51.2500,                 loss: 1.0591
env0_second_0:                 episode reward: 51.2500,                 loss: 0.6998
env1_first_0:                 episode reward: -61.3000,                 loss: nan
env1_second_0:                 episode reward: 61.3000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 472.6,                last time consumption/overall running time: 574.6599s / 203956.9648 s
env0_first_0:                 episode reward: -62.8500,                 loss: 1.1084
env0_second_0:                 episode reward: 62.8500,                 loss: 0.6890
env1_first_0:                 episode reward: -75.3500,                 loss: nan
env1_second_0:                 episode reward: 75.3500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 496.55,                last time consumption/overall running time: 602.3874s / 204559.3523 s
env0_first_0:                 episode reward: -63.1500,                 loss: 1.0867
env0_second_0:                 episode reward: 63.1500,                 loss: 0.6783
env1_first_0:                 episode reward: -59.6000,                 loss: nan
env1_second_0:                 episode reward: 59.6000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 530.75,                last time consumption/overall running time: 649.9891s / 205209.3414 s
env0_first_0:                 episode reward: -65.8000,                 loss: 1.0382
env0_second_0:                 episode reward: 65.8000,                 loss: 0.6964
env1_first_0:                 episode reward: -51.7000,                 loss: nan
env1_second_0:                 episode reward: 51.7000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 509.35,                last time consumption/overall running time: 626.1339s / 205835.4753 s
env0_first_0:                 episode reward: -58.5500,                 loss: 1.0403
env0_second_0:                 episode reward: 58.5500,                 loss: 0.6975
env1_first_0:                 episode reward: -58.4500,                 loss: nan
env1_second_0:                 episode reward: 58.4500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 590.3,                last time consumption/overall running time: 722.2036s / 206557.6789 s
env0_first_0:                 episode reward: -41.7000,                 loss: 1.0140
env0_second_0:                 episode reward: 41.7000,                 loss: 0.7166
env1_first_0:                 episode reward: -60.4000,                 loss: nan
env1_second_0:                 episode reward: 60.4000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 376.05,                last time consumption/overall running time: 462.5394s / 207020.2183 s
env0_first_0:                 episode reward: -48.8500,                 loss: 0.9269
env0_second_0:                 episode reward: 48.8500,                 loss: 0.7715
env1_first_0:                 episode reward: -70.0500,                 loss: nan
env1_second_0:                 episode reward: 70.0500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 436.55,                last time consumption/overall running time: 533.1531s / 207553.3715 s
env0_first_0:                 episode reward: -38.2000,                 loss: 0.9129
env0_second_0:                 episode reward: 38.2000,                 loss: 0.7284
env1_first_0:                 episode reward: -76.9000,                 loss: nan
env1_second_0:                 episode reward: 76.9000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 528.15,                last time consumption/overall running time: 645.4157s / 208198.7871 s
env0_first_0:                 episode reward: -55.4000,                 loss: 0.9470
env0_second_0:                 episode reward: 55.4000,                 loss: 0.7267
env1_first_0:                 episode reward: -58.7500,                 loss: nan
env1_second_0:                 episode reward: 58.7500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 566.5,                last time consumption/overall running time: 692.3519s / 208891.1391 s
env0_first_0:                 episode reward: -44.2500,                 loss: 0.8652
env0_second_0:                 episode reward: 44.2500,                 loss: 0.7614
env1_first_0:                 episode reward: -43.5000,                 loss: nan
env1_second_0:                 episode reward: 43.5000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 591.65,                last time consumption/overall running time: 722.2864s / 209613.4255 s
env0_first_0:                 episode reward: -52.2500,                 loss: 0.8545
env0_second_0:                 episode reward: 52.2500,                 loss: 0.7579
env1_first_0:                 episode reward: -44.3500,                 loss: nan
env1_second_0:                 episode reward: 44.3500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 478.5,                last time consumption/overall running time: 587.2250s / 210200.6504 s
env0_first_0:                 episode reward: -50.0000,                 loss: 0.8505
env0_second_0:                 episode reward: 50.0000,                 loss: 0.7101
env1_first_0:                 episode reward: -62.6000,                 loss: nan
env1_second_0:                 episode reward: 62.6000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 735.15,                last time consumption/overall running time: 903.5613s / 211104.2118 s
env0_first_0:                 episode reward: -40.5000,                 loss: 0.8426
env0_second_0:                 episode reward: 40.5000,                 loss: 0.7128
env1_first_0:                 episode reward: -44.4000,                 loss: nan
env1_second_0:                 episode reward: 44.4000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 608.45,                last time consumption/overall running time: 745.7638s / 211849.9756 s
env0_first_0:                 episode reward: -64.7000,                 loss: 0.7793
env0_second_0:                 episode reward: 64.7000,                 loss: 0.6829
env1_first_0:                 episode reward: -54.4500,                 loss: nan
env1_second_0:                 episode reward: 54.4500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 559.2,                last time consumption/overall running time: 686.5448s / 212536.5204 s
env0_first_0:                 episode reward: -58.6500,                 loss: 0.7390
env0_second_0:                 episode reward: 58.6500,                 loss: 0.6933
env1_first_0:                 episode reward: -47.4000,                 loss: nan
env1_second_0:                 episode reward: 47.4000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 531.85,                last time consumption/overall running time: 651.5426s / 213188.0630 s
env0_first_0:                 episode reward: -63.4000,                 loss: 0.7775
env0_second_0:                 episode reward: 63.4000,                 loss: 0.7200
env1_first_0:                 episode reward: -50.2000,                 loss: nan
env1_second_0:                 episode reward: 50.2000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 593.45,                last time consumption/overall running time: 725.0574s / 213913.1204 s
env0_first_0:                 episode reward: -48.7000,                 loss: 0.7916
env0_second_0:                 episode reward: 48.7000,                 loss: 0.7814
env1_first_0:                 episode reward: -59.9500,                 loss: nan
env1_second_0:                 episode reward: 59.9500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 632.7,                last time consumption/overall running time: 765.0305s / 214678.1509 s
env0_first_0:                 episode reward: -58.8500,                 loss: 0.9051
env0_second_0:                 episode reward: 58.8500,                 loss: 0.7874
env1_first_0:                 episode reward: -44.4500,                 loss: nan
env1_second_0:                 episode reward: 44.4500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 539.15,                last time consumption/overall running time: 654.2356s / 215332.3865 s
env0_first_0:                 episode reward: -59.9000,                 loss: 0.9455
env0_second_0:                 episode reward: 59.9000,                 loss: 0.7901
env1_first_0:                 episode reward: -42.0500,                 loss: nan
env1_second_0:                 episode reward: 42.0500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 509.55,                last time consumption/overall running time: 617.1635s / 215949.5500 s
env0_first_0:                 episode reward: -71.7500,                 loss: 1.0447
env0_second_0:                 episode reward: 71.7500,                 loss: 0.8061
env1_first_0:                 episode reward: -48.8500,                 loss: nan
env1_second_0:                 episode reward: 48.8500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 564.4,                last time consumption/overall running time: 688.7193s / 216638.2693 s
env0_first_0:                 episode reward: -62.0500,                 loss: 1.0788
env0_second_0:                 episode reward: 62.0500,                 loss: 0.8358
env1_first_0:                 episode reward: -56.4500,                 loss: nan
env1_second_0:                 episode reward: 56.4500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 557.25,                last time consumption/overall running time: 665.7899s / 217304.0592 s
env0_first_0:                 episode reward: -42.2500,                 loss: 1.0670
env0_second_0:                 episode reward: 42.2500,                 loss: 0.8078
env1_first_0:                 episode reward: -62.4500,                 loss: nan
env1_second_0:                 episode reward: 62.4500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 499.1,                last time consumption/overall running time: 595.1877s / 217899.2469 s
env0_first_0:                 episode reward: -54.4000,                 loss: 1.1869
env0_second_0:                 episode reward: 54.4000,                 loss: 0.8060
env1_first_0:                 episode reward: -70.9500,                 loss: nan
env1_second_0:                 episode reward: 70.9500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 524.6,                last time consumption/overall running time: 627.1278s / 218526.3747 s
env0_first_0:                 episode reward: -52.9000,                 loss: 1.1912
env0_second_0:                 episode reward: 52.9000,                 loss: 0.8180
env1_first_0:                 episode reward: -54.5500,                 loss: nan
env1_second_0:                 episode reward: 54.5500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 487.8,                last time consumption/overall running time: 588.6638s / 219115.0386 s