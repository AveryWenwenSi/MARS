pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
SlimeVolley-v0 slimevolley
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
random seed: 37
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7f1b25322b70>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
No agent are not learnable.
{'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 1, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000, 'exploiter_update_itr': 1}, 'num_process': 5, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f1b2532a1d0>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
random seed: 37
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 1, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000, 'exploiter_update_itr': 1}, 'num_process': 5, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f5583c26eb8>}}
Save models to : /home/zihan/research/MARS/data/model/4/slimevolley_SlimeVolley-v0_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/4/slimevolley_SlimeVolley-v0_nash_dqn_exploiter.
Process ID: 4, episode: 20/10000 (0.2000%),                     avg. length: 685.3,                    last time consumption/overall running time: 33.2472s / 33.2472 s
first_0:                     episode reward: -0.5500
second_0:                     episode reward: 0.5500
Process ID: 4, episode: 40/10000 (0.4000%),                     avg. length: 590.0,                    last time consumption/overall running time: 29.5739s / 62.8212 s
first_0:                     episode reward: -0.0500
second_0:                     episode reward: 0.0500
Process ID: 4, episode: 60/10000 (0.6000%),                     avg. length: 641.9,                    last time consumption/overall running time: 32.9393s / 95.7605 s
first_0:                     episode reward: -0.6500
second_0:                     episode reward: 0.6500
Process ID: 4, episode: 80/10000 (0.8000%),                     avg. length: 617.55,                    last time consumption/overall running time: 30.6174s / 126.3778 s
first_0:                     episode reward: 0.0500
second_0:                     episode reward: -0.0500
Process ID: 4, episode: 100/10000 (1.0000%),                     avg. length: 630.25,                    last time consumption/overall running time: 31.8086s / 158.1864 s
first_0:                     episode reward: -0.3500
second_0:                     episode reward: 0.3500
Process ID: 4, episode: 120/10000 (1.2000%),                     avg. length: 652.2,                    last time consumption/overall running time: 35.0182s / 193.2046 s
first_0:                     episode reward: 0.1000
second_0:                     episode reward: -0.1000
Process ID: 4, episode: 140/10000 (1.4000%),                     avg. length: 594.5,                    last time consumption/overall running time: 32.1236s / 225.3283 s
first_0:                     episode reward: -0.3500
second_0:                     episode reward: 0.3500
Process ID: 4, episode: 160/10000 (1.6000%),                     avg. length: 637.35,                    last time consumption/overall running time: 32.5233s / 257.8515 s
first_0:                     episode reward: 0.1000
second_0:                     episode reward: -0.1000
Process ID: 4, episode: 180/10000 (1.8000%),                     avg. length: 663.6,                    last time consumption/overall running time: 34.6332s / 292.4847 s
first_0:                     episode reward: 0.4000
second_0:                     episode reward: -0.4000
Process ID: 4, episode: 200/10000 (2.0000%),                     avg. length: 664.5,                    last time consumption/overall running time: 36.3489s / 328.8336 s
first_0:                     episode reward: -0.7000
second_0:                     episode reward: 0.7000
Process ID: 4, episode: 220/10000 (2.2000%),                     avg. length: 638.6,                    last time consumption/overall running time: 34.8517s / 363.6853 s
first_0:                     episode reward: -1.0500
second_0:                     episode reward: 1.0500
Process ID: 4, episode: 240/10000 (2.4000%),                     avg. length: 636.15,                    last time consumption/overall running time: 35.8308s / 399.5161 s
first_0:                     episode reward: -0.3500
second_0:                     episode reward: 0.3500
Process ID: 4, episode: 260/10000 (2.6000%),                     avg. length: 616.4,                    last time consumption/overall running time: 36.5601s / 436.0763 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 4, episode: 280/10000 (2.8000%),                     avg. length: 617.9,                    last time consumption/overall running time: 35.1669s / 471.2432 s
first_0:                     episode reward: 0.3500
second_0:                     episode reward: -0.3500
Process ID: 4, episode: 300/10000 (3.0000%),                     avg. length: 634.75,                    last time consumption/overall running time: 37.2280s / 508.4712 s
first_0:                     episode reward: 0.3500
second_0:                     episode reward: -0.3500
Process ID: 4, episode: 320/10000 (3.2000%),                     avg. length: 647.15,                    last time consumption/overall running time: 37.5564s / 546.0275 s
first_0:                     episode reward: -0.5000
second_0:                     episode reward: 0.5000
Process ID: 4, episode: 340/10000 (3.4000%),                     avg. length: 602.4,                    last time consumption/overall running time: 36.7011s / 582.7286 s
first_0:                     episode reward: -0.8000
second_0:                     episode reward: 0.8000
Process ID: 4, episode: 360/10000 (3.6000%),                     avg. length: 643.35,                    last time consumption/overall running time: 41.1468s / 623.8754 s
first_0:                     episode reward: 0.8500
second_0:                     episode reward: -0.8500
Process ID: 4, episode: 380/10000 (3.8000%),                     avg. length: 670.75,                    last time consumption/overall running time: 42.3458s / 666.2213 s
first_0:                     episode reward: -0.8500
second_0:                     episode reward: 0.8500
Process ID: 4, episode: 400/10000 (4.0000%),                     avg. length: 643.15,                    last time consumption/overall running time: 41.2095s / 707.4308 s
first_0:                     episode reward: 0.8500
second_0:                     episode reward: -0.8500
Process ID: 4, episode: 420/10000 (4.2000%),                     avg. length: 618.45,                    last time consumption/overall running time: 40.3590s / 747.7897 s
first_0:                     episode reward: 0.7500
second_0:                     episode reward: -0.7500
Process ID: 4, episode: 440/10000 (4.4000%),                     avg. length: 628.75,                    last time consumption/overall running time: 43.2212s / 791.0110 s
first_0:                     episode reward: 0.4000
second_0:                     episode reward: -0.4000
Process ID: 4, episode: 460/10000 (4.6000%),                     avg. length: 635.25,                    last time consumption/overall running time: 42.0021s / 833.0131 s
first_0:                     episode reward: -0.2500
second_0:                     episode reward: 0.2500
Process ID: 4, episode: 480/10000 (4.8000%),                     avg. length: 641.2,                    last time consumption/overall running time: 43.8228s / 876.8359 spygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
random seed: 37
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 1, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000, 'exploiter_update_itr': 1}, 'num_process': 5, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f5583c26eb8>}}
Save models to : /home/zihan/research/MARS/data/model/1/slimevolley_SlimeVolley-v0_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/1/slimevolley_SlimeVolley-v0_nash_dqn_exploiter.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 665.7,                    last time consumption/overall running time: 31.5526s / 31.5526 s
first_0:                     episode reward: -0.0500
second_0:                     episode reward: 0.0500
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 640.3,                    last time consumption/overall running time: 33.2682s / 64.8208 s
first_0:                     episode reward: -0.9000
second_0:                     episode reward: 0.9000
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 649.3,                    last time consumption/overall running time: 32.3430s / 97.1638 s
first_0:                     episode reward: -1.4000
second_0:                     episode reward: 1.4000
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 601.2,                    last time consumption/overall running time: 30.0669s / 127.2307 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 640.45,                    last time consumption/overall running time: 32.8137s / 160.0445 s
first_0:                     episode reward: -0.1000
second_0:                     episode reward: 0.1000
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 608.75,                    last time consumption/overall running time: 35.1039s / 195.1483 s
first_0:                     episode reward: -0.0500
second_0:                     episode reward: 0.0500
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 615.05,                    last time consumption/overall running time: 34.1718s / 229.3202 s
first_0:                     episode reward: 0.2000
second_0:                     episode reward: -0.2000
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 656.0,                    last time consumption/overall running time: 33.5544s / 262.8745 s
first_0:                     episode reward: 0.1000
second_0:                     episode reward: -0.1000
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 651.3,                    last time consumption/overall running time: 34.3976s / 297.2721 s
first_0:                     episode reward: 0.5000
second_0:                     episode reward: -0.5000
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 647.0,                    last time consumption/overall running time: 36.4357s / 333.7078 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 634.5,                    last time consumption/overall running time: 35.5923s / 369.3001 s
first_0:                     episode reward: -0.9000
second_0:                     episode reward: 0.9000
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 632.85,                    last time consumption/overall running time: 36.8718s / 406.1718 s
first_0:                     episode reward: -0.6000
second_0:                     episode reward: 0.6000
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 655.2,                    last time consumption/overall running time: 38.4565s / 444.6283 s
first_0:                     episode reward: 0.1000
second_0:                     episode reward: -0.1000
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 654.7,                    last time consumption/overall running time: 39.0534s / 483.6817 s
first_0:                     episode reward: 0.6000
second_0:                     episode reward: -0.6000
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 625.15,                    last time consumption/overall running time: 36.3976s / 520.0794 s
first_0:                     episode reward: 0.3000
second_0:                     episode reward: -0.3000
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 651.55,                    last time consumption/overall running time: 39.1825s / 559.2618 s
first_0:                     episode reward: -0.5000
second_0:                     episode reward: 0.5000
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 611.8,                    last time consumption/overall running time: 37.7846s / 597.0465 s
first_0:                     episode reward: -0.1500
second_0:                     episode reward: 0.1500
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 646.75,                    last time consumption/overall running time: 41.6279s / 638.6744 s
first_0:                     episode reward: -0.2500
second_0:                     episode reward: 0.2500
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 627.1,                    last time consumption/overall running time: 40.3027s / 678.9771 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 623.6,                    last time consumption/overall running time: 39.9478s / 718.9249 s
first_0:                     episode reward: 1.0500
second_0:                     episode reward: -1.0500
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 674.05,                    last time consumption/overall running time: 44.1125s / 763.0374 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 586.05,                    last time consumption/overall running time: 40.9201s / 803.9575 s
first_0:                     episode reward: 1.2500
second_0:                     episode reward: -1.2500
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 674.9,                    last time consumption/overall running time: 45.5238s / 849.4813 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 612.95,                    last time consumption/overall running time: 42.4902s / 891.9715 spygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
random seed: 37
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 1, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000, 'exploiter_update_itr': 1}, 'num_process': 5, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f5583c26eb8>}}
Save models to : /home/zihan/research/MARS/data/model/2/slimevolley_SlimeVolley-v0_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/2/slimevolley_SlimeVolley-v0_nash_dqn_exploiter.
Process ID: 2, episode: 20/10000 (0.2000%),                     avg. length: 642.7,                    last time consumption/overall running time: 30.7843s / 30.7843 s
first_0:                     episode reward: -0.0500
second_0:                     episode reward: 0.0500
Process ID: 2, episode: 40/10000 (0.4000%),                     avg. length: 622.45,                    last time consumption/overall running time: 33.0147s / 63.7990 s
first_0:                     episode reward: -0.4500
second_0:                     episode reward: 0.4500
Process ID: 2, episode: 60/10000 (0.6000%),                     avg. length: 638.75,                    last time consumption/overall running time: 32.7914s / 96.5904 s
first_0:                     episode reward: -0.7500
second_0:                     episode reward: 0.7500
Process ID: 2, episode: 80/10000 (0.8000%),                     avg. length: 628.25,                    last time consumption/overall running time: 30.4658s / 127.0562 s
first_0:                     episode reward: 0.2000
second_0:                     episode reward: -0.2000
Process ID: 2, episode: 100/10000 (1.0000%),                     avg. length: 647.2,                    last time consumption/overall running time: 32.6397s / 159.6959 s
first_0:                     episode reward: -0.2500
second_0:                     episode reward: 0.2500
Process ID: 2, episode: 120/10000 (1.2000%),                     avg. length: 630.0,                    last time consumption/overall running time: 33.9321s / 193.6281 s
first_0:                     episode reward: 0.1500
second_0:                     episode reward: -0.1500
Process ID: 2, episode: 140/10000 (1.4000%),                     avg. length: 581.6,                    last time consumption/overall running time: 31.9748s / 225.6028 s
first_0:                     episode reward: -0.1000
second_0:                     episode reward: 0.1000
Process ID: 2, episode: 160/10000 (1.6000%),                     avg. length: 645.75,                    last time consumption/overall running time: 33.6155s / 259.2183 s
first_0:                     episode reward: 0.6500
second_0:                     episode reward: -0.6500
Process ID: 2, episode: 180/10000 (1.8000%),                     avg. length: 663.45,                    last time consumption/overall running time: 34.9043s / 294.1226 s
first_0:                     episode reward: 0.5500
second_0:                     episode reward: -0.5500
Process ID: 2, episode: 200/10000 (2.0000%),                     avg. length: 678.6,                    last time consumption/overall running time: 38.3391s / 332.4617 s
first_0:                     episode reward: -0.5000
second_0:                     episode reward: 0.5000
Process ID: 2, episode: 220/10000 (2.2000%),                     avg. length: 597.0,                    last time consumption/overall running time: 33.4153s / 365.8770 s
first_0:                     episode reward: -1.8000
second_0:                     episode reward: 1.8000
Process ID: 2, episode: 240/10000 (2.4000%),                     avg. length: 674.8,                    last time consumption/overall running time: 39.1601s / 405.0371 s
first_0:                     episode reward: -0.6000
second_0:                     episode reward: 0.6000
Process ID: 2, episode: 260/10000 (2.6000%),                     avg. length: 638.55,                    last time consumption/overall running time: 37.8443s / 442.8814 s
first_0:                     episode reward: -0.4500
second_0:                     episode reward: 0.4500
Process ID: 2, episode: 280/10000 (2.8000%),                     avg. length: 662.75,                    last time consumption/overall running time: 40.9964s / 483.8778 s
first_0:                     episode reward: 0.7000
second_0:                     episode reward: -0.7000
Process ID: 2, episode: 300/10000 (3.0000%),                     avg. length: 638.25,                    last time consumption/overall running time: 37.6324s / 521.5102 s
first_0:                     episode reward: -0.1000
second_0:                     episode reward: 0.1000
Process ID: 2, episode: 320/10000 (3.2000%),                     avg. length: 669.15,                    last time consumption/overall running time: 40.3112s / 561.8213 s
first_0:                     episode reward: -0.2500
second_0:                     episode reward: 0.2500
Process ID: 2, episode: 340/10000 (3.4000%),                     avg. length: 639.75,                    last time consumption/overall running time: 39.7367s / 601.5580 s
first_0:                     episode reward: -0.2500
second_0:                     episode reward: 0.2500
Process ID: 2, episode: 360/10000 (3.6000%),                     avg. length: 644.75,                    last time consumption/overall running time: 40.6258s / 642.1838 s
first_0:                     episode reward: 0.6500
second_0:                     episode reward: -0.6500
Process ID: 2, episode: 380/10000 (3.8000%),                     avg. length: 639.45,                    last time consumption/overall running time: 39.4853s / 681.6691 s
first_0:                     episode reward: -0.2500
second_0:                     episode reward: 0.2500
Process ID: 2, episode: 400/10000 (4.0000%),                     avg. length: 701.85,                    last time consumption/overall running time: 45.3397s / 727.0088 s
first_0:                     episode reward: 0.4000
second_0:                     episode reward: -0.4000
Process ID: 2, episode: 420/10000 (4.2000%),                     avg. length: 675.4,                    last time consumption/overall running time: 45.0942s / 772.1030 s
first_0:                     episode reward: -0.2500
second_0:                     episode reward: 0.2500
Process ID: 2, episode: 440/10000 (4.4000%),                     avg. length: 674.85,                    last time consumption/overall running time: 46.4398s / 818.5428 s
first_0:                     episode reward: 1.1500
second_0:                     episode reward: -1.1500
Process ID: 2, episode: 460/10000 (4.6000%),                     avg. length: 629.2,                    last time consumption/overall running time: 42.5175s / 861.0603 s
first_0:                     episode reward: -0.5500
second_0:                     episode reward: 0.5500
Process ID: 2, episode: 480/10000 (4.8000%),                     avg. length: 622.2,                    last time consumption/overall running time: 43.2410s / 904.3013 spygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
random seed: 37
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 1, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000, 'exploiter_update_itr': 1}, 'num_process': 5, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f5583c26eb8>}}
Save models to : /home/zihan/research/MARS/data/model/0/slimevolley_SlimeVolley-v0_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/0/slimevolley_SlimeVolley-v0_nash_dqn_exploiter.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 683.3,                    last time consumption/overall running time: 34.1538s / 34.1538 s
first_0:                     episode reward: -0.2500
second_0:                     episode reward: 0.2500
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 594.65,                    last time consumption/overall running time: 30.5796s / 64.7335 s
first_0:                     episode reward: 0.2000
second_0:                     episode reward: -0.2000
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 665.7,                    last time consumption/overall running time: 34.2332s / 98.9666 s
first_0:                     episode reward: -0.9000
second_0:                     episode reward: 0.9000
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 663.4,                    last time consumption/overall running time: 34.3888s / 133.3554 s
first_0:                     episode reward: -0.0500
second_0:                     episode reward: 0.0500
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 635.05,                    last time consumption/overall running time: 33.4374s / 166.7928 s
first_0:                     episode reward: 0.2500
second_0:                     episode reward: -0.2500
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 607.85,                    last time consumption/overall running time: 33.5520s / 200.3448 s
first_0:                     episode reward: 0.8500
second_0:                     episode reward: -0.8500
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 626.9,                    last time consumption/overall running time: 34.5691s / 234.9138 s
first_0:                     episode reward: -0.4000
second_0:                     episode reward: 0.4000
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 648.25,                    last time consumption/overall running time: 34.0692s / 268.9830 s
first_0:                     episode reward: 0.2500
second_0:                     episode reward: -0.2500
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 602.75,                    last time consumption/overall running time: 33.0095s / 301.9925 s
first_0:                     episode reward: 0.4500
second_0:                     episode reward: -0.4500
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 641.1,                    last time consumption/overall running time: 36.5424s / 338.5349 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 636.9,                    last time consumption/overall running time: 36.1841s / 374.7190 s
first_0:                     episode reward: -0.6500
second_0:                     episode reward: 0.6500
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 633.8,                    last time consumption/overall running time: 38.1018s / 412.8208 s
first_0:                     episode reward: -0.6000
second_0:                     episode reward: 0.6000
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 633.8,                    last time consumption/overall running time: 38.5731s / 451.3939 s
first_0:                     episode reward: -0.1000
second_0:                     episode reward: 0.1000
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 637.9,                    last time consumption/overall running time: 39.6503s / 491.0442 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 656.65,                    last time consumption/overall running time: 40.1263s / 531.1705 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 637.8,                    last time consumption/overall running time: 39.9175s / 571.0881 s
first_0:                     episode reward: -0.0500
second_0:                     episode reward: 0.0500
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 639.05,                    last time consumption/overall running time: 40.5437s / 611.6317 s
first_0:                     episode reward: -0.5000
second_0:                     episode reward: 0.5000
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 631.15,                    last time consumption/overall running time: 41.9886s / 653.6203 s
first_0:                     episode reward: 0.4000
second_0:                     episode reward: -0.4000
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 650.25,                    last time consumption/overall running time: 41.9445s / 695.5648 s
first_0:                     episode reward: -0.2500
second_0:                     episode reward: 0.2500
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 645.6,                    last time consumption/overall running time: 42.5087s / 738.0735 s
first_0:                     episode reward: 0.5000
second_0:                     episode reward: -0.5000
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 614.25,                    last time consumption/overall running time: 42.5857s / 780.6591 s
first_0:                     episode reward: 0.3500
second_0:                     episode reward: -0.3500
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 602.95,                    last time consumption/overall running time: 41.9191s / 822.5782 s
first_0:                     episode reward: 1.1000
second_0:                     episode reward: -1.1000
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 631.8,                    last time consumption/overall running time: 44.8229s / 867.4012 s
first_0:                     episode reward: 0.0500
second_0:                     episode reward: -0.0500
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 631.05,                    last time consumption/overall running time: 43.9985s / 911.3996 spygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
random seed: 37
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 1, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000, 'exploiter_update_itr': 1}, 'num_process': 5, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f5583c26eb8>}}
Save models to : /home/zihan/research/MARS/data/model/3/slimevolley_SlimeVolley-v0_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/3/slimevolley_SlimeVolley-v0_nash_dqn_exploiter.
Process ID: 3, episode: 20/10000 (0.2000%),                     avg. length: 647.85,                    last time consumption/overall running time: 32.2047s / 32.2047 s
first_0:                     episode reward: 0.0500
second_0:                     episode reward: -0.0500
Process ID: 3, episode: 40/10000 (0.4000%),                     avg. length: 594.85,                    last time consumption/overall running time: 31.7715s / 63.9763 s
first_0:                     episode reward: -0.4500
second_0:                     episode reward: 0.4500
Process ID: 3, episode: 60/10000 (0.6000%),                     avg. length: 685.4,                    last time consumption/overall running time: 36.1585s / 100.1348 s
first_0:                     episode reward: -1.4000
second_0:                     episode reward: 1.4000
Process ID: 3, episode: 80/10000 (0.8000%),                     avg. length: 639.9,                    last time consumption/overall running time: 32.8974s / 133.0322 s
first_0:                     episode reward: -0.2000
second_0:                     episode reward: 0.2000
Process ID: 3, episode: 100/10000 (1.0000%),                     avg. length: 657.7,                    last time consumption/overall running time: 34.8258s / 167.8579 s
first_0:                     episode reward: -0.2500
second_0:                     episode reward: 0.2500
Process ID: 3, episode: 120/10000 (1.2000%),                     avg. length: 648.65,                    last time consumption/overall running time: 36.4872s / 204.3451 s
first_0:                     episode reward: 0.4000
second_0:                     episode reward: -0.4000
Process ID: 3, episode: 140/10000 (1.4000%),                     avg. length: 589.5,                    last time consumption/overall running time: 31.8174s / 236.1625 s
first_0:                     episode reward: -0.4500
second_0:                     episode reward: 0.4500
Process ID: 3, episode: 160/10000 (1.6000%),                     avg. length: 650.8,                    last time consumption/overall running time: 35.3549s / 271.5174 s
first_0:                     episode reward: 0.3500
second_0:                     episode reward: -0.3500
Process ID: 3, episode: 180/10000 (1.8000%),                     avg. length: 630.3,                    last time consumption/overall running time: 34.5685s / 306.0858 s
first_0:                     episode reward: 0.4000
second_0:                     episode reward: -0.4000
Process ID: 3, episode: 200/10000 (2.0000%),                     avg. length: 651.0,                    last time consumption/overall running time: 37.3338s / 343.4197 s
first_0:                     episode reward: -0.8500
second_0:                     episode reward: 0.8500
Process ID: 3, episode: 220/10000 (2.2000%),                     avg. length: 622.95,                    last time consumption/overall running time: 36.4702s / 379.8899 s
first_0:                     episode reward: -0.6500
second_0:                     episode reward: 0.6500
Process ID: 3, episode: 240/10000 (2.4000%),                     avg. length: 663.65,                    last time consumption/overall running time: 39.5563s / 419.4462 s
first_0:                     episode reward: -0.5500
second_0:                     episode reward: 0.5500
Process ID: 3, episode: 260/10000 (2.6000%),                     avg. length: 640.95,                    last time consumption/overall running time: 38.0758s / 457.5220 s
first_0:                     episode reward: -0.1000
second_0:                     episode reward: 0.1000
Process ID: 3, episode: 280/10000 (2.8000%),                     avg. length: 654.95,                    last time consumption/overall running time: 38.2408s / 495.7627 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 3, episode: 300/10000 (3.0000%),                     avg. length: 644.45,                    last time consumption/overall running time: 38.9331s / 534.6959 s
first_0:                     episode reward: 0.7000
second_0:                     episode reward: -0.7000
Process ID: 3, episode: 320/10000 (3.2000%),                     avg. length: 651.55,                    last time consumption/overall running time: 40.0751s / 574.7710 s
first_0:                     episode reward: -0.4000
second_0:                     episode reward: 0.4000
Process ID: 3, episode: 340/10000 (3.4000%),                     avg. length: 564.15,                    last time consumption/overall running time: 36.3290s / 611.1000 s
first_0:                     episode reward: -0.4000
second_0:                     episode reward: 0.4000
Process ID: 3, episode: 360/10000 (3.6000%),                     avg. length: 657.65,                    last time consumption/overall running time: 43.5099s / 654.6099 s
first_0:                     episode reward: 0.8500
second_0:                     episode reward: -0.8500
Process ID: 3, episode: 380/10000 (3.8000%),                     avg. length: 624.95,                    last time consumption/overall running time: 39.6048s / 694.2147 s
first_0:                     episode reward: -0.9500
second_0:                     episode reward: 0.9500
Process ID: 3, episode: 400/10000 (4.0000%),                     avg. length: 639.95,                    last time consumption/overall running time: 41.4561s / 735.6708 s
first_0:                     episode reward: 0.8000
second_0:                     episode reward: -0.8000
Process ID: 3, episode: 420/10000 (4.2000%),                     avg. length: 630.45,                    last time consumption/overall running time: 43.3851s / 779.0560 s
first_0:                     episode reward: -0.2500
second_0:                     episode reward: 0.2500
Process ID: 3, episode: 440/10000 (4.4000%),                     avg. length: 585.7,                    last time consumption/overall running time: 41.1660s / 820.2220 s
first_0:                     episode reward: 0.8000
second_0:                     episode reward: -0.8000
Process ID: 3, episode: 460/10000 (4.6000%),                     avg. length: 675.85,                    last time consumption/overall running time: 47.0407s / 867.2627 s
first_0:                     episode reward: 0.2000
second_0:                     episode reward: -0.2000
Process ID: 3, episode: 480/10000 (4.8000%),                     avg. length: 633.85,                    last time consumption/overall running time: 45.1279s / 912.3905 s