pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
SlimeVolley-v0 slimevolley
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
random seed: 37
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7f5551d5bb70>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
No agent are not learnable.
{'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 1, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000}, 'num_process': 5, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f5551d48b38>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
random seed: 37
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 1, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000}, 'num_process': 5, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7ff85d4a2e10>}}
Save models to : /home/zihan/research/MARS/data/model/1/slimevolley_SlimeVolley-v0_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/1/slimevolley_SlimeVolley-v0_nash_dqn.
Process ID: 1, episode: 20/50000 (0.0400%),                     avg. length: 678.3,                    last time consumption/overall running time: 38.1698s / 38.1698 s
first_0:                     episode reward: -0.1500
second_0:                     episode reward: 0.1500
Process ID: 1, episode: 40/50000 (0.0800%),                     avg. length: 620.4,                    last time consumption/overall running time: 42.7393s / 80.9091 s
first_0:                     episode reward: -0.7500
second_0:                     episode reward: 0.7500
Process ID: 1, episode: 60/50000 (0.1200%),                     avg. length: 634.35,                    last time consumption/overall running time: 38.0685s / 118.9776 s
first_0:                     episode reward: -0.8500
second_0:                     episode reward: 0.8500
Process ID: 1, episode: 80/50000 (0.1600%),                     avg. length: 620.5,                    last time consumption/overall running time: 34.8704s / 153.8480 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 1, episode: 100/50000 (0.2000%),                     avg. length: 609.7,                    last time consumption/overall running time: 36.7717s / 190.6196 s
first_0:                     episode reward: 0.2500
second_0:                     episode reward: -0.2500
Process ID: 1, episode: 120/50000 (0.2400%),                     avg. length: 637.35,                    last time consumption/overall running time: 36.0799s / 226.6996 s
first_0:                     episode reward: -0.1500
second_0:                     episode reward: 0.1500
Process ID: 1, episode: 140/50000 (0.2800%),                     avg. length: 617.7,                    last time consumption/overall running time: 38.2911s / 264.9907 s
first_0:                     episode reward: -0.5500
second_0:                     episode reward: 0.5500
Process ID: 1, episode: 160/50000 (0.3200%),                     avg. length: 655.75,                    last time consumption/overall running time: 37.9311s / 302.9219 s
first_0:                     episode reward: 0.4500
second_0:                     episode reward: -0.4500
Process ID: 1, episode: 180/50000 (0.3600%),                     avg. length: 657.2,                    last time consumption/overall running time: 37.8446s / 340.7665 s
first_0:                     episode reward: 0.4500
second_0:                     episode reward: -0.4500
Process ID: 1, episode: 200/50000 (0.4000%),                     avg. length: 660.25,                    last time consumption/overall running time: 37.2588s / 378.0253 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 1, episode: 220/50000 (0.4400%),                     avg. length: 635.2,                    last time consumption/overall running time: 36.9548s / 414.9801 s
first_0:                     episode reward: -0.6500
second_0:                     episode reward: 0.6500
Process ID: 1, episode: 240/50000 (0.4800%),                     avg. length: 613.05,                    last time consumption/overall running time: 37.4321s / 452.4122 s
first_0:                     episode reward: -0.8500
second_0:                     episode reward: 0.8500
Process ID: 1, episode: 260/50000 (0.5200%),                     avg. length: 626.25,                    last time consumption/overall running time: 38.7129s / 491.1250 s
first_0:                     episode reward: 0.4000
second_0:                     episode reward: -0.4000
Process ID: 1, episode: 280/50000 (0.5600%),                     avg. length: 649.05,                    last time consumption/overall running time: 41.8672s / 532.9922 s
first_0:                     episode reward: 0.3000
second_0:                     episode reward: -0.3000
Process ID: 1, episode: 300/50000 (0.6000%),                     avg. length: 622.5,                    last time consumption/overall running time: 38.6790s / 571.6712 s
first_0:                     episode reward: -0.1000
second_0:                     episode reward: 0.1000
Process ID: 1, episode: 320/50000 (0.6400%),                     avg. length: 599.3,                    last time consumption/overall running time: 35.9771s / 607.6484 s
first_0:                     episode reward: -0.8000
second_0:                     episode reward: 0.8000
Process ID: 1, episode: 340/50000 (0.6800%),                     avg. length: 656.5,                    last time consumption/overall running time: 39.3089s / 646.9573 s
first_0:                     episode reward: 0.1000
second_0:                     episode reward: -0.1000
Process ID: 1, episode: 360/50000 (0.7200%),                     avg. length: 588.0,                    last time consumption/overall running time: 38.9932s / 685.9504 s
first_0:                     episode reward: 0.4000
second_0:                     episode reward: -0.4000
Process ID: 1, episode: 380/50000 (0.7600%),                     avg. length: 641.9,                    last time consumption/overall running time: 45.9254s / 731.8758 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 1, episode: 400/50000 (0.8000%),                     avg. length: 648.2,                    last time consumption/overall running time: 45.8192s / 777.6950 s
first_0:                     episode reward: 0.7500
second_0:                     episode reward: -0.7500
Process ID: 1, episode: 420/50000 (0.8400%),                     avg. length: 635.9,                    last time consumption/overall running time: 42.6192s / 820.3142 s
first_0:                     episode reward: 0.6500
second_0:                     episode reward: -0.6500
Process ID: 1, episode: 440/50000 (0.8800%),                     avg. length: 633.05,                    last time consumption/overall running time: 42.1801s / 862.4942 s
first_0:                     episode reward: 0.9000
second_0:                     episode reward: -0.9000
Process ID: 1, episode: 460/50000 (0.9200%),                     avg. length: 648.1,                    last time consumption/overall running time: 43.9013s / 906.3955 s
first_0:                     episode reward: 0.1500
second_0:                     episode reward: -0.1500
Process ID: 1, episode: 480/50000 (0.9600%),                     avg. length: 639.45,                    last time consumption/overall running time: 43.8146s / 950.2101 s
first_0:                     episode reward: -1.0500pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
random seed: 37
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 1, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000}, 'num_process': 5, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7ff85d4a2e10>}}
Save models to : /home/zihan/research/MARS/data/model/0/slimevolley_SlimeVolley-v0_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/0/slimevolley_SlimeVolley-v0_nash_dqn.
Process ID: 0, episode: 20/50000 (0.0400%),                     avg. length: 690.95,                    last time consumption/overall running time: 38.5858s / 38.5858 s
first_0:                     episode reward: 0.2500
second_0:                     episode reward: -0.2500
Process ID: 0, episode: 40/50000 (0.0800%),                     avg. length: 597.25,                    last time consumption/overall running time: 40.7768s / 79.3627 s
first_0:                     episode reward: 0.1500
second_0:                     episode reward: -0.1500
Process ID: 0, episode: 60/50000 (0.1200%),                     avg. length: 653.0,                    last time consumption/overall running time: 40.3855s / 119.7481 s
first_0:                     episode reward: -0.6000
second_0:                     episode reward: 0.6000
Process ID: 0, episode: 80/50000 (0.1600%),                     avg. length: 605.05,                    last time consumption/overall running time: 34.0432s / 153.7914 s
first_0:                     episode reward: 0.3500
second_0:                     episode reward: -0.3500
Process ID: 0, episode: 100/50000 (0.2000%),                     avg. length: 625.7,                    last time consumption/overall running time: 38.4154s / 192.2068 s
first_0:                     episode reward: -0.1000
second_0:                     episode reward: 0.1000
Process ID: 0, episode: 120/50000 (0.2400%),                     avg. length: 617.05,                    last time consumption/overall running time: 36.7799s / 228.9867 s
first_0:                     episode reward: 0.3500
second_0:                     episode reward: -0.3500
Process ID: 0, episode: 140/50000 (0.2800%),                     avg. length: 625.5,                    last time consumption/overall running time: 41.5573s / 270.5440 s
first_0:                     episode reward: -0.4000
second_0:                     episode reward: 0.4000
Process ID: 0, episode: 160/50000 (0.3200%),                     avg. length: 659.4,                    last time consumption/overall running time: 38.4671s / 309.0111 s
first_0:                     episode reward: 0.4000
second_0:                     episode reward: -0.4000
Process ID: 0, episode: 180/50000 (0.3600%),                     avg. length: 631.85,                    last time consumption/overall running time: 36.5347s / 345.5457 s
first_0:                     episode reward: 0.2000
second_0:                     episode reward: -0.2000
Process ID: 0, episode: 200/50000 (0.4000%),                     avg. length: 632.65,                    last time consumption/overall running time: 37.5155s / 383.0612 s
first_0:                     episode reward: 0.1000
second_0:                     episode reward: -0.1000
Process ID: 0, episode: 220/50000 (0.4400%),                     avg. length: 620.75,                    last time consumption/overall running time: 37.3498s / 420.4110 s
first_0:                     episode reward: -0.9500
second_0:                     episode reward: 0.9500
Process ID: 0, episode: 240/50000 (0.4800%),                     avg. length: 628.85,                    last time consumption/overall running time: 38.8555s / 459.2665 s
first_0:                     episode reward: -1.2500
second_0:                     episode reward: 1.2500
Process ID: 0, episode: 260/50000 (0.5200%),                     avg. length: 632.7,                    last time consumption/overall running time: 40.5467s / 499.8132 s
first_0:                     episode reward: 0.3500
second_0:                     episode reward: -0.3500
Process ID: 0, episode: 280/50000 (0.5600%),                     avg. length: 636.55,                    last time consumption/overall running time: 41.4202s / 541.2335 s
first_0:                     episode reward: 0.3500
second_0:                     episode reward: -0.3500
Process ID: 0, episode: 300/50000 (0.6000%),                     avg. length: 593.55,                    last time consumption/overall running time: 37.9351s / 579.1686 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 0, episode: 320/50000 (0.6400%),                     avg. length: 661.3,                    last time consumption/overall running time: 41.4485s / 620.6171 s
first_0:                     episode reward: -0.2500
second_0:                     episode reward: 0.2500
Process ID: 0, episode: 340/50000 (0.6800%),                     avg. length: 587.2,                    last time consumption/overall running time: 36.1431s / 656.7602 s
first_0:                     episode reward: -0.2000
second_0:                     episode reward: 0.2000
Process ID: 0, episode: 360/50000 (0.7200%),                     avg. length: 587.65,                    last time consumption/overall running time: 41.4321s / 698.1923 s
first_0:                     episode reward: 0.6500
second_0:                     episode reward: -0.6500
Process ID: 0, episode: 380/50000 (0.7600%),                     avg. length: 624.0,                    last time consumption/overall running time: 45.5107s / 743.7030 s
first_0:                     episode reward: 0.3500
second_0:                     episode reward: -0.3500
Process ID: 0, episode: 400/50000 (0.8000%),                     avg. length: 657.4,                    last time consumption/overall running time: 45.7595s / 789.4625 s
first_0:                     episode reward: 0.7000
second_0:                     episode reward: -0.7000
Process ID: 0, episode: 420/50000 (0.8400%),                     avg. length: 624.05,                    last time consumption/overall running time: 41.9491s / 831.4116 s
first_0:                     episode reward: 0.9000
second_0:                     episode reward: -0.9000
Process ID: 0, episode: 440/50000 (0.8800%),                     avg. length: 626.3,                    last time consumption/overall running time: 42.8619s / 874.2735 s
first_0:                     episode reward: 0.8500
second_0:                     episode reward: -0.8500
Process ID: 0, episode: 460/50000 (0.9200%),                     avg. length: 637.55,                    last time consumption/overall running time: 43.9187s / 918.1922 s
first_0:                     episode reward: 0.7500
second_0:                     episode reward: -0.7500
Process ID: 0, episode: 480/50000 (0.9600%),                     avg. length: 656.9,                    last time consumption/overall running time: 46.9551s / 965.1473 s
first_0:                     episode reward: -0.4000pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
random seed: 37
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 1, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000}, 'num_process': 5, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7ff85d4a2e10>}}
Save models to : /home/zihan/research/MARS/data/model/3/slimevolley_SlimeVolley-v0_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/3/slimevolley_SlimeVolley-v0_nash_dqn.
Process ID: 3, episode: 20/50000 (0.0400%),                     avg. length: 673.05,                    last time consumption/overall running time: 40.0575s / 40.0575 s
first_0:                     episode reward: -0.0500
second_0:                     episode reward: 0.0500
Process ID: 3, episode: 40/50000 (0.0800%),                     avg. length: 599.35,                    last time consumption/overall running time: 41.7070s / 81.7645 s
first_0:                     episode reward: -0.4500
second_0:                     episode reward: 0.4500
Process ID: 3, episode: 60/50000 (0.1200%),                     avg. length: 640.0,                    last time consumption/overall running time: 38.8440s / 120.6085 s
first_0:                     episode reward: -1.2000
second_0:                     episode reward: 1.2000
Process ID: 3, episode: 80/50000 (0.1600%),                     avg. length: 626.2,                    last time consumption/overall running time: 35.2742s / 155.8827 s
first_0:                     episode reward: -0.3500
second_0:                     episode reward: 0.3500
Process ID: 3, episode: 100/50000 (0.2000%),                     avg. length: 658.1,                    last time consumption/overall running time: 40.2884s / 196.1711 s
first_0:                     episode reward: -0.2000
second_0:                     episode reward: 0.2000
Process ID: 3, episode: 120/50000 (0.2400%),                     avg. length: 642.5,                    last time consumption/overall running time: 36.9059s / 233.0770 s
first_0:                     episode reward: -0.4500
second_0:                     episode reward: 0.4500
Process ID: 3, episode: 140/50000 (0.2800%),                     avg. length: 649.8,                    last time consumption/overall running time: 42.9269s / 276.0039 s
first_0:                     episode reward: 0.1500
second_0:                     episode reward: -0.1500
Process ID: 3, episode: 160/50000 (0.3200%),                     avg. length: 615.7,                    last time consumption/overall running time: 36.3623s / 312.3662 s
first_0:                     episode reward: 1.2000
second_0:                     episode reward: -1.2000
Process ID: 3, episode: 180/50000 (0.3600%),                     avg. length: 660.95,                    last time consumption/overall running time: 38.7870s / 351.1532 s
first_0:                     episode reward: 0.5500
second_0:                     episode reward: -0.5500
Process ID: 3, episode: 200/50000 (0.4000%),                     avg. length: 647.35,                    last time consumption/overall running time: 38.6791s / 389.8323 s
first_0:                     episode reward: -0.4000
second_0:                     episode reward: 0.4000
Process ID: 3, episode: 220/50000 (0.4400%),                     avg. length: 671.9,                    last time consumption/overall running time: 39.9346s / 429.7669 s
first_0:                     episode reward: -0.7000
second_0:                     episode reward: 0.7000
Process ID: 3, episode: 240/50000 (0.4800%),                     avg. length: 624.25,                    last time consumption/overall running time: 37.6193s / 467.3862 s
first_0:                     episode reward: -1.4500
second_0:                     episode reward: 1.4500
Process ID: 3, episode: 260/50000 (0.5200%),                     avg. length: 603.95,                    last time consumption/overall running time: 40.2833s / 507.6695 s
first_0:                     episode reward: -0.0500
second_0:                     episode reward: 0.0500
Process ID: 3, episode: 280/50000 (0.5600%),                     avg. length: 665.5,                    last time consumption/overall running time: 43.7245s / 551.3940 s
first_0:                     episode reward: 0.2500
second_0:                     episode reward: -0.2500
Process ID: 3, episode: 300/50000 (0.6000%),                     avg. length: 613.5,                    last time consumption/overall running time: 38.0958s / 589.4899 s
first_0:                     episode reward: 0.1500
second_0:                     episode reward: -0.1500
Process ID: 3, episode: 320/50000 (0.6400%),                     avg. length: 601.4,                    last time consumption/overall running time: 37.0826s / 626.5725 s
first_0:                     episode reward: -0.4500
second_0:                     episode reward: 0.4500
Process ID: 3, episode: 340/50000 (0.6800%),                     avg. length: 593.8,                    last time consumption/overall running time: 37.0344s / 663.6069 s
first_0:                     episode reward: -0.5500
second_0:                     episode reward: 0.5500
Process ID: 3, episode: 360/50000 (0.7200%),                     avg. length: 606.3,                    last time consumption/overall running time: 44.5078s / 708.1147 s
first_0:                     episode reward: 0.7000
second_0:                     episode reward: -0.7000
Process ID: 3, episode: 380/50000 (0.7600%),                     avg. length: 603.7,                    last time consumption/overall running time: 44.6629s / 752.7777 s
first_0:                     episode reward: -0.5500
second_0:                     episode reward: 0.5500
Process ID: 3, episode: 400/50000 (0.8000%),                     avg. length: 632.35,                    last time consumption/overall running time: 46.6856s / 799.4633 s
first_0:                     episode reward: 0.3000
second_0:                     episode reward: -0.3000
Process ID: 3, episode: 420/50000 (0.8400%),                     avg. length: 608.9,                    last time consumption/overall running time: 41.3287s / 840.7920 s
first_0:                     episode reward: 1.3000
second_0:                     episode reward: -1.3000
Process ID: 3, episode: 440/50000 (0.8800%),                     avg. length: 594.9,                    last time consumption/overall running time: 41.0293s / 881.8213 s
first_0:                     episode reward: 0.9000
second_0:                     episode reward: -0.9000
Process ID: 3, episode: 460/50000 (0.9200%),                     avg. length: 621.55,                    last time consumption/overall running time: 42.8617s / 924.6829 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 3, episode: 480/50000 (0.9600%),                     avg. length: 668.45,                    last time consumption/overall running time: 48.2404s / 972.9234 s
first_0:                     episode reward: -0.5000pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
random seed: 37
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 1, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000}, 'num_process': 5, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7ff85d4a2e10>}}
Save models to : /home/zihan/research/MARS/data/model/2/slimevolley_SlimeVolley-v0_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/2/slimevolley_SlimeVolley-v0_nash_dqn.
Process ID: 2, episode: 20/50000 (0.0400%),                     avg. length: 676.05,                    last time consumption/overall running time: 36.1988s / 36.1988 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 2, episode: 40/50000 (0.0800%),                     avg. length: 606.25,                    last time consumption/overall running time: 41.6332s / 77.8320 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 2, episode: 60/50000 (0.1200%),                     avg. length: 653.65,                    last time consumption/overall running time: 41.3514s / 119.1834 s
first_0:                     episode reward: -0.7000
second_0:                     episode reward: 0.7000
Process ID: 2, episode: 80/50000 (0.1600%),                     avg. length: 632.1,                    last time consumption/overall running time: 36.0947s / 155.2781 s
first_0:                     episode reward: -0.1500
second_0:                     episode reward: 0.1500
Process ID: 2, episode: 100/50000 (0.2000%),                     avg. length: 613.0,                    last time consumption/overall running time: 37.9048s / 193.1829 s
first_0:                     episode reward: 0.3000
second_0:                     episode reward: -0.3000
Process ID: 2, episode: 120/50000 (0.2400%),                     avg. length: 649.2,                    last time consumption/overall running time: 38.4542s / 231.6371 s
first_0:                     episode reward: -0.4500
second_0:                     episode reward: 0.4500
Process ID: 2, episode: 140/50000 (0.2800%),                     avg. length: 628.75,                    last time consumption/overall running time: 41.7050s / 273.3421 s
first_0:                     episode reward: -0.9000
second_0:                     episode reward: 0.9000
Process ID: 2, episode: 160/50000 (0.3200%),                     avg. length: 643.4,                    last time consumption/overall running time: 38.6726s / 312.0148 s
first_0:                     episode reward: 0.4000
second_0:                     episode reward: -0.4000
Process ID: 2, episode: 180/50000 (0.3600%),                     avg. length: 663.65,                    last time consumption/overall running time: 38.2348s / 350.2496 s
first_0:                     episode reward: 0.2000
second_0:                     episode reward: -0.2000
Process ID: 2, episode: 200/50000 (0.4000%),                     avg. length: 687.0,                    last time consumption/overall running time: 40.1856s / 390.4352 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 2, episode: 220/50000 (0.4400%),                     avg. length: 616.95,                    last time consumption/overall running time: 37.2173s / 427.6525 s
first_0:                     episode reward: -0.1000
second_0:                     episode reward: 0.1000
Process ID: 2, episode: 240/50000 (0.4800%),                     avg. length: 629.95,                    last time consumption/overall running time: 39.6377s / 467.2902 s
first_0:                     episode reward: -0.7000
second_0:                     episode reward: 0.7000
Process ID: 2, episode: 260/50000 (0.5200%),                     avg. length: 633.85,                    last time consumption/overall running time: 42.1295s / 509.4197 s
first_0:                     episode reward: 0.2000
second_0:                     episode reward: -0.2000
Process ID: 2, episode: 280/50000 (0.5600%),                     avg. length: 639.15,                    last time consumption/overall running time: 41.3114s / 550.7311 s
first_0:                     episode reward: 0.1500
second_0:                     episode reward: -0.1500
Process ID: 2, episode: 300/50000 (0.6000%),                     avg. length: 627.35,                    last time consumption/overall running time: 37.9867s / 588.7177 s
first_0:                     episode reward: 0.2500
second_0:                     episode reward: -0.2500
Process ID: 2, episode: 320/50000 (0.6400%),                     avg. length: 688.6,                    last time consumption/overall running time: 41.4585s / 630.1762 s
first_0:                     episode reward: -0.6000
second_0:                     episode reward: 0.6000
Process ID: 2, episode: 340/50000 (0.6800%),                     avg. length: 634.3,                    last time consumption/overall running time: 39.9984s / 670.1747 s
first_0:                     episode reward: -0.4000
second_0:                     episode reward: 0.4000
Process ID: 2, episode: 360/50000 (0.7200%),                     avg. length: 622.9,                    last time consumption/overall running time: 44.7570s / 714.9316 s
first_0:                     episode reward: 0.5500
second_0:                     episode reward: -0.5500
Process ID: 2, episode: 380/50000 (0.7600%),                     avg. length: 647.65,                    last time consumption/overall running time: 44.9372s / 759.8688 s
first_0:                     episode reward: 0.1000
second_0:                     episode reward: -0.1000
Process ID: 2, episode: 400/50000 (0.8000%),                     avg. length: 662.7,                    last time consumption/overall running time: 45.9827s / 805.8515 s
first_0:                     episode reward: 0.8000
second_0:                     episode reward: -0.8000
Process ID: 2, episode: 420/50000 (0.8400%),                     avg. length: 625.7,                    last time consumption/overall running time: 42.4539s / 848.3055 s
first_0:                     episode reward: 0.2500
second_0:                     episode reward: -0.2500
Process ID: 2, episode: 440/50000 (0.8800%),                     avg. length: 651.9,                    last time consumption/overall running time: 45.4608s / 893.7663 s
first_0:                     episode reward: 1.4500
second_0:                     episode reward: -1.4500
Process ID: 2, episode: 460/50000 (0.9200%),                     avg. length: 608.1,                    last time consumption/overall running time: 41.6112s / 935.3775 s
first_0:                     episode reward: -1.1500
second_0:                     episode reward: 1.1500
Process ID: 2, episode: 480/50000 (0.9600%),                     avg. length: 656.2,                    last time consumption/overall running time: 46.6718s / 982.0493 s
first_0:                     episode reward: 0.2000pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
random seed: 37
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 1, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000}, 'num_process': 5, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7ff85d4a2e10>}}
Save models to : /home/zihan/research/MARS/data/model/4/slimevolley_SlimeVolley-v0_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/4/slimevolley_SlimeVolley-v0_nash_dqn.
Process ID: 4, episode: 20/50000 (0.0400%),                     avg. length: 647.55,                    last time consumption/overall running time: 36.6763s / 36.6763 s
first_0:                     episode reward: -0.1500
second_0:                     episode reward: 0.1500
Process ID: 4, episode: 40/50000 (0.0800%),                     avg. length: 640.15,                    last time consumption/overall running time: 45.1232s / 81.7995 s
first_0:                     episode reward: -0.0500
second_0:                     episode reward: 0.0500
Process ID: 4, episode: 60/50000 (0.1200%),                     avg. length: 646.5,                    last time consumption/overall running time: 40.9734s / 122.7729 s
first_0:                     episode reward: -1.3000
second_0:                     episode reward: 1.3000
Process ID: 4, episode: 80/50000 (0.1600%),                     avg. length: 596.2,                    last time consumption/overall running time: 34.7881s / 157.5609 s
first_0:                     episode reward: 0.2000
second_0:                     episode reward: -0.2000
Process ID: 4, episode: 100/50000 (0.2000%),                     avg. length: 625.1,                    last time consumption/overall running time: 39.6930s / 197.2540 s
first_0:                     episode reward: -0.3500
second_0:                     episode reward: 0.3500
Process ID: 4, episode: 120/50000 (0.2400%),                     avg. length: 645.25,                    last time consumption/overall running time: 39.2294s / 236.4834 s
first_0:                     episode reward: 0.0500
second_0:                     episode reward: -0.0500
Process ID: 4, episode: 140/50000 (0.2800%),                     avg. length: 614.95,                    last time consumption/overall running time: 40.8228s / 277.3062 s
first_0:                     episode reward: -0.1000
second_0:                     episode reward: 0.1000
Process ID: 4, episode: 160/50000 (0.3200%),                     avg. length: 653.65,                    last time consumption/overall running time: 38.6801s / 315.9863 s
first_0:                     episode reward: 0.2000
second_0:                     episode reward: -0.2000
Process ID: 4, episode: 180/50000 (0.3600%),                     avg. length: 640.15,                    last time consumption/overall running time: 37.0902s / 353.0765 s
first_0:                     episode reward: -0.0500
second_0:                     episode reward: 0.0500
Process ID: 4, episode: 200/50000 (0.4000%),                     avg. length: 616.45,                    last time consumption/overall running time: 36.6634s / 389.7399 s
first_0:                     episode reward: -0.7500
second_0:                     episode reward: 0.7500
Process ID: 4, episode: 220/50000 (0.4400%),                     avg. length: 639.65,                    last time consumption/overall running time: 39.3768s / 429.1167 s
first_0:                     episode reward: -0.7500
second_0:                     episode reward: 0.7500
Process ID: 4, episode: 240/50000 (0.4800%),                     avg. length: 646.7,                    last time consumption/overall running time: 40.6912s / 469.8079 s
first_0:                     episode reward: -0.5000
second_0:                     episode reward: 0.5000
Process ID: 4, episode: 260/50000 (0.5200%),                     avg. length: 626.65,                    last time consumption/overall running time: 41.5925s / 511.4004 s
first_0:                     episode reward: -0.2000
second_0:                     episode reward: 0.2000
Process ID: 4, episode: 280/50000 (0.5600%),                     avg. length: 640.9,                    last time consumption/overall running time: 41.2549s / 552.6553 s
first_0:                     episode reward: 0.2500
second_0:                     episode reward: -0.2500
Process ID: 4, episode: 300/50000 (0.6000%),                     avg. length: 641.65,                    last time consumption/overall running time: 39.6820s / 592.3373 s
first_0:                     episode reward: -0.0500
second_0:                     episode reward: 0.0500
Process ID: 4, episode: 320/50000 (0.6400%),                     avg. length: 682.2,                    last time consumption/overall running time: 41.1231s / 633.4603 s
first_0:                     episode reward: -0.1500
second_0:                     episode reward: 0.1500
Process ID: 4, episode: 340/50000 (0.6800%),                     avg. length: 593.6,                    last time consumption/overall running time: 37.6272s / 671.0875 s
first_0:                     episode reward: -0.2000
second_0:                     episode reward: 0.2000
Process ID: 4, episode: 360/50000 (0.7200%),                     avg. length: 643.25,                    last time consumption/overall running time: 46.1653s / 717.2528 s
first_0:                     episode reward: 0.6000
second_0:                     episode reward: -0.6000
Process ID: 4, episode: 380/50000 (0.7600%),                     avg. length: 630.85,                    last time consumption/overall running time: 45.3947s / 762.6475 s
first_0:                     episode reward: -0.1500
second_0:                     episode reward: 0.1500
Process ID: 4, episode: 400/50000 (0.8000%),                     avg. length: 709.55,                    last time consumption/overall running time: 49.5407s / 812.1882 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 4, episode: 420/50000 (0.8400%),                     avg. length: 657.45,                    last time consumption/overall running time: 45.2895s / 857.4777 s
first_0:                     episode reward: 0.3000
second_0:                     episode reward: -0.3000
Process ID: 4, episode: 440/50000 (0.8800%),                     avg. length: 644.75,                    last time consumption/overall running time: 44.6741s / 902.1518 s
first_0:                     episode reward: 1.2500
second_0:                     episode reward: -1.2500
Process ID: 4, episode: 460/50000 (0.9200%),                     avg. length: 644.95,                    last time consumption/overall running time: 45.4998s / 947.6516 s
first_0:                     episode reward: -0.1500
second_0:                     episode reward: 0.1500
Process ID: 4, episode: 480/50000 (0.9600%),                     avg. length: 634.7,                    last time consumption/overall running time: 44.4546s / 992.1062 s
first_0:                     episode reward: -0.1500