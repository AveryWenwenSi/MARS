pygame 1.9.6
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
random seed: 23
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7fc068c68160>
No agent are not learnable.
{'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
2022-01-06 18:27:17.542258: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/quantumiracle/.mujoco/mujoco200/bin:/home/quantumiracle/.mujoco/mujoco210/bin:/usr/lib/nvidia
2022-01-06 18:27:17.542319: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/quantumiracle/.mujoco/mujoco200/bin:/home/quantumiracle/.mujoco/mujoco210/bin:/usr/lib/nvidia
2022-01-06 18:27:17.542323: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2022-01-06 18:27:17.622642: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/quantumiracle/.mujoco/mujoco200/bin:/home/quantumiracle/.mujoco/mujoco210/bin:/usr/lib/nvidia
2022-01-06 18:27:17.622703: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/quantumiracle/.mujoco/mujoco200/bin:/home/quantumiracle/.mujoco/mujoco210/bin:/usr/lib/nvidia
2022-01-06 18:27:17.622708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2022-01-06 18:27:17.631427: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/quantumiracle/.mujoco/mujoco200/bin:/home/quantumiracle/.mujoco/mujoco210/bin:/usr/lib/nvidia
2022-01-06 18:27:17.631533: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/quantumiracle/.mujoco/mujoco200/bin:/home/quantumiracle/.mujoco/mujoco210/bin:/usr/lib/nvidia
2022-01-06 18:27:17.631542: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
pygame 1.9.6
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
random seed: 48
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/quantumiracle/research/MARS/data/model/0/pettingzoo_boxing_v1_nash_dqn. 
 Save logs to: /home/quantumiracle/research/MARS/data/log/0/pettingzoo_boxing_v1_nash_dqn.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 87.9098s / 87.9098 s
first_0:                     episode reward: -1.1000
second_0:                     episode reward: 1.1000
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 126.0719s / 213.9817 s
first_0:                     episode reward: 1.2500
second_0:                     episode reward: -1.2500
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 135.1639s / 349.1456 s
first_0:                     episode reward: -0.6000
second_0:                     episode reward: 0.6000
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 134.4076s / 483.5532 s
first_0:                     episode reward: -2.7000
second_0:                     episode reward: 2.7000
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 136.4855s / 620.0387 s
first_0:                     episode reward: -1.2000
second_0:                     episode reward: 1.2000
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 136.5583s / 756.5970 s
first_0:                     episode reward: -1.1000
second_0:                     episode reward: 1.1000
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 139.5889s / 896.1859 s
first_0:                     episode reward: -2.9500
second_0:                     episode reward: 2.9500
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 136.1391s / 1032.3250 s
first_0:                     episode reward: -2.8500
second_0:                     episode reward: 2.8500
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 138.9121s / 1171.2371 s
first_0:                     episode reward: -1.7000
second_0:                     episode reward: 1.7000
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 137.6627s / 1308.8998 s
first_0:                     episode reward: -1.9000
second_0:                     episode reward: 1.9000
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 140.8259s / 1449.7257 s
first_0:                     episode reward: 2.6000
second_0:                     episode reward: -2.6000
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 143.7737s / 1593.4994 s
first_0:                     episode reward: -2.0500
second_0:                     episode reward: 2.0500
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 141.5870s / 1735.0865 s
first_0:                     episode reward: 1.9000
second_0:                     episode reward: -1.9000
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 140.9023s / 1875.9888 s
first_0:                     episode reward: -2.5000
second_0:                     episode reward: 2.5000
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 141.6486s / 2017.6374 s
first_0:                     episode reward: -1.1000
second_0:                     episode reward: 1.1000
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 144.2599s / 2161.8973 s
first_0:                     episode reward: 0.5000
second_0:                     episode reward: -0.5000
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 139.1556s / 2301.0529 s
first_0:                     episode reward: -1.1500
second_0:                     episode reward: 1.1500
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 139.0610s / 2440.1139 s
first_0:                     episode reward: -4.2000
second_0:                     episode reward: 4.2000
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 139.1015s / 2579.2154 s
first_0:                     episode reward: -2.5000
second_0:                     episode reward: 2.5000
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 138.1857s / 2717.4011 s
first_0:                     episode reward: -1.2500
second_0:                     episode reward: 1.2500
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 138.5499s / 2855.9510 s
first_0:                     episode reward: -0.7000
second_0:                     episode reward: 0.7000
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 139.7949s / 2995.7460 s
first_0:                     episode reward: 2.1000
second_0:                     episode reward: -2.1000
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 140.4592s / 3136.2052 s
first_0:                     episode reward: -3.4000
second_0:                     episode reward: 3.4000
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 139.0962s / 3275.3014 s
first_0:                     episode reward: -3.7000
second_0:                     episode reward: 3.7000
Process ID: 0, episode: 500/10000 (5.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 139.1095s / 3414.4109 spygame 1.9.6
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
random seed: 33
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/quantumiracle/research/MARS/data/model/1/pettingzoo_boxing_v1_nash_dqn. 
 Save logs to: /home/quantumiracle/research/MARS/data/log/1/pettingzoo_boxing_v1_nash_dqn.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 86.9066s / 86.9066 s
first_0:                     episode reward: -0.7500
second_0:                     episode reward: 0.7500
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 126.7879s / 213.6945 s
first_0:                     episode reward: -0.6000
second_0:                     episode reward: 0.6000
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 135.1760s / 348.8705 s
first_0:                     episode reward: -1.3500
second_0:                     episode reward: 1.3500
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 135.0374s / 483.9079 s
first_0:                     episode reward: -0.0500
second_0:                     episode reward: 0.0500
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 136.5632s / 620.4711 s
first_0:                     episode reward: -3.0000
second_0:                     episode reward: 3.0000
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 136.3124s / 756.7835 s
first_0:                     episode reward: -1.2500
second_0:                     episode reward: 1.2500
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 139.9596s / 896.7431 s
first_0:                     episode reward: -2.3000
second_0:                     episode reward: 2.3000
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 136.6332s / 1033.3763 s
first_0:                     episode reward: -0.7000
second_0:                     episode reward: 0.7000
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 139.4562s / 1172.8325 s
first_0:                     episode reward: -0.4500
second_0:                     episode reward: 0.4500
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 137.5210s / 1310.3535 s
first_0:                     episode reward: 0.8000
second_0:                     episode reward: -0.8000
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 141.9486s / 1452.3021 s
first_0:                     episode reward: -1.9500
second_0:                     episode reward: 1.9500
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 144.0032s / 1596.3053 s
first_0:                     episode reward: -1.5000
second_0:                     episode reward: 1.5000
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 142.2697s / 1738.5750 s
first_0:                     episode reward: 1.1000
second_0:                     episode reward: -1.1000
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 140.7267s / 1879.3017 s
first_0:                     episode reward: -0.6500
second_0:                     episode reward: 0.6500
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 142.7302s / 2022.0319 s
first_0:                     episode reward: -1.4000
second_0:                     episode reward: 1.4000
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 144.4847s / 2166.5166 s
first_0:                     episode reward: -1.8500
second_0:                     episode reward: 1.8500
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 139.3349s / 2305.8515 s
first_0:                     episode reward: -2.2500
second_0:                     episode reward: 2.2500
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 139.4731s / 2445.3246 s
first_0:                     episode reward: -0.1500
second_0:                     episode reward: 0.1500
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 138.2038s / 2583.5284 s
first_0:                     episode reward: -1.6000
second_0:                     episode reward: 1.6000
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 138.2150s / 2721.7434 s
first_0:                     episode reward: -1.7000
second_0:                     episode reward: 1.7000
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 138.9953s / 2860.7387 s
first_0:                     episode reward: -0.1000
second_0:                     episode reward: 0.1000
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 141.0856s / 3001.8243 s
first_0:                     episode reward: -1.1500
second_0:                     episode reward: 1.1500
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 139.8410s / 3141.6654 s
first_0:                     episode reward: -1.5500
second_0:                     episode reward: 1.5500
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 139.1429s / 3280.8083 s
first_0:                     episode reward: -2.3500
second_0:                     episode reward: 2.3500
Process ID: 1, episode: 500/10000 (5.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 139.3791s / 3420.1874 s