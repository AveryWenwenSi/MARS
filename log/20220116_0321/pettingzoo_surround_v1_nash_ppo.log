pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [31, 19]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220116_0321/pettingzoo_surround_v1_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220116_0321/pettingzoo_surround_v1_nash_ppo.
Episode: 1/10000 (0.0100%),                 avg. length: 1061.0,                last time consumption/overall running time: 14.0891s / 14.0891 s
env0_first_0:                 episode reward: 9.0000,                 loss: -0.0207
env0_second_0:                 episode reward: -9.0000,                 loss: 0.0003
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1434.65,                last time consumption/overall running time: 367.4512s / 381.5403 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0382
env0_second_0:                 episode reward: 1.8500,                 loss: -0.0306
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1677.5,                last time consumption/overall running time: 427.1422s / 808.6825 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0412
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0393
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1996.0,                last time consumption/overall running time: 507.8586s / 1316.5411 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.0474
env0_second_0:                 episode reward: 2.8500,                 loss: -0.0450
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 2067.2,                last time consumption/overall running time: 526.7441s / 1843.2852 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.0693
env0_second_0:                 episode reward: 3.6500,                 loss: -0.0669
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 2214.55,                last time consumption/overall running time: 564.5772s / 2407.8624 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.0682
env0_second_0:                 episode reward: 1.4500,                 loss: -0.0696
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 2234.55,                last time consumption/overall running time: 567.0371s / 2974.8995 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.0838
env0_second_0:                 episode reward: 3.3000,                 loss: -0.0850
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 2142.9,                last time consumption/overall running time: 545.1678s / 3520.0673 s
env0_first_0:                 episode reward: -4.3000,                 loss: -0.0821
env0_second_0:                 episode reward: 4.3000,                 loss: -0.0804
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 2195.0,                last time consumption/overall running time: 560.1649s / 4080.2322 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.0826
env0_second_0:                 episode reward: 3.8000,                 loss: -0.0812
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 2288.95,                last time consumption/overall running time: 580.3277s / 4660.5599 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.0923
env0_second_0:                 episode reward: 3.2000,                 loss: -0.0916
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 2184.9,                last time consumption/overall running time: 556.1095s / 5216.6694 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.0993
env0_second_0:                 episode reward: 2.1000,                 loss: -0.0991
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 2111.0,                last time consumption/overall running time: 539.0500s / 5755.7194 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.1111
env0_second_0:                 episode reward: 3.9500,                 loss: -0.1112
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 2095.15,                last time consumption/overall running time: 535.1842s / 6290.9036 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.0944
env0_second_0:                 episode reward: 4.6500,                 loss: -0.0931
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 2176.25,                last time consumption/overall running time: 555.8170s / 6846.7206 s
env0_first_0:                 episode reward: -3.9000,                 loss: -0.0947
env0_second_0:                 episode reward: 3.9000,                 loss: -0.0914
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 2193.3,                last time consumption/overall running time: 562.1510s / 7408.8716 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.0862
env0_second_0:                 episode reward: 4.1000,                 loss: -0.0824
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 2316.25,                last time consumption/overall running time: 591.8012s / 8000.6728 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.0842
env0_second_0:                 episode reward: 2.4000,                 loss: -0.0780
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 2245.75,                last time consumption/overall running time: 573.5864s / 8574.2591 s
env0_first_0:                 episode reward: -4.3500,                 loss: -0.1033
env0_second_0:                 episode reward: 4.3500,                 loss: -0.0981
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 2196.45,                last time consumption/overall running time: 560.0987s / 9134.3579 s
env0_first_0:                 episode reward: -4.0500,                 loss: -0.1170
env0_second_0:                 episode reward: 4.0500,                 loss: -0.1127
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 2202.4,                last time consumption/overall running time: 559.8884s / 9694.2463 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1237
env0_second_0:                 episode reward: 4.4000,                 loss: -0.1210
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 2386.7,                last time consumption/overall running time: 606.8256s / 10301.0719 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1112
env0_second_0:                 episode reward: 1.8500,                 loss: -0.1043
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 2333.15,                last time consumption/overall running time: 592.2366s / 10893.3085 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.1059
env0_second_0:                 episode reward: 4.4500,                 loss: -0.1032
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 2376.2,                last time consumption/overall running time: 603.3813s / 11496.6899 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.0920
env0_second_0:                 episode reward: 2.9000,                 loss: -0.0884
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 2348.85,                last time consumption/overall running time: 599.0516s / 12095.7415 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.1095
env0_second_0:                 episode reward: 2.4500,                 loss: -0.1020
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 2383.8,                last time consumption/overall running time: 601.1353s / 12696.8769 s
env0_first_0:                 episode reward: -3.1500,                 loss: -0.1198
env0_second_0:                 episode reward: 3.1500,                 loss: -0.1112
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 2281.7,                last time consumption/overall running time: 577.6693s / 13274.5462 s
env0_first_0:                 episode reward: -3.6000,                 loss: -0.1357
env0_second_0:                 episode reward: 3.6000,                 loss: -0.1267
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 2131.6,                last time consumption/overall running time: 539.0024s / 13813.5486 s
env0_first_0:                 episode reward: -4.8500,                 loss: -0.1343
env0_second_0:                 episode reward: 4.8500,                 loss: -0.1335
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 2300.9,                last time consumption/overall running time: 580.3378s / 14393.8863 s
env0_first_0:                 episode reward: -3.4500,                 loss: -0.1236
env0_second_0:                 episode reward: 3.4500,                 loss: -0.1192
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 2238.0,                last time consumption/overall running time: 568.4175s / 14962.3038 s
env0_first_0:                 episode reward: -4.8500,                 loss: -0.1371
env0_second_0:                 episode reward: 4.8500,                 loss: -0.1334
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 2345.2,                last time consumption/overall running time: 593.4586s / 15555.7625 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.1344
env0_second_0:                 episode reward: 3.3000,                 loss: -0.1284
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 2331.1,                last time consumption/overall running time: 587.6906s / 16143.4531 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.1228
env0_second_0:                 episode reward: 4.5000,                 loss: -0.1157
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 2225.0,                last time consumption/overall running time: 563.9851s / 16707.4382 s
env0_first_0:                 episode reward: -5.0000,                 loss: -0.1328
env0_second_0:                 episode reward: 5.0000,                 loss: -0.1270
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 2162.9,                last time consumption/overall running time: 547.5085s / 17254.9467 s
env0_first_0:                 episode reward: -5.3500,                 loss: -0.1371
env0_second_0:                 episode reward: 5.3500,                 loss: -0.1292
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 2320.3,                last time consumption/overall running time: 585.8995s / 17840.8461 s
env0_first_0:                 episode reward: -5.0500,                 loss: -0.1317
env0_second_0:                 episode reward: 5.0500,                 loss: -0.1250
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 2189.2,                last time consumption/overall running time: 554.9237s / 18395.7698 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.1524
env0_second_0:                 episode reward: 5.4500,                 loss: -0.1469
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 2299.1,                last time consumption/overall running time: 582.8531s / 18978.6229 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.1298
env0_second_0:                 episode reward: 4.7000,                 loss: -0.1187
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 2308.15,                last time consumption/overall running time: 583.5070s / 19562.1298 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.1211
env0_second_0:                 episode reward: 3.8500,                 loss: -0.1100
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 2091.5,                last time consumption/overall running time: 529.9752s / 20092.1051 s
env0_first_0:                 episode reward: -6.9000,                 loss: -0.1500
env0_second_0:                 episode reward: 6.9000,                 loss: -0.1399
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 2079.4,                last time consumption/overall running time: 526.2204s / 20618.3255 s
env0_first_0:                 episode reward: -7.1000,                 loss: -0.1568
env0_second_0:                 episode reward: 7.1000,                 loss: -0.1483
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2269.9,                last time consumption/overall running time: 571.7922s / 21190.1176 s
env0_first_0:                 episode reward: -6.0000,                 loss: -0.1525
env0_second_0:                 episode reward: 6.0000,                 loss: -0.1410
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 2103.2,                last time consumption/overall running time: 532.5111s / 21722.6287 s
env0_first_0:                 episode reward: -4.0500,                 loss: -0.1560
env0_second_0:                 episode reward: 4.0500,                 loss: -0.1432
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 2266.2,                last time consumption/overall running time: 572.1732s / 22294.8020 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1320
env0_second_0:                 episode reward: 4.4000,                 loss: -0.1185
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 2469.85,                last time consumption/overall running time: 624.7631s / 22919.5651 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.1309
env0_second_0:                 episode reward: 2.8500,                 loss: -0.1146
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 2641.85,                last time consumption/overall running time: 667.9066s / 23587.4716 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.1233
env0_second_0:                 episode reward: 3.7500,                 loss: -0.0812
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 2713.8,                last time consumption/overall running time: 683.8647s / 24271.3364 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.1184
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0870
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 2799.1,                last time consumption/overall running time: 703.6985s / 24975.0349 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.1198
env0_second_0:                 episode reward: 2.5500,                 loss: -0.1052
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2989.7,                last time consumption/overall running time: 751.7172s / 25726.7521 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.1093
env0_second_0:                 episode reward: 0.8500,                 loss: -0.0941
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 2789.3,                last time consumption/overall running time: 699.9440s / 26426.6962 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.1129
env0_second_0:                 episode reward: 4.2000,                 loss: -0.0980
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 2731.0,                last time consumption/overall running time: 685.9621s / 27112.6583 s
env0_first_0:                 episode reward: -3.0500,                 loss: -0.0980
env0_second_0:                 episode reward: 3.0500,                 loss: -0.0798
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2909.35,                last time consumption/overall running time: 727.2019s / 27839.8602 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1055
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0893
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 2690.1,                last time consumption/overall running time: 673.2624s / 28513.1225 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.1104
env0_second_0:                 episode reward: 1.0500,                 loss: -0.0985
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 2844.15,                last time consumption/overall running time: 710.3356s / 29223.4581 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.1177
env0_second_0:                 episode reward: 2.3500,                 loss: -0.1006
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 2870.85,                last time consumption/overall running time: 717.2796s / 29940.7377 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.1100
env0_second_0:                 episode reward: 1.3500,                 loss: -0.0829
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 3046.45,                last time consumption/overall running time: 759.9164s / 30700.6541 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1155
env0_second_0:                 episode reward: -0.3500,                 loss: -0.0946
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 2891.55,                last time consumption/overall running time: 723.6187s / 31424.2727 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1135
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0917
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 3101.2,                last time consumption/overall running time: 773.1183s / 32197.3910 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.1065
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0931
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 3147.45,                last time consumption/overall running time: 786.6093s / 32984.0003 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1097
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0985
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 2978.75,                last time consumption/overall running time: 743.4811s / 33727.4813 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1177
env0_second_0:                 episode reward: -1.0500,                 loss: -0.1010
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 3065.3,                last time consumption/overall running time: 764.7620s / 34492.2433 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1194
env0_second_0:                 episode reward: 0.5500,                 loss: -0.0988
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 3063.2,                last time consumption/overall running time: 764.7908s / 35257.0341 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1085
env0_second_0:                 episode reward: 1.8500,                 loss: -0.0929
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 3077.1,                last time consumption/overall running time: 767.3802s / 36024.4143 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1025
env0_second_0:                 episode reward: -1.0500,                 loss: -0.0721
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 3258.05,                last time consumption/overall running time: 817.1125s / 36841.5267 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.1081
env0_second_0:                 episode reward: 0.9000,                 loss: -0.0840
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 2925.2,                last time consumption/overall running time: 734.0225s / 37575.5493 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1120
env0_second_0:                 episode reward: -1.4000,                 loss: -0.0925
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 3083.65,                last time consumption/overall running time: 770.5293s / 38346.0786 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.1128
env0_second_0:                 episode reward: 1.2000,                 loss: -0.0862
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 3064.95,                last time consumption/overall running time: 764.9140s / 39110.9926 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1134
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0880
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 3018.2,                last time consumption/overall running time: 752.9619s / 39863.9545 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1278
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0993
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 3065.05,                last time consumption/overall running time: 765.9835s / 40629.9380 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1155
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0892
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 3386.8,                last time consumption/overall running time: 847.1230s / 41477.0609 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1126
env0_second_0:                 episode reward: 0.1000,                 loss: -0.0688
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 3067.6,                last time consumption/overall running time: 767.3054s / 42244.3663 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.1089
env0_second_0:                 episode reward: -3.6000,                 loss: -0.0771
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 3044.75,                last time consumption/overall running time: 760.4900s / 43004.8563 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.1201
env0_second_0:                 episode reward: -3.8000,                 loss: -0.0940
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 2642.5,                last time consumption/overall running time: 661.0472s / 43665.9034 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.1107
env0_second_0:                 episode reward: -4.5000,                 loss: -0.0779
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 3015.2,                last time consumption/overall running time: 752.7850s / 44418.6885 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1189
env0_second_0:                 episode reward: -2.2000,                 loss: -0.0913
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 3337.45,                last time consumption/overall running time: 829.2226s / 45247.9111 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1251
env0_second_0:                 episode reward: -1.6500,                 loss: -0.0993
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 3070.1,                last time consumption/overall running time: 766.5331s / 46014.4442 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1194
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0879
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 3272.85,                last time consumption/overall running time: 817.4703s / 46831.9145 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1183
env0_second_0:                 episode reward: -1.7000,                 loss: -0.0918
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 3099.75,                last time consumption/overall running time: 773.3511s / 47605.2657 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1290
env0_second_0:                 episode reward: -2.1000,                 loss: -0.0997
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 3210.1,                last time consumption/overall running time: 802.0958s / 48407.3615 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.1184
env0_second_0:                 episode reward: -0.6500,                 loss: -0.0898
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 3096.8,                last time consumption/overall running time: 775.6290s / 49182.9905 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1235
env0_second_0:                 episode reward: -3.2000,                 loss: -0.0895
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 3212.15,                last time consumption/overall running time: 801.9510s / 49984.9415 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1256
env0_second_0:                 episode reward: -1.6500,                 loss: -0.0998
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 3194.0,                last time consumption/overall running time: 798.8032s / 50783.7447 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.1244
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0974
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 3161.05,                last time consumption/overall running time: 790.6054s / 51574.3501 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1273
env0_second_0:                 episode reward: -2.6500,                 loss: -0.0896
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 3370.55,                last time consumption/overall running time: 816.0044s / 52390.3544 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.1249
env0_second_0:                 episode reward: -1.1000,                 loss: -0.0914
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 3205.05,                last time consumption/overall running time: 741.2509s / 53131.6053 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1224
env0_second_0:                 episode reward: -2.4500,                 loss: -0.0982
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 3031.6,                last time consumption/overall running time: 701.2728s / 53832.8782 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1228
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0854
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 3307.9,                last time consumption/overall running time: 688.8980s / 54521.7761 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1256
env0_second_0:                 episode reward: -3.2000,                 loss: -0.0851
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 3014.45,                last time consumption/overall running time: 619.2644s / 55141.0405 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.1253
env0_second_0:                 episode reward: -3.6000,                 loss: -0.0864
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 3160.95,                last time consumption/overall running time: 648.9069s / 55789.9475 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1145
env0_second_0:                 episode reward: -3.2500,                 loss: -0.0663
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 2865.95,                last time consumption/overall running time: 532.2541s / 56322.2015 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.1272
env0_second_0:                 episode reward: -4.2000,                 loss: -0.0861
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 3227.55,                last time consumption/overall running time: 597.7512s / 56919.9528 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1076
env0_second_0:                 episode reward: -1.5000,                 loss: -0.0542
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 3080.65,                last time consumption/overall running time: 573.1309s / 57493.0837 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1161
env0_second_0:                 episode reward: -2.5000,                 loss: -0.0733
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 3141.25,                last time consumption/overall running time: 581.7027s / 58074.7864 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1197
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0757
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 3276.65,                last time consumption/overall running time: 606.8405s / 58681.6268 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1255
env0_second_0:                 episode reward: -1.6000,                 loss: -0.0742
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 2999.15,                last time consumption/overall running time: 555.3859s / 59237.0127 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1236
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0836
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 3091.7,                last time consumption/overall running time: 575.9861s / 59812.9988 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1371
env0_second_0:                 episode reward: -1.7500,                 loss: -0.0917
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 3220.7,                last time consumption/overall running time: 597.7144s / 60410.7132 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1391
env0_second_0:                 episode reward: -2.3500,                 loss: -0.0905
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 3183.95,                last time consumption/overall running time: 591.1154s / 61001.8286 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1357
env0_second_0:                 episode reward: -1.1500,                 loss: -0.0843
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 3104.4,                last time consumption/overall running time: 577.8203s / 61579.6489 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.1316
env0_second_0:                 episode reward: -4.2000,                 loss: -0.0825
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 3484.9,                last time consumption/overall running time: 644.7765s / 62224.4254 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1320
env0_second_0:                 episode reward: -2.8500,                 loss: -0.0835
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 3544.5,                last time consumption/overall running time: 654.6569s / 62879.0823 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1272
env0_second_0:                 episode reward: -0.9000,                 loss: -0.0887
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 3407.1,                last time consumption/overall running time: 629.7439s / 63508.8262 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1343
env0_second_0:                 episode reward: -2.4500,                 loss: -0.0918
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 3589.4,                last time consumption/overall running time: 665.2148s / 64174.0410 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.1382
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0947
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 3466.45,                last time consumption/overall running time: 640.6868s / 64814.7278 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1314
env0_second_0:                 episode reward: -2.4500,                 loss: -0.0819
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 3361.15,                last time consumption/overall running time: 621.5484s / 65436.2761 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.1320
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0791
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 3495.6,                last time consumption/overall running time: 586.3919s / 66022.6680 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.1263
env0_second_0:                 episode reward: 1.1000,                 loss: -0.0587
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 3368.65,                last time consumption/overall running time: 546.6023s / 66569.2703 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.1201
env0_second_0:                 episode reward: 1.8000,                 loss: -0.0372
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 3306.65,                last time consumption/overall running time: 535.9761s / 67105.2464 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.1307
env0_second_0:                 episode reward: 2.9000,                 loss: -0.0534
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 3097.55,                last time consumption/overall running time: 503.4869s / 67608.7333 s
env0_first_0:                 episode reward: -4.3500,                 loss: -0.1409
env0_second_0:                 episode reward: 4.3500,                 loss: -0.0704
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 3225.65,                last time consumption/overall running time: 525.9357s / 68134.6690 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.1413
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0827
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 3221.7,                last time consumption/overall running time: 529.3857s / 68664.0547 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.1463
env0_second_0:                 episode reward: 3.7500,                 loss: -0.0939
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 3445.65,                last time consumption/overall running time: 562.1854s / 69226.2401 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.1442
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0972
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 3044.85,                last time consumption/overall running time: 497.3470s / 69723.5871 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.1452
env0_second_0:                 episode reward: 2.3500,                 loss: -0.1006
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 3157.6,                last time consumption/overall running time: 512.8515s / 70236.4387 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.1587
env0_second_0:                 episode reward: 3.2000,                 loss: -0.1046
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 3086.5,                last time consumption/overall running time: 503.8817s / 70740.3204 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.1488
env0_second_0:                 episode reward: 3.7500,                 loss: -0.1009
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 2967.6,                last time consumption/overall running time: 487.3093s / 71227.6297 s
env0_first_0:                 episode reward: -4.8500,                 loss: -0.1405
env0_second_0:                 episode reward: 4.8500,                 loss: -0.0909
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 3361.75,                last time consumption/overall running time: 547.9969s / 71775.6266 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.1303
env0_second_0:                 episode reward: 0.8500,                 loss: -0.0848
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 3196.15,                last time consumption/overall running time: 521.2440s / 72296.8706 s
env0_first_0:                 episode reward: -4.1500,                 loss: -0.1357
env0_second_0:                 episode reward: 4.1500,                 loss: -0.0857
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 3241.4,                last time consumption/overall running time: 528.1555s / 72825.0262 s
env0_first_0:                 episode reward: -3.2500,                 loss: -0.1334
env0_second_0:                 episode reward: 3.2500,                 loss: -0.0830
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 3273.8,                last time consumption/overall running time: 536.4696s / 73361.4958 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.1332
env0_second_0:                 episode reward: 2.6000,                 loss: -0.0786
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 3161.4,                last time consumption/overall running time: 515.7221s / 73877.2178 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.1407
env0_second_0:                 episode reward: 4.4500,                 loss: -0.0750
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 2688.8,                last time consumption/overall running time: 440.3154s / 74317.5332 s
env0_first_0:                 episode reward: -6.5000,                 loss: -0.1437
env0_second_0:                 episode reward: 6.5000,                 loss: -0.0878
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 2352.0,                last time consumption/overall running time: 385.7571s / 74703.2903 s
env0_first_0:                 episode reward: -8.0000,                 loss: -0.1525
env0_second_0:                 episode reward: 8.0000,                 loss: -0.0969
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 2727.9,                last time consumption/overall running time: 449.2995s / 75152.5898 s
env0_first_0:                 episode reward: -7.6000,                 loss: -0.1501
env0_second_0:                 episode reward: 7.6000,                 loss: -0.0825
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 2682.7,                last time consumption/overall running time: 440.4167s / 75593.0065 s
env0_first_0:                 episode reward: -6.9500,                 loss: -0.1462
env0_second_0:                 episode reward: 6.9500,                 loss: -0.0830
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 2932.0,                last time consumption/overall running time: 478.4265s / 76071.4330 s
env0_first_0:                 episode reward: -7.1000,                 loss: -0.1507
env0_second_0:                 episode reward: 7.1000,                 loss: -0.0640
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 2959.85,                last time consumption/overall running time: 483.2639s / 76554.6969 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.1426
env0_second_0:                 episode reward: 5.9500,                 loss: -0.0703
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 2979.35,                last time consumption/overall running time: 486.3320s / 77041.0289 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.1458
env0_second_0:                 episode reward: 4.6000,                 loss: -0.0810
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 3004.75,                last time consumption/overall running time: 489.0101s / 77530.0390 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.1371
env0_second_0:                 episode reward: 5.7000,                 loss: -0.0828
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 2804.2,                last time consumption/overall running time: 454.9937s / 77985.0328 s
env0_first_0:                 episode reward: -6.6000,                 loss: -0.1519
env0_second_0:                 episode reward: 6.6000,                 loss: -0.0980
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 2909.5,                last time consumption/overall running time: 473.4496s / 78458.4824 s
env0_first_0:                 episode reward: -6.9000,                 loss: -0.1387
env0_second_0:                 episode reward: 6.9000,                 loss: -0.0858
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 2945.3,                last time consumption/overall running time: 478.3893s / 78936.8717 s
env0_first_0:                 episode reward: -6.1000,                 loss: -0.1352
env0_second_0:                 episode reward: 6.1000,                 loss: -0.0771
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 2961.9,                last time consumption/overall running time: 482.6517s / 79419.5234 s
env0_first_0:                 episode reward: -5.6500,                 loss: -0.1422
env0_second_0:                 episode reward: 5.6500,                 loss: -0.0798
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 2904.6,                last time consumption/overall running time: 478.8334s / 79898.3568 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.1508
env0_second_0:                 episode reward: 4.6000,                 loss: -0.0924
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 3001.0,                last time consumption/overall running time: 494.1839s / 80392.5407 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.1534
env0_second_0:                 episode reward: 5.4500,                 loss: -0.0935
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 3142.3,                last time consumption/overall running time: 514.6216s / 80907.1623 s
env0_first_0:                 episode reward: -5.8500,                 loss: -0.1581
env0_second_0:                 episode reward: 5.8500,                 loss: -0.0896
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 3193.3,                last time consumption/overall running time: 525.0527s / 81432.2150 s
env0_first_0:                 episode reward: -3.5000,                 loss: -0.1461
env0_second_0:                 episode reward: 3.5000,                 loss: -0.0855
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 2666.8,                last time consumption/overall running time: 437.7750s / 81869.9900 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.1601
env0_second_0:                 episode reward: 6.7500,                 loss: 0.3880
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 2430.7,                last time consumption/overall running time: 401.7226s / 82271.7127 s
env0_first_0:                 episode reward: -7.3000,                 loss: -0.1594
env0_second_0:                 episode reward: 7.3000,                 loss: -0.0705
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 2355.75,                last time consumption/overall running time: 389.3431s / 82661.0557 s
env0_first_0:                 episode reward: -6.2000,                 loss: -0.1507
env0_second_0:                 episode reward: 6.2000,                 loss: -0.0870
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 2393.3,                last time consumption/overall running time: 395.9164s / 83056.9721 s
env0_first_0:                 episode reward: -7.8000,                 loss: -0.1522
env0_second_0:                 episode reward: 7.8000,                 loss: -0.0668
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 2255.45,                last time consumption/overall running time: 372.5053s / 83429.4774 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.1494
env0_second_0:                 episode reward: 7.5500,                 loss: -0.0906
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 2491.3,                last time consumption/overall running time: 407.5472s / 83837.0246 s
env0_first_0:                 episode reward: -7.3500,                 loss: -0.1472
env0_second_0:                 episode reward: 7.3500,                 loss: -0.0777
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1946.75,                last time consumption/overall running time: 323.1838s / 84160.2084 s
env0_first_0:                 episode reward: -8.2500,                 loss: -0.1276
env0_second_0:                 episode reward: 8.2500,                 loss: -0.0550
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 2405.0,                last time consumption/overall running time: 396.7133s / 84556.9217 s
env0_first_0:                 episode reward: -7.7500,                 loss: -0.1447
env0_second_0:                 episode reward: 7.7500,                 loss: -0.0825
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 1916.1,                last time consumption/overall running time: 319.0111s / 84875.9328 s
env0_first_0:                 episode reward: -8.6000,                 loss: -0.1328
env0_second_0:                 episode reward: 8.6000,                 loss: -0.0787
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 2419.45,                last time consumption/overall running time: 397.9880s / 85273.9208 s
env0_first_0:                 episode reward: -7.2000,                 loss: -0.1327
env0_second_0:                 episode reward: 7.2000,                 loss: -0.0839
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 2779.35,                last time consumption/overall running time: 455.8802s / 85729.8011 s
env0_first_0:                 episode reward: -5.1500,                 loss: -0.1358
env0_second_0:                 episode reward: 5.1500,                 loss: -0.0766
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 2556.95,                last time consumption/overall running time: 418.0327s / 86147.8337 s
env0_first_0:                 episode reward: -7.1500,                 loss: -0.1546
env0_second_0:                 episode reward: 7.1500,                 loss: -0.1068
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 2298.0,                last time consumption/overall running time: 379.4293s / 86527.2631 s
env0_first_0:                 episode reward: -7.9500,                 loss: -0.1602
env0_second_0:                 episode reward: 7.9500,                 loss: -0.1161
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 2174.8,                last time consumption/overall running time: 359.9871s / 86887.2502 s
env0_first_0:                 episode reward: -8.2000,                 loss: -0.1562
env0_second_0:                 episode reward: 8.2000,                 loss: -0.1008
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 2423.1,                last time consumption/overall running time: 399.1087s / 87286.3589 s
env0_first_0:                 episode reward: -7.2000,                 loss: -0.1554
env0_second_0:                 episode reward: 7.2000,                 loss: -0.0770
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 2165.0,                last time consumption/overall running time: 357.8807s / 87644.2396 s
env0_first_0:                 episode reward: -7.6000,                 loss: -0.1357
env0_second_0:                 episode reward: 7.6000,                 loss: -0.0636
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 2146.55,                last time consumption/overall running time: 357.2122s / 88001.4518 s
env0_first_0:                 episode reward: -7.0500,                 loss: -0.1409
env0_second_0:                 episode reward: 7.0500,                 loss: -0.0738
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 2105.5,                last time consumption/overall running time: 350.1082s / 88351.5600 s
env0_first_0:                 episode reward: -8.7000,                 loss: -0.1600
env0_second_0:                 episode reward: 8.7000,                 loss: -0.0897
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 2222.1,                last time consumption/overall running time: 366.5604s / 88718.1204 s
env0_first_0:                 episode reward: -7.9500,                 loss: -0.1348
env0_second_0:                 episode reward: 7.9500,                 loss: -0.0722
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 2698.7,                last time consumption/overall running time: 442.6504s / 89160.7708 s
env0_first_0:                 episode reward: -6.7000,                 loss: -0.1492
env0_second_0:                 episode reward: 6.7000,                 loss: -0.0996
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 2311.1,                last time consumption/overall running time: 383.1022s / 89543.8730 s
env0_first_0:                 episode reward: -6.4500,                 loss: -0.1357
env0_second_0:                 episode reward: 6.4500,                 loss: -0.0885
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 2311.25,                last time consumption/overall running time: 381.5397s / 89925.4127 s
env0_first_0:                 episode reward: -7.9000,                 loss: -0.1278
env0_second_0:                 episode reward: 7.9000,                 loss: -0.0801
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 2648.4,                last time consumption/overall running time: 435.6677s / 90361.0804 s
env0_first_0:                 episode reward: -6.7000,                 loss: -0.1413
env0_second_0:                 episode reward: 6.7000,                 loss: -0.0838
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 2740.1,                last time consumption/overall running time: 456.1108s / 90817.1912 s
env0_first_0:                 episode reward: -6.1000,                 loss: -0.1425
env0_second_0:                 episode reward: 6.1000,                 loss: -0.0995
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 2330.35,                last time consumption/overall running time: 386.2359s / 91203.4272 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.1388
env0_second_0:                 episode reward: 5.8000,                 loss: -0.0811
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 2530.35,                last time consumption/overall running time: 419.4324s / 91622.8596 s
env0_first_0:                 episode reward: -5.1500,                 loss: -0.1425
env0_second_0:                 episode reward: 5.1500,                 loss: -0.0866
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 2587.35,                last time consumption/overall running time: 428.0252s / 92050.8848 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.1439
env0_second_0:                 episode reward: 6.0500,                 loss: -0.0906
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 2581.0,                last time consumption/overall running time: 425.1528s / 92476.0376 s
env0_first_0:                 episode reward: -6.7000,                 loss: -0.1441
env0_second_0:                 episode reward: 6.7000,                 loss: -0.1027
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 2406.95,                last time consumption/overall running time: 396.8876s / 92872.9252 s
env0_first_0:                 episode reward: -8.3000,                 loss: -0.1501
env0_second_0:                 episode reward: 8.3000,                 loss: -0.1015
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 2496.8,                last time consumption/overall running time: 411.1310s / 93284.0562 s
env0_first_0:                 episode reward: -5.8500,                 loss: -0.1441
env0_second_0:                 episode reward: 5.8500,                 loss: -0.0953
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 2660.95,                last time consumption/overall running time: 438.0258s / 93722.0821 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.1557
env0_second_0:                 episode reward: 5.7000,                 loss: -0.0869
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 2561.75,                last time consumption/overall running time: 419.1472s / 94141.2293 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.1627
env0_second_0:                 episode reward: 6.1500,                 loss: -0.1075
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 2672.15,                last time consumption/overall running time: 440.2775s / 94581.5068 s
env0_first_0:                 episode reward: -7.4500,                 loss: -0.1604
env0_second_0:                 episode reward: 7.4500,                 loss: -0.1131
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 2882.1,                last time consumption/overall running time: 473.0628s / 95054.5696 s
env0_first_0:                 episode reward: -6.6000,                 loss: -0.1549
env0_second_0:                 episode reward: 6.6000,                 loss: -0.0992
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 2694.35,                last time consumption/overall running time: 449.7435s / 95504.3131 s
env0_first_0:                 episode reward: -7.7000,                 loss: -0.1758
env0_second_0:                 episode reward: 7.7000,                 loss: -0.1122
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 2483.45,                last time consumption/overall running time: 408.8406s / 95913.1537 s
env0_first_0:                 episode reward: -8.5000,                 loss: -0.1774
env0_second_0:                 episode reward: 8.5000,                 loss: -0.1284
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 2369.7,                last time consumption/overall running time: 394.7692s / 96307.9229 s
env0_first_0:                 episode reward: -8.4000,                 loss: -0.1683
env0_second_0:                 episode reward: 8.4000,                 loss: -0.1225
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 2188.85,                last time consumption/overall running time: 365.1264s / 96673.0493 s
env0_first_0:                 episode reward: -8.8500,                 loss: -0.1830
env0_second_0:                 episode reward: 8.8500,                 loss: -0.1360
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 2432.55,                last time consumption/overall running time: 400.0038s / 97073.0531 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.1685
env0_second_0:                 episode reward: 7.8500,                 loss: -0.1209
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1473.2,                last time consumption/overall running time: 247.0807s / 97320.1338 s
env0_first_0:                 episode reward: -9.1000,                 loss: -0.1461
env0_second_0:                 episode reward: 9.1000,                 loss: -0.0956
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1842.5,                last time consumption/overall running time: 308.6031s / 97628.7369 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1489
env0_second_0:                 episode reward: 9.3500,                 loss: -0.0961
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1793.25,                last time consumption/overall running time: 302.9244s / 97931.6614 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1412
env0_second_0:                 episode reward: 9.7000,                 loss: -0.0908
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1884.85,                last time consumption/overall running time: 315.4526s / 98247.1140 s
env0_first_0:                 episode reward: -9.1500,                 loss: -0.1350
env0_second_0:                 episode reward: 9.1500,                 loss: -0.0904
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 2300.3,                last time consumption/overall running time: 379.6477s / 98626.7617 s
env0_first_0:                 episode reward: -7.7000,                 loss: -0.1321
env0_second_0:                 episode reward: 7.7000,                 loss: -0.0855
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 2118.25,                last time consumption/overall running time: 351.6300s / 98978.3916 s
env0_first_0:                 episode reward: -8.5500,                 loss: -0.1390
env0_second_0:                 episode reward: 8.5500,                 loss: -0.0944
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 2025.65,                last time consumption/overall running time: 337.6323s / 99316.0240 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.1581
env0_second_0:                 episode reward: 9.2000,                 loss: -0.1134
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1939.95,                last time consumption/overall running time: 322.5187s / 99638.5427 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.1400
env0_second_0:                 episode reward: 9.2000,                 loss: -0.1000
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1993.25,                last time consumption/overall running time: 333.0542s / 99971.5969 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1335
env0_second_0:                 episode reward: 9.3500,                 loss: -0.0917
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 2140.7,                last time consumption/overall running time: 354.4294s / 100326.0263 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.1316
env0_second_0:                 episode reward: 8.7500,                 loss: -0.0812
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 2410.35,                last time consumption/overall running time: 398.5104s / 100724.5367 s
env0_first_0:                 episode reward: -7.4000,                 loss: -0.1463
env0_second_0:                 episode reward: 7.4000,                 loss: -0.1001
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 2263.6,                last time consumption/overall running time: 375.8940s / 101100.4307 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.1433
env0_second_0:                 episode reward: 8.6500,                 loss: -0.1018
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1897.4,                last time consumption/overall running time: 316.2651s / 101416.6958 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.1529
env0_second_0:                 episode reward: 9.7500,                 loss: -0.1207
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 2034.8,                last time consumption/overall running time: 334.2890s / 101750.9848 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.1598
env0_second_0:                 episode reward: 9.5500,                 loss: -0.1047
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 2062.25,                last time consumption/overall running time: 345.9932s / 102096.9780 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1609
env0_second_0:                 episode reward: 9.3500,                 loss: -0.1079
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 2067.6,                last time consumption/overall running time: 340.5944s / 102437.5724 s
env0_first_0:                 episode reward: -9.2500,                 loss: -0.1680
env0_second_0:                 episode reward: 9.2500,                 loss: -0.1063
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 2124.9,                last time consumption/overall running time: 353.6825s / 102791.2549 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.1587
env0_second_0:                 episode reward: 9.3000,                 loss: -0.1036
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 2171.15,                last time consumption/overall running time: 358.9330s / 103150.1878 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.1578
env0_second_0:                 episode reward: 9.3000,                 loss: -0.0923
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 2289.45,                last time consumption/overall running time: 377.9238s / 103528.1117 s
env0_first_0:                 episode reward: -9.0000,                 loss: -0.1540
env0_second_0:                 episode reward: 9.0000,                 loss: -0.0902
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 2273.35,                last time consumption/overall running time: 373.1949s / 103901.3066 s
env0_first_0:                 episode reward: -8.7000,                 loss: -0.1625
env0_second_0:                 episode reward: 8.7000,                 loss: -0.1094
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 2308.75,                last time consumption/overall running time: 377.4802s / 104278.7868 s
env0_first_0:                 episode reward: -8.4000,                 loss: -0.1495
env0_second_0:                 episode reward: 8.4000,                 loss: -0.0982
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 2333.2,                last time consumption/overall running time: 383.2685s / 104662.0553 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.1473
env0_second_0:                 episode reward: 7.8500,                 loss: -0.1009
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 2168.8,                last time consumption/overall running time: 355.4311s / 105017.4864 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.1530
env0_second_0:                 episode reward: 9.2000,                 loss: -0.0872
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 2518.75,                last time consumption/overall running time: 411.7095s / 105429.1960 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.1316
env0_second_0:                 episode reward: 6.8000,                 loss: -0.0897
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 2406.45,                last time consumption/overall running time: 393.3327s / 105822.5287 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.1261
env0_second_0:                 episode reward: 6.0500,                 loss: -0.0850
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1957.45,                last time consumption/overall running time: 326.3354s / 106148.8641 s
env0_first_0:                 episode reward: -6.9500,                 loss: -0.1197
env0_second_0:                 episode reward: 6.9500,                 loss: -0.0520
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 2205.45,                last time consumption/overall running time: 366.3122s / 106515.1763 s
env0_first_0:                 episode reward: -4.3000,                 loss: -0.1071
env0_second_0:                 episode reward: 4.3000,                 loss: -0.0420
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 2572.45,                last time consumption/overall running time: 429.3143s / 106944.4906 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.0914
env0_second_0:                 episode reward: 2.0500,                 loss: -0.0373
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 3090.8,                last time consumption/overall running time: 505.2728s / 107449.7634 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.0768
env0_second_0:                 episode reward: 2.3000,                 loss: -0.0362
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 2757.55,                last time consumption/overall running time: 455.8291s / 107905.5925 s
env0_first_0:                 episode reward: -3.7000,                 loss: -0.0997
env0_second_0:                 episode reward: 3.7000,                 loss: -0.0539
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 3089.65,                last time consumption/overall running time: 508.2724s / 108413.8649 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.0676
env0_second_0:                 episode reward: -1.7500,                 loss: -0.0217
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 2839.7,                last time consumption/overall running time: 466.8304s / 108880.6953 s
env0_first_0:                 episode reward: -4.2500,                 loss: -0.0756
env0_second_0:                 episode reward: 4.2500,                 loss: -0.0323
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 2851.65,                last time consumption/overall running time: 466.1059s / 109346.8013 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.0797
env0_second_0:                 episode reward: 4.5000,                 loss: -0.0452
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 3143.85,                last time consumption/overall running time: 514.6637s / 109861.4649 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.0849
env0_second_0:                 episode reward: 1.6500,                 loss: -0.0504
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 2569.25,                last time consumption/overall running time: 424.3496s / 110285.8145 s
env0_first_0:                 episode reward: -5.3500,                 loss: -0.1042
env0_second_0:                 episode reward: 5.3500,                 loss: -0.0636
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 2157.7,                last time consumption/overall running time: 357.2620s / 110643.0766 s
env0_first_0:                 episode reward: -8.1500,                 loss: -0.1233
env0_second_0:                 episode reward: 8.1500,                 loss: -0.0934
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 2381.2,                last time consumption/overall running time: 393.6854s / 111036.7620 s
env0_first_0:                 episode reward: -6.3000,                 loss: -0.1277
env0_second_0:                 episode reward: 6.3000,                 loss: -0.0752
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 2617.8,                last time consumption/overall running time: 430.0307s / 111466.7927 s
env0_first_0:                 episode reward: -5.0000,                 loss: -0.1071
env0_second_0:                 episode reward: 5.0000,                 loss: -0.0790
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 2799.35,                last time consumption/overall running time: 459.5337s / 111926.3263 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0999
env0_second_0:                 episode reward: 1.8500,                 loss: -0.0621
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 2776.4,                last time consumption/overall running time: 458.7744s / 112385.1007 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.1099
env0_second_0:                 episode reward: 2.1500,                 loss: -0.0796
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 2527.85,                last time consumption/overall running time: 416.8340s / 112801.9347 s
env0_first_0:                 episode reward: -4.1500,                 loss: -0.1144
env0_second_0:                 episode reward: 4.1500,                 loss: -0.0787
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 2693.9,                last time consumption/overall running time: 444.6522s / 113246.5869 s
env0_first_0:                 episode reward: -5.7500,                 loss: -0.1319
env0_second_0:                 episode reward: 5.7500,                 loss: -0.0974
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 2014.05,                last time consumption/overall running time: 333.7437s / 113580.3306 s
env0_first_0:                 episode reward: -8.5500,                 loss: -0.1457
env0_second_0:                 episode reward: 8.5500,                 loss: -0.1019
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 2436.8,                last time consumption/overall running time: 405.1580s / 113985.4886 s
env0_first_0:                 episode reward: -7.2000,                 loss: -0.1202
env0_second_0:                 episode reward: 7.2000,                 loss: -0.0582
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 1792.05,                last time consumption/overall running time: 296.7046s / 114282.1933 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1267
env0_second_0:                 episode reward: 9.3500,                 loss: -0.0889
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1878.65,                last time consumption/overall running time: 310.6012s / 114592.7945 s
env0_first_0:                 episode reward: -8.9500,                 loss: -0.1368
env0_second_0:                 episode reward: 8.9500,                 loss: -0.0765
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 2252.05,                last time consumption/overall running time: 372.3080s / 114965.1025 s
env0_first_0:                 episode reward: -7.7500,                 loss: -0.1291
env0_second_0:                 episode reward: 7.7500,                 loss: -0.0819
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 2176.6,                last time consumption/overall running time: 359.1036s / 115324.2062 s
env0_first_0:                 episode reward: -7.7000,                 loss: -0.1174
env0_second_0:                 episode reward: 7.7000,                 loss: -0.0535
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 2679.55,                last time consumption/overall running time: 445.2817s / 115769.4878 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.1020
env0_second_0:                 episode reward: 4.1000,                 loss: -0.0569
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 2845.2,                last time consumption/overall running time: 471.0100s / 116240.4978 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.0884
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0469
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 3041.9,                last time consumption/overall running time: 492.2986s / 116732.7964 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1049
env0_second_0:                 episode reward: -1.7000,                 loss: -0.0507
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 2805.0,                last time consumption/overall running time: 417.4922s / 117150.2886 s
env0_first_0:                 episode reward: -3.4000,                 loss: -0.1125
env0_second_0:                 episode reward: 3.4000,                 loss: -0.0655
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 2907.55,                last time consumption/overall running time: 433.3414s / 117583.6300 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.1223
env0_second_0:                 episode reward: 1.0000,                 loss: -0.0609
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 2756.95,                last time consumption/overall running time: 411.4315s / 117995.0615 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.1239
env0_second_0:                 episode reward: 1.4500,                 loss: -0.0636
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 2861.3,                last time consumption/overall running time: 423.0788s / 118418.1403 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.1225
env0_second_0:                 episode reward: 2.6500,                 loss: -0.0789
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 2956.1,                last time consumption/overall running time: 436.3494s / 118854.4897 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0952
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0599
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 3155.75,                last time consumption/overall running time: 469.4144s / 119323.9041 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0952
env0_second_0:                 episode reward: -0.6000,                 loss: -0.0609
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 2955.7,                last time consumption/overall running time: 434.3152s / 119758.2193 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1028
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0693
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 2500.95,                last time consumption/overall running time: 377.8724s / 120136.0917 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.1058
env0_second_0:                 episode reward: 6.0500,                 loss: -0.0622
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 2577.25,                last time consumption/overall running time: 388.3478s / 120524.4395 s
env0_first_0:                 episode reward: -6.3000,                 loss: -0.1169
env0_second_0:                 episode reward: 6.3000,                 loss: -0.0869
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 3039.6,                last time consumption/overall running time: 449.3137s / 120973.7532 s
env0_first_0:                 episode reward: -2.8000,                 loss: -0.1050
env0_second_0:                 episode reward: 2.8000,                 loss: -0.0419
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 2437.05,                last time consumption/overall running time: 360.2503s / 121334.0035 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.1098
env0_second_0:                 episode reward: 6.1500,                 loss: -0.0835
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 2889.25,                last time consumption/overall running time: 428.6149s / 121762.6185 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.1297
env0_second_0:                 episode reward: 4.6500,                 loss: -0.0866
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 2518.55,                last time consumption/overall running time: 371.4181s / 122134.0366 s
env0_first_0:                 episode reward: -5.9000,                 loss: -0.1199
env0_second_0:                 episode reward: 5.9000,                 loss: -0.0785
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 2702.7,                last time consumption/overall running time: 400.5008s / 122534.5374 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.1356
env0_second_0:                 episode reward: 4.6500,                 loss: -0.0867
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 2714.75,                last time consumption/overall running time: 398.9379s / 122933.4753 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.1230
env0_second_0:                 episode reward: 6.0500,                 loss: -0.0793
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 2626.2,                last time consumption/overall running time: 396.5382s / 123330.0134 s
env0_first_0:                 episode reward: -5.5500,                 loss: -0.1375
env0_second_0:                 episode reward: 5.5500,                 loss: -0.0902
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 2714.95,                last time consumption/overall running time: 406.1578s / 123736.1712 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.1406
env0_second_0:                 episode reward: 3.6500,                 loss: -0.0878
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 2528.6,                last time consumption/overall running time: 371.7029s / 124107.8741 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.1293
env0_second_0:                 episode reward: 4.4500,                 loss: -0.0882
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 2662.7,                last time consumption/overall running time: 392.0883s / 124499.9624 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1434
env0_second_0:                 episode reward: 4.4000,                 loss: -0.0868
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 2977.4,                last time consumption/overall running time: 436.7592s / 124936.7216 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.1229
env0_second_0:                 episode reward: 2.0000,                 loss: -0.0653
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 2726.9,                last time consumption/overall running time: 404.1543s / 125340.8759 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1244
env0_second_0:                 episode reward: 4.4000,                 loss: -0.0713
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 2852.5,                last time consumption/overall running time: 427.5934s / 125768.4692 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.1250
env0_second_0:                 episode reward: 2.0500,                 loss: -0.0597
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 2542.05,                last time consumption/overall running time: 375.9531s / 126144.4224 s
env0_first_0:                 episode reward: -4.5500,                 loss: -0.1422
env0_second_0:                 episode reward: 4.5500,                 loss: -0.0731
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 2583.15,                last time consumption/overall running time: 385.1624s / 126529.5847 s
env0_first_0:                 episode reward: -5.8500,                 loss: -0.1365
env0_second_0:                 episode reward: 5.8500,                 loss: -0.0801
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 2680.9,                last time consumption/overall running time: 399.3132s / 126928.8979 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.1341
env0_second_0:                 episode reward: 2.0500,                 loss: -0.0778
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 2603.0,                last time consumption/overall running time: 389.8525s / 127318.7504 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.1419
env0_second_0:                 episode reward: 4.2000,                 loss: -0.0898
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 2261.4,                last time consumption/overall running time: 340.8145s / 127659.5649 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.1474
env0_second_0:                 episode reward: 5.8000,                 loss: -0.0966
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 2677.85,                last time consumption/overall running time: 402.1440s / 128061.7089 s
env0_first_0:                 episode reward: -5.6000,                 loss: -0.1458
env0_second_0:                 episode reward: 5.6000,                 loss: -0.0957
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 2710.75,                last time consumption/overall running time: 404.8615s / 128466.5704 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.1400
env0_second_0:                 episode reward: 4.7000,                 loss: -0.0910
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 2959.15,                last time consumption/overall running time: 440.5110s / 128907.0814 s
env0_first_0:                 episode reward: -5.1500,                 loss: -0.1388
env0_second_0:                 episode reward: 5.1500,                 loss: -0.0916
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 3157.6,                last time consumption/overall running time: 469.4334s / 129376.5148 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.1304
env0_second_0:                 episode reward: 3.9500,                 loss: -0.0619
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 3048.25,                last time consumption/overall running time: 458.2370s / 129834.7518 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.1423
env0_second_0:                 episode reward: 2.7000,                 loss: -0.0858
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 3038.1,                last time consumption/overall running time: 452.7189s / 130287.4707 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.1438
env0_second_0:                 episode reward: 2.6000,                 loss: -0.0917
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 2700.65,                last time consumption/overall running time: 411.3566s / 130698.8273 s
env0_first_0:                 episode reward: -3.7000,                 loss: -0.1579
env0_second_0:                 episode reward: 3.7000,                 loss: -0.0936
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 2833.55,                last time consumption/overall running time: 427.8969s / 131126.7242 s
env0_first_0:                 episode reward: -3.7000,                 loss: -0.1521
env0_second_0:                 episode reward: 3.7000,                 loss: -0.0852
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 2978.5,                last time consumption/overall running time: 439.3378s / 131566.0621 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.1480
env0_second_0:                 episode reward: 2.2500,                 loss: -0.0697
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 2973.75,                last time consumption/overall running time: 437.2948s / 132003.3569 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.1482
env0_second_0:                 episode reward: 1.7500,                 loss: -0.0735
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 3470.8,                last time consumption/overall running time: 474.2618s / 132477.6187 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1303
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0447
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 3157.6,                last time consumption/overall running time: 425.2831s / 132902.9017 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.1445
env0_second_0:                 episode reward: 3.7500,                 loss: -0.0707
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 3432.25,                last time consumption/overall running time: 461.9690s / 133364.8707 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.1336
env0_second_0:                 episode reward: 4.1000,                 loss: -0.0702
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 3588.2,                last time consumption/overall running time: 484.0514s / 133848.9221 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.1310
env0_second_0:                 episode reward: 2.1000,                 loss: -0.0661
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 3159.1,                last time consumption/overall running time: 431.3566s / 134280.2787 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.1403
env0_second_0:                 episode reward: 2.8500,                 loss: -0.0594
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 2965.95,                last time consumption/overall running time: 409.5535s / 134689.8322 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.1274
env0_second_0:                 episode reward: 2.4500,                 loss: -0.0158
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 2862.5,                last time consumption/overall running time: 389.7262s / 135079.5584 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.1402
env0_second_0:                 episode reward: 5.4000,                 loss: -0.0436
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 2787.6,                last time consumption/overall running time: 381.9322s / 135461.4906 s
env0_first_0:                 episode reward: -5.0500,                 loss: -0.1469
env0_second_0:                 episode reward: 5.0500,                 loss: -0.0318
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 2503.1,                last time consumption/overall running time: 339.6832s / 135801.1739 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.1475
env0_second_0:                 episode reward: 6.1500,                 loss: -0.0404
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 2646.4,                last time consumption/overall running time: 352.0967s / 136153.2706 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.1446
env0_second_0:                 episode reward: 3.9500,                 loss: -0.0438
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 2672.6,                last time consumption/overall running time: 377.4346s / 136530.7052 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.1719
env0_second_0:                 episode reward: 6.8000,                 loss: -0.0976
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 2702.55,                last time consumption/overall running time: 372.7829s / 136903.4881 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.1746
env0_second_0:                 episode reward: 5.7000,                 loss: -0.0902
env1_first_0:                 episode reward: -7.9000,                 loss: nan
env1_second_0:                 episode reward: 7.9000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 2712.5,                last time consumption/overall running time: 365.1594s / 137268.6475 s
env0_first_0:                 episode reward: -6.9500,                 loss: -0.1614
env0_second_0:                 episode reward: 6.9500,                 loss: -0.0857
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 2689.05,                last time consumption/overall running time: 366.6244s / 137635.2719 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.1667
env0_second_0:                 episode reward: 7.2500,                 loss: -0.0905
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 2708.85,                last time consumption/overall running time: 359.9363s / 137995.2082 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.1566
env0_second_0:                 episode reward: 6.7500,                 loss: -0.0841
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 2686.55,                last time consumption/overall running time: 364.0399s / 138359.2480 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.1598
env0_second_0:                 episode reward: 8.6500,                 loss: -0.0840
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 2699.1,                last time consumption/overall running time: 361.2915s / 138720.5396 s
env0_first_0:                 episode reward: -7.7000,                 loss: -0.1676
env0_second_0:                 episode reward: 7.7000,                 loss: -0.1005
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 2974.6,                last time consumption/overall running time: 385.8383s / 139106.3778 s
env0_first_0:                 episode reward: -7.2000,                 loss: -0.1606
env0_second_0:                 episode reward: 7.2000,                 loss: -0.0951
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 3412.0,                last time consumption/overall running time: 456.1424s / 139562.5202 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.1508
env0_second_0:                 episode reward: 6.0500,                 loss: -0.0785
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 2946.6,                last time consumption/overall running time: 396.2735s / 139958.7937 s
env0_first_0:                 episode reward: -6.2000,                 loss: -0.1462
env0_second_0:                 episode reward: 6.2000,                 loss: -0.0841
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 2817.05,                last time consumption/overall running time: 374.3000s / 140333.0937 s
env0_first_0:                 episode reward: -6.9000,                 loss: -0.1596
env0_second_0:                 episode reward: 6.9000,                 loss: -0.0994
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 3374.0,                last time consumption/overall running time: 448.3506s / 140781.4443 s
env0_first_0:                 episode reward: -5.6500,                 loss: -0.1457
env0_second_0:                 episode reward: 5.6500,                 loss: -0.0860
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 2955.3,                last time consumption/overall running time: 365.7091s / 141147.1534 s
env0_first_0:                 episode reward: -6.2500,                 loss: -0.1394
env0_second_0:                 episode reward: 6.2500,                 loss: -0.0882
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 3479.95,                last time consumption/overall running time: 426.7460s / 141573.8994 s
env0_first_0:                 episode reward: -5.7500,                 loss: -0.1339
env0_second_0:                 episode reward: 5.7500,                 loss: -0.0394
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 3620.75,                last time consumption/overall running time: 442.0515s / 142015.9509 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.1398
env0_second_0:                 episode reward: 3.7500,                 loss: -0.0599
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 3303.1,                last time consumption/overall running time: 407.7316s / 142423.6825 s
env0_first_0:                 episode reward: -6.3000,                 loss: -0.1559
env0_second_0:                 episode reward: 6.3000,                 loss: -0.0821
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 3525.65,                last time consumption/overall running time: 437.1882s / 142860.8706 s
env0_first_0:                 episode reward: -4.8000,                 loss: -0.1436
env0_second_0:                 episode reward: 4.8000,                 loss: -0.0715
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 3738.45,                last time consumption/overall running time: 464.1759s / 143325.0465 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.1462
env0_second_0:                 episode reward: 4.2000,                 loss: -0.0857
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 3400.35,                last time consumption/overall running time: 422.5325s / 143747.5790 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.1435
env0_second_0:                 episode reward: 6.8000,                 loss: -0.0878
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 3627.35,                last time consumption/overall running time: 442.4089s / 144189.9879 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.1445
env0_second_0:                 episode reward: 4.2000,                 loss: -0.0900
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 3640.9,                last time consumption/overall running time: 448.8103s / 144638.7982 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.1512
env0_second_0:                 episode reward: 2.3000,                 loss: -0.1004
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 3310.45,                last time consumption/overall running time: 408.7034s / 145047.5016 s
env0_first_0:                 episode reward: -6.5500,                 loss: -0.1569
env0_second_0:                 episode reward: 6.5500,                 loss: -0.1051
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 3814.45,                last time consumption/overall running time: 476.9856s / 145524.4872 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1446
env0_second_0:                 episode reward: 4.4000,                 loss: -0.0752
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 3474.1,                last time consumption/overall running time: 429.6745s / 145954.1618 s
env0_first_0:                 episode reward: -5.9000,                 loss: -0.1538
env0_second_0:                 episode reward: 5.9000,                 loss: -0.1027
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 3289.3,                last time consumption/overall running time: 427.9529s / 146382.1146 s
env0_first_0:                 episode reward: -7.3500,                 loss: -0.1542
env0_second_0:                 episode reward: 7.3500,                 loss: -0.1001
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 3178.7,                last time consumption/overall running time: 393.6113s / 146775.7259 s
env0_first_0:                 episode reward: -7.1000,                 loss: -0.1557
env0_second_0:                 episode reward: 7.1000,                 loss: -0.0957
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 3732.75,                last time consumption/overall running time: 461.9515s / 147237.6774 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.1341
env0_second_0:                 episode reward: 5.9500,                 loss: -0.0681
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 2791.7,                last time consumption/overall running time: 346.4831s / 147584.1605 s
env0_first_0:                 episode reward: -8.3000,                 loss: -0.1465
env0_second_0:                 episode reward: 8.3000,                 loss: -0.0818
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 2617.9,                last time consumption/overall running time: 331.0263s / 147915.1869 s
env0_first_0:                 episode reward: -7.4000,                 loss: -0.1351
env0_second_0:                 episode reward: 7.4000,                 loss: -0.0387
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 2991.35,                last time consumption/overall running time: 375.5443s / 148290.7312 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.1336
env0_second_0:                 episode reward: 5.8000,                 loss: -0.0608
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 2665.65,                last time consumption/overall running time: 334.3423s / 148625.0735 s
env0_first_0:                 episode reward: -7.3000,                 loss: -0.1440
env0_second_0:                 episode reward: 7.3000,                 loss: -0.0638
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 2886.8,                last time consumption/overall running time: 361.6886s / 148986.7621 s
env0_first_0:                 episode reward: -6.1000,                 loss: -0.1462
env0_second_0:                 episode reward: 6.1000,                 loss: -0.0526
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 3570.35,                last time consumption/overall running time: 446.3098s / 149433.0719 s
env0_first_0:                 episode reward: -4.9000,                 loss: -0.1181
env0_second_0:                 episode reward: 4.9000,                 loss: -0.0342
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 3389.95,                last time consumption/overall running time: 410.7311s / 149843.8029 s
env0_first_0:                 episode reward: -7.2000,                 loss: -0.1161
env0_second_0:                 episode reward: 7.2000,                 loss: -0.0628
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 2752.6,                last time consumption/overall running time: 341.9163s / 150185.7192 s
env0_first_0:                 episode reward: -8.2500,                 loss: -0.1414
env0_second_0:                 episode reward: 8.2500,                 loss: -0.0755
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 3284.45,                last time consumption/overall running time: 405.0924s / 150590.8117 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.1344
env0_second_0:                 episode reward: 7.2500,                 loss: -0.0762
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 3593.8,                last time consumption/overall running time: 441.4028s / 151032.2145 s
env0_first_0:                 episode reward: -6.9500,                 loss: -0.1364
env0_second_0:                 episode reward: 6.9500,                 loss: -0.0796
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 3440.1,                last time consumption/overall running time: 423.9148s / 151456.1293 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.1378
env0_second_0:                 episode reward: 5.9500,                 loss: -0.0667
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 3631.05,                last time consumption/overall running time: 448.5680s / 151904.6973 s
env0_first_0:                 episode reward: -6.5500,                 loss: -0.1279
env0_second_0:                 episode reward: 6.5500,                 loss: -0.0821
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 3462.4,                last time consumption/overall running time: 425.6295s / 152330.3268 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.1421
env0_second_0:                 episode reward: 7.2500,                 loss: -0.0936
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 3692.75,                last time consumption/overall running time: 448.0588s / 152778.3855 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.1269
env0_second_0:                 episode reward: 5.9500,                 loss: -0.0798
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 3000.35,                last time consumption/overall running time: 373.1726s / 153151.5581 s
env0_first_0:                 episode reward: -7.1000,                 loss: -0.1398
env0_second_0:                 episode reward: 7.1000,                 loss: -0.0841
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 3110.35,                last time consumption/overall running time: 388.4507s / 153540.0089 s
env0_first_0:                 episode reward: -7.3000,                 loss: -0.1317
env0_second_0:                 episode reward: 7.3000,                 loss: -0.0571
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 3805.0,                last time consumption/overall running time: 468.7362s / 154008.7450 s
env0_first_0:                 episode reward: -6.0000,                 loss: -0.1353
env0_second_0:                 episode reward: 6.0000,                 loss: -0.0785
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 2839.8,                last time consumption/overall running time: 346.0860s / 154354.8310 s
env0_first_0:                 episode reward: -7.4000,                 loss: -0.1457
env0_second_0:                 episode reward: 7.4000,                 loss: -0.0875
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 2787.55,                last time consumption/overall running time: 341.6417s / 154696.4727 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.1494
env0_second_0:                 episode reward: 8.6500,                 loss: -0.0875
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 3587.7,                last time consumption/overall running time: 439.8253s / 155136.2980 s
env0_first_0:                 episode reward: -6.6000,                 loss: -0.1359
env0_second_0:                 episode reward: 6.6000,                 loss: -0.0773
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 3167.05,                last time consumption/overall running time: 403.5052s / 155539.8033 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.1473
env0_second_0:                 episode reward: 8.7500,                 loss: -0.1060
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 3386.8,                last time consumption/overall running time: 420.2977s / 155960.1009 s
env0_first_0:                 episode reward: -7.5000,                 loss: -0.1476
env0_second_0:                 episode reward: 7.5000,                 loss: -0.1001
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 3230.5,                last time consumption/overall running time: 403.6341s / 156363.7350 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.1379
env0_second_0:                 episode reward: 7.2500,                 loss: -0.0808
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 3147.5,                last time consumption/overall running time: 388.0851s / 156751.8201 s
env0_first_0:                 episode reward: -7.0500,                 loss: -0.1677
env0_second_0:                 episode reward: 7.0500,                 loss: -0.1094
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 3356.8,                last time consumption/overall running time: 416.6447s / 157168.4649 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.1502
env0_second_0:                 episode reward: 5.9500,                 loss: -0.0929
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 3281.85,                last time consumption/overall running time: 409.3638s / 157577.8287 s
env0_first_0:                 episode reward: -6.2000,                 loss: -0.1497
env0_second_0:                 episode reward: 6.2000,                 loss: -0.0889
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 3652.35,                last time consumption/overall running time: 449.4486s / 158027.2773 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.1459
env0_second_0:                 episode reward: 5.4500,                 loss: -0.0859
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 3347.75,                last time consumption/overall running time: 414.9225s / 158442.1998 s
env0_first_0:                 episode reward: -6.2500,                 loss: -0.1510
env0_second_0:                 episode reward: 6.2500,                 loss: -0.0895
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 3123.4,                last time consumption/overall running time: 387.0227s / 158829.2225 s
env0_first_0:                 episode reward: -6.9500,                 loss: -0.1590
env0_second_0:                 episode reward: 6.9500,                 loss: -0.0956
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 3479.15,                last time consumption/overall running time: 438.0441s / 159267.2666 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.1426
env0_second_0:                 episode reward: 4.6500,                 loss: -0.0939
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 3619.6,                last time consumption/overall running time: 450.9365s / 159718.2030 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.1446
env0_second_0:                 episode reward: 4.0000,                 loss: -0.0881
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 3579.25,                last time consumption/overall running time: 454.8464s / 160173.0494 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.1290
env0_second_0:                 episode reward: 5.8000,                 loss: -0.0613
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 2414.2,                last time consumption/overall running time: 304.2283s / 160477.2777 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.1331
env0_second_0:                 episode reward: 7.8500,                 loss: -0.0746
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 3508.9,                last time consumption/overall running time: 443.6940s / 160920.9717 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.1317
env0_second_0:                 episode reward: 4.7000,                 loss: -0.0728
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 3662.15,                last time consumption/overall running time: 463.8153s / 161384.7870 s
env0_first_0:                 episode reward: -5.5000,                 loss: -0.1297
env0_second_0:                 episode reward: 5.5000,                 loss: -0.0675
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 3259.25,                last time consumption/overall running time: 407.3255s / 161792.1125 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.1296
env0_second_0:                 episode reward: 6.1500,                 loss: -0.0757
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 3068.55,                last time consumption/overall running time: 379.4170s / 162171.5295 s
env0_first_0:                 episode reward: -6.4000,                 loss: -0.1317
env0_second_0:                 episode reward: 6.4000,                 loss: -0.0760
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 3485.45,                last time consumption/overall running time: 431.2098s / 162602.7393 s
env0_first_0:                 episode reward: -4.2500,                 loss: -0.1039
env0_second_0:                 episode reward: 4.2500,                 loss: -0.0565
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 3298.25,                last time consumption/overall running time: 408.3242s / 163011.0635 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.1119
env0_second_0:                 episode reward: 6.8000,                 loss: -0.0706
env1_first_0:                 episode reward: -4.6500,                 loss: nan