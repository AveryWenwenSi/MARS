pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [31, 19]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220116_0321/pettingzoo_surround_v1_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220116_0321/pettingzoo_surround_v1_nash_ppo.
Episode: 1/10000 (0.0100%),                 avg. length: 1061.0,                last time consumption/overall running time: 14.0891s / 14.0891 s
env0_first_0:                 episode reward: 9.0000,                 loss: -0.0207
env0_second_0:                 episode reward: -9.0000,                 loss: 0.0003
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1434.65,                last time consumption/overall running time: 367.4512s / 381.5403 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0382
env0_second_0:                 episode reward: 1.8500,                 loss: -0.0306
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1677.5,                last time consumption/overall running time: 427.1422s / 808.6825 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0412
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0393
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1996.0,                last time consumption/overall running time: 507.8586s / 1316.5411 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.0474
env0_second_0:                 episode reward: 2.8500,                 loss: -0.0450
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 2067.2,                last time consumption/overall running time: 526.7441s / 1843.2852 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.0693
env0_second_0:                 episode reward: 3.6500,                 loss: -0.0669
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 2214.55,                last time consumption/overall running time: 564.5772s / 2407.8624 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.0682
env0_second_0:                 episode reward: 1.4500,                 loss: -0.0696
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 2234.55,                last time consumption/overall running time: 567.0371s / 2974.8995 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.0838
env0_second_0:                 episode reward: 3.3000,                 loss: -0.0850
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 2142.9,                last time consumption/overall running time: 545.1678s / 3520.0673 s
env0_first_0:                 episode reward: -4.3000,                 loss: -0.0821
env0_second_0:                 episode reward: 4.3000,                 loss: -0.0804
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 2195.0,                last time consumption/overall running time: 560.1649s / 4080.2322 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.0826
env0_second_0:                 episode reward: 3.8000,                 loss: -0.0812
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 2288.95,                last time consumption/overall running time: 580.3277s / 4660.5599 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.0923
env0_second_0:                 episode reward: 3.2000,                 loss: -0.0916
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 2184.9,                last time consumption/overall running time: 556.1095s / 5216.6694 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.0993
env0_second_0:                 episode reward: 2.1000,                 loss: -0.0991
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 2111.0,                last time consumption/overall running time: 539.0500s / 5755.7194 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.1111
env0_second_0:                 episode reward: 3.9500,                 loss: -0.1112
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 2095.15,                last time consumption/overall running time: 535.1842s / 6290.9036 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.0944
env0_second_0:                 episode reward: 4.6500,                 loss: -0.0931
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 2176.25,                last time consumption/overall running time: 555.8170s / 6846.7206 s
env0_first_0:                 episode reward: -3.9000,                 loss: -0.0947
env0_second_0:                 episode reward: 3.9000,                 loss: -0.0914
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 2193.3,                last time consumption/overall running time: 562.1510s / 7408.8716 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.0862
env0_second_0:                 episode reward: 4.1000,                 loss: -0.0824
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 2316.25,                last time consumption/overall running time: 591.8012s / 8000.6728 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.0842
env0_second_0:                 episode reward: 2.4000,                 loss: -0.0780
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 2245.75,                last time consumption/overall running time: 573.5864s / 8574.2591 s
env0_first_0:                 episode reward: -4.3500,                 loss: -0.1033
env0_second_0:                 episode reward: 4.3500,                 loss: -0.0981
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 2196.45,                last time consumption/overall running time: 560.0987s / 9134.3579 s
env0_first_0:                 episode reward: -4.0500,                 loss: -0.1170
env0_second_0:                 episode reward: 4.0500,                 loss: -0.1127
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 2202.4,                last time consumption/overall running time: 559.8884s / 9694.2463 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1237
env0_second_0:                 episode reward: 4.4000,                 loss: -0.1210
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 2386.7,                last time consumption/overall running time: 606.8256s / 10301.0719 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1112
env0_second_0:                 episode reward: 1.8500,                 loss: -0.1043
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 2333.15,                last time consumption/overall running time: 592.2366s / 10893.3085 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.1059
env0_second_0:                 episode reward: 4.4500,                 loss: -0.1032
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 2376.2,                last time consumption/overall running time: 603.3813s / 11496.6899 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.0920
env0_second_0:                 episode reward: 2.9000,                 loss: -0.0884
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 2348.85,                last time consumption/overall running time: 599.0516s / 12095.7415 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.1095
env0_second_0:                 episode reward: 2.4500,                 loss: -0.1020
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 2383.8,                last time consumption/overall running time: 601.1353s / 12696.8769 s
env0_first_0:                 episode reward: -3.1500,                 loss: -0.1198
env0_second_0:                 episode reward: 3.1500,                 loss: -0.1112
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 2281.7,                last time consumption/overall running time: 577.6693s / 13274.5462 s
env0_first_0:                 episode reward: -3.6000,                 loss: -0.1357
env0_second_0:                 episode reward: 3.6000,                 loss: -0.1267
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 2131.6,                last time consumption/overall running time: 539.0024s / 13813.5486 s
env0_first_0:                 episode reward: -4.8500,                 loss: -0.1343
env0_second_0:                 episode reward: 4.8500,                 loss: -0.1335
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 2300.9,                last time consumption/overall running time: 580.3378s / 14393.8863 s
env0_first_0:                 episode reward: -3.4500,                 loss: -0.1236
env0_second_0:                 episode reward: 3.4500,                 loss: -0.1192
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 2238.0,                last time consumption/overall running time: 568.4175s / 14962.3038 s
env0_first_0:                 episode reward: -4.8500,                 loss: -0.1371
env0_second_0:                 episode reward: 4.8500,                 loss: -0.1334
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 2345.2,                last time consumption/overall running time: 593.4586s / 15555.7625 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.1344
env0_second_0:                 episode reward: 3.3000,                 loss: -0.1284
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 2331.1,                last time consumption/overall running time: 587.6906s / 16143.4531 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.1228
env0_second_0:                 episode reward: 4.5000,                 loss: -0.1157
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 2225.0,                last time consumption/overall running time: 563.9851s / 16707.4382 s
env0_first_0:                 episode reward: -5.0000,                 loss: -0.1328
env0_second_0:                 episode reward: 5.0000,                 loss: -0.1270
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 2162.9,                last time consumption/overall running time: 547.5085s / 17254.9467 s
env0_first_0:                 episode reward: -5.3500,                 loss: -0.1371
env0_second_0:                 episode reward: 5.3500,                 loss: -0.1292
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 2320.3,                last time consumption/overall running time: 585.8995s / 17840.8461 s
env0_first_0:                 episode reward: -5.0500,                 loss: -0.1317
env0_second_0:                 episode reward: 5.0500,                 loss: -0.1250
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 2189.2,                last time consumption/overall running time: 554.9237s / 18395.7698 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.1524
env0_second_0:                 episode reward: 5.4500,                 loss: -0.1469
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 2299.1,                last time consumption/overall running time: 582.8531s / 18978.6229 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.1298
env0_second_0:                 episode reward: 4.7000,                 loss: -0.1187
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 2308.15,                last time consumption/overall running time: 583.5070s / 19562.1298 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.1211
env0_second_0:                 episode reward: 3.8500,                 loss: -0.1100
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 2091.5,                last time consumption/overall running time: 529.9752s / 20092.1051 s
env0_first_0:                 episode reward: -6.9000,                 loss: -0.1500
env0_second_0:                 episode reward: 6.9000,                 loss: -0.1399
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 2079.4,                last time consumption/overall running time: 526.2204s / 20618.3255 s
env0_first_0:                 episode reward: -7.1000,                 loss: -0.1568
env0_second_0:                 episode reward: 7.1000,                 loss: -0.1483
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2269.9,                last time consumption/overall running time: 571.7922s / 21190.1176 s
env0_first_0:                 episode reward: -6.0000,                 loss: -0.1525
env0_second_0:                 episode reward: 6.0000,                 loss: -0.1410
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 2103.2,                last time consumption/overall running time: 532.5111s / 21722.6287 s
env0_first_0:                 episode reward: -4.0500,                 loss: -0.1560
env0_second_0:                 episode reward: 4.0500,                 loss: -0.1432
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 2266.2,                last time consumption/overall running time: 572.1732s / 22294.8020 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1320
env0_second_0:                 episode reward: 4.4000,                 loss: -0.1185
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 2469.85,                last time consumption/overall running time: 624.7631s / 22919.5651 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.1309
env0_second_0:                 episode reward: 2.8500,                 loss: -0.1146
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 2641.85,                last time consumption/overall running time: 667.9066s / 23587.4716 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.1233
env0_second_0:                 episode reward: 3.7500,                 loss: -0.0812
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 2713.8,                last time consumption/overall running time: 683.8647s / 24271.3364 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.1184
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0870
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 2799.1,                last time consumption/overall running time: 703.6985s / 24975.0349 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.1198
env0_second_0:                 episode reward: 2.5500,                 loss: -0.1052
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2989.7,                last time consumption/overall running time: 751.7172s / 25726.7521 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.1093
env0_second_0:                 episode reward: 0.8500,                 loss: -0.0941
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 2789.3,                last time consumption/overall running time: 699.9440s / 26426.6962 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.1129
env0_second_0:                 episode reward: 4.2000,                 loss: -0.0980
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 2731.0,                last time consumption/overall running time: 685.9621s / 27112.6583 s
env0_first_0:                 episode reward: -3.0500,                 loss: -0.0980
env0_second_0:                 episode reward: 3.0500,                 loss: -0.0798
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2909.35,                last time consumption/overall running time: 727.2019s / 27839.8602 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1055
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0893
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 2690.1,                last time consumption/overall running time: 673.2624s / 28513.1225 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.1104
env0_second_0:                 episode reward: 1.0500,                 loss: -0.0985
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 2844.15,                last time consumption/overall running time: 710.3356s / 29223.4581 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.1177
env0_second_0:                 episode reward: 2.3500,                 loss: -0.1006
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 2870.85,                last time consumption/overall running time: 717.2796s / 29940.7377 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.1100
env0_second_0:                 episode reward: 1.3500,                 loss: -0.0829
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 3046.45,                last time consumption/overall running time: 759.9164s / 30700.6541 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1155
env0_second_0:                 episode reward: -0.3500,                 loss: -0.0946
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 2891.55,                last time consumption/overall running time: 723.6187s / 31424.2727 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1135
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0917
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 3101.2,                last time consumption/overall running time: 773.1183s / 32197.3910 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.1065
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0931
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 3147.45,                last time consumption/overall running time: 786.6093s / 32984.0003 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1097
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0985
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 2978.75,                last time consumption/overall running time: 743.4811s / 33727.4813 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1177
env0_second_0:                 episode reward: -1.0500,                 loss: -0.1010
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 3065.3,                last time consumption/overall running time: 764.7620s / 34492.2433 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1194
env0_second_0:                 episode reward: 0.5500,                 loss: -0.0988
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 3063.2,                last time consumption/overall running time: 764.7908s / 35257.0341 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1085
env0_second_0:                 episode reward: 1.8500,                 loss: -0.0929
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 3077.1,                last time consumption/overall running time: 767.3802s / 36024.4143 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1025
env0_second_0:                 episode reward: -1.0500,                 loss: -0.0721
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 3258.05,                last time consumption/overall running time: 817.1125s / 36841.5267 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.1081
env0_second_0:                 episode reward: 0.9000,                 loss: -0.0840
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 2925.2,                last time consumption/overall running time: 734.0225s / 37575.5493 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1120
env0_second_0:                 episode reward: -1.4000,                 loss: -0.0925
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 3083.65,                last time consumption/overall running time: 770.5293s / 38346.0786 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.1128
env0_second_0:                 episode reward: 1.2000,                 loss: -0.0862
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 3064.95,                last time consumption/overall running time: 764.9140s / 39110.9926 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1134
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0880
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 3018.2,                last time consumption/overall running time: 752.9619s / 39863.9545 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1278
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0993
env1_first_0:                 episode reward: 0.2000,                 loss: nan