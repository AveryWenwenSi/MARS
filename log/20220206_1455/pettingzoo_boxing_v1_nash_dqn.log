pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 5, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220206_1455/pettingzoo_boxing_v1_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220206_1455/pettingzoo_boxing_v1_nash_dqn.
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 105.7524s / 105.7524 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0694
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0674
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2408.5531s / 2514.3055 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0761
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0761
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2670.8771s / 5185.1826 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0734
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0735
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2953.7236s / 8138.9062 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0737
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0734
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3014.5117s / 11153.4179 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0752
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0742
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2990.4541s / 14143.8720 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0799
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0769
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3001.9688s / 17145.8408 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0769
env0_second_0:                 episode reward: 4.4500,                 loss: 0.0726
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3006.3784s / 20152.2192 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0768
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0754
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2986.7208s / 23138.9400 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0767
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0749
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3023.4716s / 26162.4117 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0787
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0776
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3037.1179s / 29199.5296 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0808
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0806
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3050.2730s / 32249.8026 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0884
env0_second_0:                 episode reward: 0.6000,                 loss: 0.0913
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3008.8901s / 35258.6927 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0942
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0974
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3040.2040s / 38298.8967 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.1009
env0_second_0:                 episode reward: -1.9500,                 loss: 0.1012
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3041.6987s / 41340.5954 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.1078
env0_second_0:                 episode reward: -1.3500,                 loss: 0.1110
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3033.7662s / 44374.3616 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.1318
env0_second_0:                 episode reward: -1.3000,                 loss: 0.1285
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3040.9826s / 47415.3443 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.1406
env0_second_0:                 episode reward: -3.9500,                 loss: 0.1435
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3047.1812s / 50462.5254 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.1628
env0_second_0:                 episode reward: 0.5000,                 loss: 0.1578
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3026.6916s / 53489.2170 s
env0_first_0:                 episode reward: 8.3500,                 loss: 0.1754
env0_second_0:                 episode reward: -8.3500,                 loss: 0.1688
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3019.0322s / 56508.2492 s
env0_first_0:                 episode reward: -5.0000,                 loss: 0.1906
env0_second_0:                 episode reward: 5.0000,                 loss: 0.1822
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3045.7669s / 59554.0161 s
env0_first_0:                 episode reward: 2.1000,                 loss: 0.2071
env0_second_0:                 episode reward: -2.1000,                 loss: 0.1969
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3040.3521s / 62594.3682 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.1908
env0_second_0:                 episode reward: 1.3000,                 loss: 0.1933
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3045.5856s / 65639.9538 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1762
env0_second_0:                 episode reward: 1.1000,                 loss: 0.1793
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3039.4751s / 68679.4289 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1555
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1521
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3003.7272s / 71683.1561 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.1584
env0_second_0:                 episode reward: -0.5000,                 loss: 0.1558
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2948.9649s / 74632.1210 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.1651
env0_second_0:                 episode reward: 2.9000,                 loss: 0.1562
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2954.3047s / 77586.4257 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.1850
env0_second_0:                 episode reward: 1.5500,                 loss: 0.1784
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2952.9252s / 80539.3509 s
env0_first_0:                 episode reward: -4.3500,                 loss: 0.1763
env0_second_0:                 episode reward: 4.3500,                 loss: 0.1749
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2950.5702s / 83489.9212 s
env0_first_0:                 episode reward: -4.3000,                 loss: 0.1720
env0_second_0:                 episode reward: 4.3000,                 loss: 0.1694
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2940.3871s / 86430.3082 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.1589
env0_second_0:                 episode reward: 5.9500,                 loss: 0.1587
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2927.9752s / 89358.2834 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.1557
env0_second_0:                 episode reward: -0.0500,                 loss: 0.1537
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2954.1460s / 92312.4294 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.1621
env0_second_0:                 episode reward: 0.5000,                 loss: 0.1553
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2972.8869s / 95285.3163 s
env0_first_0:                 episode reward: -4.8500,                 loss: 0.1939
env0_second_0:                 episode reward: 4.8500,                 loss: 0.1777
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2964.7161s / 98250.0324 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.1956
env0_second_0:                 episode reward: 1.3500,                 loss: 0.1866
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2960.9722s / 101211.0046 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.1906
env0_second_0:                 episode reward: 0.5000,                 loss: 0.1781
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2952.5314s / 104163.5360 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.1780
env0_second_0:                 episode reward: 5.8000,                 loss: 0.1712
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2892.5970s / 107056.1329 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.1711
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1681
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2904.3705s / 109960.5034 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1841
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1748
env1_first_0:                 episode reward: -9.7500,                 loss: nan
env1_second_0:                 episode reward: 9.7500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 1778.25,                last time consumption/overall running time: 2883.9103s / 112844.4137 s
env0_first_0:                 episode reward: -9.7500,                 loss: 0.2118
env0_second_0:                 episode reward: 9.7500,                 loss: 0.1955
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2895.6025s / 115740.0162 s
env0_first_0:                 episode reward: -9.5500,                 loss: 0.2413
env0_second_0:                 episode reward: 9.5500,                 loss: 0.2201
env1_first_0:                 episode reward: -16.5000,                 loss: nan
env1_second_0:                 episode reward: 16.5000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2901.6026s / 118641.6188 s
env0_first_0:                 episode reward: -10.4000,                 loss: 0.2313
env0_second_0:                 episode reward: 10.4000,                 loss: 0.2274
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2906.1978s / 121547.8165 s
env0_first_0:                 episode reward: -10.7500,                 loss: 0.2258
env0_second_0:                 episode reward: 10.7500,                 loss: 0.2323
env1_first_0:                 episode reward: -10.5000,                 loss: nan
env1_second_0:                 episode reward: 10.5000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2892.9375s / 124440.7540 s
env0_first_0:                 episode reward: -10.0000,                 loss: 0.2252
env0_second_0:                 episode reward: 10.0000,                 loss: 0.2278
env1_first_0:                 episode reward: -11.1000,                 loss: nan
env1_second_0:                 episode reward: 11.1000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2865.8923s / 127306.6464 s
env0_first_0:                 episode reward: -10.4000,                 loss: 0.2315
env0_second_0:                 episode reward: 10.4000,                 loss: 0.2312
env1_first_0:                 episode reward: -14.3000,                 loss: nan
env1_second_0:                 episode reward: 14.3000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1777.3,                last time consumption/overall running time: 2846.9257s / 130153.5721 s
env0_first_0:                 episode reward: -14.4000,                 loss: 0.2571
env0_second_0:                 episode reward: 14.4000,                 loss: 0.2619
env1_first_0:                 episode reward: -19.6000,                 loss: nan
env1_second_0:                 episode reward: 19.6000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2860.6441s / 133014.2162 s
env0_first_0:                 episode reward: -8.5000,                 loss: 0.3140
env0_second_0:                 episode reward: 8.5000,                 loss: 0.3039
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2871.0431s / 135885.2593 s
env0_first_0:                 episode reward: -10.6000,                 loss: 0.3401
env0_second_0:                 episode reward: 10.6000,                 loss: 0.3175
env1_first_0:                 episode reward: -19.3000,                 loss: nan
env1_second_0:                 episode reward: 19.3000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1764.1,                last time consumption/overall running time: 2829.8917s / 138715.1511 s
env0_first_0:                 episode reward: -7.6500,                 loss: 0.3413
env0_second_0:                 episode reward: 7.6500,                 loss: 0.3212
env1_first_0:                 episode reward: -14.3500,                 loss: nan
env1_second_0:                 episode reward: 14.3500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1774.6,                last time consumption/overall running time: 2855.3153s / 141570.4664 s
env0_first_0:                 episode reward: -12.7000,                 loss: 0.3607
env0_second_0:                 episode reward: 12.7000,                 loss: 0.3360
env1_first_0:                 episode reward: -17.3500,                 loss: nan
env1_second_0:                 episode reward: 17.3500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1778.5,                last time consumption/overall running time: 2856.9029s / 144427.3693 s
env0_first_0:                 episode reward: -9.8000,                 loss: 0.3345
env0_second_0:                 episode reward: 9.8000,                 loss: 0.3230
env1_first_0:                 episode reward: -14.8000,                 loss: nan
env1_second_0:                 episode reward: 14.8000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1742.9,                last time consumption/overall running time: 2791.1798s / 147218.5491 s
env0_first_0:                 episode reward: -19.1000,                 loss: 0.3422
env0_second_0:                 episode reward: 19.1000,                 loss: 0.3185
env1_first_0:                 episode reward: -12.4500,                 loss: nan
env1_second_0:                 episode reward: 12.4500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1780.85,                last time consumption/overall running time: 2860.8950s / 150079.4441 s
env0_first_0:                 episode reward: -28.6500,                 loss: 0.3644
env0_second_0:                 episode reward: 28.6500,                 loss: 0.3348
env1_first_0:                 episode reward: -17.1000,                 loss: nan
env1_second_0:                 episode reward: 17.1000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1778.7,                last time consumption/overall running time: 2861.7310s / 152941.1750 s
env0_first_0:                 episode reward: -20.0500,                 loss: 0.3969
env0_second_0:                 episode reward: 20.0500,                 loss: 0.3815
env1_first_0:                 episode reward: -25.4000,                 loss: nan
env1_second_0:                 episode reward: 25.4000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1714.25,                last time consumption/overall running time: 2742.3245s / 155683.4995 s
env0_first_0:                 episode reward: -27.6500,                 loss: 0.4185
env0_second_0:                 episode reward: 27.6500,                 loss: 0.3930
env1_first_0:                 episode reward: -20.4000,                 loss: nan
env1_second_0:                 episode reward: 20.4000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1740.0,                last time consumption/overall running time: 2786.6488s / 158470.1483 s
env0_first_0:                 episode reward: -33.6000,                 loss: 0.4777
env0_second_0:                 episode reward: 33.6000,                 loss: 0.4289
env1_first_0:                 episode reward: -19.2500,                 loss: nan
env1_second_0:                 episode reward: 19.2500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1771.15,                last time consumption/overall running time: 2844.9708s / 161315.1192 s
env0_first_0:                 episode reward: -28.7000,                 loss: 0.4862
env0_second_0:                 episode reward: 28.7000,                 loss: 0.4533
env1_first_0:                 episode reward: -15.4500,                 loss: nan
env1_second_0:                 episode reward: 15.4500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1732.95,                last time consumption/overall running time: 2772.7827s / 164087.9018 s
env0_first_0:                 episode reward: -14.2000,                 loss: 0.5313
env0_second_0:                 episode reward: 14.2000,                 loss: 0.4618
env1_first_0:                 episode reward: -19.1000,                 loss: nan
env1_second_0:                 episode reward: 19.1000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1721.05,                last time consumption/overall running time: 2751.0500s / 166838.9519 s
env0_first_0:                 episode reward: -11.0000,                 loss: 0.4599
env0_second_0:                 episode reward: 11.0000,                 loss: 0.4371
env1_first_0:                 episode reward: -21.9500,                 loss: nan
env1_second_0:                 episode reward: 21.9500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1640.95,                last time consumption/overall running time: 2632.1525s / 169471.1044 s
env0_first_0:                 episode reward: -28.9000,                 loss: 0.4771
env0_second_0:                 episode reward: 28.9000,                 loss: 0.4321
env1_first_0:                 episode reward: -13.5500,                 loss: nan
env1_second_0:                 episode reward: 13.5500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1720.95,                last time consumption/overall running time: 2756.2250s / 172227.3294 s
env0_first_0:                 episode reward: -14.0500,                 loss: 0.4835
env0_second_0:                 episode reward: 14.0500,                 loss: 0.4416
env1_first_0:                 episode reward: -24.2000,                 loss: nan
env1_second_0:                 episode reward: 24.2000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1772.1,                last time consumption/overall running time: 2829.6953s / 175057.0247 s
env0_first_0:                 episode reward: -11.4500,                 loss: 0.4814
env0_second_0:                 episode reward: 11.4500,                 loss: 0.4285
env1_first_0:                 episode reward: -18.0000,                 loss: nan
env1_second_0:                 episode reward: 18.0000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1709.5,                last time consumption/overall running time: 2745.3912s / 177802.4158 s
env0_first_0:                 episode reward: -13.7500,                 loss: 0.4914
env0_second_0:                 episode reward: 13.7500,                 loss: 0.4624
env1_first_0:                 episode reward: -24.3500,                 loss: nan
env1_second_0:                 episode reward: 24.3500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1688.75,                last time consumption/overall running time: 2719.2216s / 180521.6375 s
env0_first_0:                 episode reward: -22.3500,                 loss: 0.4966
env0_second_0:                 episode reward: 22.3500,                 loss: 0.4684
env1_first_0:                 episode reward: -17.9500,                 loss: nan
env1_second_0:                 episode reward: 17.9500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1706.7,                last time consumption/overall running time: 2735.7448s / 183257.3822 s
env0_first_0:                 episode reward: -16.6000,                 loss: 0.5354
env0_second_0:                 episode reward: 16.6000,                 loss: 0.5102
env1_first_0:                 episode reward: -18.8500,                 loss: nan
env1_second_0:                 episode reward: 18.8500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1644.6,                last time consumption/overall running time: 2626.8267s / 185884.2089 s
env0_first_0:                 episode reward: -19.1500,                 loss: 0.6313
env0_second_0:                 episode reward: 19.1500,                 loss: 0.5561
env1_first_0:                 episode reward: -19.2000,                 loss: nan
env1_second_0:                 episode reward: 19.2000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1706.5,                last time consumption/overall running time: 2728.2505s / 188612.4595 s
env0_first_0:                 episode reward: -18.1500,                 loss: 0.6813
env0_second_0:                 episode reward: 18.1500,                 loss: 0.5901
env1_first_0:                 episode reward: -15.4000,                 loss: nan
env1_second_0:                 episode reward: 15.4000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1681.4,                last time consumption/overall running time: 2710.3626s / 191322.8221 s
env0_first_0:                 episode reward: -17.5500,                 loss: 0.7034
env0_second_0:                 episode reward: 17.5500,                 loss: 0.6279
env1_first_0:                 episode reward: -13.6500,                 loss: nan
env1_second_0:                 episode reward: 13.6500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1753.5,                last time consumption/overall running time: 2830.4169s / 194153.2390 s
env0_first_0:                 episode reward: -18.9000,                 loss: 0.6500
env0_second_0:                 episode reward: 18.9000,                 loss: 0.5863
env1_first_0:                 episode reward: -19.0500,                 loss: nan
env1_second_0:                 episode reward: 19.0500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1689.2,                last time consumption/overall running time: 2827.2480s / 196980.4870 s
env0_first_0:                 episode reward: -15.9000,                 loss: 0.6634
env0_second_0:                 episode reward: 15.9000,                 loss: 0.6040
env1_first_0:                 episode reward: -21.3500,                 loss: nan
env1_second_0:                 episode reward: 21.3500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1702.75,                last time consumption/overall running time: 2830.3763s / 199810.8633 s
env0_first_0:                 episode reward: -22.7500,                 loss: 0.6341
env0_second_0:                 episode reward: 22.7500,                 loss: 0.5811
env1_first_0:                 episode reward: -16.4500,                 loss: nan
env1_second_0:                 episode reward: 16.4500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1708.05,                last time consumption/overall running time: 2844.2102s / 202655.0734 s
env0_first_0:                 episode reward: -13.8000,                 loss: 0.5971
env0_second_0:                 episode reward: 13.8000,                 loss: 0.5528
env1_first_0:                 episode reward: -17.7500,                 loss: nan
env1_second_0:                 episode reward: 17.7500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1678.4,                last time consumption/overall running time: 2790.5583s / 205445.6317 s
env0_first_0:                 episode reward: -18.8000,                 loss: 0.6118
env0_second_0:                 episode reward: 18.8000,                 loss: 0.5467
env1_first_0:                 episode reward: -17.8500,                 loss: nan
env1_second_0:                 episode reward: 17.8500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 1678.3,                last time consumption/overall running time: 2757.6207s / 208203.2524 s
env0_first_0:                 episode reward: -10.9000,                 loss: 0.6366
env0_second_0:                 episode reward: 10.9000,                 loss: 0.5557
env1_first_0:                 episode reward: -22.6000,                 loss: nan
env1_second_0:                 episode reward: 22.6000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1700.15,                last time consumption/overall running time: 2703.1969s / 210906.4494 s
env0_first_0:                 episode reward: -21.8500,                 loss: 0.6858
env0_second_0:                 episode reward: 21.8500,                 loss: 0.5978
env1_first_0:                 episode reward: -22.3000,                 loss: nan
env1_second_0:                 episode reward: 22.3000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1718.0,                last time consumption/overall running time: 2737.3901s / 213643.8395 s
env0_first_0:                 episode reward: -14.3000,                 loss: 0.6736
env0_second_0:                 episode reward: 14.3000,                 loss: 0.5963
env1_first_0:                 episode reward: -21.9000,                 loss: nan
env1_second_0:                 episode reward: 21.9000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1711.5,                last time consumption/overall running time: 2718.6272s / 216362.4667 s
env0_first_0:                 episode reward: -20.2500,                 loss: 0.6251
env0_second_0:                 episode reward: 20.2500,                 loss: 0.5688
env1_first_0:                 episode reward: -13.1500,                 loss: nan
env1_second_0:                 episode reward: 13.1500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1765.85,                last time consumption/overall running time: 2825.3564s / 219187.8231 s
env0_first_0:                 episode reward: -13.9500,                 loss: 0.5419
env0_second_0:                 episode reward: 13.9500,                 loss: 0.5072
env1_first_0:                 episode reward: -19.3000,                 loss: nan
env1_second_0:                 episode reward: 19.3000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1667.4,                last time consumption/overall running time: 2672.0496s / 221859.8727 s
env0_first_0:                 episode reward: -24.0000,                 loss: 0.5593
env0_second_0:                 episode reward: 24.0000,                 loss: 0.5275
env1_first_0:                 episode reward: -24.2500,                 loss: nan
env1_second_0:                 episode reward: 24.2500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1663.0,                last time consumption/overall running time: 2648.9579s / 224508.8306 s
env0_first_0:                 episode reward: -24.8000,                 loss: 0.5724
env0_second_0:                 episode reward: 24.8000,                 loss: 0.5536
env1_first_0:                 episode reward: -20.2000,                 loss: nan
env1_second_0:                 episode reward: 20.2000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1745.65,                last time consumption/overall running time: 2791.7613s / 227300.5919 s
env0_first_0:                 episode reward: -14.6500,                 loss: 0.6119
env0_second_0:                 episode reward: 14.6500,                 loss: 0.5609
env1_first_0:                 episode reward: -19.5500,                 loss: nan
env1_second_0:                 episode reward: 19.5500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1692.4,                last time consumption/overall running time: 2702.9986s / 230003.5904 s
env0_first_0:                 episode reward: -21.0500,                 loss: 0.5801
env0_second_0:                 episode reward: 21.0500,                 loss: 0.5075
env1_first_0:                 episode reward: -13.6500,                 loss: nan
env1_second_0:                 episode reward: 13.6500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1764.7,                last time consumption/overall running time: 2807.7948s / 232811.3852 s
env0_first_0:                 episode reward: -25.1500,                 loss: 0.5580
env0_second_0:                 episode reward: 25.1500,                 loss: 0.4879
env1_first_0:                 episode reward: -18.3000,                 loss: nan
env1_second_0:                 episode reward: 18.3000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1683.65,                last time consumption/overall running time: 2673.3131s / 235484.6984 s
env0_first_0:                 episode reward: -8.1000,                 loss: 0.5291
env0_second_0:                 episode reward: 8.1000,                 loss: 0.4739
env1_first_0:                 episode reward: -27.7500,                 loss: nan
env1_second_0:                 episode reward: 27.7500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1755.65,                last time consumption/overall running time: 2792.2315s / 238276.9299 s
env0_first_0:                 episode reward: -14.7500,                 loss: 0.5056
env0_second_0:                 episode reward: 14.7500,                 loss: 0.4479
env1_first_0:                 episode reward: -20.6500,                 loss: nan
env1_second_0:                 episode reward: 20.6500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1688.1,                last time consumption/overall running time: 2679.4572s / 240956.3871 s
env0_first_0:                 episode reward: -19.7000,                 loss: 0.5006
env0_second_0:                 episode reward: 19.7000,                 loss: 0.4414
env1_first_0:                 episode reward: -19.2000,                 loss: nan
env1_second_0:                 episode reward: 19.2000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1737.1,                last time consumption/overall running time: 2770.6257s / 243727.0128 s
env0_first_0:                 episode reward: -9.2500,                 loss: 0.4235
env0_second_0:                 episode reward: 9.2500,                 loss: 0.3740
env1_first_0:                 episode reward: -18.1000,                 loss: nan
env1_second_0:                 episode reward: 18.1000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1661.35,                last time consumption/overall running time: 2640.9247s / 246367.9375 s
env0_first_0:                 episode reward: -22.4500,                 loss: 0.4381
env0_second_0:                 episode reward: 22.4500,                 loss: 0.3985
env1_first_0:                 episode reward: -21.9500,                 loss: nan
env1_second_0:                 episode reward: 21.9500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1695.05,                last time consumption/overall running time: 2699.5666s / 249067.5040 s
env0_first_0:                 episode reward: -21.8000,                 loss: 0.5043
env0_second_0:                 episode reward: 21.8000,                 loss: 0.4648
env1_first_0:                 episode reward: -19.8500,                 loss: nan
env1_second_0:                 episode reward: 19.8500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1772.7,                last time consumption/overall running time: 2805.4067s / 251872.9108 s
env0_first_0:                 episode reward: -9.2500,                 loss: 0.5546
env0_second_0:                 episode reward: 9.2500,                 loss: 0.5122
env1_first_0:                 episode reward: -11.5500,                 loss: nan
env1_second_0:                 episode reward: 11.5500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1666.5,                last time consumption/overall running time: 2644.3879s / 254517.2987 s
env0_first_0:                 episode reward: -29.4000,                 loss: 0.5955
env0_second_0:                 episode reward: 29.4000,                 loss: 0.5281
env1_first_0:                 episode reward: -15.4000,                 loss: nan
env1_second_0:                 episode reward: 15.4000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1759.35,                last time consumption/overall running time: 2791.2269s / 257308.5255 s
env0_first_0:                 episode reward: -15.6000,                 loss: 0.5968
env0_second_0:                 episode reward: 15.6000,                 loss: 0.5467
env1_first_0:                 episode reward: -19.9500,                 loss: nan
env1_second_0:                 episode reward: 19.9500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1651.45,                last time consumption/overall running time: 2628.3238s / 259936.8493 s
env0_first_0:                 episode reward: -24.2000,                 loss: 0.5848
env0_second_0:                 episode reward: 24.2000,                 loss: 0.5340
env1_first_0:                 episode reward: -17.6500,                 loss: nan
env1_second_0:                 episode reward: 17.6500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1568.45,                last time consumption/overall running time: 2511.6156s / 262448.4649 s
env0_first_0:                 episode reward: -35.0500,                 loss: 0.6262
env0_second_0:                 episode reward: 35.0500,                 loss: 0.5874
env1_first_0:                 episode reward: -23.1000,                 loss: nan
env1_second_0:                 episode reward: 23.1000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1570.6,                last time consumption/overall running time: 2495.4332s / 264943.8981 s
env0_first_0:                 episode reward: -23.0000,                 loss: 0.7038
env0_second_0:                 episode reward: 23.0000,                 loss: 0.6328
env1_first_0:                 episode reward: -31.8500,                 loss: nan
env1_second_0:                 episode reward: 31.8500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1451.75,                last time consumption/overall running time: 2313.3555s / 267257.2536 s
env0_first_0:                 episode reward: -22.2500,                 loss: 0.8500
env0_second_0:                 episode reward: 22.2500,                 loss: 0.7444
env1_first_0:                 episode reward: -34.5500,                 loss: nan
env1_second_0:                 episode reward: 34.5500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1683.05,                last time consumption/overall running time: 2681.8625s / 269939.1160 s
env0_first_0:                 episode reward: -11.5500,                 loss: 0.8377
env0_second_0:                 episode reward: 11.5500,                 loss: 0.7593
env1_first_0:                 episode reward: -16.4500,                 loss: nan
env1_second_0:                 episode reward: 16.4500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1671.15,                last time consumption/overall running time: 2661.5469s / 272600.6630 s
env0_first_0:                 episode reward: -16.2500,                 loss: 0.7654
env0_second_0:                 episode reward: 16.2500,                 loss: 0.6736
env1_first_0:                 episode reward: -20.6000,                 loss: nan
env1_second_0:                 episode reward: 20.6000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 1731.85,                last time consumption/overall running time: 2915.9105s / 275516.5735 s