pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 5, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220206_1455/pettingzoo_pong_v2_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220206_1455/pettingzoo_pong_v2_nash_dqn.
Episode: 1/10000 (0.0100%),                 avg. length: 1348.0,                last time consumption/overall running time: 54.4544s / 54.4544 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0809
env0_second_0:                 episode reward: -4.0000,                 loss: 0.0785
env1_first_0:                 episode reward: -10.0000,                 loss: nan
env1_second_0:                 episode reward: 10.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1310.25,                last time consumption/overall running time: 1226.7816s / 1281.2360 s
env0_first_0:                 episode reward: 2.8000,                 loss: 0.0896
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0901
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1520.2,                last time consumption/overall running time: 1589.2458s / 2870.4818 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.1330
env0_second_0:                 episode reward: -1.7500,                 loss: 0.1278
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1590.95,                last time consumption/overall running time: 1863.7809s / 4734.2627 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.1160
env0_second_0:                 episode reward: 4.9500,                 loss: 0.1208
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1629.25,                last time consumption/overall running time: 2102.9288s / 6837.1915 s
env0_first_0:                 episode reward: -12.2500,                 loss: 0.1026
env0_second_0:                 episode reward: 12.2500,                 loss: 0.1002
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1800.5,                last time consumption/overall running time: 2346.7473s / 9183.9388 s
env0_first_0:                 episode reward: -11.8500,                 loss: 0.0899
env0_second_0:                 episode reward: 11.8500,                 loss: 0.0868
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 2108.85,                last time consumption/overall running time: 2738.8047s / 11922.7435 s
env0_first_0:                 episode reward: -8.7500,                 loss: 0.0699
env0_second_0:                 episode reward: 8.7500,                 loss: 0.0656
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 2293.6,                last time consumption/overall running time: 2967.2843s / 14890.0278 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0610
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0586
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 2226.05,                last time consumption/overall running time: 2899.7999s / 17789.8277 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0566
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0571
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 2346.35,                last time consumption/overall running time: 3082.0147s / 20871.8424 s
env0_first_0:                 episode reward: -6.5500,                 loss: 0.0581
env0_second_0:                 episode reward: 6.5500,                 loss: 0.0581
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 2352.7,                last time consumption/overall running time: 3082.4659s / 23954.3083 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0560
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0567
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 2483.3,                last time consumption/overall running time: 3246.6166s / 27200.9249 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0580
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0560
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 2349.7,                last time consumption/overall running time: 3075.8205s / 30276.7454 s
env0_first_0:                 episode reward: -7.8500,                 loss: 0.0554
env0_second_0:                 episode reward: 7.8500,                 loss: 0.0553
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 2362.4,                last time consumption/overall running time: 3090.6013s / 33367.3467 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0556
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0544
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 2349.55,                last time consumption/overall running time: 3062.3093s / 36429.6561 s
env0_first_0:                 episode reward: -7.3500,                 loss: 0.0543
env0_second_0:                 episode reward: 7.3500,                 loss: 0.0532
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 2388.0,                last time consumption/overall running time: 3123.6697s / 39553.3258 s
env0_first_0:                 episode reward: -7.6500,                 loss: 0.0524
env0_second_0:                 episode reward: 7.6500,                 loss: 0.0522
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 2453.9,                last time consumption/overall running time: 3213.5603s / 42766.8861 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0536
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0517
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 2421.2,                last time consumption/overall running time: 3154.3862s / 45921.2723 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0531
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0511
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 2468.3,                last time consumption/overall running time: 3225.4344s / 49146.7068 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0536
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0536
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 2367.2,                last time consumption/overall running time: 3116.6113s / 52263.3181 s
env0_first_0:                 episode reward: -4.7000,                 loss: 0.0545
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0546
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 2463.15,                last time consumption/overall running time: 3211.8557s / 55475.1738 s
env0_first_0:                 episode reward: -5.2500,                 loss: 0.0536
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0535
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 2208.3,                last time consumption/overall running time: 2886.2356s / 58361.4094 s
env0_first_0:                 episode reward: -7.5500,                 loss: 0.0539
env0_second_0:                 episode reward: 7.5500,                 loss: 0.0525
env1_first_0:                 episode reward: -9.9500,                 loss: nan
env1_second_0:                 episode reward: 9.9500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 2504.9,                last time consumption/overall running time: 3282.1832s / 61643.5927 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0520
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0514
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 2452.05,                last time consumption/overall running time: 3194.7820s / 64838.3747 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0514
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0515
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 2502.35,                last time consumption/overall running time: 3279.6885s / 68118.0632 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0524
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0526
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 2535.75,                last time consumption/overall running time: 3323.8324s / 71441.8956 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0520
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0516
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 2354.55,                last time consumption/overall running time: 3064.0317s / 74505.9273 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0522
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0508
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 2533.65,                last time consumption/overall running time: 3285.8748s / 77791.8021 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0519
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0513
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 2406.4,                last time consumption/overall running time: 3148.3114s / 80940.1135 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0510
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0501
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 2443.4,                last time consumption/overall running time: 3181.9811s / 84122.0946 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0497
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0496
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 2425.5,                last time consumption/overall running time: 3143.5554s / 87265.6501 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0493
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0488
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 2553.6,                last time consumption/overall running time: 3332.2158s / 90597.8659 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0497
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0496
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 2437.75,                last time consumption/overall running time: 3167.1635s / 93765.0294 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0488
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0502
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 2552.1,                last time consumption/overall running time: 3316.2687s / 97081.2981 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0485
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0483
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 2329.05,                last time consumption/overall running time: 3038.0563s / 100119.3544 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.0479
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0474
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 2400.4,                last time consumption/overall running time: 3108.7825s / 103228.1369 s
env0_first_0:                 episode reward: -7.5000,                 loss: 0.0487
env0_second_0:                 episode reward: 7.5000,                 loss: 0.0490
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 2604.45,                last time consumption/overall running time: 3382.9201s / 106611.0570 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0489
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0489
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 2494.55,                last time consumption/overall running time: 3243.6936s / 109854.7506 s
env0_first_0:                 episode reward: -7.3500,                 loss: 0.0493
env0_second_0:                 episode reward: 7.3500,                 loss: 0.0502
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2405.5,                last time consumption/overall running time: 3130.6565s / 112985.4072 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0503
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0514
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 2399.75,                last time consumption/overall running time: 3118.4859s / 116103.8931 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0511
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0504
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 2366.0,                last time consumption/overall running time: 3090.0300s / 119193.9231 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0499
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0494
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 2466.2,                last time consumption/overall running time: 3232.2803s / 122426.2034 s
env0_first_0:                 episode reward: -4.6500,                 loss: 0.0504
env0_second_0:                 episode reward: 4.6500,                 loss: 0.0498
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 2452.8,                last time consumption/overall running time: 3184.3869s / 125610.5903 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0508
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0490
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 2471.0,                last time consumption/overall running time: 3211.2385s / 128821.8288 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0497
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0488
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 2387.95,                last time consumption/overall running time: 3092.1578s / 131913.9866 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0494
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0495
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2309.1,                last time consumption/overall running time: 2983.3573s / 134897.3438 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0488
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0485
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 2477.95,                last time consumption/overall running time: 3189.5039s / 138086.8477 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0492
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0493
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 2399.5,                last time consumption/overall running time: 3102.4958s / 141189.3435 s
env0_first_0:                 episode reward: -4.3000,                 loss: 0.0507
env0_second_0:                 episode reward: 4.3000,                 loss: 0.0495
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2447.2,                last time consumption/overall running time: 3155.8250s / 144345.1686 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0511
env0_second_0:                 episode reward: 3.6000,                 loss: 0.0513
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 2392.5,                last time consumption/overall running time: 3086.6151s / 147431.7836 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0507
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0510
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 2311.8,                last time consumption/overall running time: 2997.8541s / 150429.6377 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0499
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0502
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 2076.9,                last time consumption/overall running time: 2629.4785s / 153059.1162 s
env0_first_0:                 episode reward: -8.2000,                 loss: 0.0496
env0_second_0:                 episode reward: 8.2000,                 loss: 0.0499
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 2180.5,                last time consumption/overall running time: 2716.6781s / 155775.7943 s
env0_first_0:                 episode reward: -8.5500,                 loss: 0.0506
env0_second_0:                 episode reward: 8.5500,                 loss: 0.0505
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 2271.15,                last time consumption/overall running time: 2845.1849s / 158620.9792 s
env0_first_0:                 episode reward: -9.1000,                 loss: 0.0492
env0_second_0:                 episode reward: 9.1000,                 loss: 0.0495
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 2185.8,                last time consumption/overall running time: 2742.0747s / 161363.0539 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0499
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0501
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 2431.65,                last time consumption/overall running time: 3018.1227s / 164381.1766 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0520
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0519
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 2359.75,                last time consumption/overall running time: 2945.5325s / 167326.7091 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0524
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0518
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 2277.4,                last time consumption/overall running time: 2845.5188s / 170172.2279 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0519
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0515
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 2398.75,                last time consumption/overall running time: 2979.6500s / 173151.8779 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0517
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0518
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 2313.9,                last time consumption/overall running time: 2875.9037s / 176027.7816 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0512
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0511
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 2342.85,                last time consumption/overall running time: 2947.7701s / 178975.5516 s
env0_first_0:                 episode reward: -5.3000,                 loss: 0.0508
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0510
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 2426.1,                last time consumption/overall running time: 3021.5972s / 181997.1488 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0505
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0511
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 2358.6,                last time consumption/overall running time: 2892.8298s / 184889.9786 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0515
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0519
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 2350.7,                last time consumption/overall running time: 2894.5112s / 187784.4898 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0526
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0521
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 2316.5,                last time consumption/overall running time: 2829.0348s / 190613.5246 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0525
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0523
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 2296.35,                last time consumption/overall running time: 2806.5172s / 193420.0418 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0532
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0532
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 2254.65,                last time consumption/overall running time: 2771.7147s / 196191.7565 s
env0_first_0:                 episode reward: -5.8500,                 loss: 0.0519
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0529
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 2326.5,                last time consumption/overall running time: 2855.8098s / 199047.5663 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0523
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0514
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 2310.0,                last time consumption/overall running time: 2852.3862s / 201899.9525 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0527
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0518
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 2207.45,                last time consumption/overall running time: 2732.1082s / 204632.0606 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0527
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0505
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 2471.45,                last time consumption/overall running time: 3014.2388s / 207646.2994 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0526
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0527
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 2219.3,                last time consumption/overall running time: 2712.1862s / 210358.4856 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0532
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0531
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 2211.35,                last time consumption/overall running time: 2726.6663s / 213085.1520 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0535
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0540
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 2326.55,                last time consumption/overall running time: 2840.3927s / 215925.5446 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0534
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0543
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 2333.75,                last time consumption/overall running time: 2852.2311s / 218777.7757 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0521
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0518
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 2322.3,                last time consumption/overall running time: 2855.8010s / 221633.5767 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0519
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0517
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 2394.15,                last time consumption/overall running time: 2956.4018s / 224589.9785 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0534
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0526
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 2300.25,                last time consumption/overall running time: 2824.3791s / 227414.3576 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0535
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0527
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 2279.75,                last time consumption/overall running time: 2797.1332s / 230211.4909 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0531
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0526
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 2387.1,                last time consumption/overall running time: 2920.6565s / 233132.1473 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.0526
env0_second_0:                 episode reward: 3.9000,                 loss: 0.0528
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 2280.5,                last time consumption/overall running time: 2777.7500s / 235909.8973 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0537
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0523
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 2105.9,                last time consumption/overall running time: 2549.5585s / 238459.4558 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0530
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0526
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 2135.75,                last time consumption/overall running time: 2585.3811s / 241044.8370 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0532
env0_second_0:                 episode reward: 7.7000,                 loss: 0.0536
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 2180.4,                last time consumption/overall running time: 2633.5901s / 243678.4271 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0534
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0541
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 2219.3,                last time consumption/overall running time: 2631.2737s / 246309.7008 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0534
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0538
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 2238.25,                last time consumption/overall running time: 2664.4340s / 248974.1348 s
env0_first_0:                 episode reward: -6.9000,                 loss: 0.0539
env0_second_0:                 episode reward: 6.9000,                 loss: 0.0532
env1_first_0:                 episode reward: -8.3000,                 loss: nan
env1_second_0:                 episode reward: 8.3000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 2176.65,                last time consumption/overall running time: 2586.4354s / 251560.5702 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0548
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0536
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 2194.35,                last time consumption/overall running time: 2599.9737s / 254160.5439 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.0547
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0535
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 2198.2,                last time consumption/overall running time: 2622.6215s / 256783.1654 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.0546
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0535
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 2274.8,                last time consumption/overall running time: 2702.2326s / 259485.3980 s
env0_first_0:                 episode reward: -6.5500,                 loss: 0.0536
env0_second_0:                 episode reward: 6.5500,                 loss: 0.0532
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 2103.8,                last time consumption/overall running time: 2503.6386s / 261989.0366 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0542
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0537
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 2249.9,                last time consumption/overall running time: 2675.5568s / 264664.5934 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0550
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0545
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 2157.15,                last time consumption/overall running time: 2574.1630s / 267238.7564 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0553
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0553
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 2230.2,                last time consumption/overall running time: 2663.9098s / 269902.6661 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0560
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0554
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 2279.25,                last time consumption/overall running time: 2709.5454s / 272612.2115 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0553
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0548
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 2160.35,                last time consumption/overall running time: 2570.3044s / 275182.5159 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0548
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0544
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 2293.7,                last time consumption/overall running time: 2727.1177s / 277909.6336 s
env0_first_0:                 episode reward: -5.3000,                 loss: 0.0549
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0543
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 2155.4,                last time consumption/overall running time: 2524.2402s / 280433.8738 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0538