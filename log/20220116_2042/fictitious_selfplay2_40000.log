2022-01-16 20:42:50.500239: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/quantumiracle/.mujoco/mujoco200/bin:/home/quantumiracle/.mujoco/mujoco210/bin:/usr/lib/nvidia:/home/quantumiracle/.mujoco/mujoco200/bin:/home/quantumiracle/.mujoco/mujoco210/bin:/usr/lib/nvidia
2022-01-16 20:42:50.500336: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/quantumiracle/.mujoco/mujoco200/bin:/home/quantumiracle/.mujoco/mujoco210/bin:/usr/lib/nvidia:/home/quantumiracle/.mujoco/mujoco200/bin:/home/quantumiracle/.mujoco/mujoco210/bin:/usr/lib/nvidia
2022-01-16 20:42:50.500343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
pygame 2.0.1 (SDL 2.0.14, Python 3.6.13)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7fc69addcda0>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [array([0.027, 0.027, 0.027, ..., 0.027, 0.027, 0.027]) array([0.028, 0.028, 0.028, ..., 0.028, 0.028, 0.028])]
Load checkpoints (policy family):  [list(['88', '1915', '4963', '5923', '7340', '7985', '8424', '10067', '10446', '11112', '11422', '15565', '15851', '16856', '17881', '18823', '19577', '20095', '21033', '24228', '24348', '24585', '25076', '25575', '26509', '26943', '28017', '29166', '30404', '30938', '32176', '33619', '33877', '35986', '36465', '37045', '39526'])
 list(['144', '2004', '5072', '6251', '7475', '8034', '8500', '10105', '10488', '11284', '11610', '15591', '15898', '16965', '17967', '18846', '19615', '20177', '21054', '24263', '24370', '24629', '25123', '25633', '26531', '26967', '28038', '29206', '30429', '31119', '32210', '33651', '33937', '36023', '36494', '37092'])]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220116200302/epi_40000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/quantumiracle/research/MARS/data/model/20220116200302_exploit_40000/mdp_arbitrary_mdp_fictitious_selfplay2. 
 Save logs to: /home/quantumiracle/research/MARS/data/log/20220116200302_exploit_40000/mdp_arbitrary_mdp_fictitious_selfplay2.
Episode: 1/10000 (0.0100%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3983s / 1.3983 s
agent0:                 episode reward: 1.1306,                 loss: nan
agent1:                 episode reward: -1.1306,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0280s / 8.4264 s
agent0:                 episode reward: 0.0507,                 loss: nan
agent1:                 episode reward: -0.0507,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1498s / 15.5762 s
agent0:                 episode reward: 0.2436,                 loss: nan
agent1:                 episode reward: -0.2436,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8522s / 23.4284 s
agent0:                 episode reward: 0.3138,                 loss: nan
agent1:                 episode reward: -0.3138,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7217s / 31.1501 s
agent0:                 episode reward: 0.1347,                 loss: nan
agent1:                 episode reward: -0.1347,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8181s / 38.9682 s
agent0:                 episode reward: 0.0107,                 loss: nan
agent1:                 episode reward: -0.0107,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8576s / 46.8258 s
agent0:                 episode reward: 0.4409,                 loss: nan
agent1:                 episode reward: -0.4409,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3806s / 55.2064 s
agent0:                 episode reward: 0.2016,                 loss: nan
agent1:                 episode reward: -0.2016,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9297s / 63.1361 s
agent0:                 episode reward: 0.2389,                 loss: nan
agent1:                 episode reward: -0.2389,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5279s / 71.6640 s
agent0:                 episode reward: 0.2790,                 loss: nan
agent1:                 episode reward: -0.2790,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9727s / 79.6366 s
agent0:                 episode reward: -0.0699,                 loss: nan
agent1:                 episode reward: 0.0699,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 2.0,                last time consumption/overall running time: 73.3721s / 153.0088 s
agent0:                 episode reward: 0.0547,                 loss: nan
agent1:                 episode reward: -0.0547,                 loss: 0.1932
Episode: 241/10000 (2.4100%),                 avg. length: 2.0,                last time consumption/overall running time: 163.0519s / 316.0606 s
agent0:                 episode reward: 0.4018,                 loss: nan
agent1:                 episode reward: -0.4018,                 loss: 0.1800
Episode: 261/10000 (2.6100%),                 avg. length: 2.0,                last time consumption/overall running time: 158.7561s / 474.8167 s
agent0:                 episode reward: 0.3489,                 loss: nan
agent1:                 episode reward: -0.3489,                 loss: 0.1726
Episode: 281/10000 (2.8100%),                 avg. length: 2.0,                last time consumption/overall running time: 156.9119s / 631.7287 s
agent0:                 episode reward: 0.1776,                 loss: nan
agent1:                 episode reward: -0.1776,                 loss: 0.1666
Episode: 301/10000 (3.0100%),                 avg. length: 2.0,                last time consumption/overall running time: 159.5831s / 791.3118 s
agent0:                 episode reward: 0.2416,                 loss: nan
agent1:                 episode reward: -0.2416,                 loss: 0.1608
Episode: 321/10000 (3.2100%),                 avg. length: 2.0,                last time consumption/overall running time: 158.8089s / 950.1207 s
agent0:                 episode reward: -0.0890,                 loss: nan