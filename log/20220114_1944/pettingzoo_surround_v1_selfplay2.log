pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [93, 36]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=5, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=5, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 16, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220114_1944/pettingzoo_surround_v1_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220114_1944/pettingzoo_surround_v1_selfplay2.
Episode: 1/10000 (0.0100%),                 avg. length: 1674.0,                last time consumption/overall running time: 17.4164s / 17.4164 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0033
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1305.2,                last time consumption/overall running time: 248.9617s / 266.3781 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0040
env0_second_0:                 episode reward: 6.0000,                 loss: nan
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1347.15,                last time consumption/overall running time: 267.4243s / 533.8024 s
env0_first_0:                 episode reward: 3.9000,                 loss: 0.0058
env0_second_0:                 episode reward: -3.9000,                 loss: nan
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1179.55,                last time consumption/overall running time: 236.8698s / 770.6722 s
env0_first_0:                 episode reward: 7.3000,                 loss: 0.0061
env0_second_0:                 episode reward: -7.3000,                 loss: nan
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1558.6,                last time consumption/overall running time: 306.3180s / 1076.9902 s
env0_first_0:                 episode reward: 5.2500,                 loss: nan
env0_second_0:                 episode reward: -5.2500,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Score delta: 16.2, update the opponent.
Episode: 101/10000 (1.0100%),                 avg. length: 1734.3,                last time consumption/overall running time: 344.2070s / 1421.1972 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0034
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1861.7,                last time consumption/overall running time: 375.8148s / 1797.0120 s
env0_first_0:                 episode reward: -0.4500,                 loss: nan
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0024
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1777.1,                last time consumption/overall running time: 358.5430s / 2155.5550 s
env0_first_0:                 episode reward: -3.9500,                 loss: nan
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0020
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1729.45,                last time consumption/overall running time: 350.4254s / 2505.9805 s
env0_first_0:                 episode reward: -4.7000,                 loss: nan
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0016
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1737.85,                last time consumption/overall running time: 349.0894s / 2855.0699 s
env0_first_0:                 episode reward: -3.5500,                 loss: nan
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0016
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1669.55,                last time consumption/overall running time: 336.5001s / 3191.5700 s
env0_first_0:                 episode reward: -4.2500,                 loss: nan
env0_second_0:                 episode reward: 4.2500,                 loss: 0.0014
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1618.35,                last time consumption/overall running time: 325.1034s / 3516.6734 s
env0_first_0:                 episode reward: -5.3500,                 loss: nan
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0015
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1513.7,                last time consumption/overall running time: 302.1726s / 3818.8460 s
env0_first_0:                 episode reward: -5.6000,                 loss: nan
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0014
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1427.15,                last time consumption/overall running time: 287.2429s / 4106.0888 s
env0_first_0:                 episode reward: -4.7000,                 loss: nan
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0012
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1492.2,                last time consumption/overall running time: 298.0899s / 4404.1787 s
env0_first_0:                 episode reward: -5.1000,                 loss: nan
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0011
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1543.15,                last time consumption/overall running time: 306.3356s / 4710.5143 s
env0_first_0:                 episode reward: -4.1500,                 loss: nan
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0012
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1341.05,                last time consumption/overall running time: 267.7940s / 4978.3082 s
env0_first_0:                 episode reward: -7.0500,                 loss: nan
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0011
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1316.8,                last time consumption/overall running time: 264.4743s / 5242.7825 s
env0_first_0:                 episode reward: -5.8000,                 loss: nan
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0011
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1419.8,                last time consumption/overall running time: 283.3400s / 5526.1225 s
env0_first_0:                 episode reward: -6.2000,                 loss: nan
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0010
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1391.7,                last time consumption/overall running time: 278.6066s / 5804.7290 s
env0_first_0:                 episode reward: -7.2500,                 loss: nan
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0010
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1268.05,                last time consumption/overall running time: 252.6074s / 6057.3365 s
env0_first_0:                 episode reward: -6.9000,                 loss: nan
env0_second_0:                 episode reward: 6.9000,                 loss: 0.0010
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1410.3,                last time consumption/overall running time: 281.1046s / 6338.4410 s
env0_first_0:                 episode reward: -6.6500,                 loss: nan
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0009
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1383.4,                last time consumption/overall running time: 275.7346s / 6614.1756 s
env0_first_0:                 episode reward: -6.5000,                 loss: nan
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0010
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1339.15,                last time consumption/overall running time: 267.3152s / 6881.4909 s
env0_first_0:                 episode reward: -7.0500,                 loss: nan
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0010
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1390.5,                last time consumption/overall running time: 277.9396s / 7159.4304 s
env0_first_0:                 episode reward: -6.2500,                 loss: nan
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0010
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1377.05,                last time consumption/overall running time: 275.6828s / 7435.1133 s
env0_first_0:                 episode reward: -5.5500,                 loss: nan
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0010
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1302.05,                last time consumption/overall running time: 259.4623s / 7694.5756 s
env0_first_0:                 episode reward: -6.8000,                 loss: nan
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0010
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1323.95,                last time consumption/overall running time: 262.2893s / 7956.8649 s
env0_first_0:                 episode reward: -6.0500,                 loss: nan
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0009
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1393.5,                last time consumption/overall running time: 279.6508s / 8236.5157 s
env0_first_0:                 episode reward: -7.3000,                 loss: nan
env0_second_0:                 episode reward: 7.3000,                 loss: 0.0010
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1347.55,                last time consumption/overall running time: 269.4038s / 8505.9195 s
env0_first_0:                 episode reward: -6.4500,                 loss: nan
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0009
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1595.95,                last time consumption/overall running time: 319.2625s / 8825.1820 s
env0_first_0:                 episode reward: -5.5500,                 loss: nan
env0_second_0:                 episode reward: 5.5500,                 loss: nan
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Score delta: 16.2, update the opponent.
Episode: 621/10000 (6.2100%),                 avg. length: 1827.2,                last time consumption/overall running time: 366.3531s / 9191.5351 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0021
env0_second_0:                 episode reward: -1.3500,                 loss: nan
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1520.15,                last time consumption/overall running time: 304.4018s / 9495.9370 s
env0_first_0:                 episode reward: 5.2500,                 loss: 0.0020
env0_second_0:                 episode reward: -5.2500,                 loss: nan
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1340.0,                last time consumption/overall running time: 268.3336s / 9764.2706 s
env0_first_0:                 episode reward: 2.5500,                 loss: 0.0017
env0_second_0:                 episode reward: -2.5500,                 loss: nan
env1_first_0:                 episode reward: 7.8500,                 loss: nan
env1_second_0:                 episode reward: -7.8500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1382.15,                last time consumption/overall running time: 275.5648s / 10039.8354 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0025
env0_second_0:                 episode reward: 0.8500,                 loss: nan
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1454.45,                last time consumption/overall running time: 289.4551s / 10329.2905 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.0049
env0_second_0:                 episode reward: -1.7500,                 loss: nan
env1_first_0:                 episode reward: 7.4000,                 loss: nan
env1_second_0:                 episode reward: -7.4000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1290.3,                last time consumption/overall running time: 258.6600s / 10587.9506 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0021
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 8.3500,                 loss: nan
env1_second_0:                 episode reward: -8.3500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1148.65,                last time consumption/overall running time: 229.7872s / 10817.7377 s
env0_first_0:                 episode reward: 7.0500,                 loss: 0.0013
env0_second_0:                 episode reward: -7.0500,                 loss: nan
env1_first_0:                 episode reward: 8.3000,                 loss: nan
env1_second_0:                 episode reward: -8.3000,                 loss: nan
Score delta: 16.2, update the opponent.
Episode: 761/10000 (7.6100%),                 avg. length: 1521.9,                last time consumption/overall running time: 305.2508s / 11122.9886 s
env0_first_0:                 episode reward: 6.0500,                 loss: nan
env0_second_0:                 episode reward: -6.0500,                 loss: 0.0030
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1984.75,                last time consumption/overall running time: 398.4431s / 11521.4317 s
env0_first_0:                 episode reward: 0.6000,                 loss: nan
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0023
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1914.8,                last time consumption/overall running time: 384.3354s / 11905.7671 s
env0_first_0:                 episode reward: 0.0000,                 loss: nan
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0023
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1937.05,                last time consumption/overall running time: 386.2789s / 12292.0460 s
env0_first_0:                 episode reward: 0.3000,                 loss: nan
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0024
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1893.9,                last time consumption/overall running time: 378.8114s / 12670.8574 s
env0_first_0:                 episode reward: -0.6500,                 loss: nan
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0022
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1779.3,                last time consumption/overall running time: 358.3738s / 13029.2312 s
env0_first_0:                 episode reward: -1.4000,                 loss: nan
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0021
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1674.9,                last time consumption/overall running time: 333.3722s / 13362.6034 s
env0_first_0:                 episode reward: -1.6500,                 loss: nan
env0_second_0:                 episode reward: 1.6500,                 loss: 0.0019
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1657.2,                last time consumption/overall running time: 330.4252s / 13693.0286 s
env0_first_0:                 episode reward: -0.9500,                 loss: nan
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0022
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1662.7,                last time consumption/overall running time: 333.5597s / 14026.5883 s
env0_first_0:                 episode reward: -1.1500,                 loss: nan
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0019
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1560.4,                last time consumption/overall running time: 313.1643s / 14339.7525 s
env0_first_0:                 episode reward: -2.4500,                 loss: nan
env0_second_0:                 episode reward: 2.4500,                 loss: 0.0019
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1624.55,                last time consumption/overall running time: 325.7261s / 14665.4786 s
env0_first_0:                 episode reward: -1.8000,                 loss: nan
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0018
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1558.3,                last time consumption/overall running time: 312.1290s / 14977.6077 s
env0_first_0:                 episode reward: -1.8000,                 loss: nan
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0019
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1568.85,                last time consumption/overall running time: 315.9790s / 15293.5867 s
env0_first_0:                 episode reward: -3.0500,                 loss: nan
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0017
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1539.5,                last time consumption/overall running time: 313.5359s / 15607.1226 s
env0_first_0:                 episode reward: -3.9500,                 loss: nan
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0017
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1480.25,                last time consumption/overall running time: 297.4295s / 15904.5521 s
env0_first_0:                 episode reward: -5.8500,                 loss: nan
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0016
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1486.1,                last time consumption/overall running time: 298.9926s / 16203.5447 s
env0_first_0:                 episode reward: -7.1500,                 loss: nan
env0_second_0:                 episode reward: 7.1500,                 loss: 0.0015
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1536.2,                last time consumption/overall running time: 308.9063s / 16512.4509 s
env0_first_0:                 episode reward: -5.6500,                 loss: nan
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0016
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1660.35,                last time consumption/overall running time: 335.0189s / 16847.4698 s
env0_first_0:                 episode reward: -4.8500,                 loss: nan
env0_second_0:                 episode reward: 4.8500,                 loss: 0.0016
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1466.3,                last time consumption/overall running time: 295.4965s / 17142.9663 s
env0_first_0:                 episode reward: -6.4500,                 loss: nan
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0019
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1193.45,                last time consumption/overall running time: 239.7018s / 17382.6681 s
env0_first_0:                 episode reward: -5.3000,                 loss: nan
env0_second_0:                 episode reward: 5.3000,                 loss: nan
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Score delta: 16.2, update the opponent.
Episode: 1161/10000 (11.6100%),                 avg. length: 1541.5,                last time consumption/overall running time: 308.9394s / 17691.6075 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0026
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1627.25,                last time consumption/overall running time: 325.2328s / 18016.8403 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0026
env0_second_0:                 episode reward: -1.8000,                 loss: nan
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1315.2,                last time consumption/overall running time: 259.6686s / 18276.5089 s
env0_first_0:                 episode reward: 5.7500,                 loss: 0.0018
env0_second_0:                 episode reward: -5.7500,                 loss: nan
env1_first_0:                 episode reward: 7.3500,                 loss: nan
env1_second_0:                 episode reward: -7.3500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1601.35,                last time consumption/overall running time: 317.3063s / 18593.8152 s
env0_first_0:                 episode reward: 2.9500,                 loss: 0.0019
env0_second_0:                 episode reward: -2.9500,                 loss: nan
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1660.7,                last time consumption/overall running time: 328.7978s / 18922.6129 s
env0_first_0:                 episode reward: 3.0000,                 loss: 0.0024
env0_second_0:                 episode reward: -3.0000,                 loss: nan
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1598.3,                last time consumption/overall running time: 315.9182s / 19238.5311 s
env0_first_0:                 episode reward: 1.2000,                 loss: 0.0018
env0_second_0:                 episode reward: -1.2000,                 loss: nan