pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [76, 19]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 10000, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nfsp', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'eta': 0.1}}
Save models to : /home/zihan/research/MARS/data/model/20220114_1944/pettingzoo_boxing_v1_nfsp. 
 Save logs to: /home/zihan/research/MARS/data/log/20220114_1944/pettingzoo_boxing_v1_nfsp.
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 6.9937s / 6.9937 s
env0_first_0:                 episode reward: 0.0000,                 loss: nan
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 470.1894s / 477.1831 s
env0_first_0:                 episode reward: -2.9500,                 loss: nan
env0_second_0:                 episode reward: 2.9500,                 loss: nan
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 596.1712s / 1073.3543 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0154
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0168
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1771.65,                last time consumption/overall running time: 606.8569s / 1680.2112 s
env0_first_0:                 episode reward: -10.3000,                 loss: 0.0305
env0_second_0:                 episode reward: 10.3000,                 loss: 0.0300
env1_first_0:                 episode reward: -13.3500,                 loss: nan
env1_second_0:                 episode reward: 13.3500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1622.9,                last time consumption/overall running time: 552.9605s / 2233.1717 s
env0_first_0:                 episode reward: -17.6000,                 loss: 0.0557
env0_second_0:                 episode reward: 17.6000,                 loss: 0.0573
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1021.0,                last time consumption/overall running time: 348.5600s / 2581.7317 s
env0_first_0:                 episode reward: 3.4000,                 loss: 0.1004
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0936
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 650.25,                last time consumption/overall running time: 222.8281s / 2804.5598 s
env0_first_0:                 episode reward: 18.3000,                 loss: 0.1481
env0_second_0:                 episode reward: -18.3000,                 loss: 0.1694
env1_first_0:                 episode reward: 24.4500,                 loss: nan
env1_second_0:                 episode reward: -24.4500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 521.55,                last time consumption/overall running time: 179.6305s / 2984.1902 s
env0_first_0:                 episode reward: 29.3500,                 loss: 0.1936
env0_second_0:                 episode reward: -29.3500,                 loss: 0.2356
env1_first_0:                 episode reward: 18.1500,                 loss: nan
env1_second_0:                 episode reward: -18.1500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 354.95,                last time consumption/overall running time: 121.3270s / 3105.5172 s
env0_first_0:                 episode reward: 8.3000,                 loss: 0.2367
env0_second_0:                 episode reward: -8.3000,                 loss: 0.2607
env1_first_0:                 episode reward: 20.6000,                 loss: nan
env1_second_0:                 episode reward: -20.6000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 281.7,                last time consumption/overall running time: 95.9503s / 3201.4675 s
env0_first_0:                 episode reward: 31.4000,                 loss: 0.2724
env0_second_0:                 episode reward: -31.4000,                 loss: 0.2850
env1_first_0:                 episode reward: 32.3000,                 loss: nan
env1_second_0:                 episode reward: -32.3000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 269.45,                last time consumption/overall running time: 92.3487s / 3293.8162 s
env0_first_0:                 episode reward: 55.8500,                 loss: 0.3060
env0_second_0:                 episode reward: -55.8500,                 loss: 0.3208
env1_first_0:                 episode reward: 55.5500,                 loss: nan
env1_second_0:                 episode reward: -55.5500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 305.45,                last time consumption/overall running time: 104.4688s / 3398.2850 s
env0_first_0:                 episode reward: 4.9500,                 loss: 0.3290
env0_second_0:                 episode reward: -4.9500,                 loss: 0.3523
env1_first_0:                 episode reward: 12.9500,                 loss: nan
env1_second_0:                 episode reward: -12.9500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 286.25,                last time consumption/overall running time: 97.9081s / 3496.1931 s
env0_first_0:                 episode reward: 30.1500,                 loss: 0.3704
env0_second_0:                 episode reward: -30.1500,                 loss: 0.3938
env1_first_0:                 episode reward: 38.1500,                 loss: nan
env1_second_0:                 episode reward: -38.1500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 351.9,                last time consumption/overall running time: 124.2291s / 3620.4223 s
env0_first_0:                 episode reward: 18.5500,                 loss: 0.3998
env0_second_0:                 episode reward: -18.5500,                 loss: 0.4147
env1_first_0:                 episode reward: 14.2500,                 loss: nan
env1_second_0:                 episode reward: -14.2500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 354.75,                last time consumption/overall running time: 120.9911s / 3741.4133 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.4207
env0_second_0:                 episode reward: -0.6500,                 loss: 0.4294
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 302.45,                last time consumption/overall running time: 104.4800s / 3845.8933 s
env0_first_0:                 episode reward: 20.0500,                 loss: 0.4299
env0_second_0:                 episode reward: -20.0500,                 loss: 0.4608
env1_first_0:                 episode reward: 36.4500,                 loss: nan
env1_second_0:                 episode reward: -36.4500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 367.2,                last time consumption/overall running time: 123.7844s / 3969.6777 s
env0_first_0:                 episode reward: 36.0000,                 loss: 0.4337
env0_second_0:                 episode reward: -36.0000,                 loss: 0.4593
env1_first_0:                 episode reward: 44.6000,                 loss: nan
env1_second_0:                 episode reward: -44.6000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 631.75,                last time consumption/overall running time: 216.6403s / 4186.3180 s
env0_first_0:                 episode reward: 51.8500,                 loss: 0.4320
env0_second_0:                 episode reward: -51.8500,                 loss: 0.4426
env1_first_0:                 episode reward: 47.8500,                 loss: nan
env1_second_0:                 episode reward: -47.8500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 304.3,                last time consumption/overall running time: 103.8580s / 4290.1760 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.4140
env0_second_0:                 episode reward: -5.1500,                 loss: 0.4432
env1_first_0:                 episode reward: 33.2500,                 loss: nan
env1_second_0:                 episode reward: -33.2500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 284.3,                last time consumption/overall running time: 95.2713s / 4385.4473 s
env0_first_0:                 episode reward: 28.3500,                 loss: 0.4405
env0_second_0:                 episode reward: -28.3500,                 loss: 0.4808
env1_first_0:                 episode reward: 32.0500,                 loss: nan
env1_second_0:                 episode reward: -32.0500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 263.15,                last time consumption/overall running time: 93.6836s / 4479.1309 s
env0_first_0:                 episode reward: 24.1000,                 loss: 0.4994
env0_second_0:                 episode reward: -24.1000,                 loss: 0.5286
env1_first_0:                 episode reward: 29.4500,                 loss: nan
env1_second_0:                 episode reward: -29.4500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 256.2,                last time consumption/overall running time: 88.0890s / 4567.2199 s
env0_first_0:                 episode reward: 10.8500,                 loss: 0.5108
env0_second_0:                 episode reward: -10.8500,                 loss: 0.5746
env1_first_0:                 episode reward: 16.7000,                 loss: nan
env1_second_0:                 episode reward: -16.7000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 496.85,                last time consumption/overall running time: 168.2352s / 4735.4551 s
env0_first_0:                 episode reward: -19.5000,                 loss: 0.5232
env0_second_0:                 episode reward: 19.5000,                 loss: 0.5707
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 256.15,                last time consumption/overall running time: 87.4652s / 4822.9203 s
env0_first_0:                 episode reward: 68.5500,                 loss: 0.5065
env0_second_0:                 episode reward: -68.5500,                 loss: 0.5489
env1_first_0:                 episode reward: 52.1000,                 loss: nan
env1_second_0:                 episode reward: -52.1000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 358.2,                last time consumption/overall running time: 122.7041s / 4945.6243 s
env0_first_0:                 episode reward: 42.5500,                 loss: 0.5490
env0_second_0:                 episode reward: -42.5500,                 loss: 0.6079
env1_first_0:                 episode reward: 23.4000,                 loss: nan
env1_second_0:                 episode reward: -23.4000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 270.15,                last time consumption/overall running time: 92.0416s / 5037.6660 s
env0_first_0:                 episode reward: 28.2500,                 loss: 0.6037
env0_second_0:                 episode reward: -28.2500,                 loss: 0.6728
env1_first_0:                 episode reward: 31.7000,                 loss: nan
env1_second_0:                 episode reward: -31.7000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 458.15,                last time consumption/overall running time: 155.0687s / 5192.7347 s
env0_first_0:                 episode reward: 3.4000,                 loss: 0.6631
env0_second_0:                 episode reward: -3.4000,                 loss: 0.7067
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 605.35,                last time consumption/overall running time: 205.6123s / 5398.3470 s
env0_first_0:                 episode reward: 48.5500,                 loss: 0.5563
env0_second_0:                 episode reward: -48.5500,                 loss: 0.5836
env1_first_0:                 episode reward: 63.8500,                 loss: nan
env1_second_0:                 episode reward: -63.8500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 280.65,                last time consumption/overall running time: 96.1376s / 5494.4846 s
env0_first_0:                 episode reward: 6.2500,                 loss: 0.5634
env0_second_0:                 episode reward: -6.2500,                 loss: 0.5606
env1_first_0:                 episode reward: 26.2500,                 loss: nan
env1_second_0:                 episode reward: -26.2500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 300.85,                last time consumption/overall running time: 105.0429s / 5599.5275 s
env0_first_0:                 episode reward: 36.1500,                 loss: 0.6402
env0_second_0:                 episode reward: -36.1500,                 loss: 0.6222
env1_first_0:                 episode reward: 36.2500,                 loss: nan
env1_second_0:                 episode reward: -36.2500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 302.0,                last time consumption/overall running time: 103.5620s / 5703.0895 s
env0_first_0:                 episode reward: 58.2000,                 loss: 0.6865
env0_second_0:                 episode reward: -58.2000,                 loss: 0.6542
env1_first_0:                 episode reward: 33.1000,                 loss: nan
env1_second_0:                 episode reward: -33.1000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 306.05,                last time consumption/overall running time: 103.8634s / 5806.9529 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.7491
env0_second_0:                 episode reward: -4.9000,                 loss: 0.7136
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 434.4,                last time consumption/overall running time: 150.0341s / 5956.9870 s
env0_first_0:                 episode reward: 9.5500,                 loss: 0.7636
env0_second_0:                 episode reward: -9.5500,                 loss: 0.7283
env1_first_0:                 episode reward: -18.4500,                 loss: nan
env1_second_0:                 episode reward: 18.4500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 455.35,                last time consumption/overall running time: 155.7228s / 6112.7098 s
env0_first_0:                 episode reward: 2.8500,                 loss: 0.7812
env0_second_0:                 episode reward: -2.8500,                 loss: 0.7596
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 603.2,                last time consumption/overall running time: 205.6962s / 6318.4060 s
env0_first_0:                 episode reward: 27.5500,                 loss: 0.7769
env0_second_0:                 episode reward: -27.5500,                 loss: 0.8095
env1_first_0:                 episode reward: 29.0500,                 loss: nan
env1_second_0:                 episode reward: -29.0500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 433.15,                last time consumption/overall running time: 148.9627s / 6467.3688 s
env0_first_0:                 episode reward: 43.9500,                 loss: 0.6845
env0_second_0:                 episode reward: -43.9500,                 loss: 0.7689
env1_first_0:                 episode reward: 37.4000,                 loss: nan
env1_second_0:                 episode reward: -37.4000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 309.45,                last time consumption/overall running time: 109.0991s / 6576.4679 s
env0_first_0:                 episode reward: 32.5000,                 loss: 0.7300
env0_second_0:                 episode reward: -32.5000,                 loss: 0.7778
env1_first_0:                 episode reward: 15.1500,                 loss: nan
env1_second_0:                 episode reward: -15.1500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 435.1,                last time consumption/overall running time: 149.8478s / 6726.3157 s
env0_first_0:                 episode reward: 37.9000,                 loss: 0.6541
env0_second_0:                 episode reward: -37.9000,                 loss: 0.7025
env1_first_0:                 episode reward: 28.9500,                 loss: nan
env1_second_0:                 episode reward: -28.9500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 358.75,                last time consumption/overall running time: 123.8027s / 6850.1184 s
env0_first_0:                 episode reward: 36.9500,                 loss: 0.6470
env0_second_0:                 episode reward: -36.9500,                 loss: 0.7007
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 349.35,                last time consumption/overall running time: 120.7038s / 6970.8223 s
env0_first_0:                 episode reward: 15.5500,                 loss: 0.6865
env0_second_0:                 episode reward: -15.5500,                 loss: 0.7090
env1_first_0:                 episode reward: 21.9500,                 loss: nan
env1_second_0:                 episode reward: -21.9500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 314.25,                last time consumption/overall running time: 110.9666s / 7081.7889 s
env0_first_0:                 episode reward: 29.7000,                 loss: 0.7099
env0_second_0:                 episode reward: -29.7000,                 loss: 0.7381
env1_first_0:                 episode reward: 15.1000,                 loss: nan
env1_second_0:                 episode reward: -15.1000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 274.4,                last time consumption/overall running time: 94.2368s / 7176.0257 s
env0_first_0:                 episode reward: 26.3500,                 loss: 0.7778
env0_second_0:                 episode reward: -26.3500,                 loss: 0.8497
env1_first_0:                 episode reward: 36.0000,                 loss: nan
env1_second_0:                 episode reward: -36.0000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 381.25,                last time consumption/overall running time: 134.8264s / 7310.8521 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.8694
env0_second_0:                 episode reward: 4.4000,                 loss: 0.9010
env1_first_0:                 episode reward: 12.4500,                 loss: nan
env1_second_0:                 episode reward: -12.4500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 255.1,                last time consumption/overall running time: 86.3862s / 7397.2383 s
env0_first_0:                 episode reward: 6.5000,                 loss: 0.8477
env0_second_0:                 episode reward: -6.5000,                 loss: 0.9273
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 274.3,                last time consumption/overall running time: 92.8129s / 7490.0512 s
env0_first_0:                 episode reward: 12.3500,                 loss: 0.7813
env0_second_0:                 episode reward: -12.3500,                 loss: 0.8567
env1_first_0:                 episode reward: 10.7500,                 loss: nan
env1_second_0:                 episode reward: -10.7500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 275.4,                last time consumption/overall running time: 97.6758s / 7587.7270 s
env0_first_0:                 episode reward: -8.0000,                 loss: 0.8370
env0_second_0:                 episode reward: 8.0000,                 loss: 0.8921
env1_first_0:                 episode reward: 13.7000,                 loss: nan
env1_second_0:                 episode reward: -13.7000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 391.05,                last time consumption/overall running time: 131.9266s / 7719.6536 s
env0_first_0:                 episode reward: 20.0000,                 loss: 0.8647
env0_second_0:                 episode reward: -20.0000,                 loss: 0.9319
env1_first_0:                 episode reward: 15.3500,                 loss: nan
env1_second_0:                 episode reward: -15.3500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 293.05,                last time consumption/overall running time: 98.6646s / 7818.3182 s
env0_first_0:                 episode reward: 26.7500,                 loss: 0.8525
env0_second_0:                 episode reward: -26.7500,                 loss: 0.9870
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 358.2,                last time consumption/overall running time: 123.4348s / 7941.7530 s
env0_first_0:                 episode reward: 12.9500,                 loss: 0.8561
env0_second_0:                 episode reward: -12.9500,                 loss: 0.9797
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 282.65,                last time consumption/overall running time: 100.4072s / 8042.1603 s
env0_first_0:                 episode reward: 23.1000,                 loss: 0.7832
env0_second_0:                 episode reward: -23.1000,                 loss: 0.9390
env1_first_0:                 episode reward: 30.3000,                 loss: nan
env1_second_0:                 episode reward: -30.3000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 323.25,                last time consumption/overall running time: 110.4772s / 8152.6374 s
env0_first_0:                 episode reward: 19.6500,                 loss: 0.8061
env0_second_0:                 episode reward: -19.6500,                 loss: 0.9430
env1_first_0:                 episode reward: 43.0000,                 loss: nan
env1_second_0:                 episode reward: -43.0000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 327.55,                last time consumption/overall running time: 113.7585s / 8266.3959 s
env0_first_0:                 episode reward: 40.5500,                 loss: 0.8502
env0_second_0:                 episode reward: -40.5500,                 loss: 1.0603
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 426.5,                last time consumption/overall running time: 146.0646s / 8412.4606 s
env0_first_0:                 episode reward: 13.6000,                 loss: 0.8894
env0_second_0:                 episode reward: -13.6000,                 loss: 1.0396
env1_first_0:                 episode reward: 20.0500,                 loss: nan
env1_second_0:                 episode reward: -20.0500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 272.65,                last time consumption/overall running time: 95.0519s / 8507.5125 s
env0_first_0:                 episode reward: 2.8500,                 loss: 0.8438
env0_second_0:                 episode reward: -2.8500,                 loss: 0.9962
env1_first_0:                 episode reward: 20.6000,                 loss: nan
env1_second_0:                 episode reward: -20.6000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 298.6,                last time consumption/overall running time: 103.1923s / 8610.7048 s
env0_first_0:                 episode reward: -26.3000,                 loss: 0.8835
env0_second_0:                 episode reward: 26.3000,                 loss: 0.9961
env1_first_0:                 episode reward: -12.1000,                 loss: nan
env1_second_0:                 episode reward: 12.1000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 290.8,                last time consumption/overall running time: 100.1606s / 8710.8654 s
env0_first_0:                 episode reward: -11.2000,                 loss: 0.8685
env0_second_0:                 episode reward: 11.2000,                 loss: 1.0587
env1_first_0:                 episode reward: 12.6500,                 loss: nan
env1_second_0:                 episode reward: -12.6500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 289.8,                last time consumption/overall running time: 98.5233s / 8809.3887 s
env0_first_0:                 episode reward: -37.5000,                 loss: 0.9050
env0_second_0:                 episode reward: 37.5000,                 loss: 1.0340
env1_first_0:                 episode reward: -12.8500,                 loss: nan
env1_second_0:                 episode reward: 12.8500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 274.05,                last time consumption/overall running time: 93.3280s / 8902.7167 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.8740
env0_second_0:                 episode reward: 3.1500,                 loss: 1.0387
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 277.15,                last time consumption/overall running time: 93.3871s / 8996.1038 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.9354
env0_second_0:                 episode reward: 5.8000,                 loss: 1.0658
env1_first_0:                 episode reward: -28.7500,                 loss: nan
env1_second_0:                 episode reward: 28.7500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 288.3,                last time consumption/overall running time: 98.5320s / 9094.6358 s
env0_first_0:                 episode reward: -29.0000,                 loss: 0.9393
env0_second_0:                 episode reward: 29.0000,                 loss: 1.0458
env1_first_0:                 episode reward: -28.0000,                 loss: nan
env1_second_0:                 episode reward: 28.0000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 337.4,                last time consumption/overall running time: 115.2624s / 9209.8982 s
env0_first_0:                 episode reward: -17.2500,                 loss: 0.9833
env0_second_0:                 episode reward: 17.2500,                 loss: 1.0982
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 277.9,                last time consumption/overall running time: 94.9166s / 9304.8148 s
env0_first_0:                 episode reward: -33.8500,                 loss: 1.1498
env0_second_0:                 episode reward: 33.8500,                 loss: 1.2539
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 307.8,                last time consumption/overall running time: 104.9094s / 9409.7242 s
env0_first_0:                 episode reward: 2.3500,                 loss: 1.0396
env0_second_0:                 episode reward: -2.3500,                 loss: 1.1800
env1_first_0:                 episode reward: 12.9000,                 loss: nan
env1_second_0:                 episode reward: -12.9000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 274.0,                last time consumption/overall running time: 93.2797s / 9503.0039 s
env0_first_0:                 episode reward: 7.4500,                 loss: 1.0669
env0_second_0:                 episode reward: -7.4500,                 loss: 1.2607
env1_first_0:                 episode reward: -24.6500,                 loss: nan
env1_second_0:                 episode reward: 24.6500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 318.9,                last time consumption/overall running time: 110.0463s / 9613.0503 s
env0_first_0:                 episode reward: 4.5500,                 loss: 1.1685
env0_second_0:                 episode reward: -4.5500,                 loss: 1.3490
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 283.15,                last time consumption/overall running time: 98.2588s / 9711.3091 s
env0_first_0:                 episode reward: 9.6500,                 loss: 1.2650
env0_second_0:                 episode reward: -9.6500,                 loss: 1.4673
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 297.75,                last time consumption/overall running time: 102.6802s / 9813.9892 s
env0_first_0:                 episode reward: -11.3000,                 loss: 1.2979
env0_second_0:                 episode reward: 11.3000,                 loss: 1.4443
env1_first_0:                 episode reward: -25.3500,                 loss: nan
env1_second_0:                 episode reward: 25.3500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 306.0,                last time consumption/overall running time: 105.3306s / 9919.3198 s
env0_first_0:                 episode reward: 13.4000,                 loss: 1.3432
env0_second_0:                 episode reward: -13.4000,                 loss: 1.3844
env1_first_0:                 episode reward: 12.4500,                 loss: nan
env1_second_0:                 episode reward: -12.4500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 333.25,                last time consumption/overall running time: 112.2398s / 10031.5597 s
env0_first_0:                 episode reward: 0.7000,                 loss: 1.2911
env0_second_0:                 episode reward: -0.7000,                 loss: 1.3931
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 366.8,                last time consumption/overall running time: 123.4029s / 10154.9626 s
env0_first_0:                 episode reward: -22.7500,                 loss: 1.1383
env0_second_0:                 episode reward: 22.7500,                 loss: 1.3387
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 349.85,                last time consumption/overall running time: 118.8156s / 10273.7782 s
env0_first_0:                 episode reward: 1.4000,                 loss: 1.1165
env0_second_0:                 episode reward: -1.4000,                 loss: 1.2646
env1_first_0:                 episode reward: 13.4500,                 loss: nan
env1_second_0:                 episode reward: -13.4500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 274.65,                last time consumption/overall running time: 95.1223s / 10368.9005 s
env0_first_0:                 episode reward: -22.3000,                 loss: 1.1550
env0_second_0:                 episode reward: 22.3000,                 loss: 1.3403
env1_first_0:                 episode reward: -47.9000,                 loss: nan
env1_second_0:                 episode reward: 47.9000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 281.7,                last time consumption/overall running time: 99.3737s / 10468.2741 s
env0_first_0:                 episode reward: -9.7000,                 loss: 1.2242
env0_second_0:                 episode reward: 9.7000,                 loss: 1.3748
env1_first_0:                 episode reward: -24.7500,                 loss: nan
env1_second_0:                 episode reward: 24.7500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 292.7,                last time consumption/overall running time: 102.7129s / 10570.9871 s
env0_first_0:                 episode reward: 1.6500,                 loss: 1.1884
env0_second_0:                 episode reward: -1.6500,                 loss: 1.3020
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 314.15,                last time consumption/overall running time: 105.9311s / 10676.9181 s
env0_first_0:                 episode reward: 19.6500,                 loss: 1.1730
env0_second_0:                 episode reward: -19.6500,                 loss: 1.3175
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 302.25,                last time consumption/overall running time: 103.4692s / 10780.3873 s
env0_first_0:                 episode reward: 13.5000,                 loss: 1.2238
env0_second_0:                 episode reward: -13.5000,                 loss: 1.3991
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 317.4,                last time consumption/overall running time: 110.2535s / 10890.6409 s
env0_first_0:                 episode reward: 11.2000,                 loss: 1.2985
env0_second_0:                 episode reward: -11.2000,                 loss: 1.4608
env1_first_0:                 episode reward: 15.1500,                 loss: nan
env1_second_0:                 episode reward: -15.1500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 302.2,                last time consumption/overall running time: 102.2417s / 10992.8826 s
env0_first_0:                 episode reward: 6.3000,                 loss: 1.3318
env0_second_0:                 episode reward: -6.3000,                 loss: 1.4878
env1_first_0:                 episode reward: -12.7000,                 loss: nan
env1_second_0:                 episode reward: 12.7000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 328.95,                last time consumption/overall running time: 114.3977s / 11107.2803 s
env0_first_0:                 episode reward: 9.2000,                 loss: 1.4253
env0_second_0:                 episode reward: -9.2000,                 loss: 1.5470
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 309.6,                last time consumption/overall running time: 106.8527s / 11214.1330 s
env0_first_0:                 episode reward: 24.6500,                 loss: 1.5496
env0_second_0:                 episode reward: -24.6500,                 loss: 1.6026
env1_first_0:                 episode reward: 59.6500,                 loss: nan
env1_second_0:                 episode reward: -59.6500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 273.2,                last time consumption/overall running time: 93.4790s / 11307.6120 s
env0_first_0:                 episode reward: 21.8500,                 loss: 1.4519
env0_second_0:                 episode reward: -21.8500,                 loss: 1.6008
env1_first_0:                 episode reward: 12.9000,                 loss: nan
env1_second_0:                 episode reward: -12.9000,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 258.3,                last time consumption/overall running time: 91.0962s / 11398.7082 s
env0_first_0:                 episode reward: -54.6500,                 loss: 1.4146
env0_second_0:                 episode reward: 54.6500,                 loss: 1.5063
env1_first_0:                 episode reward: -38.3500,                 loss: nan
env1_second_0:                 episode reward: 38.3500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 345.05,                last time consumption/overall running time: 116.2899s / 11514.9981 s
env0_first_0:                 episode reward: 5.6500,                 loss: 1.3925
env0_second_0:                 episode reward: -5.6500,                 loss: 1.4216
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 372.6,                last time consumption/overall running time: 128.9908s / 11643.9889 s
env0_first_0:                 episode reward: -24.1000,                 loss: 1.4033
env0_second_0:                 episode reward: 24.1000,                 loss: 1.4481
env1_first_0:                 episode reward: -19.1000,                 loss: nan
env1_second_0:                 episode reward: 19.1000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 506.4,                last time consumption/overall running time: 177.5388s / 11821.5277 s
env0_first_0:                 episode reward: -7.2000,                 loss: 1.1642
env0_second_0:                 episode reward: 7.2000,                 loss: 1.2618
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 281.7,                last time consumption/overall running time: 97.3821s / 11918.9098 s
env0_first_0:                 episode reward: 18.4000,                 loss: 1.0585
env0_second_0:                 episode reward: -18.4000,                 loss: 1.2314
env1_first_0:                 episode reward: 25.0000,                 loss: nan
env1_second_0:                 episode reward: -25.0000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 391.45,                last time consumption/overall running time: 134.0921s / 12053.0019 s
env0_first_0:                 episode reward: 12.7000,                 loss: 1.0838
env0_second_0:                 episode reward: -12.7000,                 loss: 1.2438
env1_first_0:                 episode reward: 10.7500,                 loss: nan
env1_second_0:                 episode reward: -10.7500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 400.2,                last time consumption/overall running time: 136.2421s / 12189.2440 s
env0_first_0:                 episode reward: 5.7500,                 loss: 1.1560
env0_second_0:                 episode reward: -5.7500,                 loss: 1.1837
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 309.25,                last time consumption/overall running time: 106.4568s / 12295.7008 s
env0_first_0:                 episode reward: -13.4000,                 loss: 1.1101
env0_second_0:                 episode reward: 13.4000,                 loss: 1.2196
env1_first_0:                 episode reward: 39.7500,                 loss: nan
env1_second_0:                 episode reward: -39.7500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 278.6,                last time consumption/overall running time: 94.9393s / 12390.6400 s
env0_first_0:                 episode reward: -14.5000,                 loss: 1.0867
env0_second_0:                 episode reward: 14.5000,                 loss: 1.1603
env1_first_0:                 episode reward: 14.1500,                 loss: nan
env1_second_0:                 episode reward: -14.1500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 300.9,                last time consumption/overall running time: 103.4550s / 12494.0951 s
env0_first_0:                 episode reward: 0.6500,                 loss: 1.1396
env0_second_0:                 episode reward: -0.6500,                 loss: 1.2534
env1_first_0:                 episode reward: -25.6000,                 loss: nan
env1_second_0:                 episode reward: 25.6000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 285.75,                last time consumption/overall running time: 97.7521s / 12591.8471 s
env0_first_0:                 episode reward: 40.1000,                 loss: 1.2669
env0_second_0:                 episode reward: -40.1000,                 loss: 1.3642
env1_first_0:                 episode reward: 21.5000,                 loss: nan
env1_second_0:                 episode reward: -21.5000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 305.7,                last time consumption/overall running time: 105.9391s / 12697.7862 s
env0_first_0:                 episode reward: -4.1000,                 loss: 1.3583
env0_second_0:                 episode reward: 4.1000,                 loss: 1.4576
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 285.2,                last time consumption/overall running time: 99.8391s / 12797.6253 s
env0_first_0:                 episode reward: 7.6000,                 loss: 1.3810
env0_second_0:                 episode reward: -7.6000,                 loss: 1.4551
env1_first_0:                 episode reward: 42.2000,                 loss: nan
env1_second_0:                 episode reward: -42.2000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 290.4,                last time consumption/overall running time: 100.7363s / 12898.3616 s
env0_first_0:                 episode reward: 14.7000,                 loss: 1.4181
env0_second_0:                 episode reward: -14.7000,                 loss: 1.4520
env1_first_0:                 episode reward: 30.3500,                 loss: nan
env1_second_0:                 episode reward: -30.3500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 273.45,                last time consumption/overall running time: 95.0629s / 12993.4245 s
env0_first_0:                 episode reward: 25.6500,                 loss: 1.4886
env0_second_0:                 episode reward: -25.6500,                 loss: 1.5932
env1_first_0:                 episode reward: -12.9500,                 loss: nan
env1_second_0:                 episode reward: 12.9500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 259.75,                last time consumption/overall running time: 90.9362s / 13084.3607 s
env0_first_0:                 episode reward: -18.4000,                 loss: 1.5457
env0_second_0:                 episode reward: 18.4000,                 loss: 1.6213
env1_first_0:                 episode reward: 7.9500,                 loss: nan
env1_second_0:                 episode reward: -7.9500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 268.1,                last time consumption/overall running time: 94.6193s / 13178.9799 s
env0_first_0:                 episode reward: 9.4500,                 loss: 1.4817
env0_second_0:                 episode reward: -9.4500,                 loss: 1.6804
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 570.6,                last time consumption/overall running time: 197.9639s / 13376.9438 s
env0_first_0:                 episode reward: -5.1500,                 loss: 1.3644
env0_second_0:                 episode reward: 5.1500,                 loss: 1.4251
env1_first_0:                 episode reward: -16.1000,                 loss: nan
env1_second_0:                 episode reward: 16.1000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 335.7,                last time consumption/overall running time: 114.7886s / 13491.7325 s
env0_first_0:                 episode reward: 22.4500,                 loss: 1.3093
env0_second_0:                 episode reward: -22.4500,                 loss: 1.3440
env1_first_0:                 episode reward: 28.1000,                 loss: nan
env1_second_0:                 episode reward: -28.1000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 313.35,                last time consumption/overall running time: 108.0932s / 13599.8256 s
env0_first_0:                 episode reward: 51.2500,                 loss: 1.3532
env0_second_0:                 episode reward: -51.2500,                 loss: 1.3982
env1_first_0:                 episode reward: 47.5500,                 loss: nan
env1_second_0:                 episode reward: -47.5500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 300.7,                last time consumption/overall running time: 104.3057s / 13704.1313 s
env0_first_0:                 episode reward: 30.7000,                 loss: 1.3640
env0_second_0:                 episode reward: -30.7000,                 loss: 1.3567
env1_first_0:                 episode reward: 26.0000,                 loss: nan
env1_second_0:                 episode reward: -26.0000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 369.9,                last time consumption/overall running time: 128.0204s / 13832.1517 s
env0_first_0:                 episode reward: 32.7500,                 loss: 1.3261
env0_second_0:                 episode reward: -32.7500,                 loss: 1.3348
env1_first_0:                 episode reward: 19.3000,                 loss: nan
env1_second_0:                 episode reward: -19.3000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 695.0,                last time consumption/overall running time: 240.9218s / 14073.0735 s
env0_first_0:                 episode reward: 26.5000,                 loss: 1.2497
env0_second_0:                 episode reward: -26.5000,                 loss: 1.2769
env1_first_0:                 episode reward: 19.4500,                 loss: nan
env1_second_0:                 episode reward: -19.4500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 519.5,                last time consumption/overall running time: 178.7959s / 14251.8694 s
env0_first_0:                 episode reward: -23.5500,                 loss: 1.1372
env0_second_0:                 episode reward: 23.5500,                 loss: 1.1923
env1_first_0:                 episode reward: -13.9000,                 loss: nan
env1_second_0:                 episode reward: 13.9000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 309.5,                last time consumption/overall running time: 106.7645s / 14358.6339 s
env0_first_0:                 episode reward: 25.0500,                 loss: 1.1407
env0_second_0:                 episode reward: -25.0500,                 loss: 1.3312
env1_first_0:                 episode reward: 13.7500,                 loss: nan
env1_second_0:                 episode reward: -13.7500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 298.35,                last time consumption/overall running time: 104.4561s / 14463.0900 s
env0_first_0:                 episode reward: 16.3000,                 loss: 1.1985
env0_second_0:                 episode reward: -16.3000,                 loss: 1.3509
env1_first_0:                 episode reward: 44.3000,                 loss: nan
env1_second_0:                 episode reward: -44.3000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 290.7,                last time consumption/overall running time: 101.6095s / 14564.6995 s
env0_first_0:                 episode reward: 46.7500,                 loss: 1.2694
env0_second_0:                 episode reward: -46.7500,                 loss: 1.4152
env1_first_0:                 episode reward: 34.0000,                 loss: nan
env1_second_0:                 episode reward: -34.0000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 327.55,                last time consumption/overall running time: 112.2512s / 14676.9508 s
env0_first_0:                 episode reward: 1.1000,                 loss: 1.2833
env0_second_0:                 episode reward: -1.1000,                 loss: 1.3165
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 375.55,                last time consumption/overall running time: 128.4298s / 14805.3805 s
env0_first_0:                 episode reward: -10.8500,                 loss: 1.3129
env0_second_0:                 episode reward: 10.8500,                 loss: 1.3545
env1_first_0:                 episode reward: -10.2000,                 loss: nan
env1_second_0:                 episode reward: 10.2000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 295.95,                last time consumption/overall running time: 101.1719s / 14906.5524 s
env0_first_0:                 episode reward: 22.4000,                 loss: 1.3538
env0_second_0:                 episode reward: -22.4000,                 loss: 1.4777
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 275.2,                last time consumption/overall running time: 92.9488s / 14999.5012 s
env0_first_0:                 episode reward: -7.2000,                 loss: 1.4792
env0_second_0:                 episode reward: 7.2000,                 loss: 1.6434
env1_first_0:                 episode reward: -28.6500,                 loss: nan
env1_second_0:                 episode reward: 28.6500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 286.3,                last time consumption/overall running time: 98.8465s / 15098.3477 s
env0_first_0:                 episode reward: 24.7000,                 loss: 1.5163
env0_second_0:                 episode reward: -24.7000,                 loss: 1.6815
env1_first_0:                 episode reward: 11.4500,                 loss: nan
env1_second_0:                 episode reward: -11.4500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 280.1,                last time consumption/overall running time: 96.9193s / 15195.2670 s
env0_first_0:                 episode reward: 51.7500,                 loss: 1.5111
env0_second_0:                 episode reward: -51.7500,                 loss: 1.7684
env1_first_0:                 episode reward: 7.4000,                 loss: nan
env1_second_0:                 episode reward: -7.4000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 313.3,                last time consumption/overall running time: 107.9414s / 15303.2084 s
env0_first_0:                 episode reward: 30.9500,                 loss: 1.4598
env0_second_0:                 episode reward: -30.9500,                 loss: 1.8992
env1_first_0:                 episode reward: 46.5500,                 loss: nan
env1_second_0:                 episode reward: -46.5500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 309.2,                last time consumption/overall running time: 106.3612s / 15409.5696 s
env0_first_0:                 episode reward: 39.2500,                 loss: 1.4198
env0_second_0:                 episode reward: -39.2500,                 loss: 1.8741
env1_first_0:                 episode reward: 30.8000,                 loss: nan
env1_second_0:                 episode reward: -30.8000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 297.85,                last time consumption/overall running time: 104.3156s / 15513.8852 s
env0_first_0:                 episode reward: 33.9000,                 loss: 1.4303
env0_second_0:                 episode reward: -33.9000,                 loss: 1.7779
env1_first_0:                 episode reward: 34.0000,                 loss: nan
env1_second_0:                 episode reward: -34.0000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 346.85,                last time consumption/overall running time: 120.4962s / 15634.3814 s
env0_first_0:                 episode reward: 40.3000,                 loss: 1.4591
env0_second_0:                 episode reward: -40.3000,                 loss: 1.7156
env1_first_0:                 episode reward: 35.5500,                 loss: nan
env1_second_0:                 episode reward: -35.5500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 405.7,                last time consumption/overall running time: 141.2590s / 15775.6404 s
env0_first_0:                 episode reward: 8.8500,                 loss: 1.4331
env0_second_0:                 episode reward: -8.8500,                 loss: 1.7791
env1_first_0:                 episode reward: 40.3000,                 loss: nan
env1_second_0:                 episode reward: -40.3000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 541.15,                last time consumption/overall running time: 186.4558s / 15962.0962 s
env0_first_0:                 episode reward: 9.8000,                 loss: 1.2804
env0_second_0:                 episode reward: -9.8000,                 loss: 1.6101
env1_first_0:                 episode reward: 17.9500,                 loss: nan
env1_second_0:                 episode reward: -17.9500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 426.6,                last time consumption/overall running time: 146.8674s / 16108.9636 s
env0_first_0:                 episode reward: -22.4500,                 loss: 1.1281
env0_second_0:                 episode reward: 22.4500,                 loss: 1.3919
env1_first_0:                 episode reward: -15.6000,                 loss: nan
env1_second_0:                 episode reward: 15.6000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 333.9,                last time consumption/overall running time: 115.5237s / 16224.4873 s
env0_first_0:                 episode reward: -14.7000,                 loss: 1.0530
env0_second_0:                 episode reward: 14.7000,                 loss: 1.2237
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 285.85,                last time consumption/overall running time: 98.9227s / 16323.4100 s
env0_first_0:                 episode reward: 23.3000,                 loss: 1.0848
env0_second_0:                 episode reward: -23.3000,                 loss: 1.1786
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 434.75,                last time consumption/overall running time: 149.8258s / 16473.2358 s
env0_first_0:                 episode reward: -17.0000,                 loss: 1.0205
env0_second_0:                 episode reward: 17.0000,                 loss: 1.1540
env1_first_0:                 episode reward: -21.7000,                 loss: nan
env1_second_0:                 episode reward: 21.7000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 318.15,                last time consumption/overall running time: 110.7362s / 16583.9720 s
env0_first_0:                 episode reward: -9.6000,                 loss: 0.9834
env0_second_0:                 episode reward: 9.6000,                 loss: 1.1502
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 254.65,                last time consumption/overall running time: 89.3260s / 16673.2980 s
env0_first_0:                 episode reward: -23.6500,                 loss: 1.0207
env0_second_0:                 episode reward: 23.6500,                 loss: 1.1762
env1_first_0:                 episode reward: -49.9500,                 loss: nan
env1_second_0:                 episode reward: 49.9500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 266.2,                last time consumption/overall running time: 92.1998s / 16765.4978 s
env0_first_0:                 episode reward: 19.3000,                 loss: 1.0685
env0_second_0:                 episode reward: -19.3000,                 loss: 1.2890
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 286.8,                last time consumption/overall running time: 99.5395s / 16865.0372 s
env0_first_0:                 episode reward: 27.0500,                 loss: 1.0613
env0_second_0:                 episode reward: -27.0500,                 loss: 1.3586
env1_first_0:                 episode reward: 13.8500,                 loss: nan
env1_second_0:                 episode reward: -13.8500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 291.1,                last time consumption/overall running time: 100.0655s / 16965.1028 s
env0_first_0:                 episode reward: 22.1500,                 loss: 1.1594
env0_second_0:                 episode reward: -22.1500,                 loss: 1.4303
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 324.95,                last time consumption/overall running time: 110.6969s / 17075.7997 s
env0_first_0:                 episode reward: 2.6500,                 loss: 1.2480
env0_second_0:                 episode reward: -2.6500,                 loss: 1.5235
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 264.25,                last time consumption/overall running time: 91.1950s / 17166.9947 s
env0_first_0:                 episode reward: -8.1000,                 loss: 1.3319
env0_second_0:                 episode reward: 8.1000,                 loss: 1.5955
env1_first_0:                 episode reward: 8.8000,                 loss: nan
env1_second_0:                 episode reward: -8.8000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 277.25,                last time consumption/overall running time: 95.8326s / 17262.8272 s
env0_first_0:                 episode reward: -5.6000,                 loss: 1.3647
env0_second_0:                 episode reward: 5.6000,                 loss: 1.6757
env1_first_0:                 episode reward: -30.0000,                 loss: nan
env1_second_0:                 episode reward: 30.0000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 318.25,                last time consumption/overall running time: 109.7149s / 17372.5421 s
env0_first_0:                 episode reward: 10.6000,                 loss: 1.4101
env0_second_0:                 episode reward: -10.6000,                 loss: 1.8216
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 329.15,                last time consumption/overall running time: 114.6704s / 17487.2125 s
env0_first_0:                 episode reward: 32.7500,                 loss: 1.4932
env0_second_0:                 episode reward: -32.7500,                 loss: 1.8798
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 291.5,                last time consumption/overall running time: 100.7490s / 17587.9615 s
env0_first_0:                 episode reward: 13.4000,                 loss: 1.4861
env0_second_0:                 episode reward: -13.4000,                 loss: 1.8069
env1_first_0:                 episode reward: 12.7500,                 loss: nan
env1_second_0:                 episode reward: -12.7500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 290.3,                last time consumption/overall running time: 99.2706s / 17687.2321 s
env0_first_0:                 episode reward: 21.4500,                 loss: 1.5558
env0_second_0:                 episode reward: -21.4500,                 loss: 1.8224
env1_first_0:                 episode reward: 22.3500,                 loss: nan
env1_second_0:                 episode reward: -22.3500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 340.05,                last time consumption/overall running time: 116.6937s / 17803.9258 s
env0_first_0:                 episode reward: -22.6000,                 loss: 1.5681
env0_second_0:                 episode reward: 22.6000,                 loss: 1.8259
env1_first_0:                 episode reward: -16.5000,                 loss: nan
env1_second_0:                 episode reward: 16.5000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 361.0,                last time consumption/overall running time: 124.1071s / 17928.0329 s
env0_first_0:                 episode reward: -10.5000,                 loss: 1.5310
env0_second_0:                 episode reward: 10.5000,                 loss: 1.7556
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 417.4,                last time consumption/overall running time: 144.3579s / 18072.3908 s
env0_first_0:                 episode reward: 23.3500,                 loss: 1.4613
env0_second_0:                 episode reward: -23.3500,                 loss: 1.6310
env1_first_0:                 episode reward: 20.1000,                 loss: nan
env1_second_0:                 episode reward: -20.1000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 344.3,                last time consumption/overall running time: 118.4097s / 18190.8005 s
env0_first_0:                 episode reward: 35.0500,                 loss: 1.3578
env0_second_0:                 episode reward: -35.0500,                 loss: 1.5706
env1_first_0:                 episode reward: 20.5000,                 loss: nan
env1_second_0:                 episode reward: -20.5000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 410.1,                last time consumption/overall running time: 141.3834s / 18332.1839 s
env0_first_0:                 episode reward: -18.9000,                 loss: 1.2798
env0_second_0:                 episode reward: 18.9000,                 loss: 1.4876
env1_first_0:                 episode reward: 12.6500,                 loss: nan
env1_second_0:                 episode reward: -12.6500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 310.2,                last time consumption/overall running time: 105.1102s / 18437.2941 s
env0_first_0:                 episode reward: 31.8000,                 loss: 1.3300
env0_second_0:                 episode reward: -31.8000,                 loss: 1.5247
env1_first_0:                 episode reward: 42.4000,                 loss: nan
env1_second_0:                 episode reward: -42.4000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 313.1,                last time consumption/overall running time: 109.4200s / 18546.7141 s
env0_first_0:                 episode reward: -2.4000,                 loss: 1.3609
env0_second_0:                 episode reward: 2.4000,                 loss: 1.5316
env1_first_0:                 episode reward: 49.7500,                 loss: nan
env1_second_0:                 episode reward: -49.7500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 400.1,                last time consumption/overall running time: 136.7938s / 18683.5080 s
env0_first_0:                 episode reward: 11.9000,                 loss: 1.3482
env0_second_0:                 episode reward: -11.9000,                 loss: 1.6055
env1_first_0:                 episode reward: 28.1000,                 loss: nan
env1_second_0:                 episode reward: -28.1000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 330.65,                last time consumption/overall running time: 113.8591s / 18797.3671 s
env0_first_0:                 episode reward: -19.7500,                 loss: 1.3261
env0_second_0:                 episode reward: 19.7500,                 loss: 1.6535
env1_first_0:                 episode reward: 20.1500,                 loss: nan
env1_second_0:                 episode reward: -20.1500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 282.6,                last time consumption/overall running time: 97.7489s / 18895.1159 s
env0_first_0:                 episode reward: 0.9000,                 loss: 1.4142
env0_second_0:                 episode reward: -0.9000,                 loss: 1.6388
env1_first_0:                 episode reward: 11.4000,                 loss: nan
env1_second_0:                 episode reward: -11.4000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 313.0,                last time consumption/overall running time: 108.8045s / 19003.9204 s
env0_first_0:                 episode reward: 18.4000,                 loss: 1.4856
env0_second_0:                 episode reward: -18.4000,                 loss: 1.7422
env1_first_0:                 episode reward: 13.7500,                 loss: nan
env1_second_0:                 episode reward: -13.7500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 317.9,                last time consumption/overall running time: 109.0934s / 19113.0138 s
env0_first_0:                 episode reward: 35.4500,                 loss: 1.5201
env0_second_0:                 episode reward: -35.4500,                 loss: 1.7238
env1_first_0:                 episode reward: 35.8500,                 loss: nan
env1_second_0:                 episode reward: -35.8500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 310.2,                last time consumption/overall running time: 109.2529s / 19222.2667 s
env0_first_0:                 episode reward: 13.7000,                 loss: 1.5373
env0_second_0:                 episode reward: -13.7000,                 loss: 1.6659
env1_first_0:                 episode reward: 23.2500,                 loss: nan
env1_second_0:                 episode reward: -23.2500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 408.65,                last time consumption/overall running time: 141.6969s / 19363.9635 s
env0_first_0:                 episode reward: 13.8500,                 loss: 1.5403
env0_second_0:                 episode reward: -13.8500,                 loss: 1.7179
env1_first_0:                 episode reward: 22.4500,                 loss: nan
env1_second_0:                 episode reward: -22.4500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 366.8,                last time consumption/overall running time: 124.3003s / 19488.2639 s
env0_first_0:                 episode reward: -7.0000,                 loss: 1.5146
env0_second_0:                 episode reward: 7.0000,                 loss: 1.6723
env1_first_0:                 episode reward: -23.8000,                 loss: nan
env1_second_0:                 episode reward: 23.8000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 393.85,                last time consumption/overall running time: 136.0976s / 19624.3614 s
env0_first_0:                 episode reward: -7.9000,                 loss: 1.5616
env0_second_0:                 episode reward: 7.9000,                 loss: 1.7246
env1_first_0:                 episode reward: -26.9000,                 loss: nan
env1_second_0:                 episode reward: 26.9000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 619.9,                last time consumption/overall running time: 215.1578s / 19839.5193 s
env0_first_0:                 episode reward: 36.2000,                 loss: 1.4677
env0_second_0:                 episode reward: -36.2000,                 loss: 1.6232
env1_first_0:                 episode reward: 21.0500,                 loss: nan
env1_second_0:                 episode reward: -21.0500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 269.95,                last time consumption/overall running time: 93.0425s / 19932.5618 s
env0_first_0:                 episode reward: 23.6000,                 loss: 1.2963
env0_second_0:                 episode reward: -23.6000,                 loss: 1.3582
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 359.05,                last time consumption/overall running time: 122.6705s / 20055.2323 s
env0_first_0:                 episode reward: -9.1500,                 loss: 1.2604
env0_second_0:                 episode reward: 9.1500,                 loss: 1.3746
env1_first_0:                 episode reward: 22.2500,                 loss: nan
env1_second_0:                 episode reward: -22.2500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 372.95,                last time consumption/overall running time: 129.2101s / 20184.4423 s
env0_first_0:                 episode reward: -7.4000,                 loss: 1.2297
env0_second_0:                 episode reward: 7.4000,                 loss: 1.3753
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 296.45,                last time consumption/overall running time: 103.2069s / 20287.6492 s
env0_first_0:                 episode reward: -9.3500,                 loss: 1.2017
env0_second_0:                 episode reward: 9.3500,                 loss: 1.4179
env1_first_0:                 episode reward: -41.7000,                 loss: nan
env1_second_0:                 episode reward: 41.7000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 343.3,                last time consumption/overall running time: 117.3320s / 20404.9812 s
env0_first_0:                 episode reward: 17.4500,                 loss: 1.1484
env0_second_0:                 episode reward: -17.4500,                 loss: 1.3556
env1_first_0:                 episode reward: 31.0000,                 loss: nan
env1_second_0:                 episode reward: -31.0000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 297.7,                last time consumption/overall running time: 102.4125s / 20507.3937 s
env0_first_0:                 episode reward: 35.1000,                 loss: 1.2100
env0_second_0:                 episode reward: -35.1000,                 loss: 1.4428
env1_first_0:                 episode reward: 40.8000,                 loss: nan
env1_second_0:                 episode reward: -40.8000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 329.55,                last time consumption/overall running time: 114.1994s / 20621.5931 s
env0_first_0:                 episode reward: 17.2500,                 loss: 1.2287
env0_second_0:                 episode reward: -17.2500,                 loss: 1.3993
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 377.1,                last time consumption/overall running time: 129.5333s / 20751.1263 s
env0_first_0:                 episode reward: 45.1000,                 loss: 1.3191
env0_second_0:                 episode reward: -45.1000,                 loss: 1.4904
env1_first_0:                 episode reward: 32.5500,                 loss: nan
env1_second_0:                 episode reward: -32.5500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 388.35,                last time consumption/overall running time: 135.5325s / 20886.6588 s
env0_first_0:                 episode reward: 21.5500,                 loss: 1.3442
env0_second_0:                 episode reward: -21.5500,                 loss: 1.4459
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 381.25,                last time consumption/overall running time: 134.1214s / 21020.7802 s
env0_first_0:                 episode reward: 39.2000,                 loss: 1.2988
env0_second_0:                 episode reward: -39.2000,                 loss: 1.4732
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 720.2,                last time consumption/overall running time: 246.9539s / 21267.7341 s
env0_first_0:                 episode reward: 9.0000,                 loss: 1.0923
env0_second_0:                 episode reward: -9.0000,                 loss: 1.3325
env1_first_0:                 episode reward: 27.8500,                 loss: nan
env1_second_0:                 episode reward: -27.8500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 343.55,                last time consumption/overall running time: 119.8204s / 21387.5546 s
env0_first_0:                 episode reward: 19.6500,                 loss: 0.9902
env0_second_0:                 episode reward: -19.6500,                 loss: 1.1997
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 404.65,                last time consumption/overall running time: 140.6556s / 21528.2102 s
env0_first_0:                 episode reward: 58.2000,                 loss: 1.0029
env0_second_0:                 episode reward: -58.2000,                 loss: 1.2409