pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [86, 33]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=1024, bias=True)
      (1): Tanh()
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): Tanh()
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Tanh()
      (6): Linear(in_features=1024, out_features=6, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=1024, bias=True)
      (1): Tanh()
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): Tanh()
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Tanh()
      (6): Linear(in_features=1024, out_features=6, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 2, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 32, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [1024, 1024, 1024], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 4, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2.
Episode: 1/10000 (0.0100%),                 avg. length: 649.0,                last time consumption/overall running time: 6.3733s / 6.3733 s
env0_first_0:                 episode reward: 3.0000,                 loss: 0.0048
env0_second_0:                 episode reward: -3.0000,                 loss: nan
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 611.8,                last time consumption/overall running time: 82.2831s / 88.6565 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0077
env0_second_0:                 episode reward: 0.7000,                 loss: nan
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 567.0,                last time consumption/overall running time: 77.0476s / 165.7041 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0122
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 538.55,                last time consumption/overall running time: 74.5439s / 240.2480 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0112
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 599.95,                last time consumption/overall running time: 84.9847s / 325.2327 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0118
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 603.2,                last time consumption/overall running time: 85.6773s / 410.9100 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0131
env0_second_0:                 episode reward: 0.0500,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 581.4,                last time consumption/overall running time: 82.5681s / 493.4781 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0139
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 587.1,                last time consumption/overall running time: 83.3493s / 576.8274 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0155
env0_second_0:                 episode reward: -0.8500,                 loss: nan
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 565.3,                last time consumption/overall running time: 79.9717s / 656.7991 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0161
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 548.75,                last time consumption/overall running time: 77.1696s / 733.9687 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0163
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 561.6,                last time consumption/overall running time: 80.2550s / 814.2237 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0187
env0_second_0:                 episode reward: -0.1000,                 loss: nan
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 560.85,                last time consumption/overall running time: 80.1032s / 894.3269 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0180
env0_second_0:                 episode reward: 0.9000,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 593.0,                last time consumption/overall running time: 83.7969s / 978.1238 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0191
env0_second_0:                 episode reward: -1.4000,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 597.05,                last time consumption/overall running time: 84.6677s / 1062.7915 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0207
env0_second_0:                 episode reward: -0.7500,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 566.45,                last time consumption/overall running time: 81.0246s / 1143.8161 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0202
env0_second_0:                 episode reward: -0.8000,                 loss: nan
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 558.45,                last time consumption/overall running time: 79.3445s / 1223.1606 s
env0_first_0:                 episode reward: 1.1500,                 loss: nan
env0_second_0:                 episode reward: -1.1500,                 loss: nan
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/283_0.
Episode: 321/10000 (3.2100%),                 avg. length: 587.3,                last time consumption/overall running time: 83.7046s / 1306.8652 s
env0_first_0:                 episode reward: -0.5500,                 loss: nan
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0096
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 553.65,                last time consumption/overall running time: 79.2050s / 1386.0702 s
env0_first_0:                 episode reward: 0.7500,                 loss: nan
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0107
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 537.25,                last time consumption/overall running time: 77.1418s / 1463.2119 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0107
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 545.95,                last time consumption/overall running time: 78.4357s / 1541.6476 s
env0_first_0:                 episode reward: -0.3500,                 loss: nan
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0119
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 565.95,                last time consumption/overall running time: 81.6868s / 1623.3344 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0138
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 575.0,                last time consumption/overall running time: 82.8318s / 1706.1662 s
env0_first_0:                 episode reward: -0.5500,                 loss: nan
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0142
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 568.35,                last time consumption/overall running time: 82.2208s / 1788.3871 s
env0_first_0:                 episode reward: 0.5000,                 loss: nan
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0161
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 549.45,                last time consumption/overall running time: 79.3063s / 1867.6934 s
env0_first_0:                 episode reward: -0.3500,                 loss: nan
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0171
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 580.65,                last time consumption/overall running time: 84.2941s / 1951.9875 s
env0_first_0:                 episode reward: 0.1500,                 loss: nan
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0183
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 606.8,                last time consumption/overall running time: 87.5969s / 2039.5844 s
env0_first_0:                 episode reward: 0.6500,                 loss: nan
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0184
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 547.2,                last time consumption/overall running time: 79.0757s / 2118.6601 s
env0_first_0:                 episode reward: -0.1500,                 loss: nan
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0181
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 567.9,                last time consumption/overall running time: 81.6687s / 2200.3288 s
env0_first_0:                 episode reward: -0.3000,                 loss: nan
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0194
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 595.45,                last time consumption/overall running time: 85.9352s / 2286.2640 s
env0_first_0:                 episode reward: -0.2500,                 loss: nan
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0210
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 579.55,                last time consumption/overall running time: 83.5613s / 2369.8254 s
env0_first_0:                 episode reward: 0.1000,                 loss: nan
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0220
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 591.5,                last time consumption/overall running time: 85.4959s / 2455.3213 s
env0_first_0:                 episode reward: 1.1500,                 loss: nan
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0218
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 603.25,                last time consumption/overall running time: 87.0255s / 2542.3468 s
env0_first_0:                 episode reward: 0.5500,                 loss: nan
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0226
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 597.95,                last time consumption/overall running time: 86.1051s / 2628.4519 s
env0_first_0:                 episode reward: -0.8000,                 loss: nan
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0225
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 582.05,                last time consumption/overall running time: 83.4843s / 2711.9362 s
env0_first_0:                 episode reward: 0.5500,                 loss: nan
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0217
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 600.35,                last time consumption/overall running time: 86.5911s / 2798.5273 s
env0_first_0:                 episode reward: -0.8500,                 loss: nan
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0219
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 567.6,                last time consumption/overall running time: 81.7309s / 2880.2582 s
env0_first_0:                 episode reward: 0.2500,                 loss: nan
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0205
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 592.5,                last time consumption/overall running time: 85.1983s / 2965.4565 s
env0_first_0:                 episode reward: 1.1000,                 loss: nan
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0183
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 566.4,                last time consumption/overall running time: 81.5544s / 3047.0109 s
env0_first_0:                 episode reward: -1.4500,                 loss: nan
env0_second_0:                 episode reward: 1.4500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/737_1.
Episode: 761/10000 (7.6100%),                 avg. length: 574.85,                last time consumption/overall running time: 82.4708s / 3129.4817 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0193
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 548.9,                last time consumption/overall running time: 78.0846s / 3207.5663 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0204
env0_second_0:                 episode reward: 0.7500,                 loss: nan
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 576.3,                last time consumption/overall running time: 82.4806s / 3290.0469 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0216
env0_second_0:                 episode reward: 0.0500,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 580.6,                last time consumption/overall running time: 83.1006s / 3373.1475 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0226
env0_second_0:                 episode reward: 0.0500,                 loss: nan
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 589.9,                last time consumption/overall running time: 84.4237s / 3457.5712 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0217
env0_second_0:                 episode reward: -0.4000,                 loss: nan
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 581.8,                last time consumption/overall running time: 83.9764s / 3541.5477 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0217
env0_second_0:                 episode reward: -1.3500,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 552.95,                last time consumption/overall running time: 80.0197s / 3621.5674 s
env0_first_0:                 episode reward: 1.3000,                 loss: nan
env0_second_0:                 episode reward: -1.3000,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/867_0.
Episode: 901/10000 (9.0100%),                 avg. length: 622.25,                last time consumption/overall running time: 89.7593s / 3711.3267 s
env0_first_0:                 episode reward: 0.1000,                 loss: nan
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0180
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 580.4,                last time consumption/overall running time: 83.2340s / 3794.5607 s
env0_first_0:                 episode reward: -0.6500,                 loss: nan
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0185
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 566.35,                last time consumption/overall running time: 80.7818s / 3875.3425 s
env0_first_0:                 episode reward: 0.6500,                 loss: nan
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0193
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 620.05,                last time consumption/overall running time: 89.2594s / 3964.6019 s
env0_first_0:                 episode reward: 1.0500,                 loss: nan
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0179
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 606.4,                last time consumption/overall running time: 86.8928s / 4051.4946 s
env0_first_0:                 episode reward: 0.7500,                 loss: nan
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0181
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 604.05,                last time consumption/overall running time: 86.9425s / 4138.4371 s
env0_first_0:                 episode reward: 0.5000,                 loss: nan
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0170
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 594.05,                last time consumption/overall running time: 85.6012s / 4224.0383 s
env0_first_0:                 episode reward: -0.5500,                 loss: nan
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0171
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 560.1,                last time consumption/overall running time: 80.7641s / 4304.8025 s
env0_first_0:                 episode reward: 0.0500,                 loss: nan
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0177
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 576.3,                last time consumption/overall running time: 82.7263s / 4387.5287 s
env0_first_0:                 episode reward: -0.3500,                 loss: nan
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/1055_1.
Episode: 1081/10000 (10.8100%),                 avg. length: 588.95,                last time consumption/overall running time: 84.5113s / 4472.0401 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0198
env0_second_0:                 episode reward: -0.6500,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 588.6,                last time consumption/overall running time: 84.3931s / 4556.4331 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0205
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 613.95,                last time consumption/overall running time: 88.1161s / 4644.5492 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0204
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 611.4,                last time consumption/overall running time: 88.1763s / 4732.7255 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0221
env0_second_0:                 episode reward: -0.7500,                 loss: nan
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 596.45,                last time consumption/overall running time: 86.1056s / 4818.8311 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0228
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 605.35,                last time consumption/overall running time: 86.3548s / 4905.1859 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0227
env0_second_0:                 episode reward: -0.5500,                 loss: nan
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 566.85,                last time consumption/overall running time: 81.3266s / 4986.5125 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0213
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 576.4,                last time consumption/overall running time: 83.5015s / 5070.0140 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0225
env0_second_0:                 episode reward: 0.5000,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 614.4,                last time consumption/overall running time: 88.7844s / 5158.7985 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0226
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 577.45,                last time consumption/overall running time: 83.1321s / 5241.9306 s
env0_first_0:                 episode reward: 0.7500,                 loss: nan
env0_second_0:                 episode reward: -0.7500,                 loss: nan
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Score delta: 4.2, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/1254_0.
Episode: 1281/10000 (12.8100%),                 avg. length: 629.15,                last time consumption/overall running time: 89.8459s / 5331.7764 s
env0_first_0:                 episode reward: 0.2500,                 loss: nan
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0185
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 586.35,                last time consumption/overall running time: 84.6483s / 5416.4248 s
env0_first_0:                 episode reward: -1.0500,                 loss: nan
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/1292_1.
Episode: 1321/10000 (13.2100%),                 avg. length: 583.6,                last time consumption/overall running time: 84.3375s / 5500.7622 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0205
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 592.15,                last time consumption/overall running time: 84.9864s / 5585.7486 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0212
env0_second_0:                 episode reward: -0.3000,                 loss: nan
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 572.6,                last time consumption/overall running time: 82.6522s / 5668.4009 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0197
env0_second_0:                 episode reward: -0.4000,                 loss: nan
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 591.9,                last time consumption/overall running time: 84.9747s / 5753.3756 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0195
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 610.4,                last time consumption/overall running time: 87.6344s / 5841.0100 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0185
env0_second_0:                 episode reward: -0.5500,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 616.1,                last time consumption/overall running time: 88.8884s / 5929.8985 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0184
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 599.35,                last time consumption/overall running time: 86.6559s / 6016.5543 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0182
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 560.8,                last time consumption/overall running time: 80.6923s / 6097.2467 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0198
env0_second_0:                 episode reward: -0.9000,                 loss: nan
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 553.85,                last time consumption/overall running time: 80.0480s / 6177.2947 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0180
env0_second_0:                 episode reward: -0.0500,                 loss: nan
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 556.4,                last time consumption/overall running time: 80.1531s / 6257.4478 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0184
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 559.25,                last time consumption/overall running time: 80.7581s / 6338.2059 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0194
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 599.5,                last time consumption/overall running time: 85.7923s / 6423.9983 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0190
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 580.85,                last time consumption/overall running time: 83.4800s / 6507.4782 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0191
env0_second_0:                 episode reward: -1.4500,                 loss: nan
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 625.85,                last time consumption/overall running time: 89.4996s / 6596.9779 s
env0_first_0:                 episode reward: -0.5500,                 loss: nan
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Score delta: 4.6, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/1563_0.
Episode: 1601/10000 (16.0100%),                 avg. length: 611.05,                last time consumption/overall running time: 87.3343s / 6684.3122 s
env0_first_0:                 episode reward: -0.6500,                 loss: nan
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0169
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 577.2,                last time consumption/overall running time: 82.9844s / 6767.2966 s
env0_first_0:                 episode reward: 0.1000,                 loss: nan
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0168
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 588.65,                last time consumption/overall running time: 85.0332s / 6852.3298 s
env0_first_0:                 episode reward: -0.2000,                 loss: nan
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0176
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 613.7,                last time consumption/overall running time: 87.5993s / 6939.9290 s
env0_first_0:                 episode reward: 0.7000,                 loss: nan
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0191
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 582.95,                last time consumption/overall running time: 83.6461s / 7023.5751 s
env0_first_0:                 episode reward: -0.2500,                 loss: nan
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0181
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 535.9,                last time consumption/overall running time: 77.6842s / 7101.2594 s
env0_first_0:                 episode reward: 0.4500,                 loss: nan
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0193
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 616.95,                last time consumption/overall running time: 87.9445s / 7189.2038 s
env0_first_0:                 episode reward: -0.2000,                 loss: nan
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0193
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 626.05,                last time consumption/overall running time: 90.2810s / 7279.4848 s
env0_first_0:                 episode reward: 0.8000,                 loss: nan
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0199
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 609.35,                last time consumption/overall running time: 86.8800s / 7366.3648 s
env0_first_0:                 episode reward: -1.0500,                 loss: nan
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0192
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 624.65,                last time consumption/overall running time: 89.7103s / 7456.0751 s
env0_first_0:                 episode reward: -1.3000,                 loss: nan
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Score delta: 4.2, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/1776_1.
Episode: 1801/10000 (18.0100%),                 avg. length: 601.1,                last time consumption/overall running time: 87.0321s / 7543.1072 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0202
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 620.0,                last time consumption/overall running time: 89.1509s / 7632.2582 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0219
env0_second_0:                 episode reward: -0.8000,                 loss: nan
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 612.45,                last time consumption/overall running time: 88.3181s / 7720.5763 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0218
env0_second_0:                 episode reward: -0.4000,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 549.9,                last time consumption/overall running time: 79.0504s / 7799.6267 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0208
env0_second_0:                 episode reward: -0.1000,                 loss: nan
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 607.45,                last time consumption/overall running time: 87.5717s / 7887.1983 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0221
env0_second_0:                 episode reward: -0.9500,                 loss: nan
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 588.9,                last time consumption/overall running time: 84.7737s / 7971.9720 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0214
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 566.3,                last time consumption/overall running time: 81.2118s / 8053.1838 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0200
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 570.55,                last time consumption/overall running time: 81.6538s / 8134.8376 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0215
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 565.0,                last time consumption/overall running time: 81.3206s / 8216.1582 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0202
env0_second_0:                 episode reward: -0.3000,                 loss: nan
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 608.0,                last time consumption/overall running time: 86.7677s / 8302.9259 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0200
env0_second_0:                 episode reward: 0.2000,                 loss: nan
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 582.6,                last time consumption/overall running time: 83.9911s / 8386.9170 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0196
env0_second_0:                 episode reward: -1.1500,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 583.75,                last time consumption/overall running time: 83.6660s / 8470.5830 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0205
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 579.65,                last time consumption/overall running time: 83.6077s / 8554.1907 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0209
env0_second_0:                 episode reward: -0.3000,                 loss: nan
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 588.0,                last time consumption/overall running time: 84.5838s / 8638.7745 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0203
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 576.25,                last time consumption/overall running time: 82.5893s / 8721.3638 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0206
env0_second_0:                 episode reward: -0.9000,                 loss: nan
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 539.5,                last time consumption/overall running time: 77.7717s / 8799.1355 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0207
env0_second_0:                 episode reward: -0.7000,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 587.6,                last time consumption/overall running time: 84.2445s / 8883.3800 s
env0_first_0:                 episode reward: 0.9000,                 loss: nan
env0_second_0:                 episode reward: -0.9000,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Score delta: 4.2, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/2118_0.
Episode: 2141/10000 (21.4100%),                 avg. length: 614.5,                last time consumption/overall running time: 88.5763s / 8971.9563 s
env0_first_0:                 episode reward: 0.1500,                 loss: nan
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0194
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 606.35,                last time consumption/overall running time: 87.0237s / 9058.9800 s
env0_first_0:                 episode reward: -1.3000,                 loss: nan
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Score delta: 4.8, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/2151_1.
Episode: 2181/10000 (21.8100%),                 avg. length: 611.7,                last time consumption/overall running time: 87.8109s / 9146.7909 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0180
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 604.85,                last time consumption/overall running time: 86.9901s / 9233.7810 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0190
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 625.05,                last time consumption/overall running time: 90.1861s / 9323.9671 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0191
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 598.0,                last time consumption/overall running time: 86.3660s / 9410.3331 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0197
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 590.85,                last time consumption/overall running time: 86.7976s / 9497.1307 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0205
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 634.5,                last time consumption/overall running time: 91.1750s / 9588.3057 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0229
env0_second_0:                 episode reward: -0.6000,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 561.05,                last time consumption/overall running time: 80.5028s / 9668.8085 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0243
env0_second_0:                 episode reward: -0.6000,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 577.0,                last time consumption/overall running time: 82.9818s / 9751.7903 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0220
env0_second_0:                 episode reward: 0.7500,                 loss: nan
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 589.45,                last time consumption/overall running time: 84.1377s / 9835.9279 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0200
env0_second_0:                 episode reward: 0.5000,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 594.85,                last time consumption/overall running time: 85.6064s / 9921.5343 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0200
env0_second_0:                 episode reward: 0.7000,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 640.45,                last time consumption/overall running time: 91.5413s / 10013.0755 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0200
env0_second_0:                 episode reward: -0.6500,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 583.1,                last time consumption/overall running time: 84.1336s / 10097.2092 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0192
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 617.6,                last time consumption/overall running time: 88.6065s / 10185.8156 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0194
env0_second_0:                 episode reward: 0.2000,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 580.2,                last time consumption/overall running time: 83.5704s / 10269.3861 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0206
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 594.35,                last time consumption/overall running time: 85.7312s / 10355.1172 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0215
env0_second_0:                 episode reward: 0.5000,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 618.5,                last time consumption/overall running time: 89.6051s / 10444.7224 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0213
env0_second_0:                 episode reward: 0.8500,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 577.45,                last time consumption/overall running time: 83.1366s / 10527.8590 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0218
env0_second_0:                 episode reward: 0.7500,                 loss: nan
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 660.85,                last time consumption/overall running time: 95.3047s / 10623.1637 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0220
env0_second_0:                 episode reward: 0.2000,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 575.65,                last time consumption/overall running time: 82.9966s / 10706.1603 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0221
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 634.6,                last time consumption/overall running time: 91.9450s / 10798.1054 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0223
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 610.95,                last time consumption/overall running time: 87.6392s / 10885.7446 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0224
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 562.95,                last time consumption/overall running time: 81.2720s / 10967.0166 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0199
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 649.0,                last time consumption/overall running time: 93.3783s / 11060.3949 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0194
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 629.15,                last time consumption/overall running time: 90.7480s / 11151.1429 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0195
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 616.65,                last time consumption/overall running time: 88.4721s / 11239.6149 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0187
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 646.65,                last time consumption/overall running time: 93.6581s / 11333.2731 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0186
env0_second_0:                 episode reward: -0.9000,                 loss: nan
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 662.55,                last time consumption/overall running time: 95.9394s / 11429.2125 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0188
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 659.9,                last time consumption/overall running time: 94.9079s / 11524.1204 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0184
env0_second_0:                 episode reward: -0.1000,                 loss: nan
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 657.65,                last time consumption/overall running time: 95.0777s / 11619.1981 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0176
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 693.25,                last time consumption/overall running time: 99.3030s / 11718.5011 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0181
env0_second_0:                 episode reward: -0.7000,                 loss: nan
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 606.6,                last time consumption/overall running time: 87.4166s / 11805.9177 s
env0_first_0:                 episode reward: 1.0500,                 loss: nan
env0_second_0:                 episode reward: -1.0500,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Score delta: 5.0, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/2768_0.
Episode: 2801/10000 (28.0100%),                 avg. length: 592.5,                last time consumption/overall running time: 85.2648s / 11891.1825 s
env0_first_0:                 episode reward: -0.1500,                 loss: nan
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0171
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 604.05,                last time consumption/overall running time: 86.7362s / 11977.9187 s
env0_first_0:                 episode reward: 0.0000,                 loss: nan
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0172
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 606.65,                last time consumption/overall running time: 87.5115s / 12065.4302 s
env0_first_0:                 episode reward: 0.9000,                 loss: nan
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0182
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 582.2,                last time consumption/overall running time: 83.4661s / 12148.8963 s
env0_first_0:                 episode reward: -0.3000,                 loss: nan
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0171
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 658.05,                last time consumption/overall running time: 94.9144s / 12243.8106 s
env0_first_0:                 episode reward: -0.3000,                 loss: nan
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0157
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 650.4,                last time consumption/overall running time: 93.4913s / 12337.3019 s
env0_first_0:                 episode reward: -0.6500,                 loss: nan
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0160
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 647.6,                last time consumption/overall running time: 93.3992s / 12430.7011 s
env0_first_0:                 episode reward: 0.7000,                 loss: nan
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0163
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 623.45,                last time consumption/overall running time: 89.9783s / 12520.6794 s
env0_first_0:                 episode reward: -0.6000,                 loss: nan
env0_second_0:                 episode reward: 0.6000,                 loss: 0.0177
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 617.15,                last time consumption/overall running time: 88.6489s / 12609.3283 s
env0_first_0:                 episode reward: -1.4500,                 loss: nan
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0181
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Score delta: 4.2, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/2961_1.
Episode: 2981/10000 (29.8100%),                 avg. length: 604.95,                last time consumption/overall running time: 86.4389s / 12695.7672 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0179
env0_second_0:                 episode reward: -0.5000,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 553.5,                last time consumption/overall running time: 80.1768s / 12775.9439 s
env0_first_0:                 episode reward: 1.2000,                 loss: nan
env0_second_0:                 episode reward: -1.2000,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Score delta: 4.2, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/2983_0.
Episode: 3021/10000 (30.2100%),                 avg. length: 603.65,                last time consumption/overall running time: 86.8209s / 12862.7648 s
env0_first_0:                 episode reward: 0.3000,                 loss: nan
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0170
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 634.2,                last time consumption/overall running time: 90.9336s / 12953.6985 s
env0_first_0:                 episode reward: -0.3500,                 loss: nan
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0184
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 638.45,                last time consumption/overall running time: 92.2099s / 13045.9084 s
env0_first_0:                 episode reward: 0.0000,                 loss: nan
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0171
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 625.0,                last time consumption/overall running time: 90.1613s / 13136.0697 s
env0_first_0:                 episode reward: -0.0500,                 loss: nan
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0164
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 676.15,                last time consumption/overall running time: 97.7010s / 13233.7708 s
env0_first_0:                 episode reward: -1.4000,                 loss: nan
env0_second_0:                 episode reward: 1.4000,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Score delta: 4.8, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/3093_1.
Episode: 3121/10000 (31.2100%),                 avg. length: 648.8,                last time consumption/overall running time: 92.6028s / 13326.3736 s
env0_first_0:                 episode reward: 1.2000,                 loss: nan
env0_second_0:                 episode reward: -1.2000,                 loss: nan
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Score delta: 4.2, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/3114_0.
Episode: 3141/10000 (31.4100%),                 avg. length: 624.65,                last time consumption/overall running time: 89.4559s / 13415.8295 s
env0_first_0:                 episode reward: -1.0500,                 loss: nan
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0182
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 677.0,                last time consumption/overall running time: 97.3073s / 13513.1368 s
env0_first_0:                 episode reward: 0.6500,                 loss: nan
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0183
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 650.9,                last time consumption/overall running time: 94.0606s / 13607.1974 s
env0_first_0:                 episode reward: 0.6000,                 loss: nan
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0170
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 667.75,                last time consumption/overall running time: 95.9562s / 13703.1536 s
env0_first_0:                 episode reward: 0.0500,                 loss: nan
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0163
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 672.9,                last time consumption/overall running time: 97.1138s / 13800.2674 s
env0_first_0:                 episode reward: -1.0000,                 loss: nan
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/3216_1.
Episode: 3241/10000 (32.4100%),                 avg. length: 697.9,                last time consumption/overall running time: 100.5748s / 13900.8422 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0174
env0_second_0:                 episode reward: -1.3000,                 loss: nan
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 768.4,                last time consumption/overall running time: 110.6330s / 14011.4752 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0163
env0_second_0:                 episode reward: -1.4000,                 loss: nan
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 634.5,                last time consumption/overall running time: 91.6995s / 14103.1747 s
env0_first_0:                 episode reward: 1.7500,                 loss: nan
env0_second_0:                 episode reward: -1.7500,                 loss: nan
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/3264_0.
Episode: 3301/10000 (33.0100%),                 avg. length: 612.8,                last time consumption/overall running time: 87.9000s / 14191.0747 s
env0_first_0:                 episode reward: 0.7500,                 loss: nan
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0163
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 590.4,                last time consumption/overall running time: 84.4534s / 14275.5281 s
env0_first_0:                 episode reward: -0.3500,                 loss: nan
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0179
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 610.15,                last time consumption/overall running time: 87.9815s / 14363.5096 s
env0_first_0:                 episode reward: 0.7000,                 loss: nan
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0188
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 598.0,                last time consumption/overall running time: 86.3987s / 14449.9083 s
env0_first_0:                 episode reward: 0.3000,                 loss: nan
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0194
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 643.45,                last time consumption/overall running time: 92.8019s / 14542.7102 s
env0_first_0:                 episode reward: 0.5500,                 loss: nan
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0180
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 649.95,                last time consumption/overall running time: 93.7386s / 14636.4487 s
env0_first_0:                 episode reward: 0.0500,                 loss: nan
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0169
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 614.0,                last time consumption/overall running time: 88.2140s / 14724.6627 s
env0_first_0:                 episode reward: 0.0500,                 loss: nan
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0173
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 647.1,                last time consumption/overall running time: 93.3792s / 14818.0419 s
env0_first_0:                 episode reward: -0.4000,                 loss: nan
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0171
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 621.25,                last time consumption/overall running time: 88.7755s / 14906.8174 s
env0_first_0:                 episode reward: 0.2500,                 loss: nan
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0189
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 614.8,                last time consumption/overall running time: 87.9430s / 14994.7605 s
env0_first_0:                 episode reward: 1.4000,                 loss: nan
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0194
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 617.45,                last time consumption/overall running time: 88.7886s / 15083.5491 s
env0_first_0:                 episode reward: -0.3500,                 loss: nan
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0186
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 648.1,                last time consumption/overall running time: 92.5713s / 15176.1203 s
env0_first_0:                 episode reward: -0.1500,                 loss: nan
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0184
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 640.3,                last time consumption/overall running time: 91.7523s / 15267.8727 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0168
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 620.1,                last time consumption/overall running time: 88.9398s / 15356.8125 s
env0_first_0:                 episode reward: -0.4000,                 loss: nan
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Score delta: 4.2, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/3559_1.
Episode: 3581/10000 (35.8100%),                 avg. length: 671.4,                last time consumption/overall running time: 96.9718s / 15453.7843 s
env0_first_0:                 episode reward: 2.2000,                 loss: nan
env0_second_0:                 episode reward: -2.2000,                 loss: nan
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Score delta: 5.0, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/3580_0.
Episode: 3601/10000 (36.0100%),                 avg. length: 665.25,                last time consumption/overall running time: 96.0444s / 15549.8287 s
env0_first_0:                 episode reward: 0.6500,                 loss: nan
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0161
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 666.7,                last time consumption/overall running time: 95.4650s / 15645.2937 s
env0_first_0:                 episode reward: -0.1000,                 loss: nan
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0165
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 676.55,                last time consumption/overall running time: 97.4580s / 15742.7517 s
env0_first_0:                 episode reward: -0.3500,                 loss: nan
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0169
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 696.85,                last time consumption/overall running time: 100.0452s / 15842.7969 s
env0_first_0:                 episode reward: -0.4500,                 loss: nan
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0172
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 683.65,                last time consumption/overall running time: 98.1732s / 15940.9701 s
env0_first_0:                 episode reward: -0.2000,                 loss: nan
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0171
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 679.45,                last time consumption/overall running time: 97.6726s / 16038.6427 s
env0_first_0:                 episode reward: 0.1500,                 loss: nan
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0168
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 707.95,                last time consumption/overall running time: 101.7555s / 16140.3981 s
env0_first_0:                 episode reward: -0.5000,                 loss: nan
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0157
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/3721_1.
Episode: 3741/10000 (37.4100%),                 avg. length: 636.4,                last time consumption/overall running time: 91.1412s / 16231.5394 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0154
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 580.1,                last time consumption/overall running time: 82.8646s / 16314.4039 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0159
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 644.55,                last time consumption/overall running time: 92.4930s / 16406.8969 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0183
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 638.95,                last time consumption/overall running time: 91.8894s / 16498.7863 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0187
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 579.2,                last time consumption/overall running time: 83.2274s / 16582.0137 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0176
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 589.8,                last time consumption/overall running time: 84.7049s / 16666.7186 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0166
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 587.8,                last time consumption/overall running time: 84.4505s / 16751.1691 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0202
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 658.0,                last time consumption/overall running time: 94.2472s / 16845.4164 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0200
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 629.35,                last time consumption/overall running time: 90.3342s / 16935.7506 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0182
env0_second_0:                 episode reward: -0.5000,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 607.75,                last time consumption/overall running time: 86.9293s / 17022.6799 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0183
env0_second_0:                 episode reward: -0.6500,                 loss: nan
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 614.95,                last time consumption/overall running time: 88.3418s / 17111.0217 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0188
env0_second_0:                 episode reward: -0.7500,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 625.35,                last time consumption/overall running time: 89.5783s / 17200.6001 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0190
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 605.05,                last time consumption/overall running time: 86.8078s / 17287.4079 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0181
env0_second_0:                 episode reward: -0.8500,                 loss: nan
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 645.85,                last time consumption/overall running time: 92.1114s / 17379.5193 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0176
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 604.6,                last time consumption/overall running time: 87.2474s / 17466.7667 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0191
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 621.15,                last time consumption/overall running time: 89.4887s / 17556.2554 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0192
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 602.25,                last time consumption/overall running time: 86.1023s / 17642.3578 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0194
env0_second_0:                 episode reward: -0.9500,                 loss: nan
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 638.4,                last time consumption/overall running time: 91.9641s / 17734.3219 s
env0_first_0:                 episode reward: -0.1000,                 loss: nan
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Score delta: 4.2, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/4065_0.
Episode: 4101/10000 (41.0100%),                 avg. length: 701.05,                last time consumption/overall running time: 100.8869s / 17835.2088 s
env0_first_0:                 episode reward: -1.0000,                 loss: nan
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0173
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 683.25,                last time consumption/overall running time: 99.0359s / 17934.2447 s
env0_first_0:                 episode reward: -0.3500,                 loss: nan
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0178
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 704.85,                last time consumption/overall running time: 101.2600s / 18035.5048 s
env0_first_0:                 episode reward: -1.2500,                 loss: nan
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0171
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 733.2,                last time consumption/overall running time: 105.6969s / 18141.2017 s
env0_first_0:                 episode reward: -0.7500,                 loss: nan
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0162
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 671.0,                last time consumption/overall running time: 96.6989s / 18237.9006 s
env0_first_0:                 episode reward: -0.6500,                 loss: nan
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0156
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 694.05,                last time consumption/overall running time: 100.2451s / 18338.1457 s
env0_first_0:                 episode reward: -1.5000,                 loss: nan
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0156
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 751.55,                last time consumption/overall running time: 108.2172s / 18446.3630 s
env0_first_0:                 episode reward: 0.2500,                 loss: nan
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Score delta: 5.2, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/4203_1.
Episode: 4241/10000 (42.4100%),                 avg. length: 719.2,                last time consumption/overall running time: 103.2707s / 18549.6337 s
env0_first_0:                 episode reward: 1.7000,                 loss: nan
env0_second_0:                 episode reward: -1.7000,                 loss: nan
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Score delta: 4.6, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/4233_0.
Episode: 4261/10000 (42.6100%),                 avg. length: 690.0,                last time consumption/overall running time: 100.0188s / 18649.6525 s
env0_first_0:                 episode reward: -0.1000,                 loss: nan
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0160
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 801.45,                last time consumption/overall running time: 115.2759s / 18764.9284 s
env0_first_0:                 episode reward: -0.4500,                 loss: nan
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0153
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 645.65,                last time consumption/overall running time: 93.6468s / 18858.5752 s
env0_first_0:                 episode reward: -2.1000,                 loss: nan
env0_second_0:                 episode reward: 2.1000,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/4290_1.
Episode: 4321/10000 (43.2100%),                 avg. length: 632.9,                last time consumption/overall running time: 91.6910s / 18950.2662 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0183
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 580.3,                last time consumption/overall running time: 84.0733s / 19034.3395 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0186
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 648.85,                last time consumption/overall running time: 92.9011s / 19127.2406 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0188
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 630.35,                last time consumption/overall running time: 91.9400s / 19219.1806 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0189
env0_second_0:                 episode reward: -0.0500,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 728.85,                last time consumption/overall running time: 105.1872s / 19324.3678 s
env0_first_0:                 episode reward: -1.5500,                 loss: nan
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Score delta: 4.8, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/4385_0.
Episode: 4421/10000 (44.2100%),                 avg. length: 751.85,                last time consumption/overall running time: 108.4491s / 19432.8169 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/4406_1.
Episode: 4441/10000 (44.4100%),                 avg. length: 830.3,                last time consumption/overall running time: 119.0013s / 19551.8181 s
env0_first_0:                 episode reward: 1.2500,                 loss: nan
env0_second_0:                 episode reward: -1.2500,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Score delta: 4.2, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/4430_0.
Episode: 4461/10000 (44.6100%),                 avg. length: 659.4,                last time consumption/overall running time: 95.1113s / 19646.9295 s
env0_first_0:                 episode reward: -1.7500,                 loss: nan
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Score delta: 4.4, save the model to .//data/model/20220114_1944/slimevolley_SlimeVolley-v0_fictitious_selfplay2/4452_1.
Episode: 4481/10000 (44.8100%),                 avg. length: 583.3,                last time consumption/overall running time: 83.6544s / 19730.5839 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0168
env0_second_0:                 episode reward: 0.7500,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 638.6,                last time consumption/overall running time: 92.2927s / 19822.8766 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0168
env0_second_0:                 episode reward: 0.7000,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 652.15,                last time consumption/overall running time: 94.0537s / 19916.9303 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0184
env0_second_0:                 episode reward: 1.3500,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 575.45,                last time consumption/overall running time: 82.9970s / 19999.9273 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0185
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 613.5,                last time consumption/overall running time: 88.6113s / 20088.5387 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0181
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 626.65,                last time consumption/overall running time: 91.1975s / 20179.7361 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0186
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 679.7,                last time consumption/overall running time: 97.6115s / 20277.3476 s