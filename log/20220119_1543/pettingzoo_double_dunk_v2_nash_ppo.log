pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
double_dunk_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'double_dunk_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220119_1543/pettingzoo_double_dunk_v2_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_1543/pettingzoo_double_dunk_v2_nash_ppo.
Episode: 1/10000 (0.0100%),                 avg. length: 4189.0,                last time consumption/overall running time: 73.7810s / 73.7810 s
env0_first_0:                 episode reward: -36.0000,                 loss: -0.0809
env0_second_0:                 episode reward: 36.0000,                 loss: -0.0907
env1_first_0:                 episode reward: -38.0000,                 loss: nan
env1_second_0:                 episode reward: 38.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 3815.05,                last time consumption/overall running time: 1440.8919s / 1514.6729 s
env0_first_0:                 episode reward: -22.4500,                 loss: -0.0530
env0_second_0:                 episode reward: 22.4500,                 loss: -0.0508
env1_first_0:                 episode reward: -25.8500,                 loss: nan
env1_second_0:                 episode reward: 25.8500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 3821.3,                last time consumption/overall running time: 1459.6578s / 2974.3307 s
env0_first_0:                 episode reward: -23.2500,                 loss: 0.0143
env0_second_0:                 episode reward: 23.2500,                 loss: 0.0119
env1_first_0:                 episode reward: -23.3000,                 loss: nan
env1_second_0:                 episode reward: 23.3000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 4208.55,                last time consumption/overall running time: 1605.0778s / 4579.4085 s
env0_first_0:                 episode reward: -30.0500,                 loss: 0.0247
env0_second_0:                 episode reward: 30.0500,                 loss: 0.0230
env1_first_0:                 episode reward: -27.9000,                 loss: nan
env1_second_0:                 episode reward: 27.9000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 3790.9,                last time consumption/overall running time: 1451.5053s / 6030.9139 s
env0_first_0:                 episode reward: -22.9000,                 loss: 0.0603
env0_second_0:                 episode reward: 22.9000,                 loss: 0.0562
env1_first_0:                 episode reward: -21.3500,                 loss: nan
env1_second_0:                 episode reward: 21.3500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 3989.25,                last time consumption/overall running time: 1519.2427s / 7550.1566 s
env0_first_0:                 episode reward: -24.1500,                 loss: 0.0517
env0_second_0:                 episode reward: 24.1500,                 loss: 0.0607
env1_first_0:                 episode reward: -28.3000,                 loss: nan
env1_second_0:                 episode reward: 28.3000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 3912.6,                last time consumption/overall running time: 1496.7725s / 9046.9291 s
env0_first_0:                 episode reward: -23.8500,                 loss: 0.0688
env0_second_0:                 episode reward: 23.8500,                 loss: 0.0768
env1_first_0:                 episode reward: -23.8000,                 loss: nan
env1_second_0:                 episode reward: 23.8000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 4067.2,                last time consumption/overall running time: 1556.5775s / 10603.5066 s
env0_first_0:                 episode reward: -24.8500,                 loss: 0.0622
env0_second_0:                 episode reward: 24.8500,                 loss: 0.0668
env1_first_0:                 episode reward: -24.2000,                 loss: nan
env1_second_0:                 episode reward: 24.2000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 3704.1,                last time consumption/overall running time: 1415.2508s / 12018.7574 s
env0_first_0:                 episode reward: -22.8500,                 loss: 0.0737
env0_second_0:                 episode reward: 22.8500,                 loss: 0.0777
env1_first_0:                 episode reward: -23.0000,                 loss: nan
env1_second_0:                 episode reward: 23.0000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 4069.4,                last time consumption/overall running time: 1555.7926s / 13574.5501 s
env0_first_0:                 episode reward: -21.7000,                 loss: 0.0552
env0_second_0:                 episode reward: 21.7000,                 loss: 0.0547
env1_first_0:                 episode reward: -24.7500,                 loss: nan
env1_second_0:                 episode reward: 24.7500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 3866.4,                last time consumption/overall running time: 1477.5377s / 15052.0877 s
env0_first_0:                 episode reward: -24.8500,                 loss: 0.0530
env0_second_0:                 episode reward: 24.8500,                 loss: 0.0556
env1_first_0:                 episode reward: -28.0500,                 loss: nan
env1_second_0:                 episode reward: 28.0500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 3981.1,                last time consumption/overall running time: 1519.4253s / 16571.5130 s
env0_first_0:                 episode reward: -23.9500,                 loss: 0.0461
env0_second_0:                 episode reward: 23.9500,                 loss: 0.0479
env1_first_0:                 episode reward: -25.9500,                 loss: nan
env1_second_0:                 episode reward: 25.9500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 3933.4,                last time consumption/overall running time: 1504.9235s / 18076.4365 s
env0_first_0:                 episode reward: -27.0000,                 loss: 0.0489
env0_second_0:                 episode reward: 27.0000,                 loss: 0.0550
env1_first_0:                 episode reward: -23.0500,                 loss: nan
env1_second_0:                 episode reward: 23.0500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 4249.5,                last time consumption/overall running time: 1624.1940s / 19700.6305 s
env0_first_0:                 episode reward: -29.0000,                 loss: 0.0469
env0_second_0:                 episode reward: 29.0000,                 loss: 0.0508
env1_first_0:                 episode reward: -28.2500,                 loss: nan
env1_second_0:                 episode reward: 28.2500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 4769.5,                last time consumption/overall running time: 1818.3650s / 21518.9955 s
env0_first_0:                 episode reward: -34.2000,                 loss: 0.0340
env0_second_0:                 episode reward: 34.2000,                 loss: 0.0397
env1_first_0:                 episode reward: -33.0000,                 loss: nan
env1_second_0:                 episode reward: 33.0000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 4298.65,                last time consumption/overall running time: 1640.2107s / 23159.2063 s
env0_first_0:                 episode reward: -27.3500,                 loss: 0.0359
env0_second_0:                 episode reward: 27.3500,                 loss: 0.0376
env1_first_0:                 episode reward: -28.6000,                 loss: nan
env1_second_0:                 episode reward: 28.6000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 4583.55,                last time consumption/overall running time: 1750.4830s / 24909.6893 s
env0_first_0:                 episode reward: -32.3000,                 loss: 0.0256
env0_second_0:                 episode reward: 32.3000,                 loss: 0.0238
env1_first_0:                 episode reward: -31.6000,                 loss: nan
env1_second_0:                 episode reward: 31.6000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 4647.4,                last time consumption/overall running time: 1771.3922s / 26681.0815 s
env0_first_0:                 episode reward: -33.9500,                 loss: 0.0041
env0_second_0:                 episode reward: 33.9500,                 loss: 0.0072
env1_first_0:                 episode reward: -36.6000,                 loss: nan
env1_second_0:                 episode reward: 36.6000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 4757.6,                last time consumption/overall running time: 1817.9292s / 28499.0107 s
env0_first_0:                 episode reward: -35.7500,                 loss: 0.0017
env0_second_0:                 episode reward: 35.7500,                 loss: 0.0017
env1_first_0:                 episode reward: -33.0500,                 loss: nan
env1_second_0:                 episode reward: 33.0500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 4771.25,                last time consumption/overall running time: 1822.1537s / 30321.1644 s
env0_first_0:                 episode reward: -36.1500,                 loss: -0.0005
env0_second_0:                 episode reward: 36.1500,                 loss: -0.0002
env1_first_0:                 episode reward: -36.2500,                 loss: nan
env1_second_0:                 episode reward: 36.2500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 5135.65,                last time consumption/overall running time: 1960.1036s / 32281.2679 s
env0_first_0:                 episode reward: -39.2500,                 loss: -0.0114
env0_second_0:                 episode reward: 39.2500,                 loss: -0.0126
env1_first_0:                 episode reward: -38.8500,                 loss: nan
env1_second_0:                 episode reward: 38.8500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 4929.5,                last time consumption/overall running time: 1880.4358s / 34161.7038 s
env0_first_0:                 episode reward: -37.8000,                 loss: -0.0281
env0_second_0:                 episode reward: 37.8000,                 loss: -0.0295
env1_first_0:                 episode reward: -35.7000,                 loss: nan
env1_second_0:                 episode reward: 35.7000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 4899.7,                last time consumption/overall running time: 1865.5981s / 36027.3018 s
env0_first_0:                 episode reward: -37.3000,                 loss: -0.0315
env0_second_0:                 episode reward: 37.3000,                 loss: -0.0377
env1_first_0:                 episode reward: -37.2000,                 loss: nan
env1_second_0:                 episode reward: 37.2000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 4806.65,                last time consumption/overall running time: 1828.6169s / 37855.9188 s
env0_first_0:                 episode reward: -38.2000,                 loss: -0.0399
env0_second_0:                 episode reward: 38.2000,                 loss: -0.0411
env1_first_0:                 episode reward: -35.4500,                 loss: nan
env1_second_0:                 episode reward: 35.4500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 4482.85,                last time consumption/overall running time: 1714.6675s / 39570.5862 s
env0_first_0:                 episode reward: -33.5000,                 loss: -0.0674
env0_second_0:                 episode reward: 33.5000,                 loss: -0.0677
env1_first_0:                 episode reward: -31.4000,                 loss: nan
env1_second_0:                 episode reward: 31.4000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 4270.25,                last time consumption/overall running time: 1625.7132s / 41196.2995 s
env0_first_0:                 episode reward: -27.0500,                 loss: -0.1063
env0_second_0:                 episode reward: 27.0500,                 loss: -0.1048
env1_first_0:                 episode reward: -25.2500,                 loss: nan
env1_second_0:                 episode reward: 25.2500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 4319.65,                last time consumption/overall running time: 1641.9453s / 42838.2447 s
env0_first_0:                 episode reward: -22.6500,                 loss: -0.1209
env0_second_0:                 episode reward: 22.6500,                 loss: -0.1131
env1_first_0:                 episode reward: -24.5000,                 loss: nan
env1_second_0:                 episode reward: 24.5000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 4140.0,                last time consumption/overall running time: 1578.1032s / 44416.3479 s
env0_first_0:                 episode reward: -15.1500,                 loss: -0.1917
env0_second_0:                 episode reward: 15.1500,                 loss: -0.1868
env1_first_0:                 episode reward: -16.2500,                 loss: nan
env1_second_0:                 episode reward: 16.2500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 3851.75,                last time consumption/overall running time: 1467.5946s / 45883.9426 s
env0_first_0:                 episode reward: -7.3500,                 loss: -0.2378
env0_second_0:                 episode reward: 7.3500,                 loss: -0.2349
env1_first_0:                 episode reward: -10.1500,                 loss: nan
env1_second_0:                 episode reward: 10.1500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 4210.25,                last time consumption/overall running time: 1603.1675s / 47487.1101 s
env0_first_0:                 episode reward: -6.5000,                 loss: -0.2634
env0_second_0:                 episode reward: 6.5000,                 loss: -0.2591
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 4150.95,                last time consumption/overall running time: 1582.2060s / 49069.3161 s
env0_first_0:                 episode reward: -6.2500,                 loss: -0.2699
env0_second_0:                 episode reward: 6.2500,                 loss: -0.2663
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 4205.55,                last time consumption/overall running time: 1608.2911s / 50677.6071 s
env0_first_0:                 episode reward: -3.1500,                 loss: -0.2811
env0_second_0:                 episode reward: 3.1500,                 loss: -0.2796
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 4056.4,                last time consumption/overall running time: 1555.8892s / 52233.4964 s
env0_first_0:                 episode reward: -3.3500,                 loss: -0.2960
env0_second_0:                 episode reward: 3.3500,                 loss: -0.2934
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 4042.85,                last time consumption/overall running time: 1551.5885s / 53785.0849 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.3018
env0_second_0:                 episode reward: 1.3000,                 loss: -0.3004
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 3523.25,                last time consumption/overall running time: 1351.4585s / 55136.5434 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3123
env0_second_0:                 episode reward: 1.3500,                 loss: -0.3219
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 3710.9,                last time consumption/overall running time: 1430.5635s / 56567.1069 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.3272
env0_second_0:                 episode reward: 0.4000,                 loss: -0.3221
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 4058.9,                last time consumption/overall running time: 1560.3850s / 58127.4919 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3342
env0_second_0:                 episode reward: -1.1000,                 loss: -0.3327
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 3990.8,                last time consumption/overall running time: 1451.8946s / 59579.3866 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3374
env0_second_0:                 episode reward: -0.2500,                 loss: -0.3317
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 3750.45,                last time consumption/overall running time: 1253.8800s / 60833.2666 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.3431
env0_second_0:                 episode reward: -0.6500,                 loss: -0.3454
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 3501.65,                last time consumption/overall running time: 1093.0299s / 61926.2965 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3495
env0_second_0:                 episode reward: -1.0000,                 loss: -0.3484
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 3713.85,                last time consumption/overall running time: 1162.3897s / 63088.6862 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3675
env0_second_0:                 episode reward: -0.9500,                 loss: -0.3624
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 3620.4,                last time consumption/overall running time: 1136.0358s / 64224.7220 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.3513
env0_second_0:                 episode reward: -0.4500,                 loss: -0.3435
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 3638.9,                last time consumption/overall running time: 1143.0516s / 65367.7736 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3537
env0_second_0:                 episode reward: -2.4500,                 loss: -0.3523
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 3750.95,                last time consumption/overall running time: 1175.3062s / 66543.0798 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3531
env0_second_0:                 episode reward: -1.3500,                 loss: -0.3525
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 3238.0,                last time consumption/overall running time: 1015.2996s / 67558.3795 s
env0_first_0:                 episode reward: 9.3500,                 loss: -0.2913
env0_second_0:                 episode reward: -9.3500,                 loss: -0.2899
env1_first_0:                 episode reward: 10.2000,                 loss: nan
env1_second_0:                 episode reward: -10.2000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2159.45,                last time consumption/overall running time: 686.4944s / 68244.8739 s
env0_first_0:                 episode reward: 16.5000,                 loss: -0.2378
env0_second_0:                 episode reward: -16.5000,                 loss: -0.2402
env1_first_0:                 episode reward: 15.5500,                 loss: nan
env1_second_0:                 episode reward: -15.5500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1983.15,                last time consumption/overall running time: 634.1570s / 68879.0309 s
env0_first_0:                 episode reward: 15.8000,                 loss: -0.2430
env0_second_0:                 episode reward: -15.8000,                 loss: -0.2405
env1_first_0:                 episode reward: 16.9000,                 loss: nan
env1_second_0:                 episode reward: -16.9000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 2018.8,                last time consumption/overall running time: 641.6151s / 69520.6460 s
env0_first_0:                 episode reward: 15.1000,                 loss: -0.2466
env0_second_0:                 episode reward: -15.1000,                 loss: -0.2505
env1_first_0:                 episode reward: 15.8500,                 loss: nan
env1_second_0:                 episode reward: -15.8500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2105.2,                last time consumption/overall running time: 669.8012s / 70190.4472 s
env0_first_0:                 episode reward: 17.3500,                 loss: -0.2494
env0_second_0:                 episode reward: -17.3500,                 loss: -0.2437
env1_first_0:                 episode reward: 14.6000,                 loss: nan
env1_second_0:                 episode reward: -14.6000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1962.25,                last time consumption/overall running time: 628.7106s / 70819.1578 s
env0_first_0:                 episode reward: 15.5000,                 loss: -0.2508
env0_second_0:                 episode reward: -15.5000,                 loss: -0.2456
env1_first_0:                 episode reward: 17.0500,                 loss: nan
env1_second_0:                 episode reward: -17.0500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1973.15,                last time consumption/overall running time: 629.9476s / 71449.1054 s
env0_first_0:                 episode reward: 16.6500,                 loss: -0.2428
env0_second_0:                 episode reward: -16.6500,                 loss: -0.2406
env1_first_0:                 episode reward: 15.8000,                 loss: nan
env1_second_0:                 episode reward: -15.8000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1861.9,                last time consumption/overall running time: 592.8821s / 72041.9875 s
env0_first_0:                 episode reward: 16.8500,                 loss: -0.2450
env0_second_0:                 episode reward: -16.8500,                 loss: -0.2430
env1_first_0:                 episode reward: 14.9500,                 loss: nan
env1_second_0:                 episode reward: -14.9500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1859.3,                last time consumption/overall running time: 598.0680s / 72640.0555 s
env0_first_0:                 episode reward: 15.2000,                 loss: -0.2500
env0_second_0:                 episode reward: -15.2000,                 loss: -0.2524
env1_first_0:                 episode reward: 16.1500,                 loss: nan
env1_second_0:                 episode reward: -16.1500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1953.5,                last time consumption/overall running time: 625.4859s / 73265.5413 s
env0_first_0:                 episode reward: 14.6500,                 loss: -0.2507
env0_second_0:                 episode reward: -14.6500,                 loss: -0.2479
env1_first_0:                 episode reward: 16.3500,                 loss: nan
env1_second_0:                 episode reward: -16.3500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1909.8,                last time consumption/overall running time: 610.6417s / 73876.1830 s
env0_first_0:                 episode reward: 16.3000,                 loss: -0.2599
env0_second_0:                 episode reward: -16.3000,                 loss: -0.2523
env1_first_0:                 episode reward: 15.1500,                 loss: nan
env1_second_0:                 episode reward: -15.1500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1925.1,                last time consumption/overall running time: 616.1527s / 74492.3357 s
env0_first_0:                 episode reward: 14.4500,                 loss: -0.2564
env0_second_0:                 episode reward: -14.4500,                 loss: -0.2531
env1_first_0:                 episode reward: 15.6000,                 loss: nan
env1_second_0:                 episode reward: -15.6000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1980.25,                last time consumption/overall running time: 630.6885s / 75123.0242 s
env0_first_0:                 episode reward: 16.2000,                 loss: -0.2608
env0_second_0:                 episode reward: -16.2000,                 loss: -0.2444
env1_first_0:                 episode reward: 15.9000,                 loss: nan
env1_second_0:                 episode reward: -15.9000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1965.0,                last time consumption/overall running time: 631.5321s / 75754.5563 s
env0_first_0:                 episode reward: 16.3500,                 loss: -0.2508
env0_second_0:                 episode reward: -16.3500,                 loss: -0.2487
env1_first_0:                 episode reward: 14.9500,                 loss: nan
env1_second_0:                 episode reward: -14.9500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1932.05,                last time consumption/overall running time: 617.1747s / 76371.7310 s
env0_first_0:                 episode reward: 16.0500,                 loss: -0.2558
env0_second_0:                 episode reward: -16.0500,                 loss: -0.2544
env1_first_0:                 episode reward: 13.7500,                 loss: nan
env1_second_0:                 episode reward: -13.7500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1945.6,                last time consumption/overall running time: 618.2301s / 76989.9610 s
env0_first_0:                 episode reward: 15.8000,                 loss: -0.2582
env0_second_0:                 episode reward: -15.8000,                 loss: -0.2559
env1_first_0:                 episode reward: 13.7500,                 loss: nan
env1_second_0:                 episode reward: -13.7500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 2004.8,                last time consumption/overall running time: 641.8974s / 77631.8584 s
env0_first_0:                 episode reward: 15.0500,                 loss: -0.2504
env0_second_0:                 episode reward: -15.0500,                 loss: -0.2506
env1_first_0:                 episode reward: 13.9500,                 loss: nan
env1_second_0:                 episode reward: -13.9500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1888.7,                last time consumption/overall running time: 610.0840s / 78241.9424 s
env0_first_0:                 episode reward: 14.6500,                 loss: -0.2656
env0_second_0:                 episode reward: -14.6500,                 loss: -0.2639
env1_first_0:                 episode reward: 15.1000,                 loss: nan
env1_second_0:                 episode reward: -15.1000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1971.7,                last time consumption/overall running time: 633.0362s / 78874.9786 s
env0_first_0:                 episode reward: 14.4500,                 loss: -0.2468
env0_second_0:                 episode reward: -14.4500,                 loss: -0.2448
env1_first_0:                 episode reward: 13.3000,                 loss: nan
env1_second_0:                 episode reward: -13.3000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1894.45,                last time consumption/overall running time: 611.9670s / 79486.9456 s
env0_first_0:                 episode reward: 12.6500,                 loss: -0.2514
env0_second_0:                 episode reward: -12.6500,                 loss: -0.2515
env1_first_0:                 episode reward: 13.1500,                 loss: nan
env1_second_0:                 episode reward: -13.1500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1943.45,                last time consumption/overall running time: 625.9300s / 80112.8757 s
env0_first_0:                 episode reward: 13.4500,                 loss: -0.2640