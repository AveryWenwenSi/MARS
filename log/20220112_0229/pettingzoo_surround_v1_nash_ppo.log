pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
random seed: [44, 57]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220112_0229/pettingzoo_surround_v1_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220112_0229/pettingzoo_surround_v1_nash_ppo.
Episode: 1/10000 (0.0100%),                 avg. length: 1255.0,                last time consumption/overall running time: 8.6033s / 8.6033 s
env0_first_0:                 episode reward: -5.0000,                 loss: -0.0322
env0_second_0:                 episode reward: 5.0000,                 loss: -0.0438
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1387.3,                last time consumption/overall running time: 196.9343s / 205.5377 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.0410
env0_second_0:                 episode reward: -2.6500,                 loss: -0.0360
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1563.4,                last time consumption/overall running time: 220.7714s / 426.3091 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.0409
env0_second_0:                 episode reward: -2.0000,                 loss: -0.0392
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1505.1,                last time consumption/overall running time: 211.9856s / 638.2947 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.0412
env0_second_0:                 episode reward: -4.8500,                 loss: -0.0414
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1855.05,                last time consumption/overall running time: 258.9883s / 897.2830 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.0703
env0_second_0:                 episode reward: -4.4500,                 loss: -0.0698
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 2159.0,                last time consumption/overall running time: 300.4141s / 1197.6971 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.0712
env0_second_0:                 episode reward: 1.3500,                 loss: -0.0690
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 2339.45,                last time consumption/overall running time: 322.2306s / 1519.9277 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.0883
env0_second_0:                 episode reward: 1.0500,                 loss: -0.0863
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 2333.1,                last time consumption/overall running time: 321.9575s / 1841.8852 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.1011
env0_second_0:                 episode reward: 2.1500,                 loss: -0.0994
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 2295.2,                last time consumption/overall running time: 317.6884s / 2159.5736 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.0958
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0939
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 2415.45,                last time consumption/overall running time: 332.5584s / 2492.1319 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.1039
env0_second_0:                 episode reward: 0.6500,                 loss: -0.0995
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 2530.3,                last time consumption/overall running time: 348.0184s / 2840.1503 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.1048
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0988
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 2574.4,                last time consumption/overall running time: 352.1389s / 3192.2891 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1047
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1014
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 2438.7,                last time consumption/overall running time: 336.7981s / 3529.0872 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.1076
env0_second_0:                 episode reward: 2.2000,                 loss: -0.1047
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 2514.05,                last time consumption/overall running time: 346.7076s / 3875.7949 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.1043
env0_second_0:                 episode reward: 2.9000,                 loss: -0.1035
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 2722.95,                last time consumption/overall running time: 372.4308s / 4248.2256 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0884
env0_second_0:                 episode reward: -0.6000,                 loss: -0.0833
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 2544.2,                last time consumption/overall running time: 348.0271s / 4596.2527 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.1016
env0_second_0:                 episode reward: 1.2500,                 loss: -0.0924
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 2716.55,                last time consumption/overall running time: 366.3210s / 4962.5737 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0946
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0876
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 2709.8,                last time consumption/overall running time: 365.9712s / 5328.5449 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1057
env0_second_0:                 episode reward: -0.7000,                 loss: -0.0987
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 2678.85,                last time consumption/overall running time: 364.4052s / 5692.9501 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.1044
env0_second_0:                 episode reward: 1.1000,                 loss: -0.1033
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 2514.55,                last time consumption/overall running time: 341.1841s / 6034.1342 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.1118
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1069
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 2589.55,                last time consumption/overall running time: 349.5052s / 6383.6394 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1064
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1036
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 2594.2,                last time consumption/overall running time: 354.2928s / 6737.9321 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.1066
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0995
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 2741.05,                last time consumption/overall running time: 373.7552s / 7111.6873 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1054
env0_second_0:                 episode reward: -1.3000,                 loss: -0.1025
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 2737.4,                last time consumption/overall running time: 372.7497s / 7484.4370 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.1016
env0_second_0:                 episode reward: 0.9000,                 loss: -0.0966
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 2736.65,                last time consumption/overall running time: 373.2525s / 7857.6895 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.1161
env0_second_0:                 episode reward: 2.5500,                 loss: -0.1037
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 2923.95,                last time consumption/overall running time: 397.7093s / 8255.3988 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.1129
env0_second_0:                 episode reward: 0.9500,                 loss: -0.1059
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 2714.0,                last time consumption/overall running time: 370.3029s / 8625.7017 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.1114
env0_second_0:                 episode reward: 1.0000,                 loss: -0.1055
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 2868.35,                last time consumption/overall running time: 393.7010s / 9019.4027 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.1123
env0_second_0:                 episode reward: 1.8000,                 loss: -0.1065
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 2861.45,                last time consumption/overall running time: 389.0107s / 9408.4133 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1113
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0974
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 2820.05,                last time consumption/overall running time: 382.3330s / 9790.7463 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.1216
env0_second_0:                 episode reward: 0.7500,                 loss: -0.1125
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 2606.85,                last time consumption/overall running time: 357.7732s / 10148.5196 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.1172
env0_second_0:                 episode reward: -2.8000,                 loss: -0.1063
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 2609.7,                last time consumption/overall running time: 355.3163s / 10503.8358 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1174
env0_second_0:                 episode reward: -3.1500,                 loss: -0.1069
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 2691.4,                last time consumption/overall running time: 366.7138s / 10870.5497 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1092
env0_second_0:                 episode reward: -2.2000,                 loss: -0.0998
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 2780.15,                last time consumption/overall running time: 373.4095s / 11243.9592 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1217
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1131
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 2918.1,                last time consumption/overall running time: 392.7762s / 11636.7354 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1185
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1138
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 2761.05,                last time consumption/overall running time: 370.7851s / 12007.5206 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.1183
env0_second_0:                 episode reward: 1.5500,                 loss: -0.1042
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 2912.25,                last time consumption/overall running time: 390.5615s / 12398.0820 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1167
env0_second_0:                 episode reward: 0.4500,                 loss: -0.1077
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 2661.1,                last time consumption/overall running time: 359.4702s / 12757.5522 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.1209
env0_second_0:                 episode reward: 3.8500,                 loss: -0.1041
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2759.5,                last time consumption/overall running time: 370.9509s / 13128.5031 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.1179
env0_second_0:                 episode reward: 1.5500,                 loss: -0.1130
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 2931.3,                last time consumption/overall running time: 391.0758s / 13519.5789 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.1261
env0_second_0:                 episode reward: 2.1000,                 loss: -0.1105
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 2908.65,                last time consumption/overall running time: 393.7955s / 13913.3743 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1212
env0_second_0:                 episode reward: -0.7500,                 loss: -0.1067
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 2949.2,                last time consumption/overall running time: 395.7661s / 14309.1404 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.1209
env0_second_0:                 episode reward: 1.5000,                 loss: -0.1041
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 2940.95,                last time consumption/overall running time: 394.2505s / 14703.3908 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1221
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1094
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 2882.6,                last time consumption/overall running time: 386.4367s / 15089.8275 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.1222
env0_second_0:                 episode reward: 1.6500,                 loss: -0.1102
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 2917.2,                last time consumption/overall running time: 392.2027s / 15482.0302 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.1275
env0_second_0:                 episode reward: 1.4500,                 loss: -0.1147
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2851.55,                last time consumption/overall running time: 386.2326s / 15868.2628 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.1317
env0_second_0:                 episode reward: 1.9000,                 loss: -0.1159
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 2707.7,                last time consumption/overall running time: 366.5850s / 16234.8478 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.1305
env0_second_0:                 episode reward: 1.7000,                 loss: -0.1096
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 2905.05,                last time consumption/overall running time: 390.1394s / 16624.9872 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.1290
env0_second_0:                 episode reward: 2.3500,                 loss: -0.1112
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2725.65,                last time consumption/overall running time: 365.7645s / 16990.7517 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.1227
env0_second_0:                 episode reward: 2.4500,                 loss: -0.1070
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 3124.9,                last time consumption/overall running time: 419.9690s / 17410.7207 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.1205
env0_second_0:                 episode reward: 0.6000,                 loss: -0.1072
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 2967.05,                last time consumption/overall running time: 395.8239s / 17806.5446 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.1330
env0_second_0:                 episode reward: 1.0000,                 loss: -0.1129
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 2924.6,                last time consumption/overall running time: 391.9907s / 18198.5353 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1336
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1188
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 2852.2,                last time consumption/overall running time: 386.4542s / 18584.9894 s
env0_first_0:                 episode reward: -3.0000,                 loss: -0.1349
env0_second_0:                 episode reward: 3.0000,                 loss: -0.1173
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 2944.9,                last time consumption/overall running time: 395.6553s / 18980.6447 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.1332
env0_second_0:                 episode reward: 1.1500,                 loss: -0.1148
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 2973.75,                last time consumption/overall running time: 405.1329s / 19385.7776 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.1393
env0_second_0:                 episode reward: 2.0500,                 loss: -0.1224
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 2870.1,                last time consumption/overall running time: 388.1357s / 19773.9133 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1320
env0_second_0:                 episode reward: 1.3000,                 loss: -0.1113
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 2938.1,                last time consumption/overall running time: 391.3112s / 20165.2245 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1391
env0_second_0:                 episode reward: 1.3000,                 loss: -0.1163
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 2987.0,                last time consumption/overall running time: 400.6605s / 20565.8850 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1295
env0_second_0:                 episode reward: 0.5500,                 loss: -0.1124
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 2857.1,                last time consumption/overall running time: 381.9728s / 20947.8578 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.1223
env0_second_0:                 episode reward: 2.4000,                 loss: -0.0960
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 2887.2,                last time consumption/overall running time: 389.0809s / 21336.9387 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.1270
env0_second_0:                 episode reward: 1.7000,                 loss: -0.1055
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 2962.55,                last time consumption/overall running time: 395.6511s / 21732.5898 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1369
env0_second_0:                 episode reward: 1.8500,                 loss: -0.1167
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 2849.1,                last time consumption/overall running time: 381.5250s / 22114.1148 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.1356
env0_second_0:                 episode reward: 3.6500,                 loss: -0.1113
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 2921.95,                last time consumption/overall running time: 390.5204s / 22504.6352 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.1385
env0_second_0:                 episode reward: 3.1000,                 loss: -0.1148
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 3105.85,                last time consumption/overall running time: 414.3510s / 22918.9862 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.1317
env0_second_0:                 episode reward: 1.5000,                 loss: -0.0982
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 3004.3,                last time consumption/overall running time: 402.6823s / 23321.6685 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.1292
env0_second_0:                 episode reward: 1.5000,                 loss: -0.1001
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 3180.6,                last time consumption/overall running time: 426.1871s / 23747.8557 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1229
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0954
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 3063.55,                last time consumption/overall running time: 410.3723s / 24158.2280 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1288
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1054
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 3107.15,                last time consumption/overall running time: 422.4385s / 24580.6665 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1253
env0_second_0:                 episode reward: -1.9500,                 loss: -0.0972
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 3010.45,                last time consumption/overall running time: 412.9434s / 24993.6099 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.1200
env0_second_0:                 episode reward: -3.6500,                 loss: -0.0835
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 3016.5,                last time consumption/overall running time: 412.2856s / 25405.8955 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.1236
env0_second_0:                 episode reward: -4.5500,                 loss: -0.0951
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 2925.1,                last time consumption/overall running time: 400.4545s / 25806.3500 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.1403
env0_second_0:                 episode reward: -3.6000,                 loss: -0.1078
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 2892.55,                last time consumption/overall running time: 397.4277s / 26203.7777 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.1413
env0_second_0:                 episode reward: -3.1000,                 loss: -0.1093
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 2910.65,                last time consumption/overall running time: 394.6721s / 26598.4498 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.1356
env0_second_0:                 episode reward: -3.0500,                 loss: -0.1021
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 2934.4,                last time consumption/overall running time: 394.3617s / 26992.8115 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.1363
env0_second_0:                 episode reward: -3.8500,                 loss: -0.1120
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 3082.5,                last time consumption/overall running time: 413.8033s / 27406.6148 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1338
env0_second_0:                 episode reward: -2.8500,                 loss: -0.1057
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 3015.95,                last time consumption/overall running time: 407.0295s / 27813.6443 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.1450
env0_second_0:                 episode reward: -4.1000,                 loss: -0.1122
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 2930.45,                last time consumption/overall running time: 399.5664s / 28213.2107 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.1448
env0_second_0:                 episode reward: -5.0500,                 loss: -0.1129
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 2996.1,                last time consumption/overall running time: 401.3205s / 28614.5312 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.1345
env0_second_0:                 episode reward: -4.3000,                 loss: -0.1076
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 2908.45,                last time consumption/overall running time: 392.4953s / 29007.0265 s
env0_first_0:                 episode reward: 5.2000,                 loss: -0.1339
env0_second_0:                 episode reward: -5.2000,                 loss: -0.0983
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 3153.6,                last time consumption/overall running time: 423.0931s / 29430.1196 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.1316
env0_second_0:                 episode reward: -5.1000,                 loss: -0.1030
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 3329.05,                last time consumption/overall running time: 444.2138s / 29874.3334 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.1320
env0_second_0:                 episode reward: -3.7000,                 loss: -0.1005
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 3173.7,                last time consumption/overall running time: 422.7717s / 30297.1051 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1340
env0_second_0:                 episode reward: -4.8000,                 loss: -0.1021
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 3079.15,                last time consumption/overall running time: 411.9541s / 30709.0592 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.1378
env0_second_0:                 episode reward: -4.8500,                 loss: -0.1014
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 2920.85,                last time consumption/overall running time: 392.3397s / 31101.3989 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.1370
env0_second_0:                 episode reward: -5.1500,                 loss: -0.0968
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 3001.1,                last time consumption/overall running time: 402.6952s / 31504.0941 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.1364
env0_second_0:                 episode reward: -4.1500,                 loss: -0.0949
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 3246.95,                last time consumption/overall running time: 433.5535s / 31937.6476 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1393
env0_second_0:                 episode reward: -3.2000,                 loss: -0.0943
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 2870.9,                last time consumption/overall running time: 390.1674s / 32327.8150 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.1402
env0_second_0:                 episode reward: -4.9500,                 loss: -0.1024
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 3040.15,                last time consumption/overall running time: 409.7182s / 32737.5332 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.1266
env0_second_0:                 episode reward: -4.4000,                 loss: -0.0851
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 3132.25,                last time consumption/overall running time: 420.4404s / 33157.9736 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1269
env0_second_0:                 episode reward: -3.4000,                 loss: -0.0885
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 3156.45,                last time consumption/overall running time: 421.7677s / 33579.7413 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.1261
env0_second_0:                 episode reward: -5.2500,                 loss: -0.0773
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 3010.35,                last time consumption/overall running time: 400.8410s / 33980.5823 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1219
env0_second_0:                 episode reward: -4.6000,                 loss: -0.0816
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 3291.7,                last time consumption/overall running time: 438.6360s / 34419.2183 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.1193
env0_second_0:                 episode reward: -3.5500,                 loss: -0.0864
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 2984.95,                last time consumption/overall running time: 398.3827s / 34817.6010 s
env0_first_0:                 episode reward: 5.5500,                 loss: -0.1247
env0_second_0:                 episode reward: -5.5500,                 loss: -0.0915
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 3198.7,                last time consumption/overall running time: 424.1300s / 35241.7310 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.1254
env0_second_0:                 episode reward: -3.6000,                 loss: -0.0842
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 3181.95,                last time consumption/overall running time: 421.6617s / 35663.3928 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1272
env0_second_0:                 episode reward: -2.7500,                 loss: -0.0849
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 3342.2,                last time consumption/overall running time: 446.1259s / 36109.5187 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1208
env0_second_0:                 episode reward: -3.4000,                 loss: -0.0866
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 3685.25,                last time consumption/overall running time: 491.8006s / 36601.3193 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1213
env0_second_0:                 episode reward: -1.3000,                 loss: -0.0901
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 3376.2,                last time consumption/overall running time: 450.8768s / 37052.1961 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.1254
env0_second_0:                 episode reward: -3.7000,                 loss: -0.0917
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 3709.35,                last time consumption/overall running time: 487.3749s / 37539.5710 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.1315
env0_second_0:                 episode reward: -2.0500,                 loss: -0.0927
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 3556.15,                last time consumption/overall running time: 479.2693s / 38018.8403 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.1340
env0_second_0:                 episode reward: -0.8500,                 loss: -0.0959
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 3556.7,                last time consumption/overall running time: 480.8499s / 38499.6902 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.1416
env0_second_0:                 episode reward: -3.3500,                 loss: -0.1050
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 3740.25,                last time consumption/overall running time: 500.5047s / 39000.1949 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1393
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0997
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 4098.45,                last time consumption/overall running time: 543.7965s / 39543.9913 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.1326
env0_second_0:                 episode reward: 2.3500,                 loss: -0.0937
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 3625.65,                last time consumption/overall running time: 485.3961s / 40029.3875 s
env0_first_0:                 episode reward: -4.0500,                 loss: -0.1336
env0_second_0:                 episode reward: 4.0500,                 loss: -0.0920
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 3427.3,                last time consumption/overall running time: 459.8900s / 40489.2775 s
env0_first_0:                 episode reward: -4.7500,                 loss: -0.1428
env0_second_0:                 episode reward: 4.7500,                 loss: -0.0908
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 3573.15,                last time consumption/overall running time: 478.5422s / 40967.8197 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.1397
env0_second_0:                 episode reward: 2.2000,                 loss: -0.0863
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 3430.35,                last time consumption/overall running time: 458.9008s / 41426.7204 s
env0_first_0:                 episode reward: -3.5000,                 loss: -0.1351
env0_second_0:                 episode reward: 3.5000,                 loss: -0.0837
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 3801.05,                last time consumption/overall running time: 513.8219s / 41940.5423 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.1316
env0_second_0:                 episode reward: 1.1500,                 loss: -0.0846
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 3778.9,                last time consumption/overall running time: 510.1395s / 42450.6818 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.1333
env0_second_0:                 episode reward: 4.1000,                 loss: -0.0857
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 3763.75,                last time consumption/overall running time: 511.5875s / 42962.2693 s
env0_first_0:                 episode reward: -3.5500,                 loss: -0.1392
env0_second_0:                 episode reward: 3.5500,                 loss: -0.0888
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 3754.65,                last time consumption/overall running time: 510.8270s / 43473.0963 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.1367
env0_second_0:                 episode reward: 3.8500,                 loss: -0.0948
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 3515.9,                last time consumption/overall running time: 477.1950s / 43950.2914 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.1458
env0_second_0:                 episode reward: 4.7000,                 loss: -0.0921
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 3323.9,                last time consumption/overall running time: 446.2622s / 44396.5535 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.1480
env0_second_0:                 episode reward: 3.8000,                 loss: -0.1075
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 3283.55,                last time consumption/overall running time: 436.5373s / 44833.0908 s
env0_first_0:                 episode reward: -5.3500,                 loss: -0.1461
env0_second_0:                 episode reward: 5.3500,                 loss: -0.0905
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 3801.0,                last time consumption/overall running time: 508.6114s / 45341.7022 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.1335
env0_second_0:                 episode reward: 3.3000,                 loss: -0.0770
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 3654.95,                last time consumption/overall running time: 499.2008s / 45840.9030 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.1284
env0_second_0:                 episode reward: 2.1000,                 loss: -0.0682
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 3836.45,                last time consumption/overall running time: 513.3804s / 46354.2834 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.1361
env0_second_0:                 episode reward: 2.4000,                 loss: -0.0682
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 3718.85,                last time consumption/overall running time: 496.7222s / 46851.0056 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.1468
env0_second_0:                 episode reward: 3.8000,                 loss: -0.0973
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 3585.3,                last time consumption/overall running time: 480.4461s / 47331.4516 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.1521
env0_second_0:                 episode reward: 5.1000,                 loss: -0.1039
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 3316.4,                last time consumption/overall running time: 449.1444s / 47780.5960 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.1556
env0_second_0:                 episode reward: 5.9500,                 loss: -0.1054
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 3489.9,                last time consumption/overall running time: 471.6674s / 48252.2634 s
env0_first_0:                 episode reward: -4.5500,                 loss: -0.1524
env0_second_0:                 episode reward: 4.5500,                 loss: -0.0990
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 3548.8,                last time consumption/overall running time: 477.4291s / 48729.6925 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.1467
env0_second_0:                 episode reward: 2.7000,                 loss: -0.0935
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 3918.65,                last time consumption/overall running time: 527.4295s / 49257.1220 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.1509
env0_second_0:                 episode reward: 2.4500,                 loss: -0.1055
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 3611.15,                last time consumption/overall running time: 485.6686s / 49742.7906 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.1462
env0_second_0:                 episode reward: 3.1000,                 loss: -0.1039
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 3796.3,                last time consumption/overall running time: 508.1517s / 50250.9423 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.1440
env0_second_0:                 episode reward: 2.9000,                 loss: -0.0890
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 3526.65,                last time consumption/overall running time: 473.0692s / 50724.0115 s
env0_first_0:                 episode reward: -4.7500,                 loss: -0.1434
env0_second_0:                 episode reward: 4.7500,                 loss: -0.0904
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 3820.25,                last time consumption/overall running time: 513.4604s / 51237.4719 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.1522
env0_second_0:                 episode reward: 4.0000,                 loss: -0.1100
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 3627.6,                last time consumption/overall running time: 484.0381s / 51721.5099 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.1534
env0_second_0:                 episode reward: 1.9000,                 loss: -0.1068
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 3774.5,                last time consumption/overall running time: 504.0682s / 52225.5782 s
env0_first_0:                 episode reward: -3.4500,                 loss: -0.1503
env0_second_0:                 episode reward: 3.4500,                 loss: -0.1047
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 4004.55,                last time consumption/overall running time: 538.1373s / 52763.7155 s
env0_first_0:                 episode reward: -1.9500,                 loss: -0.1403
env0_second_0:                 episode reward: 1.9500,                 loss: -0.0940
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 3883.9,                last time consumption/overall running time: 519.6057s / 53283.3212 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.1303
env0_second_0:                 episode reward: 2.2500,                 loss: -0.0823
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 3905.1,                last time consumption/overall running time: 524.4480s / 53807.7692 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.1409
env0_second_0:                 episode reward: 3.6500,                 loss: -0.0982
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 3668.8,                last time consumption/overall running time: 492.3624s / 54300.1316 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.1301
env0_second_0:                 episode reward: 3.9500,                 loss: -0.0862
env1_first_0:                 episode reward: -4.6500,                 loss: nan