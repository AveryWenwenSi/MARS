pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
random seed: [11, 67]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220112_0229/pettingzoo_pong_v2_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220112_0229/pettingzoo_pong_v2_nash_ppo.
Episode: 1/10000 (0.0100%),                 avg. length: 1331.0,                last time consumption/overall running time: 18.6005s / 18.6005 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.2931
env0_second_0:                 episode reward: -1.0000,                 loss: 0.2756
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1184.1,                last time consumption/overall running time: 171.0943s / 189.6947 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2708
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2757
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1288.6,                last time consumption/overall running time: 185.1716s / 374.8663 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.4219
env0_second_0:                 episode reward: -0.8000,                 loss: 0.4170
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1207.5,                last time consumption/overall running time: 175.6900s / 550.5563 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.5259
env0_second_0:                 episode reward: -5.4500,                 loss: 0.5043
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1111.35,                last time consumption/overall running time: 160.6449s / 711.2011 s
env0_first_0:                 episode reward: 7.9000,                 loss: 0.5704
env0_second_0:                 episode reward: -7.9000,                 loss: 0.5448
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1089.9,                last time consumption/overall running time: 158.4206s / 869.6218 s
env0_first_0:                 episode reward: 2.7500,                 loss: 0.6514
env0_second_0:                 episode reward: -2.7500,                 loss: 0.6416
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1115.85,                last time consumption/overall running time: 162.5775s / 1032.1993 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.7093
env0_second_0:                 episode reward: -0.9000,                 loss: 0.7009
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1115.0,                last time consumption/overall running time: 162.8467s / 1195.0460 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.6854
env0_second_0:                 episode reward: 1.2000,                 loss: 0.6692
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1034.25,                last time consumption/overall running time: 150.3633s / 1345.4092 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.6505
env0_second_0:                 episode reward: -1.6500,                 loss: 0.6365
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1112.55,                last time consumption/overall running time: 162.2067s / 1507.6160 s
env0_first_0:                 episode reward: 2.8000,                 loss: 0.6691
env0_second_0:                 episode reward: -2.8000,                 loss: 0.6613
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1193.3,                last time consumption/overall running time: 172.2752s / 1679.8912 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.6954
env0_second_0:                 episode reward: 1.3000,                 loss: 0.6825
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1168.75,                last time consumption/overall running time: 167.3184s / 1847.2096 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.6877
env0_second_0:                 episode reward: 1.7000,                 loss: 0.6802
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1128.7,                last time consumption/overall running time: 160.7543s / 2007.9638 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.6714
env0_second_0:                 episode reward: 1.1000,                 loss: 0.6680
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1113.3,                last time consumption/overall running time: 159.1272s / 2167.0910 s
env0_first_0:                 episode reward: 2.1500,                 loss: 0.6179
env0_second_0:                 episode reward: -2.1500,                 loss: 0.6195
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1136.9,                last time consumption/overall running time: 162.9816s / 2330.0726 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.7013
env0_second_0:                 episode reward: -1.1000,                 loss: 0.7027
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1160.45,                last time consumption/overall running time: 166.0314s / 2496.1041 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.6707
env0_second_0:                 episode reward: -1.8000,                 loss: 0.6648
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1173.75,                last time consumption/overall running time: 167.7057s / 2663.8097 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.6358
env0_second_0:                 episode reward: 2.3500,                 loss: 0.6343
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1250.7,                last time consumption/overall running time: 178.6137s / 2842.4235 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.6633
env0_second_0:                 episode reward: -1.2500,                 loss: 0.6430
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1117.0,                last time consumption/overall running time: 156.7094s / 2999.1329 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.6231
env0_second_0:                 episode reward: -1.4500,                 loss: 0.6162
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1217.1,                last time consumption/overall running time: 171.1298s / 3170.2627 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.6660
env0_second_0:                 episode reward: -2.4500,                 loss: 0.6370
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1128.65,                last time consumption/overall running time: 160.6518s / 3330.9145 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.6387
env0_second_0:                 episode reward: -5.9000,                 loss: 0.6243
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1226.75,                last time consumption/overall running time: 173.4760s / 3504.3904 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.6375
env0_second_0:                 episode reward: -1.1000,                 loss: 0.6186
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1167.1,                last time consumption/overall running time: 167.5591s / 3671.9495 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.6017
env0_second_0:                 episode reward: -0.5000,                 loss: 0.5928
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1064.7,                last time consumption/overall running time: 150.5399s / 3822.4895 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.5699
env0_second_0:                 episode reward: -1.0000,                 loss: 0.5903
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1173.55,                last time consumption/overall running time: 167.7528s / 3990.2423 s
env0_first_0:                 episode reward: -6.5500,                 loss: 0.6179
env0_second_0:                 episode reward: 6.5500,                 loss: 0.6129
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1201.8,                last time consumption/overall running time: 171.4399s / 4161.6821 s
env0_first_0:                 episode reward: 3.9000,                 loss: 0.6445
env0_second_0:                 episode reward: -3.9000,                 loss: 0.6525
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1200.8,                last time consumption/overall running time: 170.5837s / 4332.2658 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.5720
env0_second_0:                 episode reward: -0.2500,                 loss: 0.5721
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1232.85,                last time consumption/overall running time: 173.6717s / 4505.9375 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.6139
env0_second_0:                 episode reward: -3.9500,                 loss: 0.6113
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1150.05,                last time consumption/overall running time: 165.5909s / 4671.5284 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.5766
env0_second_0:                 episode reward: 0.8500,                 loss: 0.5710
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1158.5,                last time consumption/overall running time: 167.2450s / 4838.7735 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.5864
env0_second_0:                 episode reward: -0.6000,                 loss: 0.5875
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1075.25,                last time consumption/overall running time: 155.5947s / 4994.3682 s
env0_first_0:                 episode reward: 3.7500,                 loss: 0.5915
env0_second_0:                 episode reward: -3.7500,                 loss: 0.5828
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1121.8,                last time consumption/overall running time: 162.3745s / 5156.7427 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.5753
env0_second_0:                 episode reward: 1.7000,                 loss: 0.5663
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1202.95,                last time consumption/overall running time: 173.2347s / 5329.9774 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.6020
env0_second_0:                 episode reward: 3.2500,                 loss: 0.6114
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1235.9,                last time consumption/overall running time: 177.4319s / 5507.4093 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.6000
env0_second_0:                 episode reward: -0.9000,                 loss: 0.6193
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1205.35,                last time consumption/overall running time: 174.4510s / 5681.8603 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.6369
env0_second_0:                 episode reward: -4.1000,                 loss: 0.6262
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1222.95,                last time consumption/overall running time: 175.7852s / 5857.6456 s
env0_first_0:                 episode reward: 5.0500,                 loss: 0.6449
env0_second_0:                 episode reward: -5.0500,                 loss: 0.6397
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1138.15,                last time consumption/overall running time: 165.0048s / 6022.6504 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.6058
env0_second_0:                 episode reward: 1.3500,                 loss: 0.5999
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1299.2,                last time consumption/overall running time: 184.8240s / 6207.4744 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.5916
env0_second_0:                 episode reward: 2.6500,                 loss: 0.5913
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 1283.95,                last time consumption/overall running time: 181.5292s / 6389.0036 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.5789
env0_second_0:                 episode reward: 2.7500,                 loss: 0.5824
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1276.15,                last time consumption/overall running time: 181.8101s / 6570.8137 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.5739
env0_second_0:                 episode reward: 5.9000,                 loss: 0.5605
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1304.0,                last time consumption/overall running time: 185.5770s / 6756.3907 s
env0_first_0:                 episode reward: -7.8500,                 loss: 0.5820
env0_second_0:                 episode reward: 7.8500,                 loss: 0.5732
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1265.7,                last time consumption/overall running time: 178.8184s / 6935.2091 s
env0_first_0:                 episode reward: -9.1000,                 loss: 0.5447
env0_second_0:                 episode reward: 9.1000,                 loss: 0.5533
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1423.35,                last time consumption/overall running time: 200.5913s / 7135.8004 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.5688
env0_second_0:                 episode reward: 4.0500,                 loss: 0.5764
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1469.55,                last time consumption/overall running time: 204.6124s / 7340.4127 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.5273
env0_second_0:                 episode reward: 3.2000,                 loss: 0.5464
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1523.4,                last time consumption/overall running time: 209.7687s / 7550.1814 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.5060
env0_second_0:                 episode reward: 5.1500,                 loss: 0.5052
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1680.35,                last time consumption/overall running time: 233.9144s / 7784.0958 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.4650
env0_second_0:                 episode reward: 3.9000,                 loss: 0.4535
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1663.55,                last time consumption/overall running time: 231.7438s / 8015.8396 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.4625
env0_second_0:                 episode reward: 1.4500,                 loss: 0.4566
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1739.15,                last time consumption/overall running time: 245.2331s / 8261.0727 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.4171
env0_second_0:                 episode reward: 2.4000,                 loss: 0.4037
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1725.1,                last time consumption/overall running time: 238.5973s / 8499.6700 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.3893
env0_second_0:                 episode reward: 2.0500,                 loss: 0.3830
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1724.4,                last time consumption/overall running time: 240.6187s / 8740.2887 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.3846
env0_second_0:                 episode reward: 1.9500,                 loss: 0.3652
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1854.95,                last time consumption/overall running time: 259.7180s / 9000.0068 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.3504
env0_second_0:                 episode reward: 1.5500,                 loss: 0.3306
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1895.0,                last time consumption/overall running time: 265.2588s / 9265.2656 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.3379
env0_second_0:                 episode reward: 0.4500,                 loss: 0.3220
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1905.95,                last time consumption/overall running time: 266.3780s / 9531.6436 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.3219
env0_second_0:                 episode reward: 1.7500,                 loss: 0.3107
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1864.3,                last time consumption/overall running time: 260.4661s / 9792.1097 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.3288
env0_second_0:                 episode reward: 3.8000,                 loss: 0.3060
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1893.05,                last time consumption/overall running time: 263.7057s / 10055.8154 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.3116
env0_second_0:                 episode reward: 0.0000,                 loss: 0.2903
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1861.55,                last time consumption/overall running time: 258.4155s / 10314.2309 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.2934
env0_second_0:                 episode reward: -0.6000,                 loss: 0.2750
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1923.1,                last time consumption/overall running time: 270.7607s / 10584.9916 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.2825
env0_second_0:                 episode reward: 2.8000,                 loss: 0.2598
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1986.4,                last time consumption/overall running time: 273.5891s / 10858.5807 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.2831
env0_second_0:                 episode reward: 1.3500,                 loss: 0.2568
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1900.9,                last time consumption/overall running time: 266.7891s / 11125.3698 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.2629
env0_second_0:                 episode reward: 2.2000,                 loss: 0.2422
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 2008.65,                last time consumption/overall running time: 282.7893s / 11408.1591 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.2620
env0_second_0:                 episode reward: -1.3000,                 loss: 0.2370
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 2026.95,                last time consumption/overall running time: 288.6581s / 11696.8173 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.2522
env0_second_0:                 episode reward: -0.9500,                 loss: 0.2374
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1991.1,                last time consumption/overall running time: 287.1661s / 11983.9834 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2413
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2191
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1982.05,                last time consumption/overall running time: 285.9388s / 12269.9221 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.2607
env0_second_0:                 episode reward: 0.2500,                 loss: 0.2489
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 2067.6,                last time consumption/overall running time: 297.3154s / 12567.2375 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.2207
env0_second_0:                 episode reward: -0.9500,                 loss: 0.2007
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1965.85,                last time consumption/overall running time: 281.7291s / 12848.9666 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.2378
env0_second_0:                 episode reward: -0.8000,                 loss: 0.2247
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1976.8,                last time consumption/overall running time: 282.6308s / 13131.5974 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.2321
env0_second_0:                 episode reward: 0.2500,                 loss: 0.2201
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1933.75,                last time consumption/overall running time: 278.6635s / 13410.2609 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.2423
env0_second_0:                 episode reward: -1.1000,                 loss: 0.2313
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1922.2,                last time consumption/overall running time: 285.3921s / 13695.6530 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.2344
env0_second_0:                 episode reward: 3.5500,                 loss: 0.2259
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1948.3,                last time consumption/overall running time: 280.0999s / 13975.7529 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.2095
env0_second_0:                 episode reward: 0.7500,                 loss: 0.2013
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1895.9,                last time consumption/overall running time: 272.9104s / 14248.6633 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.2182
env0_second_0:                 episode reward: 1.9500,                 loss: 0.2102
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1991.35,                last time consumption/overall running time: 285.8371s / 14534.5003 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.2178
env0_second_0:                 episode reward: -1.1500,                 loss: 0.2090
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1950.65,                last time consumption/overall running time: 285.9739s / 14820.4742 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2277
env0_second_0:                 episode reward: -0.0500,                 loss: 0.2172
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 1908.6,                last time consumption/overall running time: 272.6967s / 15093.1709 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2153
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2117
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1959.95,                last time consumption/overall running time: 283.5783s / 15376.7493 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1939
env0_second_0:                 episode reward: 1.1000,                 loss: 0.1835
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1978.05,                last time consumption/overall running time: 284.0605s / 15660.8097 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.1914
env0_second_0:                 episode reward: 1.0000,                 loss: 0.1833
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 2009.5,                last time consumption/overall running time: 289.9529s / 15950.7627 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.1909
env0_second_0:                 episode reward: -0.2500,                 loss: 0.1901
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 2028.7,                last time consumption/overall running time: 291.4011s / 16242.1638 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.1898
env0_second_0:                 episode reward: 0.3500,                 loss: 0.1814
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 2007.55,                last time consumption/overall running time: 288.8381s / 16531.0019 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.2042
env0_second_0:                 episode reward: 1.8000,                 loss: 0.2027
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1952.5,                last time consumption/overall running time: 282.3626s / 16813.3645 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1981
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1811
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1960.45,                last time consumption/overall running time: 281.8756s / 17095.2401 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1925
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1861
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1962.4,                last time consumption/overall running time: 284.4474s / 17379.6875 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.1942
env0_second_0:                 episode reward: 2.5000,                 loss: 0.1803
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1923.65,                last time consumption/overall running time: 275.9095s / 17655.5971 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.2016
env0_second_0:                 episode reward: 1.5500,                 loss: 0.1917
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 2011.95,                last time consumption/overall running time: 289.7451s / 17945.3422 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.2000
env0_second_0:                 episode reward: 0.9000,                 loss: 0.1891
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 2001.75,                last time consumption/overall running time: 286.1318s / 18231.4740 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.1952
env0_second_0:                 episode reward: 1.5000,                 loss: 0.1948
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 2056.25,                last time consumption/overall running time: 296.3387s / 18527.8127 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.1885
env0_second_0:                 episode reward: 1.5500,                 loss: 0.1923
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1985.35,                last time consumption/overall running time: 286.0687s / 18813.8814 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.1811
env0_second_0:                 episode reward: 1.0500,                 loss: 0.1734
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 2031.25,                last time consumption/overall running time: 294.5282s / 19108.4096 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.1881
env0_second_0:                 episode reward: -0.8500,                 loss: 0.1850
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 2008.35,                last time consumption/overall running time: 289.2295s / 19397.6391 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.1705
env0_second_0:                 episode reward: 0.9000,                 loss: 0.1688
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 2027.05,                last time consumption/overall running time: 291.0964s / 19688.7355 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.1744
env0_second_0:                 episode reward: 2.1500,                 loss: 0.1852
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1952.85,                last time consumption/overall running time: 280.2397s / 19968.9752 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.1770
env0_second_0:                 episode reward: 2.6000,                 loss: 0.1925
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1922.05,                last time consumption/overall running time: 279.1022s / 20248.0773 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.2213
env0_second_0:                 episode reward: 0.4000,                 loss: 0.1988
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 2033.8,                last time consumption/overall running time: 295.4169s / 20543.4943 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.1849
env0_second_0:                 episode reward: 0.2000,                 loss: 0.1887
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1985.45,                last time consumption/overall running time: 289.0957s / 20832.5900 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1756
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1856
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 2050.05,                last time consumption/overall running time: 296.0553s / 21128.6453 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.1780
env0_second_0:                 episode reward: -0.5500,                 loss: 0.1888
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1961.25,                last time consumption/overall running time: 285.1283s / 21413.7735 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.2049
env0_second_0:                 episode reward: -0.5500,                 loss: 0.2087
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1851.75,                last time consumption/overall running time: 268.6316s / 21682.4052 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1834
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1921
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1994.2,                last time consumption/overall running time: 285.8681s / 21968.2732 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.1829
env0_second_0:                 episode reward: 0.4500,                 loss: 0.1861
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 1929.45,                last time consumption/overall running time: 277.7871s / 22246.0604 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.1645
env0_second_0:                 episode reward: 1.7500,                 loss: 0.1666
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 2027.85,                last time consumption/overall running time: 291.8155s / 22537.8758 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1444
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1544
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 2062.55,                last time consumption/overall running time: 297.5416s / 22835.4175 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.1530
env0_second_0:                 episode reward: 3.6500,                 loss: 0.1558
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 2049.15,                last time consumption/overall running time: 292.9120s / 23128.3295 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.1618
env0_second_0:                 episode reward: 0.4000,                 loss: 0.1641
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1982.65,                last time consumption/overall running time: 287.5441s / 23415.8735 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.1814
env0_second_0:                 episode reward: -0.4500,                 loss: 0.1810
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1953.2,                last time consumption/overall running time: 284.7060s / 23700.5795 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.1935
env0_second_0:                 episode reward: 2.1500,                 loss: 0.1932
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 2025.95,                last time consumption/overall running time: 294.1237s / 23994.7032 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.1720
env0_second_0:                 episode reward: 1.2500,                 loss: 0.1744
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 2050.1,                last time consumption/overall running time: 293.3663s / 24288.0696 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.1525
env0_second_0:                 episode reward: -0.5500,                 loss: 0.1591
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 2066.55,                last time consumption/overall running time: 300.4791s / 24588.5486 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.1840
env0_second_0:                 episode reward: 2.4000,                 loss: 0.1755
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 1971.45,                last time consumption/overall running time: 285.2191s / 24873.7678 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.1656
env0_second_0:                 episode reward: 0.6500,                 loss: 0.1721
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 2099.6,                last time consumption/overall running time: 303.4652s / 25177.2330 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.1600
env0_second_0:                 episode reward: 0.0500,                 loss: 0.1662
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 2025.55,                last time consumption/overall running time: 291.3789s / 25468.6119 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.1776
env0_second_0:                 episode reward: 1.0000,                 loss: 0.1826
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 2041.35,                last time consumption/overall running time: 293.7624s / 25762.3743 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.1474
env0_second_0:                 episode reward: -0.9000,                 loss: 0.1542
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 2169.65,                last time consumption/overall running time: 313.1925s / 26075.5668 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.1514
env0_second_0:                 episode reward: 1.6500,                 loss: 0.1591
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 2053.05,                last time consumption/overall running time: 296.9365s / 26372.5033 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.1714
env0_second_0:                 episode reward: -0.7000,                 loss: 0.1728
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 2056.35,                last time consumption/overall running time: 298.7390s / 26671.2423 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.1667
env0_second_0:                 episode reward: 0.0500,                 loss: 0.1676
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 2098.2,                last time consumption/overall running time: 300.7384s / 26971.9807 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.1575
env0_second_0:                 episode reward: 1.4000,                 loss: 0.1626
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 2037.8,                last time consumption/overall running time: 294.1824s / 27266.1632 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.1685
env0_second_0:                 episode reward: 1.5500,                 loss: 0.1730
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 2075.05,                last time consumption/overall running time: 297.5716s / 27563.7348 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.1781
env0_second_0:                 episode reward: -1.0500,                 loss: 0.1753
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 1941.85,                last time consumption/overall running time: 281.2863s / 27845.0211 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.1821
env0_second_0:                 episode reward: 1.0000,                 loss: 0.1906
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 2021.2,                last time consumption/overall running time: 292.7339s / 28137.7550 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.1946
env0_second_0:                 episode reward: -0.6000,                 loss: 0.2045
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 2001.5,                last time consumption/overall running time: 288.7723s / 28426.5273 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.1810
env0_second_0:                 episode reward: -1.6500,                 loss: 0.1941
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 1936.95,                last time consumption/overall running time: 279.4274s / 28705.9547 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.1954
env0_second_0:                 episode reward: 1.9000,                 loss: 0.1942
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 2046.5,                last time consumption/overall running time: 292.6794s / 28998.6342 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.1980
env0_second_0:                 episode reward: 0.9500,                 loss: 0.1914
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 2055.2,                last time consumption/overall running time: 297.8136s / 29296.4478 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.1798
env0_second_0:                 episode reward: -0.2500,                 loss: 0.1924
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 1923.0,                last time consumption/overall running time: 280.0572s / 29576.5050 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.1958
env0_second_0:                 episode reward: 2.4000,                 loss: 0.2006
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1953.55,                last time consumption/overall running time: 283.9953s / 29860.5003 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.1895
env0_second_0:                 episode reward: -1.3500,                 loss: 0.1990
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 2042.95,                last time consumption/overall running time: 295.1982s / 30155.6985 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.1875
env0_second_0:                 episode reward: -0.8500,                 loss: 0.2099
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 2077.6,                last time consumption/overall running time: 297.1327s / 30452.8312 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.1684
env0_second_0:                 episode reward: -0.2000,                 loss: 0.1921
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 2051.3,                last time consumption/overall running time: 296.2898s / 30749.1210 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.1648
env0_second_0:                 episode reward: -0.8500,                 loss: 0.1681
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 2131.05,                last time consumption/overall running time: 307.3354s / 31056.4564 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.1611
env0_second_0:                 episode reward: 2.4500,                 loss: 0.1671
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 2016.5,                last time consumption/overall running time: 292.4081s / 31348.8645 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.1790
env0_second_0:                 episode reward: 0.2000,                 loss: 0.1979
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1906.5,                last time consumption/overall running time: 275.2666s / 31624.1311 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.2085
env0_second_0:                 episode reward: -0.5500,                 loss: 0.2120
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 1942.9,                last time consumption/overall running time: 283.2914s / 31907.4225 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.2057
env0_second_0:                 episode reward: 3.0500,                 loss: 0.2000
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 2035.7,                last time consumption/overall running time: 294.9208s / 32202.3433 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1835
env0_second_0:                 episode reward: 0.3000,                 loss: 0.1912
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 2097.35,                last time consumption/overall running time: 300.9613s / 32503.3045 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.1901
env0_second_0:                 episode reward: 1.0000,                 loss: 0.1833
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 2005.55,                last time consumption/overall running time: 292.5563s / 32795.8609 s
env0_first_0:                 episode reward: 1.2000,                 loss: 0.1839
env0_second_0:                 episode reward: -1.2000,                 loss: 0.1909
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 2129.0,                last time consumption/overall running time: 307.8161s / 33103.6770 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.1787
env0_second_0:                 episode reward: 0.4000,                 loss: 0.1828
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 2024.2,                last time consumption/overall running time: 295.9683s / 33399.6453 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.1907
env0_second_0:                 episode reward: 2.9000,                 loss: 0.1847
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1956.5,                last time consumption/overall running time: 285.3339s / 33684.9792 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.1887
env0_second_0:                 episode reward: 2.7000,                 loss: 0.2079
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 2070.85,                last time consumption/overall running time: 300.2074s / 33985.1866 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1652
env0_second_0:                 episode reward: 0.3000,                 loss: 0.1714
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 2097.45,                last time consumption/overall running time: 300.7735s / 34285.9601 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.1719
env0_second_0:                 episode reward: 1.7500,                 loss: 0.1708
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 2055.0,                last time consumption/overall running time: 296.1475s / 34582.1077 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.1985
env0_second_0:                 episode reward: -1.9500,                 loss: 0.2062
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1950.6,                last time consumption/overall running time: 284.0526s / 34866.1603 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.1940
env0_second_0:                 episode reward: -1.1500,                 loss: 0.2052
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 2104.75,                last time consumption/overall running time: 302.5414s / 35168.7017 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.1889
env0_second_0:                 episode reward: -0.0500,                 loss: 0.1857
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 2022.8,                last time consumption/overall running time: 289.4969s / 35458.1985 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.2042
env0_second_0:                 episode reward: 1.1500,                 loss: 0.2071
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 2034.95,                last time consumption/overall running time: 294.9060s / 35753.1045 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.2033
env0_second_0:                 episode reward: 0.9000,                 loss: 0.2009
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 2061.75,                last time consumption/overall running time: 296.7700s / 36049.8745 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.1723
env0_second_0:                 episode reward: 2.4000,                 loss: 0.1744
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 2056.3,                last time consumption/overall running time: 296.7547s / 36346.6292 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.1662
env0_second_0:                 episode reward: 0.9000,                 loss: 0.1705
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 2041.35,                last time consumption/overall running time: 291.0403s / 36637.6695 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.2031
env0_second_0:                 episode reward: 0.4500,                 loss: 0.2170
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 2041.5,                last time consumption/overall running time: 293.5328s / 36931.2023 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.1839
env0_second_0:                 episode reward: -1.3500,                 loss: 0.1932
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 2002.8,                last time consumption/overall running time: 288.4719s / 37219.6743 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.2136
env0_second_0:                 episode reward: -1.0000,                 loss: 0.2181
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 2122.65,                last time consumption/overall running time: 305.7303s / 37525.4046 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.1950
env0_second_0:                 episode reward: -0.7000,                 loss: 0.1948
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 2080.1,                last time consumption/overall running time: 299.8098s / 37825.2144 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.1941
env0_second_0:                 episode reward: 2.3000,                 loss: 0.2120
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 2187.15,                last time consumption/overall running time: 311.4011s / 38136.6154 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.1808
env0_second_0:                 episode reward: -0.7000,                 loss: 0.1757
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 1944.3,                last time consumption/overall running time: 279.3984s / 38416.0138 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.2278
env0_second_0:                 episode reward: 0.6000,                 loss: 0.2282
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 2170.5,                last time consumption/overall running time: 309.9472s / 38725.9610 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.1695
env0_second_0:                 episode reward: -1.3000,                 loss: 0.1702
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 2120.6,                last time consumption/overall running time: 302.9916s / 39028.9526 s
env0_first_0:                 episode reward: 3.2000,                 loss: 0.1805
env0_second_0:                 episode reward: -3.2000,                 loss: 0.1779
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 2089.25,                last time consumption/overall running time: 300.2863s / 39329.2389 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.1774
env0_second_0:                 episode reward: 0.1000,                 loss: 0.1983
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 2088.9,                last time consumption/overall running time: 297.8955s / 39627.1344 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.1791
env0_second_0:                 episode reward: 4.2000,                 loss: 0.1739
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 1907.95,                last time consumption/overall running time: 273.5637s / 39900.6981 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.2151
env0_second_0:                 episode reward: 2.6500,                 loss: 0.2056
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 2133.6,                last time consumption/overall running time: 306.2017s / 40206.8998 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1698
env0_second_0:                 episode reward: 1.1000,                 loss: 0.1736
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 2110.75,                last time consumption/overall running time: 303.7881s / 40510.6879 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.1942
env0_second_0:                 episode reward: 1.8500,                 loss: 0.1985
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 2096.6,                last time consumption/overall running time: 300.1714s / 40810.8593 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.2134
env0_second_0:                 episode reward: 1.6000,                 loss: 0.2067
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 1978.5,                last time consumption/overall running time: 281.4750s / 41092.3343 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.2053
env0_second_0:                 episode reward: -0.3000,                 loss: 0.2021
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 2096.4,                last time consumption/overall running time: 299.4533s / 41391.7876 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.1865
env0_second_0:                 episode reward: 2.8500,                 loss: 0.1830
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 2153.3,                last time consumption/overall running time: 308.5109s / 41700.2986 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.1782
env0_second_0:                 episode reward: 2.0500,                 loss: 0.1836
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 2036.9,                last time consumption/overall running time: 293.0516s / 41993.3501 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.2195
env0_second_0:                 episode reward: 1.2000,                 loss: 0.2255
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 2111.85,                last time consumption/overall running time: 302.7379s / 42296.0880 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.1989
env0_second_0:                 episode reward: 1.3500,                 loss: 0.2006
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 2176.1,                last time consumption/overall running time: 312.0042s / 42608.0922 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.1826
env0_second_0:                 episode reward: 1.2000,                 loss: 0.1825
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 2164.0,                last time consumption/overall running time: 310.6052s / 42918.6974 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.1760
env0_second_0:                 episode reward: 2.2000,                 loss: 0.1840
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 2135.95,                last time consumption/overall running time: 305.4062s / 43224.1037 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.1842
env0_second_0:                 episode reward: 2.2500,                 loss: 0.1723
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 2133.1,                last time consumption/overall running time: 306.3734s / 43530.4771 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.1845
env0_second_0:                 episode reward: 2.1000,                 loss: 0.1762
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 2064.45,                last time consumption/overall running time: 295.6562s / 43826.1333 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.1863
env0_second_0:                 episode reward: 2.9500,                 loss: 0.2146
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 2250.4,                last time consumption/overall running time: 323.0491s / 44149.1824 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.1500
env0_second_0:                 episode reward: 1.7500,                 loss: 0.1558
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 2039.1,                last time consumption/overall running time: 290.1082s / 44439.2906 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.1848
env0_second_0:                 episode reward: 1.3500,                 loss: 0.1870
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 2053.15,                last time consumption/overall running time: 291.8657s / 44731.1563 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.1857
env0_second_0:                 episode reward: 0.9500,                 loss: 0.2059
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 2096.2,                last time consumption/overall running time: 303.3759s / 45034.5322 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.2033
env0_second_0:                 episode reward: 0.2500,                 loss: 0.2400
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 2232.0,                last time consumption/overall running time: 321.6384s / 45356.1706 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.1548
env0_second_0:                 episode reward: 2.9500,                 loss: 0.1627
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 2150.7,                last time consumption/overall running time: 312.4425s / 45668.6131 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1921
env0_second_0:                 episode reward: 0.3000,                 loss: 0.1808
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 2111.85,                last time consumption/overall running time: 305.0631s / 45973.6762 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1811
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1760
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 2065.6,                last time consumption/overall running time: 298.0731s / 46271.7494 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.2220
env0_second_0:                 episode reward: 3.8000,                 loss: 0.2264
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 2131.5,                last time consumption/overall running time: 304.9018s / 46576.6511 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.1907
env0_second_0:                 episode reward: 1.9000,                 loss: 0.1902
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 2192.55,                last time consumption/overall running time: 321.1043s / 46897.7554 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.2061
env0_second_0:                 episode reward: 1.1500,                 loss: 0.2002
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 2160.65,                last time consumption/overall running time: 310.1244s / 47207.8798 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.1750
env0_second_0:                 episode reward: 1.2500,                 loss: 0.2138
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 2172.45,                last time consumption/overall running time: 314.2932s / 47522.1729 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.1607
env0_second_0:                 episode reward: 1.8500,                 loss: 0.1676
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 2145.95,                last time consumption/overall running time: 308.1825s / 47830.3555 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.1855
env0_second_0:                 episode reward: 1.2500,                 loss: 0.1895
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 2137.0,                last time consumption/overall running time: 307.1279s / 48137.4833 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.1928
env0_second_0:                 episode reward: 2.9500,                 loss: 0.1964
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 2079.05,                last time consumption/overall running time: 300.3117s / 48437.7951 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.1962
env0_second_0:                 episode reward: -0.9000,                 loss: 0.2067
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 2239.5,                last time consumption/overall running time: 323.2967s / 48761.0917 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.2813
env0_second_0:                 episode reward: 3.5000,                 loss: 0.3264
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 2039.45,                last time consumption/overall running time: 296.4412s / 49057.5330 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.2216
env0_second_0:                 episode reward: 2.4500,                 loss: 0.2242
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 2117.85,                last time consumption/overall running time: 305.4615s / 49362.9945 s
env0_first_0:                 episode reward: -4.5500,                 loss: 0.1761
env0_second_0:                 episode reward: 4.5500,                 loss: 0.1813
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 2196.35,                last time consumption/overall running time: 317.8109s / 49680.8054 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1850
env0_second_0:                 episode reward: 1.1000,                 loss: 0.2074
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 2128.8,                last time consumption/overall running time: 305.9628s / 49986.7682 s
env0_first_0:                 episode reward: -4.3000,                 loss: 0.1499
env0_second_0:                 episode reward: 4.3000,                 loss: 0.1595
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 2151.85,                last time consumption/overall running time: 310.4159s / 50297.1842 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.1600
env0_second_0:                 episode reward: 4.6000,                 loss: 0.2029
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 2257.85,                last time consumption/overall running time: 324.6475s / 50621.8317 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.1708
env0_second_0:                 episode reward: 2.1500,                 loss: 0.2051
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 2208.3,                last time consumption/overall running time: 320.7298s / 50942.5614 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.1521
env0_second_0:                 episode reward: 3.9500,                 loss: 0.1488
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 2258.95,                last time consumption/overall running time: 329.4063s / 51271.9677 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.1564
env0_second_0:                 episode reward: 2.5000,                 loss: 0.1541
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 2078.05,                last time consumption/overall running time: 302.9365s / 51574.9042 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1710
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1710
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 2168.2,                last time consumption/overall running time: 311.8194s / 51886.7236 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.1377
env0_second_0:                 episode reward: 6.4500,                 loss: 0.1489
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 2162.35,                last time consumption/overall running time: 311.5428s / 52198.2664 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.1708
env0_second_0:                 episode reward: 2.5000,                 loss: 0.1811
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 2135.4,                last time consumption/overall running time: 307.0723s / 52505.3387 s
env0_first_0:                 episode reward: -5.0000,                 loss: 0.1545
env0_second_0:                 episode reward: 5.0000,                 loss: 0.1782
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 2146.85,                last time consumption/overall running time: 311.3808s / 52816.7195 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.1523
env0_second_0:                 episode reward: 5.1000,                 loss: 0.1710
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 2130.7,                last time consumption/overall running time: 311.0905s / 53127.8100 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.1646
env0_second_0:                 episode reward: 3.9500,                 loss: 0.1721
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 2220.45,                last time consumption/overall running time: 320.6213s / 53448.4314 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.1874