pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 2, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'num_process': 1, 'batch_size': 32, 'max_episodes': 50000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'Tanh', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'Tanh', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220128_1512/slimevolley_SlimeVolley-v0_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220128_1512/slimevolley_SlimeVolley-v0_nash_ppo.
Episode: 1/50000 (0.0020%),                 avg. length: 582.0,                last time consumption/overall running time: 3.4399s / 3.4399 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.0403
env0_second_0:                 episode reward: 1.0000,                 loss: -0.0490
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 553.95,                last time consumption/overall running time: 54.8772s / 58.3171 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0371
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0780
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 625.85,                last time consumption/overall running time: 60.5328s / 118.8499 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.1408
env0_second_0:                 episode reward: 0.6500,                 loss: 0.1783
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 569.15,                last time consumption/overall running time: 55.0642s / 173.9140 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.1372
env0_second_0:                 episode reward: -1.1500,                 loss: 0.1614
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 585.15,                last time consumption/overall running time: 57.5136s / 231.4276 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.1468
env0_second_0:                 episode reward: 0.6000,                 loss: 0.1565
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 619.45,                last time consumption/overall running time: 59.9152s / 291.3428 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.1466
env0_second_0:                 episode reward: -0.7500,                 loss: 0.1533
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 558.8,                last time consumption/overall running time: 54.3630s / 345.7058 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.1812
env0_second_0:                 episode reward: -1.2500,                 loss: 0.1848
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 561.9,                last time consumption/overall running time: 55.2495s / 400.9554 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.1871
env0_second_0:                 episode reward: -0.5500,                 loss: 0.1933
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 582.6,                last time consumption/overall running time: 57.2586s / 458.2140 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1746
env0_second_0:                 episode reward: 1.1000,                 loss: 0.1884
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 565.75,                last time consumption/overall running time: 55.2828s / 513.4968 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1643
env0_second_0:                 episode reward: 1.1000,                 loss: 0.1954
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 576.45,                last time consumption/overall running time: 55.9533s / 569.4501 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.1727
env0_second_0:                 episode reward: -0.1500,                 loss: 0.1877
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 618.35,                last time consumption/overall running time: 59.9690s / 629.4191 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.1655
env0_second_0:                 episode reward: -0.1500,                 loss: 0.1861
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 551.35,                last time consumption/overall running time: 55.1565s / 684.5757 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1837
env0_second_0:                 episode reward: 1.1000,                 loss: 0.1885
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 599.55,                last time consumption/overall running time: 58.0959s / 742.6716 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.1896
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2107
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 613.0,                last time consumption/overall running time: 59.3300s / 802.0016 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.1968
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2115
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 539.95,                last time consumption/overall running time: 53.6446s / 855.6461 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2053
env0_second_0:                 episode reward: -0.0500,                 loss: 0.2232
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 568.95,                last time consumption/overall running time: 55.8830s / 911.5291 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.2193
env0_second_0:                 episode reward: 0.2000,                 loss: 0.2314
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 596.7,                last time consumption/overall running time: 57.9379s / 969.4670 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.2271
env0_second_0:                 episode reward: 0.3500,                 loss: 0.2427
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 541.0,                last time consumption/overall running time: 53.8992s / 1023.3662 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.2401
env0_second_0:                 episode reward: -0.3000,                 loss: 0.2397
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 560.35,                last time consumption/overall running time: 54.6079s / 1077.9741 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2522
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2640
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 587.25,                last time consumption/overall running time: 57.0440s / 1135.0181 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.2966
env0_second_0:                 episode reward: -0.3500,                 loss: 0.3011
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 616.55,                last time consumption/overall running time: 60.6331s / 1195.6513 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.2921
env0_second_0:                 episode reward: -0.9500,                 loss: 0.3041
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 564.5,                last time consumption/overall running time: 56.6459s / 1252.2971 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.3113
env0_second_0:                 episode reward: -0.2000,                 loss: 0.3094
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 550.95,                last time consumption/overall running time: 55.4939s / 1307.7911 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2904
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2881
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 581.9,                last time consumption/overall running time: 57.2046s / 1364.9957 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.3092
env0_second_0:                 episode reward: -0.5000,                 loss: 0.3177
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 546.25,                last time consumption/overall running time: 53.8526s / 1418.8483 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.2947
env0_second_0:                 episode reward: 0.7000,                 loss: 0.2955
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 560.3,                last time consumption/overall running time: 55.6201s / 1474.4684 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2846
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2975
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 556.5,                last time consumption/overall running time: 54.6691s / 1529.1375 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.3265
env0_second_0:                 episode reward: 0.6500,                 loss: 0.3337
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 571.05,                last time consumption/overall running time: 55.3745s / 1584.5120 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.3011
env0_second_0:                 episode reward: 0.7000,                 loss: 0.3114
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 557.95,                last time consumption/overall running time: 54.8894s / 1639.4013 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.2995
env0_second_0:                 episode reward: -0.4500,                 loss: 0.3129
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 556.3,                last time consumption/overall running time: 54.3057s / 1693.7070 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.3386
env0_second_0:                 episode reward: 0.6000,                 loss: 0.3564
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 579.1,                last time consumption/overall running time: 56.0948s / 1749.8018 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.3616
env0_second_0:                 episode reward: -0.3000,                 loss: 0.3826
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 641/50000 (1.2820%),                 avg. length: 584.85,                last time consumption/overall running time: 57.6171s / 1807.4189 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.3382
env0_second_0:                 episode reward: 0.2500,                 loss: 0.3478
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 661/50000 (1.3220%),                 avg. length: 579.55,                last time consumption/overall running time: 56.4845s / 1863.9034 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.3294
env0_second_0:                 episode reward: 0.4000,                 loss: 0.3390
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 681/50000 (1.3620%),                 avg. length: 547.05,                last time consumption/overall running time: 53.8362s / 1917.7396 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.3136
env0_second_0:                 episode reward: -0.3500,                 loss: 0.3289
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 701/50000 (1.4020%),                 avg. length: 512.7,                last time consumption/overall running time: 50.7508s / 1968.4904 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.3033
env0_second_0:                 episode reward: 1.1000,                 loss: 0.3232
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 721/50000 (1.4420%),                 avg. length: 554.35,                last time consumption/overall running time: 54.5088s / 2022.9992 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2586
env0_second_0:                 episode reward: -0.0500,                 loss: 0.2723
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 741/50000 (1.4820%),                 avg. length: 571.75,                last time consumption/overall running time: 55.6476s / 2078.6468 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.2536
env0_second_0:                 episode reward: 0.3500,                 loss: 0.2751
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 761/50000 (1.5220%),                 avg. length: 557.15,                last time consumption/overall running time: 54.5273s / 2133.1741 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2716
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2794
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 781/50000 (1.5620%),                 avg. length: 596.6,                last time consumption/overall running time: 58.8149s / 2191.9890 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.2546
env0_second_0:                 episode reward: -0.4000,                 loss: 0.2667
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 801/50000 (1.6020%),                 avg. length: 598.6,                last time consumption/overall running time: 57.8450s / 2249.8340 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2706
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2765
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 821/50000 (1.6420%),                 avg. length: 537.05,                last time consumption/overall running time: 53.3972s / 2303.2312 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.2818
env0_second_0:                 episode reward: -0.4000,                 loss: 0.2818
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 841/50000 (1.6820%),                 avg. length: 594.6,                last time consumption/overall running time: 58.0197s / 2361.2509 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2827
env0_second_0:                 episode reward: 0.6500,                 loss: 0.2888
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 861/50000 (1.7220%),                 avg. length: 570.45,                last time consumption/overall running time: 55.7378s / 2416.9887 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.2439
env0_second_0:                 episode reward: 0.2500,                 loss: 0.2507
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 881/50000 (1.7620%),                 avg. length: 605.75,                last time consumption/overall running time: 58.4735s / 2475.4623 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.2982
env0_second_0:                 episode reward: 0.8500,                 loss: 0.3069
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 901/50000 (1.8020%),                 avg. length: 546.65,                last time consumption/overall running time: 53.3491s / 2528.8113 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2584
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2605
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 921/50000 (1.8420%),                 avg. length: 574.7,                last time consumption/overall running time: 56.6650s / 2585.4764 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.2432
env0_second_0:                 episode reward: 0.5500,                 loss: 0.2571
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 941/50000 (1.8820%),                 avg. length: 594.0,                last time consumption/overall running time: 58.5329s / 2644.0092 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2621
env0_second_0:                 episode reward: 0.6500,                 loss: 0.2707
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 961/50000 (1.9220%),                 avg. length: 590.8,                last time consumption/overall running time: 57.5379s / 2701.5471 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.2565
env0_second_0:                 episode reward: -0.1500,                 loss: 0.2619
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 981/50000 (1.9620%),                 avg. length: 620.05,                last time consumption/overall running time: 59.2746s / 2760.8217 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.2600
env0_second_0:                 episode reward: -0.1000,                 loss: 0.2701
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1001/50000 (2.0020%),                 avg. length: 580.35,                last time consumption/overall running time: 56.2255s / 2817.0472 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.2511
env0_second_0:                 episode reward: 0.2000,                 loss: 0.2604
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 1021/50000 (2.0420%),                 avg. length: 576.95,                last time consumption/overall running time: 56.7308s / 2873.7780 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2574
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2626
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1041/50000 (2.0820%),                 avg. length: 599.6,                last time consumption/overall running time: 58.4121s / 2932.1901 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2605
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2744
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1061/50000 (2.1220%),                 avg. length: 574.2,                last time consumption/overall running time: 57.0271s / 2989.2171 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2569
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2859
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1081/50000 (2.1620%),                 avg. length: 606.35,                last time consumption/overall running time: 59.2055s / 3048.4227 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.2549
env0_second_0:                 episode reward: 0.0000,                 loss: 0.2776
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 1101/50000 (2.2020%),                 avg. length: 580.4,                last time consumption/overall running time: 56.2070s / 3104.6297 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.2654
env0_second_0:                 episode reward: -0.6000,                 loss: 0.2832
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1121/50000 (2.2420%),                 avg. length: 578.65,                last time consumption/overall running time: 54.9527s / 3159.5824 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.2579
env0_second_0:                 episode reward: 0.4500,                 loss: 0.2622
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 1141/50000 (2.2820%),                 avg. length: 554.35,                last time consumption/overall running time: 54.3399s / 3213.9223 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.2646
env0_second_0:                 episode reward: 0.8500,                 loss: 0.2780
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1161/50000 (2.3220%),                 avg. length: 614.5,                last time consumption/overall running time: 59.9652s / 3273.8874 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2815
env0_second_0:                 episode reward: 0.6500,                 loss: 0.2975
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1181/50000 (2.3620%),                 avg. length: 570.2,                last time consumption/overall running time: 55.7938s / 3329.6812 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.2586
env0_second_0:                 episode reward: 0.4500,                 loss: 0.2907
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 1201/50000 (2.4020%),                 avg. length: 591.3,                last time consumption/overall running time: 57.2196s / 3386.9008 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.2462
env0_second_0:                 episode reward: 1.2500,                 loss: 0.2640
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 1221/50000 (2.4420%),                 avg. length: 575.9,                last time consumption/overall running time: 55.8656s / 3442.7664 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.2592
env0_second_0:                 episode reward: 1.0000,                 loss: 0.2713
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1241/50000 (2.4820%),                 avg. length: 564.85,                last time consumption/overall running time: 55.1383s / 3497.9047 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.2793
env0_second_0:                 episode reward: -0.6500,                 loss: 0.2938
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1261/50000 (2.5220%),                 avg. length: 539.05,                last time consumption/overall running time: 53.2819s / 3551.1866 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.3040
env0_second_0:                 episode reward: -0.1500,                 loss: 0.3113
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1281/50000 (2.5620%),                 avg. length: 573.25,                last time consumption/overall running time: 56.4288s / 3607.6154 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.2643
env0_second_0:                 episode reward: -0.9000,                 loss: 0.2851
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1301/50000 (2.6020%),                 avg. length: 588.3,                last time consumption/overall running time: 56.9162s / 3664.5316 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2717
env0_second_0:                 episode reward: 0.6500,                 loss: 0.2803
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1321/50000 (2.6420%),                 avg. length: 576.9,                last time consumption/overall running time: 56.1429s / 3720.6745 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2296
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2302
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1341/50000 (2.6820%),                 avg. length: 600.8,                last time consumption/overall running time: 57.5440s / 3778.2184 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.2469
env0_second_0:                 episode reward: 0.4500,                 loss: 0.2477
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1361/50000 (2.7220%),                 avg. length: 555.55,                last time consumption/overall running time: 53.8404s / 3832.0588 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.2718
env0_second_0:                 episode reward: -0.3000,                 loss: 0.2863
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1381/50000 (2.7620%),                 avg. length: 564.65,                last time consumption/overall running time: 55.0081s / 3887.0669 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.2647
env0_second_0:                 episode reward: 0.0500,                 loss: 0.2701
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1401/50000 (2.8020%),                 avg. length: 610.35,                last time consumption/overall running time: 59.4561s / 3946.5231 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2572
env0_second_0:                 episode reward: 0.6500,                 loss: 0.2696
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1421/50000 (2.8420%),                 avg. length: 602.5,                last time consumption/overall running time: 58.6200s / 4005.1431 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.2517
env0_second_0:                 episode reward: 0.8500,                 loss: 0.2675
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1441/50000 (2.8820%),                 avg. length: 536.3,                last time consumption/overall running time: 52.4023s / 4057.5454 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.2461
env0_second_0:                 episode reward: 0.5500,                 loss: 0.2608
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1461/50000 (2.9220%),                 avg. length: 566.4,                last time consumption/overall running time: 54.7635s / 4112.3089 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.2809
env0_second_0:                 episode reward: 0.3500,                 loss: 0.2935
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1481/50000 (2.9620%),                 avg. length: 551.75,                last time consumption/overall running time: 53.8109s / 4166.1198 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.2634
env0_second_0:                 episode reward: -0.1500,                 loss: 0.2788
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 1501/50000 (3.0020%),                 avg. length: 575.4,                last time consumption/overall running time: 55.7045s / 4221.8243 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.2444
env0_second_0:                 episode reward: 0.2500,                 loss: 0.2609
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 1521/50000 (3.0420%),                 avg. length: 588.75,                last time consumption/overall running time: 56.8014s / 4278.6258 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.2952
env0_second_0:                 episode reward: 0.3000,                 loss: 0.2993
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1541/50000 (3.0820%),                 avg. length: 613.55,                last time consumption/overall running time: 59.1507s / 4337.7765 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2732
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2795
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1561/50000 (3.1220%),                 avg. length: 557.15,                last time consumption/overall running time: 54.1271s / 4391.9037 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.2886
env0_second_0:                 episode reward: 0.5000,                 loss: 0.2891
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1581/50000 (3.1620%),                 avg. length: 598.9,                last time consumption/overall running time: 57.1097s / 4449.0134 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.3008
env0_second_0:                 episode reward: -0.1500,                 loss: 0.2952
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1601/50000 (3.2020%),                 avg. length: 604.05,                last time consumption/overall running time: 57.8907s / 4506.9041 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.2981
env0_second_0:                 episode reward: -0.1500,                 loss: 0.3107
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 1621/50000 (3.2420%),                 avg. length: 551.65,                last time consumption/overall running time: 55.2177s / 4562.1218 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.3210
env0_second_0:                 episode reward: -0.1000,                 loss: 0.3190
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 1641/50000 (3.2820%),                 avg. length: 536.9,                last time consumption/overall running time: 53.8695s / 4615.9913 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.3310
env0_second_0:                 episode reward: 0.2500,                 loss: 0.3326
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 1661/50000 (3.3220%),                 avg. length: 598.3,                last time consumption/overall running time: 58.6227s / 4674.6140 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.3483
env0_second_0:                 episode reward: 0.3500,                 loss: 0.3454
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1681/50000 (3.3620%),                 avg. length: 601.65,                last time consumption/overall running time: 59.0706s / 4733.6846 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.3078
env0_second_0:                 episode reward: 0.8000,                 loss: 0.3162
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1701/50000 (3.4020%),                 avg. length: 557.9,                last time consumption/overall running time: 54.7033s / 4788.3879 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.3207
env0_second_0:                 episode reward: -0.2500,                 loss: 0.3160
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1721/50000 (3.4420%),                 avg. length: 575.15,                last time consumption/overall running time: 56.0542s / 4844.4420 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2818
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2815
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1741/50000 (3.4820%),                 avg. length: 530.4,                last time consumption/overall running time: 52.5731s / 4897.0151 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.3015
env0_second_0:                 episode reward: 1.4000,                 loss: 0.3048
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1761/50000 (3.5220%),                 avg. length: 574.15,                last time consumption/overall running time: 56.2615s / 4953.2766 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2971
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2934
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1781/50000 (3.5620%),                 avg. length: 552.65,                last time consumption/overall running time: 54.3146s / 5007.5913 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.2968
env0_second_0:                 episode reward: -0.5000,                 loss: 0.3002
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 1801/50000 (3.6020%),                 avg. length: 540.45,                last time consumption/overall running time: 52.8459s / 5060.4372 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.2959
env0_second_0:                 episode reward: -1.0000,                 loss: 0.2989
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 1821/50000 (3.6420%),                 avg. length: 499.15,                last time consumption/overall running time: 49.4670s / 5109.9042 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.3169
env0_second_0:                 episode reward: 0.3000,                 loss: 0.3156
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 1841/50000 (3.6820%),                 avg. length: 563.5,                last time consumption/overall running time: 54.3282s / 5164.2324 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.3084
env0_second_0:                 episode reward: 0.3000,                 loss: 0.3101
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1861/50000 (3.7220%),                 avg. length: 557.85,                last time consumption/overall running time: 54.6677s / 5218.9001 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.2990
env0_second_0:                 episode reward: -0.6500,                 loss: 0.2886
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1881/50000 (3.7620%),                 avg. length: 592.9,                last time consumption/overall running time: 57.5875s / 5276.4876 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.2838
env0_second_0:                 episode reward: -0.4500,                 loss: 0.2787
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1901/50000 (3.8020%),                 avg. length: 591.7,                last time consumption/overall running time: 57.0508s / 5333.5384 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2561
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2492
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1921/50000 (3.8420%),                 avg. length: 582.95,                last time consumption/overall running time: 55.8049s / 5389.3433 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.2228
env0_second_0:                 episode reward: 0.0000,                 loss: 0.2183
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 1941/50000 (3.8820%),                 avg. length: 591.05,                last time consumption/overall running time: 57.6606s / 5447.0039 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.2932
env0_second_0:                 episode reward: -0.5500,                 loss: 0.2850
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1961/50000 (3.9220%),                 avg. length: 559.45,                last time consumption/overall running time: 54.2992s / 5501.3031 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.2982
env0_second_0:                 episode reward: -0.4000,                 loss: 0.2982
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1981/50000 (3.9620%),                 avg. length: 577.7,                last time consumption/overall running time: 56.5477s / 5557.8507 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.2902
env0_second_0:                 episode reward: -0.3000,                 loss: 0.2901
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2001/50000 (4.0020%),                 avg. length: 574.4,                last time consumption/overall running time: 56.4018s / 5614.2525 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.3164
env0_second_0:                 episode reward: -0.5000,                 loss: 0.3224
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 2021/50000 (4.0420%),                 avg. length: 580.35,                last time consumption/overall running time: 57.1251s / 5671.3776 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.3079
env0_second_0:                 episode reward: 1.0500,                 loss: 0.3087
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2041/50000 (4.0820%),                 avg. length: 565.4,                last time consumption/overall running time: 54.5110s / 5725.8886 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.3140
env0_second_0:                 episode reward: -0.6000,                 loss: 0.3137
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2061/50000 (4.1220%),                 avg. length: 569.05,                last time consumption/overall running time: 55.0943s / 5780.9829 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.2543
env0_second_0:                 episode reward: 1.7500,                 loss: 0.2462
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 2081/50000 (4.1620%),                 avg. length: 584.85,                last time consumption/overall running time: 57.2202s / 5838.2031 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.2960
env0_second_0:                 episode reward: 0.2000,                 loss: 0.2932
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2101/50000 (4.2020%),                 avg. length: 586.85,                last time consumption/overall running time: 57.7315s / 5895.9346 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.2931
env0_second_0:                 episode reward: -0.3500,                 loss: 0.2957
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 2121/50000 (4.2420%),                 avg. length: 584.25,                last time consumption/overall running time: 57.0654s / 5952.9999 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.2805
env0_second_0:                 episode reward: 0.5500,                 loss: 0.2831
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2141/50000 (4.2820%),                 avg. length: 588.25,                last time consumption/overall running time: 56.6110s / 6009.6110 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.2837
env0_second_0:                 episode reward: 0.8500,                 loss: 0.2801
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 2161/50000 (4.3220%),                 avg. length: 582.95,                last time consumption/overall running time: 56.7701s / 6066.3811 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.2945
env0_second_0:                 episode reward: -0.4500,                 loss: 0.2927
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2181/50000 (4.3620%),                 avg. length: 549.6,                last time consumption/overall running time: 53.6563s / 6120.0374 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.3120
env0_second_0:                 episode reward: -0.2000,                 loss: 0.3118
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2201/50000 (4.4020%),                 avg. length: 578.85,                last time consumption/overall running time: 56.6032s / 6176.6406 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.2964
env0_second_0:                 episode reward: -0.4500,                 loss: 0.3059
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2221/50000 (4.4420%),                 avg. length: 586.75,                last time consumption/overall running time: 57.6951s / 6234.3357 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.3011
env0_second_0:                 episode reward: 0.8000,                 loss: 0.2913
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2241/50000 (4.4820%),                 avg. length: 573.6,                last time consumption/overall running time: 57.2699s / 6291.6056 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.3008
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2987
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 2261/50000 (4.5220%),                 avg. length: 561.45,                last time consumption/overall running time: 55.9399s / 6347.5456 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.2871
env0_second_0:                 episode reward: -0.1000,                 loss: 0.2872
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2281/50000 (4.5620%),                 avg. length: 569.55,                last time consumption/overall running time: 55.6654s / 6403.2109 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2860
env0_second_0:                 episode reward: 0.6500,                 loss: 0.2963
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 2301/50000 (4.6020%),                 avg. length: 572.55,                last time consumption/overall running time: 55.4356s / 6458.6465 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.2751
env0_second_0:                 episode reward: -0.1000,                 loss: 0.2768
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2321/50000 (4.6420%),                 avg. length: 549.8,                last time consumption/overall running time: 54.9480s / 6513.5945 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.2964
env0_second_0:                 episode reward: 0.5000,                 loss: 0.2903
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2341/50000 (4.6820%),                 avg. length: 547.85,                last time consumption/overall running time: 54.4463s / 6568.0409 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2898
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2895
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 2361/50000 (4.7220%),                 avg. length: 559.95,                last time consumption/overall running time: 54.8299s / 6622.8707 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.3020
env0_second_0:                 episode reward: -0.1000,                 loss: 0.2936
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 2381/50000 (4.7620%),                 avg. length: 597.55,                last time consumption/overall running time: 57.8131s / 6680.6838 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.2934
env0_second_0:                 episode reward: -0.1500,                 loss: 0.3009
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 2401/50000 (4.8020%),                 avg. length: 552.3,                last time consumption/overall running time: 54.3121s / 6734.9959 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2988
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2968
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 2421/50000 (4.8420%),                 avg. length: 541.95,                last time consumption/overall running time: 53.6076s / 6788.6035 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.2976
env0_second_0:                 episode reward: 0.5000,                 loss: 0.2967
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 2441/50000 (4.8820%),                 avg. length: 568.6,                last time consumption/overall running time: 55.5361s / 6844.1396 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.3120
env0_second_0:                 episode reward: -0.1000,                 loss: 0.3098
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2461/50000 (4.9220%),                 avg. length: 569.25,                last time consumption/overall running time: 55.6305s / 6899.7701 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.3136
env0_second_0:                 episode reward: -0.1000,                 loss: 0.3103
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2481/50000 (4.9620%),                 avg. length: 583.6,                last time consumption/overall running time: 56.9135s / 6956.6835 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.2795
env0_second_0:                 episode reward: 0.0500,                 loss: 0.2850
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 2501/50000 (5.0020%),                 avg. length: 542.35,                last time consumption/overall running time: 53.2145s / 7009.8980 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.3164
env0_second_0:                 episode reward: 0.8500,                 loss: 0.3160
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2521/50000 (5.0420%),                 avg. length: 588.45,                last time consumption/overall running time: 56.6468s / 7066.5448 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.3095
env0_second_0:                 episode reward: -0.3000,                 loss: 0.3101
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 2541/50000 (5.0820%),                 avg. length: 542.5,                last time consumption/overall running time: 53.4956s / 7120.0405 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.3011
env0_second_0:                 episode reward: -0.2000,                 loss: 0.3003
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 2561/50000 (5.1220%),                 avg. length: 577.45,                last time consumption/overall running time: 56.9616s / 7177.0021 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.2898
env0_second_0:                 episode reward: -0.7500,                 loss: 0.2935
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 2581/50000 (5.1620%),                 avg. length: 562.75,                last time consumption/overall running time: 55.3251s / 7232.3271 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2875
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2929
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 2601/50000 (5.2020%),                 avg. length: 568.85,                last time consumption/overall running time: 54.9800s / 7287.3071 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.3181
env0_second_0:                 episode reward: 0.0000,                 loss: 0.3173
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2621/50000 (5.2420%),                 avg. length: 539.4,                last time consumption/overall running time: 52.9147s / 7340.2218 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.2945
env0_second_0:                 episode reward: -0.2000,                 loss: 0.2966
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2641/50000 (5.2820%),                 avg. length: 616.9,                last time consumption/overall running time: 59.5849s / 7399.8067 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2702
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2761
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2661/50000 (5.3220%),                 avg. length: 623.85,                last time consumption/overall running time: 59.7223s / 7459.5290 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.2274
env0_second_0:                 episode reward: -0.6000,                 loss: 0.2368
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2681/50000 (5.3620%),                 avg. length: 621.25,                last time consumption/overall running time: 60.1964s / 7519.7255 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.2546
env0_second_0:                 episode reward: -0.5000,                 loss: 0.2640
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2701/50000 (5.4020%),                 avg. length: 594.45,                last time consumption/overall running time: 57.9147s / 7577.6402 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2486
env0_second_0:                 episode reward: -0.0500,                 loss: 0.2634
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 2721/50000 (5.4420%),                 avg. length: 609.1,                last time consumption/overall running time: 59.7478s / 7637.3880 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2608
env0_second_0:                 episode reward: -0.0500,                 loss: 0.2712
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 2741/50000 (5.4820%),                 avg. length: 570.55,                last time consumption/overall running time: 55.9405s / 7693.3284 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.2447
env0_second_0:                 episode reward: 0.3500,                 loss: 0.2554
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 2761/50000 (5.5220%),                 avg. length: 541.25,                last time consumption/overall running time: 53.5373s / 7746.8658 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.2653
env0_second_0:                 episode reward: -0.4000,                 loss: 0.2655
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 2781/50000 (5.5620%),                 avg. length: 581.3,                last time consumption/overall running time: 57.1203s / 7803.9861 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.3134
env0_second_0:                 episode reward: 0.1000,                 loss: 0.3198
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2801/50000 (5.6020%),                 avg. length: 621.4,                last time consumption/overall running time: 60.1115s / 7864.0976 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.2988
env0_second_0:                 episode reward: 0.0000,                 loss: 0.3035
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 2821/50000 (5.6420%),                 avg. length: 552.7,                last time consumption/overall running time: 54.5685s / 7918.6662 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.2818
env0_second_0:                 episode reward: 0.8000,                 loss: 0.2929
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2841/50000 (5.6820%),                 avg. length: 543.5,                last time consumption/overall running time: 54.3386s / 7973.0048 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.2802
env0_second_0:                 episode reward: 0.6000,                 loss: 0.2929
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2861/50000 (5.7220%),                 avg. length: 578.95,                last time consumption/overall running time: 58.0834s / 8031.0882 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2837
env0_second_0:                 episode reward: 0.6500,                 loss: 0.2952
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 2881/50000 (5.7620%),                 avg. length: 561.25,                last time consumption/overall running time: 55.9064s / 8086.9946 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.2964
env0_second_0:                 episode reward: -0.9000,                 loss: 0.2931
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 2901/50000 (5.8020%),                 avg. length: 578.95,                last time consumption/overall running time: 56.5851s / 8143.5798 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.3008
env0_second_0:                 episode reward: -0.5000,                 loss: 0.3055
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2921/50000 (5.8420%),                 avg. length: 548.25,                last time consumption/overall running time: 54.4544s / 8198.0341 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.2969
env0_second_0:                 episode reward: -0.1000,                 loss: 0.3064
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2941/50000 (5.8820%),                 avg. length: 545.0,                last time consumption/overall running time: 54.0041s / 8252.0383 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.2810
env0_second_0:                 episode reward: -0.5500,                 loss: 0.2892
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2961/50000 (5.9220%),                 avg. length: 588.1,                last time consumption/overall running time: 57.2619s / 8309.3001 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.3070
env0_second_0:                 episode reward: -0.3500,                 loss: 0.3092
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 2981/50000 (5.9620%),                 avg. length: 607.3,                last time consumption/overall running time: 58.3068s / 8367.6070 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.2794
env0_second_0:                 episode reward: -0.5500,                 loss: 0.2901
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 3001/50000 (6.0020%),                 avg. length: 574.05,                last time consumption/overall running time: 56.0096s / 8423.6166 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.2974
env0_second_0:                 episode reward: 1.1000,                 loss: 0.3033
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3021/50000 (6.0420%),                 avg. length: 545.95,                last time consumption/overall running time: 52.9692s / 8476.5858 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.3034
env0_second_0:                 episode reward: -0.4000,                 loss: 0.3088
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 3041/50000 (6.0820%),                 avg. length: 544.95,                last time consumption/overall running time: 52.7372s / 8529.3230 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.3229
env0_second_0:                 episode reward: -0.4000,                 loss: 0.3307
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3061/50000 (6.1220%),                 avg. length: 596.15,                last time consumption/overall running time: 57.9977s / 8587.3207 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.3231
env0_second_0:                 episode reward: 0.1000,                 loss: 0.3301
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3081/50000 (6.1620%),                 avg. length: 563.2,                last time consumption/overall running time: 55.2953s / 8642.6160 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.2609
env0_second_0:                 episode reward: 1.1000,                 loss: 0.2692
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 3101/50000 (6.2020%),                 avg. length: 611.4,                last time consumption/overall running time: 59.3813s / 8701.9973 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.3004
env0_second_0:                 episode reward: 0.8000,                 loss: 0.3073
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3121/50000 (6.2420%),                 avg. length: 555.45,                last time consumption/overall running time: 54.4011s / 8756.3984 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.2771
env0_second_0:                 episode reward: -0.4000,                 loss: 0.2881
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 3141/50000 (6.2820%),                 avg. length: 529.3,                last time consumption/overall running time: 51.8912s / 8808.2896 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.2565
env0_second_0:                 episode reward: 0.7500,                 loss: 0.2663
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 3161/50000 (6.3220%),                 avg. length: 587.85,                last time consumption/overall running time: 56.3071s / 8864.5967 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2732
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2815
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3181/50000 (6.3620%),                 avg. length: 581.65,                last time consumption/overall running time: 56.0298s / 8920.6265 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.2380
env0_second_0:                 episode reward: 0.5500,                 loss: 0.2526
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 3201/50000 (6.4020%),                 avg. length: 581.6,                last time consumption/overall running time: 56.1614s / 8976.7880 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2672
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2818
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3221/50000 (6.4420%),                 avg. length: 583.1,                last time consumption/overall running time: 57.0693s / 9033.8573 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.2415
env0_second_0:                 episode reward: 0.2000,                 loss: 0.2509
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 3241/50000 (6.4820%),                 avg. length: 621.2,                last time consumption/overall running time: 60.0845s / 9093.9418 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.2229
env0_second_0:                 episode reward: 0.0500,                 loss: 0.2232
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 3261/50000 (6.5220%),                 avg. length: 565.9,                last time consumption/overall running time: 55.4576s / 9149.3995 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.2680
env0_second_0:                 episode reward: -0.5500,                 loss: 0.2710
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3281/50000 (6.5620%),                 avg. length: 619.7,                last time consumption/overall running time: 60.1631s / 9209.5626 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.2720
env0_second_0:                 episode reward: -0.3000,                 loss: 0.2779
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 3301/50000 (6.6020%),                 avg. length: 621.9,                last time consumption/overall running time: 61.5239s / 9271.0865 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2577
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2669
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 3321/50000 (6.6420%),                 avg. length: 606.25,                last time consumption/overall running time: 59.6112s / 9330.6977 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.2705
env0_second_0:                 episode reward: 0.2500,                 loss: 0.2803
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3341/50000 (6.6820%),                 avg. length: 638.0,                last time consumption/overall running time: 61.5848s / 9392.2824 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.2111
env0_second_0:                 episode reward: 0.8000,                 loss: 0.2119
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 3361/50000 (6.7220%),                 avg. length: 570.25,                last time consumption/overall running time: 56.3838s / 9448.6662 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.2531
env0_second_0:                 episode reward: -0.5000,                 loss: 0.2586
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3381/50000 (6.7620%),                 avg. length: 647.5,                last time consumption/overall running time: 63.1622s / 9511.8284 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.2170
env0_second_0:                 episode reward: -0.4500,                 loss: 0.2215
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3401/50000 (6.8020%),                 avg. length: 621.85,                last time consumption/overall running time: 60.4479s / 9572.2764 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.2515
env0_second_0:                 episode reward: -0.8000,                 loss: 0.2568
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 3421/50000 (6.8420%),                 avg. length: 584.4,                last time consumption/overall running time: 56.5885s / 9628.8649 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.2680
env0_second_0:                 episode reward: 1.0500,                 loss: 0.2723
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 3441/50000 (6.8820%),                 avg. length: 657.5,                last time consumption/overall running time: 63.5561s / 9692.4210 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.2188
env0_second_0:                 episode reward: 0.0000,                 loss: 0.2211
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 3461/50000 (6.9220%),                 avg. length: 630.25,                last time consumption/overall running time: 60.9292s / 9753.3502 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2195
env0_second_0:                 episode reward: -0.0500,                 loss: 0.2306
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3481/50000 (6.9620%),                 avg. length: 634.8,                last time consumption/overall running time: 60.7117s / 9814.0619 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1860
env0_second_0:                 episode reward: 0.3000,                 loss: 0.1948
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3501/50000 (7.0020%),                 avg. length: 622.0,                last time consumption/overall running time: 60.1822s / 9874.2442 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.1824
env0_second_0:                 episode reward: 0.4000,                 loss: 0.1826
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3521/50000 (7.0420%),                 avg. length: 643.0,                last time consumption/overall running time: 62.4663s / 9936.7105 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.1942
env0_second_0:                 episode reward: 0.6000,                 loss: 0.2028
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3541/50000 (7.0820%),                 avg. length: 636.15,                last time consumption/overall running time: 61.6349s / 9998.3454 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.1852
env0_second_0:                 episode reward: 0.2500,                 loss: 0.1903
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 3561/50000 (7.1220%),                 avg. length: 570.75,                last time consumption/overall running time: 55.3069s / 10053.6522 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.2130
env0_second_0:                 episode reward: 0.7500,                 loss: 0.2135
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 3581/50000 (7.1620%),                 avg. length: 637.95,                last time consumption/overall running time: 62.4706s / 10116.1228 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.1924
env0_second_0:                 episode reward: 0.3500,                 loss: 0.2058
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 3601/50000 (7.2020%),                 avg. length: 595.9,                last time consumption/overall running time: 57.6756s / 10173.7984 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.2129
env0_second_0:                 episode reward: 1.3500,                 loss: 0.2161
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3621/50000 (7.2420%),                 avg. length: 623.85,                last time consumption/overall running time: 59.7348s / 10233.5332 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.1896
env0_second_0:                 episode reward: 1.4000,                 loss: 0.1970
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3641/50000 (7.2820%),                 avg. length: 636.7,                last time consumption/overall running time: 61.3238s / 10294.8570 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1802
env0_second_0:                 episode reward: 1.1000,                 loss: 0.1823
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 3661/50000 (7.3220%),                 avg. length: 641.65,                last time consumption/overall running time: 63.6540s / 10358.5110 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.2092
env0_second_0:                 episode reward: 0.5000,                 loss: 0.2180
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3681/50000 (7.3620%),                 avg. length: 635.05,                last time consumption/overall running time: 62.3935s / 10420.9046 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1862
env0_second_0:                 episode reward: 0.3000,                 loss: 0.1923
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 3701/50000 (7.4020%),                 avg. length: 655.6,                last time consumption/overall running time: 62.6325s / 10483.5370 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.1913
env0_second_0:                 episode reward: 0.4000,                 loss: 0.1882
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3721/50000 (7.4420%),                 avg. length: 619.7,                last time consumption/overall running time: 60.0145s / 10543.5516 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1858
env0_second_0:                 episode reward: 1.1000,                 loss: 0.1984
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 3741/50000 (7.4820%),                 avg. length: 709.95,                last time consumption/overall running time: 66.1963s / 10609.7478 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.1606
env0_second_0:                 episode reward: 0.6000,                 loss: 0.1788
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 3761/50000 (7.5220%),                 avg. length: 629.75,                last time consumption/overall running time: 60.9902s / 10670.7380 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.1811
env0_second_0:                 episode reward: 0.8500,                 loss: 0.1922
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 3781/50000 (7.5620%),                 avg. length: 659.25,                last time consumption/overall running time: 63.6386s / 10734.3766 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.1831
env0_second_0:                 episode reward: 1.3000,                 loss: 0.2030
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 3801/50000 (7.6020%),                 avg. length: 644.15,                last time consumption/overall running time: 61.9020s / 10796.2786 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.1598
env0_second_0:                 episode reward: 0.9000,                 loss: 0.1766
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 3821/50000 (7.6420%),                 avg. length: 614.45,                last time consumption/overall running time: 59.9037s / 10856.1824 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.1586
env0_second_0:                 episode reward: 1.5500,                 loss: 0.1645
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 3841/50000 (7.6820%),                 avg. length: 664.45,                last time consumption/overall running time: 64.3424s / 10920.5248 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.1657
env0_second_0:                 episode reward: 1.7000,                 loss: 0.1737
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 3861/50000 (7.7220%),                 avg. length: 687.75,                last time consumption/overall running time: 65.6182s / 10986.1430 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.1843
env0_second_0:                 episode reward: 0.7000,                 loss: 0.1963
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 3881/50000 (7.7620%),                 avg. length: 642.6,                last time consumption/overall running time: 61.4644s / 11047.6074 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.1521
env0_second_0:                 episode reward: 1.6000,                 loss: 0.1589
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 3901/50000 (7.8020%),                 avg. length: 694.35,                last time consumption/overall running time: 66.2074s / 11113.8148 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.1273
env0_second_0:                 episode reward: 1.8000,                 loss: 0.1364
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 3921/50000 (7.8420%),                 avg. length: 708.9,                last time consumption/overall running time: 67.4726s / 11181.2874 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.1351
env0_second_0:                 episode reward: 2.0000,                 loss: 0.1440
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 3941/50000 (7.8820%),                 avg. length: 693.25,                last time consumption/overall running time: 65.7950s / 11247.0824 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.1537
env0_second_0:                 episode reward: 2.9500,                 loss: 0.1630
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 3961/50000 (7.9220%),                 avg. length: 614.4,                last time consumption/overall running time: 59.5960s / 11306.6784 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1360
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1450
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 3981/50000 (7.9620%),                 avg. length: 690.45,                last time consumption/overall running time: 65.4598s / 11372.1382 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.1392
env0_second_0:                 episode reward: 2.4000,                 loss: 0.1481
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 4001/50000 (8.0020%),                 avg. length: 615.55,                last time consumption/overall running time: 59.2152s / 11431.3534 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.1281
env0_second_0:                 episode reward: 2.6500,                 loss: 0.1433
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 4021/50000 (8.0420%),                 avg. length: 649.45,                last time consumption/overall running time: 62.1847s / 11493.5381 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.1234
env0_second_0:                 episode reward: 2.3500,                 loss: 0.1485
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 4041/50000 (8.0820%),                 avg. length: 604.85,                last time consumption/overall running time: 57.7648s / 11551.3029 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1291
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1495
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 4061/50000 (8.1220%),                 avg. length: 651.3,                last time consumption/overall running time: 62.1393s / 11613.4422 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.1347
env0_second_0:                 episode reward: 2.2000,                 loss: 0.1508
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 4081/50000 (8.1620%),                 avg. length: 626.1,                last time consumption/overall running time: 59.3578s / 11672.8000 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.1331
env0_second_0:                 episode reward: 2.1000,                 loss: 0.1466
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4101/50000 (8.2020%),                 avg. length: 642.25,                last time consumption/overall running time: 60.6041s / 11733.4041 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.1979
env0_second_0:                 episode reward: 1.1500,                 loss: 0.2203
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 4121/50000 (8.2420%),                 avg. length: 605.9,                last time consumption/overall running time: 57.7371s / 11791.1412 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.1548
env0_second_0:                 episode reward: 2.7000,                 loss: 0.1565
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 4141/50000 (8.2820%),                 avg. length: 608.9,                last time consumption/overall running time: 58.6871s / 11849.8282 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.1645
env0_second_0:                 episode reward: 3.0500,                 loss: 0.1829
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 4161/50000 (8.3220%),                 avg. length: 698.25,                last time consumption/overall running time: 65.6297s / 11915.4579 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.1567
env0_second_0:                 episode reward: 2.5500,                 loss: 0.1763
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 4181/50000 (8.3620%),                 avg. length: 646.4,                last time consumption/overall running time: 61.5704s / 11977.0283 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.1279
env0_second_0:                 episode reward: 2.6500,                 loss: 0.1388
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 4201/50000 (8.4020%),                 avg. length: 623.1,                last time consumption/overall running time: 60.3879s / 12037.4162 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.1539
env0_second_0:                 episode reward: 3.8500,                 loss: 0.1747
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 4221/50000 (8.4420%),                 avg. length: 595.25,                last time consumption/overall running time: 57.5720s / 12094.9883 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.1626
env0_second_0:                 episode reward: 1.9500,                 loss: 0.1822
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 4241/50000 (8.4820%),                 avg. length: 681.35,                last time consumption/overall running time: 64.7841s / 12159.7724 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.2036
env0_second_0:                 episode reward: 2.3500,                 loss: 0.2161
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 4261/50000 (8.5220%),                 avg. length: 639.05,                last time consumption/overall running time: 61.7276s / 12221.5000 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.1391
env0_second_0:                 episode reward: 2.3000,                 loss: 0.1567
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 4281/50000 (8.5620%),                 avg. length: 629.05,                last time consumption/overall running time: 60.2386s / 12281.7385 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.1193
env0_second_0:                 episode reward: 3.0500,                 loss: 0.1407
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 4301/50000 (8.6020%),                 avg. length: 607.9,                last time consumption/overall running time: 57.7585s / 12339.4970 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1068
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1167
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 4321/50000 (8.6420%),                 avg. length: 606.35,                last time consumption/overall running time: 58.7216s / 12398.2186 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1122
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1278
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 4341/50000 (8.6820%),                 avg. length: 643.2,                last time consumption/overall running time: 61.6329s / 12459.8515 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1126
env0_second_0:                 episode reward: 3.0000,                 loss: 0.1252
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 4361/50000 (8.7220%),                 avg. length: 628.1,                last time consumption/overall running time: 60.4218s / 12520.2733 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.1276
env0_second_0:                 episode reward: 3.8500,                 loss: 0.1467
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 4381/50000 (8.7620%),                 avg. length: 596.55,                last time consumption/overall running time: 57.6752s / 12577.9485 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0981
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1000
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 4401/50000 (8.8020%),                 avg. length: 657.65,                last time consumption/overall running time: 62.7693s / 12640.7178 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1228
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1436
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4421/50000 (8.8420%),                 avg. length: 617.1,                last time consumption/overall running time: 59.3228s / 12700.0406 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0992
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1240
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 4441/50000 (8.8820%),                 avg. length: 669.95,                last time consumption/overall running time: 63.3108s / 12763.3514 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0998
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1053
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 4461/50000 (8.9220%),                 avg. length: 636.8,                last time consumption/overall running time: 60.8938s / 12824.2452 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1379
env0_second_0:                 episode reward: 3.0000,                 loss: 0.1786
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 4481/50000 (8.9620%),                 avg. length: 594.3,                last time consumption/overall running time: 57.4165s / 12881.6616 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1202
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1457
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 4501/50000 (9.0020%),                 avg. length: 687.2,                last time consumption/overall running time: 63.8306s / 12945.4923 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1082
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1275
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 4521/50000 (9.0420%),                 avg. length: 610.3,                last time consumption/overall running time: 58.3126s / 13003.8048 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.1035
env0_second_0:                 episode reward: 2.1000,                 loss: 0.1377
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 4541/50000 (9.0820%),                 avg. length: 606.1,                last time consumption/overall running time: 58.2768s / 13062.0816 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1262
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1474
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 4561/50000 (9.1220%),                 avg. length: 548.4,                last time consumption/overall running time: 53.8842s / 13115.9658 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1828
env0_second_0:                 episode reward: 3.0000,                 loss: 0.2137
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 4581/50000 (9.1620%),                 avg. length: 611.3,                last time consumption/overall running time: 58.6891s / 13174.6549 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1055
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1425
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4601/50000 (9.2020%),                 avg. length: 564.45,                last time consumption/overall running time: 53.9992s / 13228.6541 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1269
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1695
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 4621/50000 (9.2420%),                 avg. length: 610.9,                last time consumption/overall running time: 58.5945s / 13287.2487 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1583
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1993
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 4641/50000 (9.2820%),                 avg. length: 552.9,                last time consumption/overall running time: 52.9577s / 13340.2064 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1650
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1942
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 4661/50000 (9.3220%),                 avg. length: 560.45,                last time consumption/overall running time: 53.7473s / 13393.9536 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.1328
env0_second_0:                 episode reward: 4.0500,                 loss: 0.1635
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4681/50000 (9.3620%),                 avg. length: 591.0,                last time consumption/overall running time: 57.2341s / 13451.1878 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1601
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1735
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 4701/50000 (9.4020%),                 avg. length: 579.55,                last time consumption/overall running time: 56.4079s / 13507.5957 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1430
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1595
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4721/50000 (9.4420%),                 avg. length: 595.6,                last time consumption/overall running time: 58.0240s / 13565.6197 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1281
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1730
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 4741/50000 (9.4820%),                 avg. length: 615.0,                last time consumption/overall running time: 58.8330s / 13624.4527 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1703
env0_second_0:                 episode reward: 3.4000,                 loss: 0.2098
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 4761/50000 (9.5220%),                 avg. length: 604.2,                last time consumption/overall running time: 58.0555s / 13682.5082 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1480
env0_second_0:                 episode reward: 3.2500,                 loss: 0.1760
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 4781/50000 (9.5620%),                 avg. length: 585.7,                last time consumption/overall running time: 56.2573s / 13738.7655 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1086
env0_second_0:                 episode reward: 3.2500,                 loss: 0.1348
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 4801/50000 (9.6020%),                 avg. length: 534.85,                last time consumption/overall running time: 52.5279s / 13791.2934 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1446
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1727
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 4821/50000 (9.6420%),                 avg. length: 559.05,                last time consumption/overall running time: 54.0872s / 13845.3806 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1427
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1708
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 4841/50000 (9.6820%),                 avg. length: 576.2,                last time consumption/overall running time: 55.8827s / 13901.2633 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1418
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1657
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4861/50000 (9.7220%),                 avg. length: 529.85,                last time consumption/overall running time: 51.7580s / 13953.0213 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1650
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1907
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 4881/50000 (9.7620%),                 avg. length: 581.45,                last time consumption/overall running time: 56.2918s / 14009.3131 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1338
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1544
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 4901/50000 (9.8020%),                 avg. length: 571.6,                last time consumption/overall running time: 55.0677s / 14064.3808 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.1574
env0_second_0:                 episode reward: 3.0500,                 loss: 0.1731
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 4921/50000 (9.8420%),                 avg. length: 589.5,                last time consumption/overall running time: 56.5678s / 14120.9486 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0959
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1192
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 4941/50000 (9.8820%),                 avg. length: 587.85,                last time consumption/overall running time: 57.7205s / 14178.6690 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1266
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1480
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 4961/50000 (9.9220%),                 avg. length: 670.65,                last time consumption/overall running time: 63.5955s / 14242.2646 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.1357
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1626
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 4981/50000 (9.9620%),                 avg. length: 605.45,                last time consumption/overall running time: 57.7945s / 14300.0591 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1286
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1692
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 5001/50000 (10.0020%),                 avg. length: 645.25,                last time consumption/overall running time: 61.2240s / 14361.2831 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1248
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1512
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 5021/50000 (10.0420%),                 avg. length: 564.25,                last time consumption/overall running time: 54.6149s / 14415.8980 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1102
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1444
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 5041/50000 (10.0820%),                 avg. length: 603.0,                last time consumption/overall running time: 58.3563s / 14474.2543 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1319
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1721
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 5061/50000 (10.1220%),                 avg. length: 604.55,                last time consumption/overall running time: 57.6485s / 14531.9028 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1371
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1760
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 5081/50000 (10.1620%),                 avg. length: 587.25,                last time consumption/overall running time: 56.2645s / 14588.1673 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1404
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1869
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 5101/50000 (10.2020%),                 avg. length: 608.2,                last time consumption/overall running time: 58.8735s / 14647.0408 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1147
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1406
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 5121/50000 (10.2420%),                 avg. length: 580.9,                last time consumption/overall running time: 57.2546s / 14704.2954 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.1258
env0_second_0:                 episode reward: 3.6500,                 loss: 0.1625
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 5141/50000 (10.2820%),                 avg. length: 584.75,                last time consumption/overall running time: 56.6955s / 14760.9910 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1198
env0_second_0:                 episode reward: 3.2500,                 loss: 0.1481
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 5161/50000 (10.3220%),                 avg. length: 619.85,                last time consumption/overall running time: 59.0328s / 14820.0238 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.1063
env0_second_0:                 episode reward: 3.8500,                 loss: 0.1481
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 5181/50000 (10.3620%),                 avg. length: 598.45,                last time consumption/overall running time: 57.8051s / 14877.8289 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1519
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1929
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5201/50000 (10.4020%),                 avg. length: 636.65,                last time consumption/overall running time: 60.5967s / 14938.4256 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1349
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1962
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 5221/50000 (10.4420%),                 avg. length: 578.35,                last time consumption/overall running time: 55.8334s / 14994.2590 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1202
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1399
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 5241/50000 (10.4820%),                 avg. length: 638.6,                last time consumption/overall running time: 61.0576s / 15055.3166 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.1455
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1745
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5261/50000 (10.5220%),                 avg. length: 561.35,                last time consumption/overall running time: 56.2463s / 15111.5629 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1650
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1969
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 5281/50000 (10.5620%),                 avg. length: 599.7,                last time consumption/overall running time: 57.8440s / 15169.4070 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1556
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1834
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 5301/50000 (10.6020%),                 avg. length: 568.85,                last time consumption/overall running time: 55.1122s / 15224.5192 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.2047
env0_second_0:                 episode reward: 2.4000,                 loss: 0.2460
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 5321/50000 (10.6420%),                 avg. length: 632.8,                last time consumption/overall running time: 60.8532s / 15285.3724 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1741
env0_second_0:                 episode reward: 3.0000,                 loss: 0.2107
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 5341/50000 (10.6820%),                 avg. length: 556.45,                last time consumption/overall running time: 53.5792s / 15338.9516 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.1492
env0_second_0:                 episode reward: 3.0500,                 loss: 0.1773
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 5361/50000 (10.7220%),                 avg. length: 589.05,                last time consumption/overall running time: 56.6935s / 15395.6451 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1384
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1664
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 5381/50000 (10.7620%),                 avg. length: 635.7,                last time consumption/overall running time: 61.0305s / 15456.6756 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0959
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1203
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 5401/50000 (10.8020%),                 avg. length: 617.8,                last time consumption/overall running time: 58.8293s / 15515.5049 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1320
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1658
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 5421/50000 (10.8420%),                 avg. length: 605.9,                last time consumption/overall running time: 57.9088s / 15573.4137 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.1106
env0_second_0:                 episode reward: 2.7500,                 loss: 0.1421
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5441/50000 (10.8820%),                 avg. length: 573.5,                last time consumption/overall running time: 54.9390s / 15628.3527 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1313
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1578
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 5461/50000 (10.9220%),                 avg. length: 560.15,                last time consumption/overall running time: 55.9617s / 15684.3144 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1379
env0_second_0:                 episode reward: 3.0000,                 loss: 0.1521
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 5481/50000 (10.9620%),                 avg. length: 631.95,                last time consumption/overall running time: 60.0868s / 15744.4011 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1212
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1422
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 5501/50000 (11.0020%),                 avg. length: 608.05,                last time consumption/overall running time: 58.8724s / 15803.2735 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1507
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1832
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 5521/50000 (11.0420%),                 avg. length: 625.0,                last time consumption/overall running time: 59.2733s / 15862.5468 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1671
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1873
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 5541/50000 (11.0820%),                 avg. length: 570.25,                last time consumption/overall running time: 54.0836s / 15916.6305 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1575
env0_second_0:                 episode reward: 3.0000,                 loss: 0.1799
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 5561/50000 (11.1220%),                 avg. length: 547.55,                last time consumption/overall running time: 54.0267s / 15970.6572 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1375
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1666
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 5581/50000 (11.1620%),                 avg. length: 577.7,                last time consumption/overall running time: 55.7260s / 16026.3831 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.1404
env0_second_0:                 episode reward: 4.0000,                 loss: 0.1624
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 5601/50000 (11.2020%),                 avg. length: 567.8,                last time consumption/overall running time: 55.6173s / 16082.0005 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1611
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1834
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 5621/50000 (11.2420%),                 avg. length: 609.4,                last time consumption/overall running time: 58.3579s / 16140.3584 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1215
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1346
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 5641/50000 (11.2820%),                 avg. length: 615.9,                last time consumption/overall running time: 59.0079s / 16199.3663 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.1173
env0_second_0:                 episode reward: 3.9500,                 loss: 0.1534
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 5661/50000 (11.3220%),                 avg. length: 644.7,                last time consumption/overall running time: 60.4354s / 16259.8017 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.2184
env0_second_0:                 episode reward: 1.9500,                 loss: 0.2504
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 5681/50000 (11.3620%),                 avg. length: 598.85,                last time consumption/overall running time: 57.5218s / 16317.3235 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1139
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1292
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5701/50000 (11.4020%),                 avg. length: 628.15,                last time consumption/overall running time: 59.5562s / 16376.8798 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1511
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1862
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 5721/50000 (11.4420%),                 avg. length: 607.65,                last time consumption/overall running time: 58.3498s / 16435.2296 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0936
env0_second_0:                 episode reward: 4.0000,                 loss: 0.1196
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 5741/50000 (11.4820%),                 avg. length: 587.5,                last time consumption/overall running time: 56.8501s / 16492.0797 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1108
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1228
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 5761/50000 (11.5220%),                 avg. length: 589.65,                last time consumption/overall running time: 57.6153s / 16549.6949 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1297
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1589
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 5781/50000 (11.5620%),                 avg. length: 539.9,                last time consumption/overall running time: 53.2041s / 16602.8990 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.1668
env0_second_0:                 episode reward: 3.8500,                 loss: 0.1952
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 5801/50000 (11.6020%),                 avg. length: 606.95,                last time consumption/overall running time: 59.7832s / 16662.6822 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1956
env0_second_0:                 episode reward: 3.4000,                 loss: 0.2238
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 5821/50000 (11.6420%),                 avg. length: 619.9,                last time consumption/overall running time: 60.2542s / 16722.9365 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1416
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1580
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 5841/50000 (11.6820%),                 avg. length: 601.6,                last time consumption/overall running time: 58.4588s / 16781.3953 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1775
env0_second_0:                 episode reward: 3.2000,                 loss: 0.2035
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 5861/50000 (11.7220%),                 avg. length: 549.4,                last time consumption/overall running time: 53.7152s / 16835.1105 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1522
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1833
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 5881/50000 (11.7620%),                 avg. length: 545.3,                last time consumption/overall running time: 53.0729s / 16888.1834 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1294
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1769
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 5901/50000 (11.8020%),                 avg. length: 604.65,                last time consumption/overall running time: 58.0836s / 16946.2670 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.0978
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1384
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 5921/50000 (11.8420%),                 avg. length: 575.65,                last time consumption/overall running time: 55.5043s / 17001.7713 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1195
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1451
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 5941/50000 (11.8820%),                 avg. length: 609.75,                last time consumption/overall running time: 59.5507s / 17061.3220 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0825
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1107
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5961/50000 (11.9220%),                 avg. length: 588.4,                last time consumption/overall running time: 57.4404s / 17118.7623 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1241
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1483
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 5981/50000 (11.9620%),                 avg. length: 556.25,                last time consumption/overall running time: 54.2938s / 17173.0561 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1150
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1457
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 6001/50000 (12.0020%),                 avg. length: 597.8,                last time consumption/overall running time: 57.8568s / 17230.9129 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.1568
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1884
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 6021/50000 (12.0420%),                 avg. length: 614.7,                last time consumption/overall running time: 58.8515s / 17289.7645 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1271
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1484
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 6041/50000 (12.0820%),                 avg. length: 602.35,                last time consumption/overall running time: 58.1351s / 17347.8996 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1267
env0_second_0:                 episode reward: 3.2500,                 loss: 0.1586
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 6061/50000 (12.1220%),                 avg. length: 578.2,                last time consumption/overall running time: 56.3994s / 17404.2990 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1357
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1493
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 6081/50000 (12.1620%),                 avg. length: 633.7,                last time consumption/overall running time: 60.9668s / 17465.2657 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1508
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1846
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 6101/50000 (12.2020%),                 avg. length: 604.65,                last time consumption/overall running time: 57.6307s / 17522.8964 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1354
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1845
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 6121/50000 (12.2420%),                 avg. length: 584.1,                last time consumption/overall running time: 56.9299s / 17579.8264 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1492
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1925
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 6141/50000 (12.2820%),                 avg. length: 700.6,                last time consumption/overall running time: 66.8328s / 17646.6591 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.1711
env0_second_0:                 episode reward: 2.7000,                 loss: 0.2047
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 6161/50000 (12.3220%),                 avg. length: 647.2,                last time consumption/overall running time: 61.9291s / 17708.5882 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1052
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1312
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 6181/50000 (12.3620%),                 avg. length: 609.7,                last time consumption/overall running time: 59.6113s / 17768.1995 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.1241
env0_second_0:                 episode reward: 3.6500,                 loss: 0.1446
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 6201/50000 (12.4020%),                 avg. length: 588.2,                last time consumption/overall running time: 56.7450s / 17824.9445 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.1174
env0_second_0:                 episode reward: 2.8000,                 loss: 0.1370
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 6221/50000 (12.4420%),                 avg. length: 578.65,                last time consumption/overall running time: 56.6496s / 17881.5940 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1348
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1537
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 6241/50000 (12.4820%),                 avg. length: 587.95,                last time consumption/overall running time: 57.1051s / 17938.6991 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1264
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1754
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 6261/50000 (12.5220%),                 avg. length: 554.0,                last time consumption/overall running time: 54.5219s / 17993.2210 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.1158
env0_second_0:                 episode reward: 4.6000,                 loss: 0.1185
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 6281/50000 (12.5620%),                 avg. length: 605.05,                last time consumption/overall running time: 58.1893s / 18051.4103 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1208
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1409
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 6301/50000 (12.6020%),                 avg. length: 620.9,                last time consumption/overall running time: 59.9059s / 18111.3163 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1351
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1569
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 6321/50000 (12.6420%),                 avg. length: 602.5,                last time consumption/overall running time: 58.1666s / 18169.4829 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1286
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1631
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 6341/50000 (12.6820%),                 avg. length: 572.25,                last time consumption/overall running time: 56.8699s / 18226.3528 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.1458
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1691
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 6361/50000 (12.7220%),                 avg. length: 639.95,                last time consumption/overall running time: 61.6142s / 18287.9669 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.1595
env0_second_0:                 episode reward: 2.6500,                 loss: 0.1781
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 6381/50000 (12.7620%),                 avg. length: 610.4,                last time consumption/overall running time: 59.6211s / 18347.5881 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1459
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1657
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 6401/50000 (12.8020%),                 avg. length: 559.25,                last time consumption/overall running time: 54.8601s / 18402.4482 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1127
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1339
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 6421/50000 (12.8420%),                 avg. length: 595.6,                last time consumption/overall running time: 58.1709s / 18460.6191 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1489
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1737
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 6441/50000 (12.8820%),                 avg. length: 615.0,                last time consumption/overall running time: 59.9377s / 18520.5568 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1663
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1997
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 6461/50000 (12.9220%),                 avg. length: 552.15,                last time consumption/overall running time: 53.3711s / 18573.9279 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1528
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1740
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 6481/50000 (12.9620%),                 avg. length: 538.2,                last time consumption/overall running time: 53.0806s / 18627.0086 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1235
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1483
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 6501/50000 (13.0020%),                 avg. length: 581.65,                last time consumption/overall running time: 56.2558s / 18683.2644 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.1333
env0_second_0:                 episode reward: 4.0000,                 loss: 0.1445
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 6521/50000 (13.0420%),                 avg. length: 604.8,                last time consumption/overall running time: 58.7581s / 18742.0225 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1419
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1578
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 6541/50000 (13.0820%),                 avg. length: 528.3,                last time consumption/overall running time: 52.5115s / 18794.5340 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1793
env0_second_0:                 episode reward: 3.7000,                 loss: 0.2010
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 6561/50000 (13.1220%),                 avg. length: 577.0,                last time consumption/overall running time: 55.7742s / 18850.3082 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1516
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1920
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 6581/50000 (13.1620%),                 avg. length: 549.0,                last time consumption/overall running time: 52.9784s / 18903.2867 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1172
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1397
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 6601/50000 (13.2020%),                 avg. length: 621.95,                last time consumption/overall running time: 59.9166s / 18963.2033 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1904
env0_second_0:                 episode reward: 3.1500,                 loss: 0.2419
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 6621/50000 (13.2420%),                 avg. length: 575.35,                last time consumption/overall running time: 56.4945s / 19019.6977 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1665
env0_second_0:                 episode reward: 3.0000,                 loss: 0.2315
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 6641/50000 (13.2820%),                 avg. length: 589.4,                last time consumption/overall running time: 56.0135s / 19075.7112 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1610
env0_second_0:                 episode reward: 3.2500,                 loss: 0.2060
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 6661/50000 (13.3220%),                 avg. length: 543.0,                last time consumption/overall running time: 52.7722s / 19128.4834 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.1320
env0_second_0:                 episode reward: 4.2500,                 loss: 0.1647
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 6681/50000 (13.3620%),                 avg. length: 550.35,                last time consumption/overall running time: 53.4318s / 19181.9152 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1596
env0_second_0:                 episode reward: 3.4000,                 loss: 0.2240
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 6701/50000 (13.4020%),                 avg. length: 573.9,                last time consumption/overall running time: 55.7383s / 19237.6535 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1897
env0_second_0:                 episode reward: 3.1500,                 loss: 0.2129
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 6721/50000 (13.4420%),                 avg. length: 526.1,                last time consumption/overall running time: 51.7637s / 19289.4172 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1371
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1979
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 6741/50000 (13.4820%),                 avg. length: 596.9,                last time consumption/overall running time: 58.0513s / 19347.4685 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1294
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1499
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 6761/50000 (13.5220%),                 avg. length: 593.95,                last time consumption/overall running time: 57.6439s / 19405.1124 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1317
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1692
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 6781/50000 (13.5620%),                 avg. length: 577.45,                last time consumption/overall running time: 56.6060s / 19461.7184 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.1392
env0_second_0:                 episode reward: 4.4000,                 loss: 0.1705
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 6801/50000 (13.6020%),                 avg. length: 542.3,                last time consumption/overall running time: 53.8690s / 19515.5874 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1736
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1942
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 6821/50000 (13.6420%),                 avg. length: 653.4,                last time consumption/overall running time: 62.2135s / 19577.8009 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1123
env0_second_0:                 episode reward: 3.2500,                 loss: 0.1483
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 6841/50000 (13.6820%),                 avg. length: 575.85,                last time consumption/overall running time: 55.0789s / 19632.8797 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0817
env0_second_0:                 episode reward: 4.2000,                 loss: 0.1574
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 6861/50000 (13.7220%),                 avg. length: 616.95,                last time consumption/overall running time: 58.8830s / 19691.7628 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1340
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1791
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 6881/50000 (13.7620%),                 avg. length: 614.85,                last time consumption/overall running time: 58.6447s / 19750.4075 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.1330
env0_second_0:                 episode reward: 4.2000,                 loss: 0.1670
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 6901/50000 (13.8020%),                 avg. length: 558.6,                last time consumption/overall running time: 54.4393s / 19804.8468 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1122
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1345
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 6921/50000 (13.8420%),                 avg. length: 608.3,                last time consumption/overall running time: 58.6860s / 19863.5328 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0953
env0_second_0:                 episode reward: 2.9000,                 loss: 0.1456
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 6941/50000 (13.8820%),                 avg. length: 576.65,                last time consumption/overall running time: 56.3798s / 19919.9125 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1117
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1577
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 6961/50000 (13.9220%),                 avg. length: 570.65,                last time consumption/overall running time: 55.4933s / 19975.4058 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1763
env0_second_0:                 episode reward: 3.5500,                 loss: 0.2011
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 6981/50000 (13.9620%),                 avg. length: 636.75,                last time consumption/overall running time: 61.5308s / 20036.9366 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.1060
env0_second_0:                 episode reward: 2.9000,                 loss: 0.1425
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 7001/50000 (14.0020%),                 avg. length: 610.05,                last time consumption/overall running time: 59.2138s / 20096.1504 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1107
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1389
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 7021/50000 (14.0420%),                 avg. length: 569.2,                last time consumption/overall running time: 55.6952s / 20151.8457 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1399
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1622
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 7041/50000 (14.0820%),                 avg. length: 595.4,                last time consumption/overall running time: 58.0325s / 20209.8782 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1813
env0_second_0:                 episode reward: 3.2500,                 loss: 0.2051
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 7061/50000 (14.1220%),                 avg. length: 585.65,                last time consumption/overall running time: 56.7861s / 20266.6643 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1863
env0_second_0:                 episode reward: 3.6000,                 loss: 0.2070
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 7081/50000 (14.1620%),                 avg. length: 606.6,                last time consumption/overall running time: 58.0673s / 20324.7317 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1775
env0_second_0:                 episode reward: 3.3000,                 loss: 0.2022
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 7101/50000 (14.2020%),                 avg. length: 550.0,                last time consumption/overall running time: 53.6932s / 20378.4249 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1944
env0_second_0:                 episode reward: 3.1000,                 loss: 0.2231
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7121/50000 (14.2420%),                 avg. length: 593.6,                last time consumption/overall running time: 57.5011s / 20435.9260 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.1270
env0_second_0:                 episode reward: 4.0000,                 loss: 0.1471
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 7141/50000 (14.2820%),                 avg. length: 579.35,                last time consumption/overall running time: 56.4490s / 20492.3751 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.2044
env0_second_0:                 episode reward: 3.4500,                 loss: 0.2284
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 7161/50000 (14.3220%),                 avg. length: 627.75,                last time consumption/overall running time: 61.2023s / 20553.5773 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1896
env0_second_0:                 episode reward: 3.3500,                 loss: 0.2147
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7181/50000 (14.3620%),                 avg. length: 575.0,                last time consumption/overall running time: 56.7182s / 20610.2956 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1498
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1768
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 7201/50000 (14.4020%),                 avg. length: 539.5,                last time consumption/overall running time: 53.5169s / 20663.8124 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.1204
env0_second_0:                 episode reward: 4.1000,                 loss: 0.1366
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 7221/50000 (14.4420%),                 avg. length: 581.9,                last time consumption/overall running time: 56.5923s / 20720.4048 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1034
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1267
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 7241/50000 (14.4820%),                 avg. length: 551.05,                last time consumption/overall running time: 53.7093s / 20774.1141 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1218
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1463
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7261/50000 (14.5220%),                 avg. length: 552.2,                last time consumption/overall running time: 53.3384s / 20827.4524 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1371
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1667
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 7281/50000 (14.5620%),                 avg. length: 682.3,                last time consumption/overall running time: 64.4582s / 20891.9107 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1786
env0_second_0:                 episode reward: 3.1000,                 loss: 0.2179
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 7301/50000 (14.6020%),                 avg. length: 600.1,                last time consumption/overall running time: 57.7788s / 20949.6895 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1486
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1642
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7321/50000 (14.6420%),                 avg. length: 566.3,                last time consumption/overall running time: 54.4172s / 21004.1066 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.1554
env0_second_0:                 episode reward: 2.9000,                 loss: 0.1783
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 7341/50000 (14.6820%),                 avg. length: 587.05,                last time consumption/overall running time: 57.2467s / 21061.3534 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1267
env0_second_0:                 episode reward: 3.0000,                 loss: 0.1442
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 7361/50000 (14.7220%),                 avg. length: 634.75,                last time consumption/overall running time: 59.8674s / 21121.2207 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.1287
env0_second_0:                 episode reward: 4.0500,                 loss: 0.1870
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 7381/50000 (14.7620%),                 avg. length: 669.35,                last time consumption/overall running time: 63.1832s / 21184.4040 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0739
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0896
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 7401/50000 (14.8020%),                 avg. length: 587.95,                last time consumption/overall running time: 56.8309s / 21241.2349 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.1498
env0_second_0:                 episode reward: 2.5500,                 loss: 0.1721
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 7421/50000 (14.8420%),                 avg. length: 560.0,                last time consumption/overall running time: 54.7014s / 21295.9363 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1284
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1372
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 7441/50000 (14.8820%),                 avg. length: 613.2,                last time consumption/overall running time: 59.6952s / 21355.6314 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.1083
env0_second_0:                 episode reward: 3.8500,                 loss: 0.1178
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 7461/50000 (14.9220%),                 avg. length: 660.85,                last time consumption/overall running time: 63.3619s / 21418.9933 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1181
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1607
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 7481/50000 (14.9620%),                 avg. length: 627.55,                last time consumption/overall running time: 60.6174s / 21479.6107 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1681
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1847
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 7501/50000 (15.0020%),                 avg. length: 643.6,                last time consumption/overall running time: 62.4060s / 21542.0167 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1002
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1180
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 7521/50000 (15.0420%),                 avg. length: 583.9,                last time consumption/overall running time: 57.2397s / 21599.2565 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1118
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1363
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 7541/50000 (15.0820%),                 avg. length: 622.95,                last time consumption/overall running time: 59.7625s / 21659.0189 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.1326
env0_second_0:                 episode reward: 2.9500,                 loss: 0.1656
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 7561/50000 (15.1220%),                 avg. length: 662.45,                last time consumption/overall running time: 63.2478s / 21722.2668 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1287
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1595
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 7581/50000 (15.1620%),                 avg. length: 594.15,                last time consumption/overall running time: 57.4255s / 21779.6923 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1563
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1741
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7601/50000 (15.2020%),                 avg. length: 593.4,                last time consumption/overall running time: 57.6206s / 21837.3129 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.1163
env0_second_0:                 episode reward: 4.0500,                 loss: 0.1367
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 7621/50000 (15.2420%),                 avg. length: 670.3,                last time consumption/overall running time: 64.0448s / 21901.3578 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.1068
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1302
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7641/50000 (15.2820%),                 avg. length: 625.85,                last time consumption/overall running time: 59.3870s / 21960.7448 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.1971
env0_second_0:                 episode reward: 2.8500,                 loss: 0.2251
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 7661/50000 (15.3220%),                 avg. length: 651.8,                last time consumption/overall running time: 62.3348s / 22023.0796 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.1244
env0_second_0:                 episode reward: 4.1500,                 loss: 0.1358
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 7681/50000 (15.3620%),                 avg. length: 620.95,                last time consumption/overall running time: 59.6862s / 22082.7658 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.1428
env0_second_0:                 episode reward: 3.8500,                 loss: 0.1623
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7701/50000 (15.4020%),                 avg. length: 599.2,                last time consumption/overall running time: 57.8678s / 22140.6336 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.1311
env0_second_0:                 episode reward: 4.2000,                 loss: 0.1516
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 7721/50000 (15.4420%),                 avg. length: 642.4,                last time consumption/overall running time: 61.3006s / 22201.9342 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1262
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1514
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7741/50000 (15.4820%),                 avg. length: 586.1,                last time consumption/overall running time: 56.9569s / 22258.8912 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.1549
env0_second_0:                 episode reward: 3.6500,                 loss: 0.1728
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 7761/50000 (15.5220%),                 avg. length: 630.65,                last time consumption/overall running time: 60.5327s / 22319.4239 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1130
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1318
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 7781/50000 (15.5620%),                 avg. length: 601.2,                last time consumption/overall running time: 58.7957s / 22378.2196 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1154
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1378
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 7801/50000 (15.6020%),                 avg. length: 639.45,                last time consumption/overall running time: 62.1021s / 22440.3217 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1203
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1372
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 7821/50000 (15.6420%),                 avg. length: 616.0,                last time consumption/overall running time: 59.2943s / 22499.6159 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1359
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1509
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7841/50000 (15.6820%),                 avg. length: 614.05,                last time consumption/overall running time: 59.5162s / 22559.1322 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1383
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1564
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7861/50000 (15.7220%),                 avg. length: 585.95,                last time consumption/overall running time: 56.7650s / 22615.8972 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1746
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1929
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 7881/50000 (15.7620%),                 avg. length: 680.8,                last time consumption/overall running time: 64.6195s / 22680.5167 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.2582
env0_second_0:                 episode reward: 1.3500,                 loss: 0.2954
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 7901/50000 (15.8020%),                 avg. length: 647.4,                last time consumption/overall running time: 61.1573s / 22741.6740 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1464
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1557
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7921/50000 (15.8420%),                 avg. length: 655.6,                last time consumption/overall running time: 63.2828s / 22804.9568 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1680
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1928
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 7941/50000 (15.8820%),                 avg. length: 657.3,                last time consumption/overall running time: 63.5137s / 22868.4705 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1383
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1528
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 7961/50000 (15.9220%),                 avg. length: 657.55,                last time consumption/overall running time: 62.7401s / 22931.2106 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1326
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1571
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 7981/50000 (15.9620%),                 avg. length: 733.05,                last time consumption/overall running time: 68.7220s / 22999.9325 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.1057
env0_second_0:                 episode reward: 2.9500,                 loss: 0.1451
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 8001/50000 (16.0020%),                 avg. length: 713.7,                last time consumption/overall running time: 68.1102s / 23068.0428 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.3402
env0_second_0:                 episode reward: 0.1000,                 loss: 0.3964
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 8021/50000 (16.0420%),                 avg. length: 750.1,                last time consumption/overall running time: 69.9191s / 23137.9619 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.1903
env0_second_0:                 episode reward: 2.3500,                 loss: 0.2153
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 8041/50000 (16.0820%),                 avg. length: 657.0,                last time consumption/overall running time: 62.5568s / 23200.5186 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.2293
env0_second_0:                 episode reward: 1.9500,                 loss: 0.2448
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 8061/50000 (16.1220%),                 avg. length: 705.55,                last time consumption/overall running time: 66.2965s / 23266.8152 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.1702
env0_second_0:                 episode reward: 2.7500,                 loss: 0.1844
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 8081/50000 (16.1620%),                 avg. length: 658.8,                last time consumption/overall running time: 61.2876s / 23328.1027 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.1650
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1788
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 8101/50000 (16.2020%),                 avg. length: 654.65,                last time consumption/overall running time: 61.3058s / 23389.4086 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1950
env0_second_0:                 episode reward: 3.1000,                 loss: 0.2072
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 8121/50000 (16.2420%),                 avg. length: 661.95,                last time consumption/overall running time: 63.6667s / 23453.0753 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.1539
env0_second_0:                 episode reward: 3.0500,                 loss: 0.1858
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 8141/50000 (16.2820%),                 avg. length: 720.75,                last time consumption/overall running time: 68.6012s / 23521.6765 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1082
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1273
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 8161/50000 (16.3220%),                 avg. length: 626.45,                last time consumption/overall running time: 60.0696s / 23581.7461 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1206
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1429
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 8181/50000 (16.3620%),                 avg. length: 648.4,                last time consumption/overall running time: 62.0874s / 23643.8335 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.1442
env0_second_0:                 episode reward: 3.0500,                 loss: 0.1786
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 8201/50000 (16.4020%),                 avg. length: 641.2,                last time consumption/overall running time: 60.4343s / 23704.2678 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0964
env0_second_0:                 episode reward: 3.9500,                 loss: 0.1285
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 8221/50000 (16.4420%),                 avg. length: 649.85,                last time consumption/overall running time: 62.5400s / 23766.8078 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1183
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1517
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 8241/50000 (16.4820%),                 avg. length: 646.4,                last time consumption/overall running time: 62.2403s / 23829.0481 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1223
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1541
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 8261/50000 (16.5220%),                 avg. length: 590.9,                last time consumption/overall running time: 57.7151s / 23886.7632 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1382
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1683
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 8281/50000 (16.5620%),                 avg. length: 627.85,                last time consumption/overall running time: 60.6131s / 23947.3763 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.1703
env0_second_0:                 episode reward: 2.9500,                 loss: 0.1948
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 8301/50000 (16.6020%),                 avg. length: 683.25,                last time consumption/overall running time: 65.2412s / 24012.6175 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1053
env0_second_0:                 episode reward: 3.2500,                 loss: 0.1393
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 8321/50000 (16.6420%),                 avg. length: 699.4,                last time consumption/overall running time: 66.1487s / 24078.7662 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1298
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1395
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 8341/50000 (16.6820%),                 avg. length: 629.65,                last time consumption/overall running time: 60.7566s / 24139.5228 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0975
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1229
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 8361/50000 (16.7220%),                 avg. length: 729.0,                last time consumption/overall running time: 69.0480s / 24208.5707 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0913
env0_second_0:                 episode reward: 4.0500,                 loss: 0.1131
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 8381/50000 (16.7620%),                 avg. length: 681.8,                last time consumption/overall running time: 65.5508s / 24274.1215 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0897
env0_second_0:                 episode reward: 4.2000,                 loss: 0.1191
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 8401/50000 (16.8020%),                 avg. length: 723.6,                last time consumption/overall running time: 68.1248s / 24342.2463 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0826
env0_second_0:                 episode reward: 3.6500,                 loss: 0.1063
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 8421/50000 (16.8420%),                 avg. length: 624.5,                last time consumption/overall running time: 60.3036s / 24402.5499 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1037
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1267
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 8441/50000 (16.8820%),                 avg. length: 700.55,                last time consumption/overall running time: 65.9848s / 24468.5348 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0906
env0_second_0:                 episode reward: 4.0500,                 loss: 0.1099
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 8461/50000 (16.9220%),                 avg. length: 668.9,                last time consumption/overall running time: 64.0536s / 24532.5884 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1081
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1186
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 8481/50000 (16.9620%),                 avg. length: 679.5,                last time consumption/overall running time: 63.8606s / 24596.4489 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0746
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1009
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 8501/50000 (17.0020%),                 avg. length: 678.05,                last time consumption/overall running time: 63.4013s / 24659.8502 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1159
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1417
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 8521/50000 (17.0420%),                 avg. length: 706.0,                last time consumption/overall running time: 65.9840s / 24725.8342 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.1165
env0_second_0:                 episode reward: 2.9500,                 loss: 0.1328
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 8541/50000 (17.0820%),                 avg. length: 697.5,                last time consumption/overall running time: 65.6308s / 24791.4649 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1378
env0_second_0:                 episode reward: 3.2500,                 loss: 0.1455
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 8561/50000 (17.1220%),                 avg. length: 801.7,                last time consumption/overall running time: 73.7790s / 24865.2439 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0785
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1038
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 8581/50000 (17.1620%),                 avg. length: 817.65,                last time consumption/overall running time: 75.9814s / 24941.2253 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0702
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0800
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 8601/50000 (17.2020%),                 avg. length: 744.75,                last time consumption/overall running time: 70.5184s / 25011.7437 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.1044
env0_second_0:                 episode reward: 2.4500,                 loss: 0.1166
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 8621/50000 (17.2420%),                 avg. length: 813.25,                last time consumption/overall running time: 75.4493s / 25087.1930 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0504
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0763
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 8641/50000 (17.2820%),                 avg. length: 862.75,                last time consumption/overall running time: 79.6226s / 25166.8156 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0513
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0688
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 8661/50000 (17.3220%),                 avg. length: 920.3,                last time consumption/overall running time: 84.6612s / 25251.4767 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0459
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0651
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 8681/50000 (17.3620%),                 avg. length: 873.6,                last time consumption/overall running time: 80.9583s / 25332.4351 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0567
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0736
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 8701/50000 (17.4020%),                 avg. length: 898.65,                last time consumption/overall running time: 82.0542s / 25414.4893 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0827
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0924
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 8721/50000 (17.4420%),                 avg. length: 823.05,                last time consumption/overall running time: 76.4630s / 25490.9523 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0601
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0712
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 8741/50000 (17.4820%),                 avg. length: 863.2,                last time consumption/overall running time: 78.3273s / 25569.2796 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0485
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0646
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 8761/50000 (17.5220%),                 avg. length: 909.55,                last time consumption/overall running time: 82.2426s / 25651.5222 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0609
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0699
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 8781/50000 (17.5620%),                 avg. length: 841.0,                last time consumption/overall running time: 78.4983s / 25730.0205 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0681
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0906
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 8801/50000 (17.6020%),                 avg. length: 783.2,                last time consumption/overall running time: 72.7779s / 25802.7984 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0800
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0899
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 8821/50000 (17.6420%),                 avg. length: 894.95,                last time consumption/overall running time: 81.4565s / 25884.2549 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0256
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0488
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 8841/50000 (17.6820%),                 avg. length: 808.9,                last time consumption/overall running time: 75.5651s / 25959.8200 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0605
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0839
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 8861/50000 (17.7220%),                 avg. length: 801.4,                last time consumption/overall running time: 73.9506s / 26033.7705 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0757
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0984
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 8881/50000 (17.7620%),                 avg. length: 708.7,                last time consumption/overall running time: 66.2759s / 26100.0465 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0911
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1250
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 8901/50000 (17.8020%),                 avg. length: 835.5,                last time consumption/overall running time: 77.2221s / 26177.2686 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0238
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0498
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 8921/50000 (17.8420%),                 avg. length: 812.25,                last time consumption/overall running time: 75.3280s / 26252.5965 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0835
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1096
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 8941/50000 (17.8820%),                 avg. length: 725.85,                last time consumption/overall running time: 67.7192s / 26320.3157 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0836
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1079
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 8961/50000 (17.9220%),                 avg. length: 902.3,                last time consumption/overall running time: 82.1898s / 26402.5056 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0478
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0680
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 8981/50000 (17.9620%),                 avg. length: 827.4,                last time consumption/overall running time: 76.1278s / 26478.6334 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0738
env0_second_0:                 episode reward: 4.0500,                 loss: 0.0898
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 9001/50000 (18.0020%),                 avg. length: 871.75,                last time consumption/overall running time: 79.6222s / 26558.2556 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0655
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0981
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 9021/50000 (18.0420%),                 avg. length: 928.25,                last time consumption/overall running time: 85.3637s / 26643.6193 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0654
env0_second_0:                 episode reward: 2.5500,                 loss: 0.0893
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 9041/50000 (18.0820%),                 avg. length: 895.9,                last time consumption/overall running time: 82.0122s / 26725.6316 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0354
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0545
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 9061/50000 (18.1220%),                 avg. length: 927.1,                last time consumption/overall running time: 83.5158s / 26809.1474 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0403
env0_second_0:                 episode reward: 3.6000,                 loss: 0.0689
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 9081/50000 (18.1620%),                 avg. length: 1048.35,                last time consumption/overall running time: 93.9355s / 26903.0828 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0361
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0644
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 9101/50000 (18.2020%),                 avg. length: 1224.0,                last time consumption/overall running time: 110.5759s / 27013.6587 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.0080
env0_second_0:                 episode reward: 2.8500,                 loss: 0.0153
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 9121/50000 (18.2420%),                 avg. length: 1191.1,                last time consumption/overall running time: 105.6724s / 27119.3312 s
env0_first_0:                 episode reward: -3.9000,                 loss: -0.0023
env0_second_0:                 episode reward: 3.9000,                 loss: 0.0142
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 9141/50000 (18.2820%),                 avg. length: 1091.3,                last time consumption/overall running time: 97.9632s / 27217.2944 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.0026
env0_second_0:                 episode reward: 2.9000,                 loss: 0.0160
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 9161/50000 (18.3220%),                 avg. length: 1373.2,                last time consumption/overall running time: 119.4326s / 27336.7270 s
env0_first_0:                 episode reward: -3.7000,                 loss: -0.0247
env0_second_0:                 episode reward: 3.7000,                 loss: -0.0189
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 9181/50000 (18.3620%),                 avg. length: 1876.3,                last time consumption/overall running time: 161.1373s / 27497.8642 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.0637
env0_second_0:                 episode reward: 2.5500,                 loss: -0.0476
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 9201/50000 (18.4020%),                 avg. length: 2014.1,                last time consumption/overall running time: 172.2552s / 27670.1195 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.0673
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0559
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 9221/50000 (18.4420%),                 avg. length: 2285.5,                last time consumption/overall running time: 196.4248s / 27866.5443 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.1020
env0_second_0:                 episode reward: 2.8500,                 loss: -0.0898
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 9241/50000 (18.4820%),                 avg. length: 2253.15,                last time consumption/overall running time: 193.0699s / 28059.6142 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.0997
env0_second_0:                 episode reward: 2.6500,                 loss: -0.0812
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 9261/50000 (18.5220%),                 avg. length: 2631.8,                last time consumption/overall running time: 223.4913s / 28283.1055 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.1179
env0_second_0:                 episode reward: 1.4500,                 loss: -0.1011
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 9281/50000 (18.5620%),                 avg. length: 2908.65,                last time consumption/overall running time: 248.8217s / 28531.9272 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.1319
env0_second_0:                 episode reward: 1.4000,                 loss: -0.1062
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 9301/50000 (18.6020%),                 avg. length: 2944.95,                last time consumption/overall running time: 249.2877s / 28781.2149 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.1392
env0_second_0:                 episode reward: 1.1000,                 loss: -0.1211
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 9321/50000 (18.6420%),                 avg. length: 2801.2,                last time consumption/overall running time: 239.1619s / 29020.3767 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.1312
env0_second_0:                 episode reward: 1.0500,                 loss: -0.1044
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 9341/50000 (18.6820%),                 avg. length: 2924.55,                last time consumption/overall running time: 248.9556s / 29269.3324 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.1251
env0_second_0:                 episode reward: 0.8000,                 loss: -0.0981
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 9361/50000 (18.7220%),                 avg. length: 2952.05,                last time consumption/overall running time: 251.1590s / 29520.4913 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.1553
env0_second_0:                 episode reward: 0.6500,                 loss: -0.1195
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 9381/50000 (18.7620%),                 avg. length: 2997.55,                last time consumption/overall running time: 253.3498s / 29773.8411 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1684
env0_second_0:                 episode reward: 0.4500,                 loss: -0.1340
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 9401/50000 (18.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.8914s / 30027.7325 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.1782
env0_second_0:                 episode reward: 0.8000,                 loss: -0.1495
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 9421/50000 (18.8420%),                 avg. length: 2843.9,                last time consumption/overall running time: 241.3074s / 30269.0399 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1572
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1287
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 9441/50000 (18.8820%),                 avg. length: 2945.45,                last time consumption/overall running time: 252.6473s / 30521.6872 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1808
env0_second_0:                 episode reward: 0.5500,                 loss: -0.1411
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 9461/50000 (18.9220%),                 avg. length: 2901.6,                last time consumption/overall running time: 246.9721s / 30768.6593 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.1721
env0_second_0:                 episode reward: 0.6000,                 loss: -0.1460
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 9481/50000 (18.9620%),                 avg. length: 2959.0,                last time consumption/overall running time: 249.4426s / 31018.1019 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.1751
env0_second_0:                 episode reward: 0.9000,                 loss: -0.1526
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 9501/50000 (19.0020%),                 avg. length: 2930.85,                last time consumption/overall running time: 248.7166s / 31266.8185 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1776
env0_second_0:                 episode reward: 0.4500,                 loss: -0.1461
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 9521/50000 (19.0420%),                 avg. length: 2983.8,                last time consumption/overall running time: 250.3465s / 31517.1650 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1871
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1639
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 9541/50000 (19.0820%),                 avg. length: 2945.05,                last time consumption/overall running time: 249.2144s / 31766.3794 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1818
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1515
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 9561/50000 (19.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.3448s / 32019.7242 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1905
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1627
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 9581/50000 (19.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 251.5325s / 32271.2567 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.1719
env0_second_0:                 episode reward: 1.0500,                 loss: -0.1448
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 9601/50000 (19.2020%),                 avg. length: 2954.6,                last time consumption/overall running time: 250.3926s / 32521.6493 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1624
env0_second_0:                 episode reward: 0.5500,                 loss: -0.1270
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 9621/50000 (19.2420%),                 avg. length: 2947.2,                last time consumption/overall running time: 251.0670s / 32772.7163 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1843
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1485
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 9641/50000 (19.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 251.8274s / 33024.5436 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.1892
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1601
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9661/50000 (19.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 252.1132s / 33276.6568 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1874
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1580
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 9681/50000 (19.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.0259s / 33529.6827 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.1747
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1557
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 9701/50000 (19.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 251.7482s / 33781.4308 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1893
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1688
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9721/50000 (19.4420%),                 avg. length: 2925.85,                last time consumption/overall running time: 247.2691s / 34028.6999 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1765
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1419
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 9741/50000 (19.4820%),                 avg. length: 2927.05,                last time consumption/overall running time: 247.2907s / 34275.9906 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1782
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1500
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 9761/50000 (19.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.1872s / 34532.1778 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1833
env0_second_0:                 episode reward: 0.7000,                 loss: -0.1585
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 9781/50000 (19.5620%),                 avg. length: 2974.4,                last time consumption/overall running time: 253.8870s / 34786.0647 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.1899
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1596
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 9801/50000 (19.6020%),                 avg. length: 2908.35,                last time consumption/overall running time: 246.4383s / 35032.5031 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1802
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1441
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9821/50000 (19.6420%),                 avg. length: 2953.35,                last time consumption/overall running time: 251.0379s / 35283.5409 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.1840
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1511
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 9841/50000 (19.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.2078s / 35538.7487 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.1927
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1641
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 9861/50000 (19.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5628s / 35794.3115 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.1904
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1690
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 9881/50000 (19.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.8369s / 36049.1484 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.1874
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1622
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 9901/50000 (19.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.5478s / 36302.6962 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1906
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1617
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 9921/50000 (19.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.2859s / 36558.9822 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1906
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1535
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 9941/50000 (19.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1635s / 36816.1457 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1922
env0_second_0:                 episode reward: 0.4000,                 loss: -0.1577
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9961/50000 (19.9220%),                 avg. length: 2998.35,                last time consumption/overall running time: 254.8294s / 37070.9751 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1757
env0_second_0:                 episode reward: 0.4000,                 loss: -0.1434
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 9981/50000 (19.9620%),                 avg. length: 2983.5,                last time consumption/overall running time: 253.0471s / 37324.0222 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.1846
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1557
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10001/50000 (20.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.9701s / 37577.9923 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1910
env0_second_0:                 episode reward: 0.5500,                 loss: -0.1614
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 10021/50000 (20.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5293s / 37833.5216 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.2007
env0_second_0:                 episode reward: 0.6500,                 loss: -0.1802
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 10041/50000 (20.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.8829s / 38089.4045 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2036
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1727
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 10061/50000 (20.1220%),                 avg. length: 2991.7,                last time consumption/overall running time: 254.8149s / 38344.2194 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1861
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1560
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 10081/50000 (20.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.1460s / 38600.3654 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1967
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1691
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 10101/50000 (20.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0656s / 38857.4309 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2021
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1722
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 10121/50000 (20.2420%),                 avg. length: 2983.1,                last time consumption/overall running time: 252.6979s / 39110.1288 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1861
env0_second_0:                 episode reward: 0.4500,                 loss: -0.1591
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 10141/50000 (20.2820%),                 avg. length: 2967.8,                last time consumption/overall running time: 253.1934s / 39363.3222 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1652
env0_second_0:                 episode reward: -0.7500,                 loss: -0.1350
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 10161/50000 (20.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.1810s / 39617.5031 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2074
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1753
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 10181/50000 (20.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.1679s / 39871.6710 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.1973
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1803
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10201/50000 (20.4020%),                 avg. length: 2966.75,                last time consumption/overall running time: 253.4112s / 40125.0823 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.1940
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1739
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 10221/50000 (20.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.6335s / 40379.7158 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2010
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1769
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 10241/50000 (20.4820%),                 avg. length: 2747.65,                last time consumption/overall running time: 232.2498s / 40611.9655 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1799
env0_second_0:                 episode reward: -0.7500,                 loss: -0.1459
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 10261/50000 (20.5220%),                 avg. length: 2998.35,                last time consumption/overall running time: 255.9636s / 40867.9292 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.2032
env0_second_0:                 episode reward: 0.6000,                 loss: -0.1727
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 10281/50000 (20.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.2386s / 41124.1677 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2140
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1873
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 10301/50000 (20.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.7580s / 41377.9257 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2194
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1851
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 10321/50000 (20.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.8248s / 41632.7505 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2215
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1852
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 10341/50000 (20.6820%),                 avg. length: 2820.65,                last time consumption/overall running time: 241.1452s / 41873.8957 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1479
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1156
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 10361/50000 (20.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5389s / 42130.4345 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2188
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1834
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 10381/50000 (20.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.9342s / 42384.3687 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2218
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1887
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 10401/50000 (20.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.5822s / 42638.9509 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2234
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1952
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 10421/50000 (20.8420%),                 avg. length: 2955.2,                last time consumption/overall running time: 252.3824s / 42891.3333 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2067
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1807
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 10441/50000 (20.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.7072s / 43146.0405 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2150
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1874
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 10461/50000 (20.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4839s / 43402.5243 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2102
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1920
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 10481/50000 (20.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.9698s / 43657.4941 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2021
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1762
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 10501/50000 (21.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5455s / 43913.0395 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2006
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1766
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10521/50000 (21.0420%),                 avg. length: 2958.1,                last time consumption/overall running time: 252.0905s / 44165.1301 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.1977
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1779
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 10541/50000 (21.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5219s / 44421.6520 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2265
env0_second_0:                 episode reward: -0.4000,                 loss: -0.2024
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 10561/50000 (21.1220%),                 avg. length: 2946.65,                last time consumption/overall running time: 249.2985s / 44670.9504 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2034
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1787
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 10581/50000 (21.1620%),                 avg. length: 2971.0,                last time consumption/overall running time: 253.4881s / 44924.4386 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.1949
env0_second_0:                 episode reward: 1.0000,                 loss: -0.1569
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 10601/50000 (21.2020%),                 avg. length: 2941.2,                last time consumption/overall running time: 249.2350s / 45173.6736 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1711
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1400
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 10621/50000 (21.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.4269s / 45427.1005 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1965
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1717
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10641/50000 (21.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.6134s / 45682.7139 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2014
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1763
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10661/50000 (21.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.9775s / 45937.6915 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2249
env0_second_0:                 episode reward: 0.3000,                 loss: -0.2010
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 10681/50000 (21.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.0901s / 46193.7815 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2122
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1918
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 10701/50000 (21.4020%),                 avg. length: 2930.6,                last time consumption/overall running time: 250.4807s / 46444.2622 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1748
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1450
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 10721/50000 (21.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6004s / 46701.8625 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2197
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1967
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 10741/50000 (21.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8152s / 46958.6777 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.2130
env0_second_0:                 episode reward: 0.6500,                 loss: -0.1763
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 10761/50000 (21.5220%),                 avg. length: 2978.05,                last time consumption/overall running time: 253.4931s / 47212.1709 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2058
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1800
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 10781/50000 (21.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3740s / 47468.5449 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2128
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1857
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 10801/50000 (21.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.2310s / 47724.7759 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1977
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1744
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 10821/50000 (21.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8599s / 47982.6357 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2042
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1728
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10841/50000 (21.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0928s / 48239.7285 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2094
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1769
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 10861/50000 (21.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.1698s / 48494.8983 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2002
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1808
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 10881/50000 (21.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.9670s / 48750.8653 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2173
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1893
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 10901/50000 (21.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5154s / 49008.3807 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2323
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2052
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 10921/50000 (21.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.5342s / 49262.9149 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2203
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1848
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 10941/50000 (21.8820%),                 avg. length: 2982.9,                last time consumption/overall running time: 256.0038s / 49518.9187 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2130
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1878
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 10961/50000 (21.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.2029s / 49774.1216 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2002
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1826
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10981/50000 (21.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5198s / 50029.6414 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2106
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1793
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 11001/50000 (22.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.4042s / 50284.0455 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2104
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1840
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11021/50000 (22.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4684s / 50540.5139 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2208
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1796
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11041/50000 (22.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.4015s / 50801.9155 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2182
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1929
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 11061/50000 (22.1220%),                 avg. length: 2922.35,                last time consumption/overall running time: 250.4575s / 51052.3730 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.1913
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1680
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 11081/50000 (22.1620%),                 avg. length: 2997.15,                last time consumption/overall running time: 257.7956s / 51310.1686 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2060
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1815
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 11101/50000 (22.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5854s / 51567.7540 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2227
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1949
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 11121/50000 (22.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.3794s / 51823.1334 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2190
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1955
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 11141/50000 (22.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.8839s / 52078.0174 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2118
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1892
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 11161/50000 (22.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.1600s / 52333.1773 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.2285
env0_second_0:                 episode reward: 0.4000,                 loss: -0.1995
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 11181/50000 (22.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9949s / 52592.1723 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2146
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1927
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 11201/50000 (22.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.6860s / 52847.8583 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2228
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2016
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 11221/50000 (22.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.4991s / 53105.3574 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2230
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2008
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11241/50000 (22.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1310s / 53363.4884 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2070
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1808
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 11261/50000 (22.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7151s / 53622.2035 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2274
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2028
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 11281/50000 (22.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.7722s / 53875.9757 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2287
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2027
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 11301/50000 (22.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5982s / 54131.5739 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2260
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2022
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 11321/50000 (22.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.7825s / 54387.3564 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2142
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1955
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 11341/50000 (22.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.6716s / 54644.0281 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2240
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1910
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 11361/50000 (22.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5394s / 54899.5675 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2208
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1980
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 11381/50000 (22.7620%),                 avg. length: 2966.85,                last time consumption/overall running time: 256.2789s / 55155.8464 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2040
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1797
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 11401/50000 (22.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2384s / 55413.0848 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2221
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1960
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11421/50000 (22.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3680s / 55671.4528 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2297
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2005
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 11441/50000 (22.8820%),                 avg. length: 2980.45,                last time consumption/overall running time: 254.5518s / 55926.0046 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2103
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1881
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 11461/50000 (22.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1266s / 56183.1311 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2215
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1940
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 11481/50000 (22.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.9414s / 56439.0725 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2316
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2093
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 11501/50000 (23.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.7620s / 56694.8345 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2373
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2003
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 11521/50000 (23.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1813s / 56953.0158 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2287
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2116
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11541/50000 (23.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5563s / 57208.5721 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2251
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1993
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 11561/50000 (23.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6484s / 57466.2205 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2320
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2103
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 11581/50000 (23.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.6732s / 57721.8936 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.2229
env0_second_0:                 episode reward: 0.4000,                 loss: -0.1922
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 11601/50000 (23.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4705s / 57978.3641 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2366
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2003
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 11621/50000 (23.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5821s / 58233.9463 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.2239
env0_second_0:                 episode reward: 0.6000,                 loss: -0.2046
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 11641/50000 (23.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.8541s / 58489.8004 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2293
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1882
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 11661/50000 (23.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.8817s / 58744.6820 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2336
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2056
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 11681/50000 (23.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.1020s / 58999.7840 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2233
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2071
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11701/50000 (23.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.1575s / 59252.9415 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.2320
env0_second_0:                 episode reward: 0.4500,                 loss: -0.2082
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 11721/50000 (23.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.7329s / 59509.6744 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2356
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2050
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 11741/50000 (23.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.7116s / 59763.3859 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2254
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1943
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 11761/50000 (23.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5877s / 60018.9736 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2259
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2005
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 11781/50000 (23.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5362s / 60274.5098 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2455
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2115
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11801/50000 (23.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.9688s / 60529.4787 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2297
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2070
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 11821/50000 (23.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.9355s / 60786.4141 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2275
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2051
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 11841/50000 (23.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0033s / 61043.4174 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2304
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2082
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 11861/50000 (23.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5743s / 61298.9917 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2249
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2002
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 11881/50000 (23.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.9618s / 61555.9536 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2221
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1919
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 11901/50000 (23.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9947s / 61813.9483 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2254
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1916
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 11921/50000 (23.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6636s / 62071.6119 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2038
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1777
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 11941/50000 (23.8820%),                 avg. length: 2951.3,                last time consumption/overall running time: 254.3585s / 62325.9704 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2225
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1940
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11961/50000 (23.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0352s / 62583.0056 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2216
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1876
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11981/50000 (23.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1691s / 62840.1747 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2277
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2001
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 12001/50000 (24.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0651s / 63098.2397 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2222
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1934
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 12021/50000 (24.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.2581s / 63354.4979 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2326
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1819
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 12041/50000 (24.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.6626s / 63610.1605 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2310
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2008
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 12061/50000 (24.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.1217s / 63864.2823 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2383
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2055
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 12081/50000 (24.1620%),                 avg. length: 2997.05,                last time consumption/overall running time: 256.0143s / 64120.2966 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2117
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1809
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 12101/50000 (24.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4793s / 64376.7759 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2135
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1794
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 12121/50000 (24.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.9610s / 64631.7369 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2196
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1943
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 12141/50000 (24.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.9814s / 64887.7184 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2290
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2025
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 12161/50000 (24.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.4960s / 65141.2144 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2177
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1949
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 12181/50000 (24.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.2134s / 65396.4278 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2233
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1927
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 12201/50000 (24.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 252.9875s / 65649.4154 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2339
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2030
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 12221/50000 (24.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 252.1804s / 65901.5958 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2270
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1887
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 12241/50000 (24.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.5284s / 66155.1241 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2198
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1941
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 12261/50000 (24.5220%),                 avg. length: 2945.75,                last time consumption/overall running time: 250.1414s / 66405.2655 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2190
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1899
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 12281/50000 (24.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5732s / 66662.8387 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2256
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1865
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 12301/50000 (24.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7322s / 66920.5709 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2109
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1872
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 12321/50000 (24.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1850s / 67178.7559 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2082
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1912
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 12341/50000 (24.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8079s / 67436.5639 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2146
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1880
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 12361/50000 (24.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.6018s / 67691.1656 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2165
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1919
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 12381/50000 (24.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.0318s / 67947.1974 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2161
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1834
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 12401/50000 (24.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3188s / 68203.5162 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2234
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1960
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 12421/50000 (24.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4008s / 68459.9170 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2219
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1990
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 12441/50000 (24.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.8176s / 68715.7346 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2290
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1920
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 12461/50000 (24.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.0591s / 68971.7937 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2349
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2083
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 12481/50000 (24.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.0319s / 69224.8256 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2252
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2011
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 12501/50000 (25.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9444s / 69483.7700 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2283
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1936
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 12521/50000 (25.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.6653s / 69740.4354 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2377
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2022
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 12541/50000 (25.0820%),                 avg. length: 2923.25,                last time consumption/overall running time: 246.4576s / 69986.8930 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2161
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1901
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 12561/50000 (25.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.6057s / 70240.4986 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2326
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1968
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 12581/50000 (25.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.6106s / 70496.1093 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2267
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2032
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 12601/50000 (25.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.8002s / 70751.9095 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2208
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1917
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 12621/50000 (25.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.3986s / 71005.3081 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2223
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1994
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 12641/50000 (25.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.6247s / 71261.9328 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2211
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1985
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 12661/50000 (25.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.4259s / 71517.3587 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2287
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2081
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 12681/50000 (25.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.2267s / 71770.5855 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2210
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1883
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 12701/50000 (25.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.9396s / 72025.5251 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2381
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1999
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 12721/50000 (25.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.4955s / 72280.0206 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2344
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2122
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 12741/50000 (25.4820%),                 avg. length: 2964.75,                last time consumption/overall running time: 251.6111s / 72531.6317 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2047
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1760
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 12761/50000 (25.5220%),                 avg. length: 2944.2,                last time consumption/overall running time: 252.1594s / 72783.7911 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2046
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1763
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 12781/50000 (25.5620%),                 avg. length: 2931.5,                last time consumption/overall running time: 251.1989s / 73034.9900 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2005
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1691
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 12801/50000 (25.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.9258s / 73288.9157 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2169
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1902
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 12821/50000 (25.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.5596s / 73543.4753 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2105
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1810
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 12841/50000 (25.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.2942s / 73798.7695 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2146
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1845
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 12861/50000 (25.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.9083s / 74054.6778 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2216
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1910
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 12881/50000 (25.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7591s / 74312.4369 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2189
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1750
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 12901/50000 (25.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.8545s / 74568.2914 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2176
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1934
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 12921/50000 (25.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4169s / 74824.7083 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.1981
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1583
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 12941/50000 (25.8820%),                 avg. length: 2951.1,                last time consumption/overall running time: 251.1769s / 75075.8851 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2096
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1837
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 12961/50000 (25.9220%),                 avg. length: 2933.0,                last time consumption/overall running time: 250.8764s / 75326.7615 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.1775
env0_second_0:                 episode reward: -0.9500,                 loss: -0.1542
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 12981/50000 (25.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.3071s / 75582.0686 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2068
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1855
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 13001/50000 (26.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.0334s / 75835.1020 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2134
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1896
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 13021/50000 (26.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4102s / 76091.5123 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2256
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1962
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 13041/50000 (26.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 251.4919s / 76343.0042 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2226
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1981
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 13061/50000 (26.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5779s / 76599.5821 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2329
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2064
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 13081/50000 (26.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.8753s / 76855.4574 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2242
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1988
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 13101/50000 (26.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.3916s / 77110.8490 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2205
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1903
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 13121/50000 (26.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6121s / 77370.4611 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2268
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1909
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 13141/50000 (26.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.3235s / 77625.7846 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2198
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1940
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 13161/50000 (26.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5219s / 77882.3065 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2182
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1944
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 13181/50000 (26.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.9999s / 78139.3064 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2206
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1903
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 13201/50000 (26.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.4248s / 78393.7312 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2238
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2016
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 13221/50000 (26.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 252.6158s / 78646.3470 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2281
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1994
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 13241/50000 (26.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 252.5599s / 78898.9069 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2206
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1985
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 13261/50000 (26.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.4903s / 79154.3973 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2186
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1963
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 13281/50000 (26.5620%),                 avg. length: 2878.85,                last time consumption/overall running time: 246.4475s / 79400.8448 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1921
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1510
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 13301/50000 (26.6020%),                 avg. length: 2615.75,                last time consumption/overall running time: 225.0712s / 79625.9160 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1284
env0_second_0:                 episode reward: -0.9000,                 loss: -0.0608
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 13321/50000 (26.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.6281s / 79881.5441 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2027
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1615
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 13341/50000 (26.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.1265s / 80136.6706 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2076
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1611
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 13361/50000 (26.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.9642s / 80392.6349 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2114
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1635
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 13381/50000 (26.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6509s / 80651.2857 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2123
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1691
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 13401/50000 (26.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4400s / 80909.7257 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2177
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1610
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 13421/50000 (26.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2109s / 81166.9367 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2133
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1633
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 13441/50000 (26.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.7276s / 81423.6643 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2135
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1772
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 13461/50000 (26.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7680s / 81683.4323 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2276
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1900
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 13481/50000 (26.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.7404s / 81939.1727 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2108
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1776
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 13501/50000 (27.0020%),                 avg. length: 2794.0,                last time consumption/overall running time: 238.5889s / 82177.7615 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1770
env0_second_0:                 episode reward: 1.3000,                 loss: -0.1364
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 13521/50000 (27.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2043s / 82435.9659 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2132
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1787
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 13541/50000 (27.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2712s / 82694.2371 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2119
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1768
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 13561/50000 (27.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9799s / 82953.2169 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2195
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1791
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 13581/50000 (27.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3360s / 83209.5530 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2088
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1821
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 13601/50000 (27.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6126s / 83468.1656 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2069
env0_second_0:                 episode reward: -0.6500,                 loss: -0.1791
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 13621/50000 (27.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.7669s / 83723.9325 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2090
env0_second_0:                 episode reward: -1.0500,                 loss: -0.1791
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 13641/50000 (27.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.9991s / 83980.9316 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2061
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1818
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 13661/50000 (27.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5305s / 84238.4622 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2079
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1833
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 13681/50000 (27.3620%),                 avg. length: 2951.95,                last time consumption/overall running time: 251.4183s / 84489.8805 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1923
env0_second_0:                 episode reward: -0.9000,                 loss: -0.1726
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 13701/50000 (27.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3603s / 84747.2408 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2068
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1827
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 13721/50000 (27.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0205s / 85005.2613 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.2067
env0_second_0:                 episode reward: -0.6000,                 loss: -0.1818
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 13741/50000 (27.4820%),                 avg. length: 2863.8,                last time consumption/overall running time: 245.5755s / 85250.8368 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1814
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1547
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 13761/50000 (27.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1469s / 85507.9837 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.2069
env0_second_0:                 episode reward: -0.9000,                 loss: -0.1807
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 13781/50000 (27.5620%),                 avg. length: 2904.35,                last time consumption/overall running time: 248.1130s / 85756.0967 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1954
env0_second_0:                 episode reward: -0.7500,                 loss: -0.1700
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 13801/50000 (27.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3563s / 86013.4529 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2222
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1885
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 13821/50000 (27.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8176s / 86270.2705 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2177
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1879
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 13841/50000 (27.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3489s / 86526.6194 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2146
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1802
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 13861/50000 (27.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5881s / 86784.2075 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2136
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1903
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 13881/50000 (27.7620%),                 avg. length: 2948.3,                last time consumption/overall running time: 252.5654s / 87036.7730 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1972
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1640
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 13901/50000 (27.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8587s / 87295.6317 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2157
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1761
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 13921/50000 (27.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7783s / 87553.4100 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2208
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1891
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 13941/50000 (27.8820%),                 avg. length: 2944.1,                last time consumption/overall running time: 254.0299s / 87807.4399 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2068
env0_second_0:                 episode reward: -0.8500,                 loss: -0.1832
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 13961/50000 (27.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1068s / 88064.5466 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2190
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1893
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 13981/50000 (27.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9027s / 88322.4494 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2143
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1818
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 14001/50000 (28.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2940s / 88580.7434 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2143
env0_second_0:                 episode reward: -0.6500,                 loss: -0.1926
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14021/50000 (28.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0516s / 88837.7949 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2217
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1896
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 14041/50000 (28.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.1867s / 89092.9816 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2199
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1886
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 14061/50000 (28.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8135s / 89350.7951 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2228
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1881
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14081/50000 (28.1620%),                 avg. length: 2990.95,                last time consumption/overall running time: 256.4786s / 89607.2738 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2209
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1747
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 14101/50000 (28.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6140s / 89866.8878 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2231
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1900
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 14121/50000 (28.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7949s / 90127.6827 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2248
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1977
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 14141/50000 (28.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3684s / 90385.0511 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2337
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1723
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 14161/50000 (28.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2592s / 90642.3103 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2211
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1832
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 14181/50000 (28.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0548s / 90899.3651 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1969
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1660
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14201/50000 (28.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3607s / 91156.7258 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2156
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1865
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 14221/50000 (28.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2303s / 91415.9561 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2216
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1880
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 14241/50000 (28.4820%),                 avg. length: 2977.2,                last time consumption/overall running time: 255.7613s / 91671.7174 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.1816
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1486
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 14261/50000 (28.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5737s / 91928.2911 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2111
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1879
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14281/50000 (28.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7296s / 92188.0207 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2165
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1898
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 14301/50000 (28.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1769s / 92445.1975 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2521
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1945
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 14321/50000 (28.6420%),                 avg. length: 1815.05,                last time consumption/overall running time: 157.8272s / 92603.0247 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.0709
env0_second_0:                 episode reward: -2.6000,                 loss: -0.0275
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 14341/50000 (28.6820%),                 avg. length: 2810.65,                last time consumption/overall running time: 241.6472s / 92844.6719 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1622
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1283
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14361/50000 (28.7220%),                 avg. length: 2990.2,                last time consumption/overall running time: 257.8499s / 93102.5219 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2119
env0_second_0:                 episode reward: -1.1500,                 loss: -0.1857
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 14381/50000 (28.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9238s / 93360.4457 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2195
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1826
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 14401/50000 (28.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2081s / 93619.6537 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2289
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1641
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 14421/50000 (28.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7121s / 93878.3658 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2257
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1848
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14441/50000 (28.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.7061s / 94134.0719 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2278
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1817
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 14461/50000 (28.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2406s / 94391.3125 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2389
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1695
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14481/50000 (28.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5446s / 94647.8571 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2142
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1838
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 14501/50000 (29.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.6745s / 94904.5316 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2161
env0_second_0:                 episode reward: -0.6500,                 loss: -0.1805
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 14521/50000 (29.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.8577s / 95165.3893 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2206
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1758
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 14541/50000 (29.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2411s / 95422.6304 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2132
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1771
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 14561/50000 (29.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3501s / 95679.9805 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2217
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1931
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 14581/50000 (29.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.9157s / 95935.8962 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2323
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1996
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 14601/50000 (29.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.2928s / 96192.1891 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2332
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1943
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 14621/50000 (29.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.4476s / 96449.6367 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2263
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1958
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 14641/50000 (29.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8788s / 96707.5155 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2290
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1992
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 14661/50000 (29.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.7919s / 96964.3074 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2288
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1980
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 14681/50000 (29.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7334s / 97223.0408 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2338
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2066
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14701/50000 (29.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1529s / 97480.1937 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2294
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2071
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 14721/50000 (29.4420%),                 avg. length: 2924.05,                last time consumption/overall running time: 252.8420s / 97733.0357 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2053
env0_second_0:                 episode reward: -1.0500,                 loss: -0.1802
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 14741/50000 (29.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3809s / 97991.4166 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2265
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1940
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 14761/50000 (29.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2422s / 98249.6588 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2302
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2008
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 14781/50000 (29.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3875s / 98509.0463 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2083
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1952
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 14801/50000 (29.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0247s / 98769.0710 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2218
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1864
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 14821/50000 (29.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9967s / 99028.0677 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2080
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1023
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 14841/50000 (29.6820%),                 avg. length: 2964.5,                last time consumption/overall running time: 257.3931s / 99285.4609 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2009
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1484
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 14861/50000 (29.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5613s / 99544.0222 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2265
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1936
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 14881/50000 (29.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9263s / 99802.9485 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2263
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1785
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 14901/50000 (29.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9473s / 100063.8958 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2218
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1676
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 14921/50000 (29.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3397s / 100323.2356 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2298
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1977
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 14941/50000 (29.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4809s / 100583.7165 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2206
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1945
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 14961/50000 (29.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4142s / 100843.1307 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2361
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2008
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 14981/50000 (29.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5518s / 101102.6825 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2395
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2073
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15001/50000 (30.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8752s / 101361.5577 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2339
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1924
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 15021/50000 (30.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6288s / 101622.1865 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2343
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1878
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15041/50000 (30.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9084s / 101883.0950 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2165
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1878
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 15061/50000 (30.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0152s / 102142.1102 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2309
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1911
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 15081/50000 (30.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1230s / 102402.2331 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2250
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1729
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 15101/50000 (30.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8879s / 102662.1210 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2260
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1794
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 15121/50000 (30.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0877s / 102921.2087 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2288
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1865
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15141/50000 (30.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7105s / 103178.9193 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2297
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1891
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 15161/50000 (30.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5344s / 103438.4536 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2232
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2023
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15181/50000 (30.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8418s / 103697.2955 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2305
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1860
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 15201/50000 (30.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8842s / 103956.1797 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2221
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1596
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 15221/50000 (30.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4052s / 104216.5849 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2249
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1944
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15241/50000 (30.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9972s / 104477.5821 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2244
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1846
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 15261/50000 (30.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3668s / 104736.9488 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2348
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1986
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 15281/50000 (30.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0469s / 104996.9957 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2358
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1909
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15301/50000 (30.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.5226s / 105258.5183 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2354
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1940
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 15321/50000 (30.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4922s / 105518.0105 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2221
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1969
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15341/50000 (30.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9985s / 105776.0090 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2175
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1911
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 15361/50000 (30.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6594s / 106035.6684 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2253
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1840
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 15381/50000 (30.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.8820s / 106297.5504 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2313
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1931
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15401/50000 (30.8020%),                 avg. length: 2460.7,                last time consumption/overall running time: 215.2118s / 106512.7622 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1562
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0947
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 15421/50000 (30.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.0210s / 106773.7832 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2247
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1960
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 15441/50000 (30.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7608s / 107033.5440 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2352
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2003
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 15461/50000 (30.9220%),                 avg. length: 2951.85,                last time consumption/overall running time: 256.6048s / 107290.1487 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.2236
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1397
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 15481/50000 (30.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.3833s / 107551.5321 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2386
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1942
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 15501/50000 (31.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4195s / 107810.9516 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2328
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1926
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 15521/50000 (31.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.8320s / 108071.7836 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2332
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1845
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 15541/50000 (31.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9204s / 108331.7041 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2281
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1803
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 15561/50000 (31.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8083s / 108591.5124 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2273
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1974
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 15581/50000 (31.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9057s / 108851.4181 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2312
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1981
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 15601/50000 (31.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3669s / 109111.7850 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2226
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1776
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 15621/50000 (31.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4420s / 109372.2269 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.2197
env0_second_0:                 episode reward: -0.6000,                 loss: -0.1884
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 15641/50000 (31.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9932s / 109630.2201 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2260
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2046
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 15661/50000 (31.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1650s / 109889.3851 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2307
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1909
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 15681/50000 (31.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2339s / 110147.6190 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2325
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1935
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15701/50000 (31.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1985s / 110406.8175 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2272
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1864
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15721/50000 (31.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6770s / 110664.4945 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2330
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2000
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15741/50000 (31.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6460s / 110924.1404 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2272
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1896
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 15761/50000 (31.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2041s / 111182.3445 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2344
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1810
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 15781/50000 (31.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2456s / 111440.5902 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2347
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2046
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15801/50000 (31.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1410s / 111699.7312 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2360
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2046
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15821/50000 (31.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2154s / 111959.9466 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2363
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2077
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 15841/50000 (31.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5180s / 112220.4646 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2292
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2027
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 15861/50000 (31.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2466s / 112478.7112 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2307
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1784
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 15881/50000 (31.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2411s / 112737.9524 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2215
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1768
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 15901/50000 (31.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7372s / 112997.6896 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2337
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2010
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15921/50000 (31.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1865s / 113256.8761 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2341
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1877
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15941/50000 (31.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8368s / 113515.7129 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2330
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2070
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15961/50000 (31.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5566s / 113775.2695 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2293
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1969
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 15981/50000 (31.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.1940s / 114036.4635 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2330
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1966
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 16001/50000 (32.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5112s / 114294.9747 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2331
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2089
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 16021/50000 (32.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2285s / 114554.2032 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2242
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1828
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 16041/50000 (32.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1363s / 114813.3395 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2250
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1935
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 16061/50000 (32.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5371s / 115069.8766 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2290
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1902
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 16081/50000 (32.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0615s / 115329.9381 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2324
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1677
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 16101/50000 (32.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5710s / 115589.5091 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2313
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2002
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 16121/50000 (32.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9385s / 115848.4476 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2326
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2034
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 16141/50000 (32.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9125s / 116106.3601 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2337
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1879
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16161/50000 (32.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5204s / 116364.8806 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2276
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1957
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 16181/50000 (32.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6242s / 116622.5048 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2283
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1927
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 16201/50000 (32.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0505s / 116882.5553 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2194
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1849
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 16221/50000 (32.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9812s / 117142.5365 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2300
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1881
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16241/50000 (32.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3018s / 117400.8383 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2179
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1835
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 16261/50000 (32.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6672s / 117658.5055 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2255
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1520
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 16281/50000 (32.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5376s / 117916.0431 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2233
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1739
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 16301/50000 (32.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0118s / 118176.0549 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2271
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1984
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 16321/50000 (32.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3300s / 118436.3849 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2049
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1797
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 16341/50000 (32.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8459s / 118694.2308 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2215
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1894
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 16361/50000 (32.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5589s / 118951.7898 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2213
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1902
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16381/50000 (32.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6225s / 119211.4123 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2242
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2022
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16401/50000 (32.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0892s / 119470.5015 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2190
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1925
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 16421/50000 (32.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4611s / 119729.9626 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2253
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1945
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 16441/50000 (32.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7999s / 119989.7624 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2175
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1765
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16461/50000 (32.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7722s / 120248.5346 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2320
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1859
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 16481/50000 (32.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8487s / 120507.3834 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2357
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1893
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 16501/50000 (33.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2231s / 120766.6065 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2211
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1815
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 16521/50000 (33.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5372s / 121026.1437 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2207
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1667
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 16541/50000 (33.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4773s / 121284.6210 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2274
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1945
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 16561/50000 (33.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9814s / 121544.6024 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2272
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1984
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 16581/50000 (33.1620%),                 avg. length: 2936.5,                last time consumption/overall running time: 251.3987s / 121796.0011 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2102
env0_second_0:                 episode reward: -0.6500,                 loss: -0.1584
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 16601/50000 (33.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1041s / 122055.1053 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2237
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1909
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 16621/50000 (33.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4710s / 122313.5763 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2082
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1818
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 16641/50000 (33.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5886s / 122574.1648 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2105
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1895
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16661/50000 (33.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3032s / 122833.4680 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2257
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1904
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 16681/50000 (33.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9903s / 123091.4583 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2240
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1879
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 16701/50000 (33.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8322s / 123348.2905 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2286
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2019
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 16721/50000 (33.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5073s / 123603.7979 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2263
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1983
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16741/50000 (33.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0150s / 123861.8128 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2171
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1860
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16761/50000 (33.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5611s / 124120.3740 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2249
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1974
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 16781/50000 (33.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8427s / 124378.2167 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2319
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1945
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 16801/50000 (33.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6716s / 124636.8883 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2357
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2104
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 16821/50000 (33.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7691s / 124895.6574 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2252
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1849
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16841/50000 (33.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3574s / 125153.0148 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2197
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1855
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 16861/50000 (33.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.3784s / 125414.3932 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2229
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1912
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 16881/50000 (33.7620%),                 avg. length: 2928.45,                last time consumption/overall running time: 253.2654s / 125667.6586 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2076
env0_second_0:                 episode reward: -1.0500,                 loss: -0.1709
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 16901/50000 (33.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6858s / 125926.3444 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2247
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2036
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16921/50000 (33.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6129s / 126186.9573 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2073
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1823
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 16941/50000 (33.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5419s / 126446.4992 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2215
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1899
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 16961/50000 (33.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4587s / 126705.9580 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2336
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1875
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 16981/50000 (33.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4492s / 126965.4072 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2213
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1968
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 17001/50000 (34.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0650s / 127225.4722 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2298
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2057
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 17021/50000 (34.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7259s / 127485.1981 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2253
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2009
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 17041/50000 (34.0820%),                 avg. length: 2957.9,                last time consumption/overall running time: 256.6756s / 127741.8737 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2219
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1654
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 17061/50000 (34.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2542s / 128002.1280 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2266
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1940
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 17081/50000 (34.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5654s / 128261.6934 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2298
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1984
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 17101/50000 (34.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2012s / 128520.8946 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2359
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1996
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 17121/50000 (34.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6307s / 128779.5253 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2257
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1978
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 17141/50000 (34.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4438s / 129037.9692 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2253
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2022
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 17161/50000 (34.3220%),                 avg. length: 2739.55,                last time consumption/overall running time: 237.6923s / 129275.6615 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2045
env0_second_0:                 episode reward: -0.8500,                 loss: -0.1551
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 17181/50000 (34.3620%),                 avg. length: 2811.7,                last time consumption/overall running time: 245.0761s / 129520.7376 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.1646
env0_second_0:                 episode reward: -1.0000,                 loss: -0.1400
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 17201/50000 (34.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.1114s / 129781.8490 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.1981
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1676
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 17221/50000 (34.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.0273s / 130042.8763 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2054
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1810
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 17241/50000 (34.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2201s / 130303.0964 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2316
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1876
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 17261/50000 (34.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2579s / 130562.3542 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2353
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1991
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 17281/50000 (34.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.6979s / 130824.0521 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2307
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1817
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 17301/50000 (34.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6563s / 131083.7084 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2345
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1927
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 17321/50000 (34.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9555s / 131343.6640 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2284
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1915
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 17341/50000 (34.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8883s / 131601.5522 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2342
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1903
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 17361/50000 (34.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1451s / 131861.6973 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2112
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1809
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 17381/50000 (34.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0777s / 132121.7750 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2122
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1787
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 17401/50000 (34.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9344s / 132381.7094 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2266
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1974
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 17421/50000 (34.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1584s / 132641.8677 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2238
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1816
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 17441/50000 (34.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5008s / 132902.3686 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2263
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1947
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 17461/50000 (34.9220%),                 avg. length: 2909.35,                last time consumption/overall running time: 252.4450s / 133154.8135 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2200
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1855
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 17481/50000 (34.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6996s / 133414.5132 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2276
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1948
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 17501/50000 (35.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0502s / 133674.5634 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2351
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2049
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 17521/50000 (35.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0363s / 133934.5998 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2326
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2081
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 17541/50000 (35.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9448s / 134195.5446 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2303
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1998
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 17561/50000 (35.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8894s / 134455.4339 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2381
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1954
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 17581/50000 (35.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0956s / 134715.5296 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2255
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1997
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 17601/50000 (35.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3265s / 134974.8561 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2381
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2069
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 17621/50000 (35.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7568s / 135234.6129 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2357
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2048
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 17641/50000 (35.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3010s / 135494.9139 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2405
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1926
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 17661/50000 (35.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9106s / 135754.8245 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2298
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1927
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 17681/50000 (35.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7128s / 136013.5373 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2304
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1905
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 17701/50000 (35.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8166s / 136273.3539 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2242
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1901
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 17721/50000 (35.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7415s / 136533.0954 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2292
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1962
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 17741/50000 (35.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2513s / 136791.3467 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2244
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1852
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 17761/50000 (35.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1408s / 137050.4875 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2241
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1915
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 17781/50000 (35.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2848s / 137308.7723 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2197
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1865
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 17801/50000 (35.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6723s / 137568.4446 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2333
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1967
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 17821/50000 (35.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5979s / 137827.0425 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2301
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1944
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 17841/50000 (35.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7615s / 138085.8040 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2350
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2026
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 17861/50000 (35.7220%),                 avg. length: 2968.25,                last time consumption/overall running time: 257.2077s / 138343.0117 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2158
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1788
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 17881/50000 (35.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1831s / 138601.1948 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2296
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2042
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 17901/50000 (35.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2832s / 138860.4780 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2370
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2066
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 17921/50000 (35.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6535s / 139120.1314 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2295
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1957
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 17941/50000 (35.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2053s / 139379.3367 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2279
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1985
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 17961/50000 (35.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7542s / 139638.0909 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2300
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2082
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 17981/50000 (35.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5992s / 139897.6902 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2311
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2009
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 18001/50000 (36.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7855s / 140156.4757 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2223
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1959
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 18021/50000 (36.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9914s / 140415.4671 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2282
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2046
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 18041/50000 (36.0820%),                 avg. length: 2939.4,                last time consumption/overall running time: 253.4088s / 140668.8759 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2092
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1838
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 18061/50000 (36.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1761s / 140928.0520 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2190
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1956
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 18081/50000 (36.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9097s / 141186.9617 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2297
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1911
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 18101/50000 (36.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8007s / 141446.7624 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2337
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2021
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 18121/50000 (36.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3244s / 141705.0867 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2390
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2041
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 18141/50000 (36.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8682s / 141962.9549 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2226
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1917
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 18161/50000 (36.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0390s / 142221.9939 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2381
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2000
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 18181/50000 (36.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6748s / 142481.6687 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2495
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1939
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 18201/50000 (36.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2525s / 142741.9212 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2409
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1877
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 18221/50000 (36.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5015s / 143001.4227 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2257
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2005
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 18241/50000 (36.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9782s / 143260.4009 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2310
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2014
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 18261/50000 (36.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7114s / 143520.1123 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2233
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1927
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18281/50000 (36.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1032s / 143780.2154 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2410
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1682
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 18301/50000 (36.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7441s / 144039.9595 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2407
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2090
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 18321/50000 (36.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1671s / 144298.1267 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2362
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2002
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18341/50000 (36.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7536s / 144558.8803 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2261
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1995
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 18361/50000 (36.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0013s / 144818.8816 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2360
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2066
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 18381/50000 (36.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5608s / 145079.4424 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2257
env0_second_0:                 episode reward: -0.6500,                 loss: -0.1866
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 18401/50000 (36.8020%),                 avg. length: 2776.75,                last time consumption/overall running time: 239.9588s / 145319.4012 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1759
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1480
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 18421/50000 (36.8420%),                 avg. length: 2860.65,                last time consumption/overall running time: 248.4443s / 145567.8455 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1959
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1548
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 18441/50000 (36.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6931s / 145826.5387 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2342
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1867
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 18461/50000 (36.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0281s / 146086.5667 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2253
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1813
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 18481/50000 (36.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3745s / 146345.9413 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2185
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1884
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 18501/50000 (37.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2185s / 146603.1597 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2350
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1956
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 18521/50000 (37.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.9918s / 146860.1515 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2343
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1985
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 18541/50000 (37.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3279s / 147119.4794 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2270
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1875
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 18561/50000 (37.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7448s / 147378.2242 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2280
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1597
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 18581/50000 (37.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5321s / 147636.7563 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2327
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2043
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18601/50000 (37.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7449s / 147896.5012 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2326
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1866
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 18621/50000 (37.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3370s / 148156.8381 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2321
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1186
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 18641/50000 (37.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0686s / 148415.9067 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2200
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1869
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 18661/50000 (37.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4362s / 148675.3430 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2211
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1591
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 18681/50000 (37.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7577s / 148935.1006 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2262
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1698
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 18701/50000 (37.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1276s / 149193.2282 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2222
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1872
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 18721/50000 (37.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6318s / 149450.8600 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2371
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1886
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 18741/50000 (37.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1282s / 149708.9882 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2386
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1986
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 18761/50000 (37.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5143s / 149966.5025 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2386
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1993
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18781/50000 (37.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5499s / 150225.0524 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2345
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1867
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18801/50000 (37.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9383s / 150483.9907 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2318
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1957
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 18821/50000 (37.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0842s / 150742.0749 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2180
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1743
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 18841/50000 (37.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4822s / 151000.5572 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2299
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2013
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18861/50000 (37.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1013s / 151257.6585 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2258
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1977
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 18881/50000 (37.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5711s / 151517.2295 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2383
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2048
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 18901/50000 (37.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3568s / 151775.5864 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2346
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1882
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 18921/50000 (37.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2753s / 152034.8617 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2325
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1865
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 18941/50000 (37.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.0877s / 152295.9494 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2273
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1991
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 18961/50000 (37.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7270s / 152555.6763 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2334
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2005
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18981/50000 (37.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7343s / 152816.4106 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2280
env0_second_0:                 episode reward: -0.4000,                 loss: -0.2096
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19001/50000 (38.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4471s / 153075.8578 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2274
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2003
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 19021/50000 (38.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4253s / 153336.2831 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2408
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1967
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 19041/50000 (38.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.2332s / 153592.5164 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2346
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2026
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 19061/50000 (38.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6363s / 153852.1527 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2344
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2068
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 19081/50000 (38.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0379s / 154111.1906 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2303
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2008
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19101/50000 (38.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8770s / 154369.0676 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2314
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1899
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19121/50000 (38.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3916s / 154629.4592 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2393
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2075
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 19141/50000 (38.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2394s / 154887.6986 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2419
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2048
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 19161/50000 (38.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3114s / 155147.0100 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2256
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1889
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19181/50000 (38.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1345s / 155407.1446 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2263
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1865
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 19201/50000 (38.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5634s / 155666.7080 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2341
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1998
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 19221/50000 (38.4420%),                 avg. length: 2963.0,                last time consumption/overall running time: 255.6969s / 155922.4049 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2359
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1759
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19241/50000 (38.4820%),                 avg. length: 2902.35,                last time consumption/overall running time: 252.8024s / 156175.2073 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1936
env0_second_0:                 episode reward: -1.8000,                 loss: -0.0851
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 19261/50000 (38.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8716s / 156434.0789 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2297
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1908
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 19281/50000 (38.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0279s / 156692.1068 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2165
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1702
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 19301/50000 (38.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7137s / 156950.8205 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2260
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1872
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19321/50000 (38.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8135s / 157209.6340 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2364
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1520
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 19341/50000 (38.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3154s / 157468.9494 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2347
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1788
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19361/50000 (38.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4301s / 157729.3795 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2233
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1681
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19381/50000 (38.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1909s / 157988.5705 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2244
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1805
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 19401/50000 (38.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0081s / 158247.5786 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2282
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1703
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 19421/50000 (38.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6192s / 158508.1978 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2280
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1985
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 19441/50000 (38.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7248s / 158767.9226 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2215
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1881
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 19461/50000 (38.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4674s / 159028.3900 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2229
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1819
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 19481/50000 (38.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8129s / 159287.2028 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2309
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1956
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19501/50000 (39.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4693s / 159546.6722 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2263
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1875
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 19521/50000 (39.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5639s / 159806.2361 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2056
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1715
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 19541/50000 (39.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7116s / 160066.9477 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2260
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1730
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 19561/50000 (39.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7331s / 160326.6808 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2235
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1912
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 19581/50000 (39.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 262.1283s / 160588.8091 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2223
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1910
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 19601/50000 (39.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5114s / 160849.3205 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2238
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1876
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 19621/50000 (39.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9671s / 161107.2876 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2166
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1565
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 19641/50000 (39.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1137s / 161367.4013 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2224
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1879
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19661/50000 (39.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2539s / 161627.6552 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2233
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1824
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19681/50000 (39.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1967s / 161885.8519 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2179
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1592
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19701/50000 (39.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.4369s / 162147.2888 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2143
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1838
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 19721/50000 (39.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8730s / 162407.1618 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2288
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1968
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19741/50000 (39.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2597s / 162666.4215 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2204
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1891
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 19761/50000 (39.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.8857s / 162927.3073 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2126
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1852
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 19781/50000 (39.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9362s / 163188.2435 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2230
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1864
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 19801/50000 (39.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7816s / 163447.0251 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2271
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1985
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19821/50000 (39.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3663s / 163706.3914 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2199
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1917
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 19841/50000 (39.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6534s / 163965.0449 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2289
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1934
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19861/50000 (39.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3904s / 164223.4353 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2370
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1874
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19881/50000 (39.7620%),                 avg. length: 1855.5,                last time consumption/overall running time: 164.9830s / 164388.4183 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.0852
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0735
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 19901/50000 (39.8020%),                 avg. length: 2673.8,                last time consumption/overall running time: 234.3856s / 164622.8039 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1230
env0_second_0:                 episode reward: -1.1500,                 loss: -0.0647
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 19921/50000 (39.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.3011s / 164884.1050 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1999
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1239
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19941/50000 (39.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3851s / 165142.4901 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2061
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1266
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 19961/50000 (39.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.9274s / 165399.4175 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2020
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1193
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 19981/50000 (39.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7454s / 165659.1628 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2027
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1190
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 20001/50000 (40.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.1024s / 165920.2652 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2080
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1414
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20021/50000 (40.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1211s / 166180.3862 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2179
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1403
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 20041/50000 (40.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6891s / 166440.0753 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2175
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1434
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 20061/50000 (40.1220%),                 avg. length: 2986.2,                last time consumption/overall running time: 258.3353s / 166698.4106 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2208
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1558
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 20081/50000 (40.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7313s / 166957.1420 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2264
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1618
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20101/50000 (40.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2193s / 167217.3613 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2264
env0_second_0:                 episode reward: -0.2500,                 loss: 0.1548
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20121/50000 (40.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8499s / 167477.2112 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2284
env0_second_0:                 episode reward: -0.0500,                 loss: 0.1216
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 20141/50000 (40.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7516s / 167736.9629 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2166
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0558
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 20161/50000 (40.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2464s / 167997.2092 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2111
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1163
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 20181/50000 (40.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2849s / 168255.4941 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2098
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0957
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20201/50000 (40.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6143s / 168516.1085 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1940
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1176
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 20221/50000 (40.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1889s / 168775.2974 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2019
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0888
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 20241/50000 (40.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8804s / 169035.1778 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2105
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1424
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20261/50000 (40.5220%),                 avg. length: 2790.6,                last time consumption/overall running time: 241.6807s / 169276.8585 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.1925
env0_second_0:                 episode reward: -0.8000,                 loss: -0.1397
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 20281/50000 (40.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9068s / 169534.7652 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2098
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1567
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20301/50000 (40.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4548s / 169794.2201 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2190
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1544
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20321/50000 (40.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3292s / 170053.5492 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2134
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1581
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20341/50000 (40.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0943s / 170313.6436 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2115
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1529
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20361/50000 (40.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0674s / 170572.7109 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2126
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1564
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 20381/50000 (40.7620%),                 avg. length: 2696.65,                last time consumption/overall running time: 234.2868s / 170806.9977 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1701
env0_second_0:                 episode reward: -1.6500,                 loss: -0.1178
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 20401/50000 (40.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6642s / 171066.6620 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2151
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1528
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 20421/50000 (40.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7000s / 171324.3620 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2131
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1599
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 20441/50000 (40.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7287s / 171584.0907 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2171
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1543
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20461/50000 (40.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1035s / 171842.1942 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2143
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1547
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 20481/50000 (40.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0960s / 172100.2902 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2114
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1655
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 20501/50000 (41.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1574s / 172358.4475 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2103
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1670
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 20521/50000 (41.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7580s / 172616.2056 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2076
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1568
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20541/50000 (41.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.6769s / 172877.8825 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2092
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1471
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 20561/50000 (41.1220%),                 avg. length: 2977.05,                last time consumption/overall running time: 258.6520s / 173136.5346 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1978
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1405
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 20581/50000 (41.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.2005s / 173397.7351 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2140
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1487
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 20601/50000 (41.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7220s / 173658.4571 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2084
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1703
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20621/50000 (41.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0049s / 173918.4619 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2127
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1658
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20641/50000 (41.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2272s / 174178.6892 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2214
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1649
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 20661/50000 (41.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.8583s / 174439.5475 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2283
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1748
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20681/50000 (41.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8625s / 174699.4100 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2208
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1505
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20701/50000 (41.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8893s / 174959.2993 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2260
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1671
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 20721/50000 (41.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6281s / 175219.9274 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2331
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1755
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20741/50000 (41.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 262.0699s / 175481.9974 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2379
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1825
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 20761/50000 (41.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7996s / 175739.7970 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2278
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1853
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 20781/50000 (41.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6651s / 175999.4621 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2381
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1903
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 20801/50000 (41.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5597s / 176260.0218 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2286
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1864
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 20821/50000 (41.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7655s / 176520.7873 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2276
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1932
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 20841/50000 (41.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8340s / 176780.6213 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2346
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1935
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 20861/50000 (41.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3263s / 177038.9475 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2346
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1918
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20881/50000 (41.7620%),                 avg. length: 2797.35,                last time consumption/overall running time: 244.1456s / 177283.0932 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.2077
env0_second_0:                 episode reward: -0.8000,                 loss: -0.1625
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 20901/50000 (41.8020%),                 avg. length: 2991.4,                last time consumption/overall running time: 259.8704s / 177542.9636 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2115
env0_second_0:                 episode reward: -1.1500,                 loss: -0.1842
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 20921/50000 (41.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6032s / 177801.5668 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2376
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2017
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20941/50000 (41.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8001s / 178060.3669 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2312
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1935
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20961/50000 (41.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 263.7478s / 178324.1147 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2292
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0278
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 20981/50000 (41.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7417s / 178584.8564 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2412
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0476
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21001/50000 (42.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5355s / 178844.3919 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2375
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1249
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21021/50000 (42.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8316s / 179101.2235 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2389
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1640
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 21041/50000 (42.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8997s / 179359.1232 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2371
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1574
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21061/50000 (42.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4068s / 179619.5299 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2263
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1827
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 21081/50000 (42.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.0962s / 179880.6261 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2240
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1816
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21101/50000 (42.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2483s / 180140.8744 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2363
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1673
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21121/50000 (42.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.6861s / 180397.5605 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2252
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1718
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 21141/50000 (42.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8854s / 180655.4459 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2336
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1910
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 21161/50000 (42.3220%),                 avg. length: 2967.3,                last time consumption/overall running time: 254.9586s / 180910.4045 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2273
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1754
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 21181/50000 (42.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9063s / 181168.3108 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2427
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2046
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 21201/50000 (42.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8053s / 181427.1161 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2387
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1996
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 21221/50000 (42.4420%),                 avg. length: 2855.2,                last time consumption/overall running time: 248.1406s / 181675.2567 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2132
env0_second_0:                 episode reward: -0.8500,                 loss: -0.1430
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 21241/50000 (42.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6306s / 181935.8873 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2383
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1984
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 21261/50000 (42.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9019s / 182195.7892 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2329
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1932
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 21281/50000 (42.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4793s / 182456.2685 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2397
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1932
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21301/50000 (42.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6884s / 182715.9570 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2422
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1788
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 21321/50000 (42.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6253s / 182974.5822 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2464
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1984
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 21341/50000 (42.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1958s / 183233.7780 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2368
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1937
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 21361/50000 (42.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0388s / 183491.8168 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2368
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2005
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 21381/50000 (42.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6704s / 183752.4871 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2270
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1895
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 21401/50000 (42.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2965s / 184010.7836 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2243
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1880
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 21421/50000 (42.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2932s / 184268.0768 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2156
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1679
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 21441/50000 (42.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4192s / 184526.4960 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2450
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1907
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 21461/50000 (42.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0904s / 184786.5865 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2379
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2039
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 21481/50000 (42.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2924s / 185044.8789 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2412
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2116
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21501/50000 (43.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2435s / 185303.1224 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2344
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1800
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 21521/50000 (43.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1938s / 185562.3162 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2356
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1873
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21541/50000 (43.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8280s / 185820.1442 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2419
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2116
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 21561/50000 (43.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7162s / 186080.8604 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2387
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1696
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21581/50000 (43.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8971s / 186337.7575 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2408
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2035
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 21601/50000 (43.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.0674s / 186598.8249 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2478
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2219
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21621/50000 (43.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2366s / 186859.0615 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2352
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1835
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 21641/50000 (43.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9097s / 187117.9712 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2289
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1530
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 21661/50000 (43.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.4618s / 187375.4331 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2353
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1970
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21681/50000 (43.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4768s / 187633.9098 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2479
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2019
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 21701/50000 (43.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5592s / 187891.4690 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2434
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2120
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21721/50000 (43.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6966s / 188149.1656 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2434
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2113
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21741/50000 (43.4820%),                 avg. length: 2982.05,                last time consumption/overall running time: 255.5936s / 188404.7592 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2220
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1694
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 21761/50000 (43.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4628s / 188664.2220 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2293
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1946
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 21781/50000 (43.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.2683s / 188925.4903 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2479
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1831
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 21801/50000 (43.6020%),                 avg. length: 2980.6,                last time consumption/overall running time: 258.1877s / 189183.6780 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2275
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1840
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 21821/50000 (43.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5752s / 189441.2532 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2140
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1760
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21841/50000 (43.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7172s / 189700.9703 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2331
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1892
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21861/50000 (43.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9136s / 189960.8839 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2302
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1974
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 21881/50000 (43.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3487s / 190220.2326 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2357
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1901
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21901/50000 (43.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5200s / 190476.7526 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2431
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1754
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 21921/50000 (43.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3886s / 190733.1412 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2386
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1900
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21941/50000 (43.8820%),                 avg. length: 2995.25,                last time consumption/overall running time: 255.9204s / 190989.0616 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2229
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1803
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 21961/50000 (43.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2058s / 191247.2674 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2529
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1605
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21981/50000 (43.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2572s / 191504.5246 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2480
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2047
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 22001/50000 (44.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7299s / 191764.2546 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2395
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2057
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 22021/50000 (44.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9258s / 192022.1804 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2323
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1886
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 22041/50000 (44.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3181s / 192278.4985 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2388
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1913
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 22061/50000 (44.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6950s / 192536.1935 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2381
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1881
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 22081/50000 (44.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.0176s / 192792.2112 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2336
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1862
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 22101/50000 (44.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8876s / 193049.0988 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2238
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1706
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 22121/50000 (44.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 262.4701s / 193311.5689 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2278
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1857
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 22141/50000 (44.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0715s / 193569.6404 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2385
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1941
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 22161/50000 (44.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5241s / 193830.1644 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2283
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1911
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 22181/50000 (44.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8207s / 194088.9851 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2263
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1855
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 22201/50000 (44.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0745s / 194347.0596 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2328
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1951
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 22221/50000 (44.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0287s / 194605.0883 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2246
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1874
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 22241/50000 (44.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6357s / 194864.7240 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2376
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1998
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 22261/50000 (44.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1917s / 195122.9157 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2161
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1541
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 22281/50000 (44.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8954s / 195379.8111 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2270
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1742
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 22301/50000 (44.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.1634s / 195635.9745 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2342
env0_second_0:                 episode reward: 0.0500,                 loss: -0.0647
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 22321/50000 (44.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7773s / 195893.7518 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2392
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1298
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 22341/50000 (44.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.2601s / 196149.0119 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2398
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1556
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 22361/50000 (44.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0994s / 196409.1113 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2362
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1845
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 22381/50000 (44.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8710s / 196668.9823 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2239
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1886
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 22401/50000 (44.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3938s / 196926.3760 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2357
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1969
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 22421/50000 (44.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2802s / 197185.6562 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2353
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1830
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 22441/50000 (44.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8809s / 197445.5371 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2398
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1748
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 22461/50000 (44.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1939s / 197705.7310 s