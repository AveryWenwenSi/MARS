pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [23, 24]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 2, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220327_2204/slimevolley_SlimeVolley-v0_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220327_2204/slimevolley_SlimeVolley-v0_nash_dqn.
Episode: 1/50000 (0.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 6.2644s / 6.2644 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0068
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0065
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 198.6794s / 204.9438 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0070
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0073
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 204.9796s / 409.9234 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0103
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0101
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 204.5468s / 614.4702 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0139
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0139
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 207.2628s / 821.7329 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0154
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0155
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 210.7902s / 1032.5231 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0159
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0171
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 214.8521s / 1247.3752 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0161
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0163
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 217.4676s / 1464.8428 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0159
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0150
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 220.4029s / 1685.2457 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0159
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0144
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 225.5136s / 1910.7593 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0162
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0145
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 228.9890s / 2139.7483 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0158
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0153
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 231.9427s / 2371.6909 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0167
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0158
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 235.6333s / 2607.3242 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0163
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0157
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 241.5140s / 2848.8382 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0162
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0161
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 243.5140s / 3092.3522 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0162
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0156
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 248.6347s / 3340.9870 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0159
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0159
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 253.0592s / 3594.0461 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0159
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0164
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 255.9862s / 3850.0323 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0160
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0159
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 258.6509s / 4108.6832 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0164
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0163
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 256.7996s / 4365.4828 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0159
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0162
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 257.1894s / 4622.6722 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0162
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0159
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 257.4218s / 4880.0940 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0164
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0166
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 257.7555s / 5137.8495 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0161
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0164
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 257.0729s / 5394.9225 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0159
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0161
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 258.2612s / 5653.1837 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0160
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0151
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 259.9236s / 5913.1073 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0159
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0153
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 259.6328s / 6172.7401 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0156
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0149
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 264.2303s / 6436.9704 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0156
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0137
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 258.8902s / 6695.8606 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0160
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0138
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 259.0715s / 6954.9321 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0151
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0138
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 258.6889s / 7213.6210 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0147
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0137
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 259.0677s / 7472.6887 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0149
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0142
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 641/50000 (1.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 259.6606s / 7732.3493 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0149
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0141
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 661/50000 (1.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 256.7446s / 7989.0939 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0144
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0138
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 681/50000 (1.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 257.4442s / 8246.5381 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0144
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0140
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 701/50000 (1.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 257.6292s / 8504.1673 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0140
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0139
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 721/50000 (1.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 257.1513s / 8761.3186 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0139
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0138
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 741/50000 (1.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 257.8489s / 9019.1675 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0133
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0139
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 761/50000 (1.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 260.5730s / 9279.7405 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0137
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0134
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 781/50000 (1.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 257.8819s / 9537.6225 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0137
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0133
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 801/50000 (1.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 268.7643s / 9806.3868 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0133
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0128
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 821/50000 (1.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 275.8047s / 10082.1915 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0138
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0133
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 841/50000 (1.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 267.1971s / 10349.3886 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0138
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0134
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 861/50000 (1.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 265.9340s / 10615.3225 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0135
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0131
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 881/50000 (1.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 260.1067s / 10875.4292 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0132
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0130
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 901/50000 (1.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 260.9529s / 11136.3821 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0134
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0129
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 921/50000 (1.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 261.0692s / 11397.4513 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0128
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0126
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 941/50000 (1.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 260.1176s / 11657.5688 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0127
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0125
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 961/50000 (1.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 261.2965s / 11918.8653 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0127
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0123
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 981/50000 (1.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 259.9617s / 12178.8270 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0123
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0120
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1001/50000 (2.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 259.8859s / 12438.7129 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0120
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0115
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 1021/50000 (2.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 260.1523s / 12698.8651 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0120
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0118
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1041/50000 (2.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 259.0665s / 12957.9316 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0121
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0117
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1061/50000 (2.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 261.3974s / 13219.3290 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0120
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0114
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1081/50000 (2.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 261.3625s / 13480.6916 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0120
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0119
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1101/50000 (2.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 260.7069s / 13741.3985 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0120
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0117
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1121/50000 (2.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 262.4778s / 14003.8763 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0118
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0121
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1141/50000 (2.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 261.3651s / 14265.2414 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0117
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0121
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1161/50000 (2.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 262.1161s / 14527.3575 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0120
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0122
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1181/50000 (2.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 262.2921s / 14789.6496 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0114
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0120
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1201/50000 (2.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 262.9278s / 15052.5774 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0116
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0120
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 1221/50000 (2.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 264.3985s / 15316.9759 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0114
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0119
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 1241/50000 (2.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 263.2268s / 15580.2027 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0111
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0112
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1261/50000 (2.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 261.6465s / 15841.8492 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0115
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0115
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1281/50000 (2.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 261.2582s / 16103.1075 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0115
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0112
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1301/50000 (2.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 260.8831s / 16363.9906 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0116
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0111
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1321/50000 (2.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 262.4861s / 16626.4766 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0115
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0109
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1341/50000 (2.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 264.1467s / 16890.6234 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0114
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0108
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1361/50000 (2.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 263.3273s / 17153.9506 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0117
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0114
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1381/50000 (2.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 262.8522s / 17416.8029 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0113
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0114
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1401/50000 (2.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 262.9064s / 17679.7093 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0116
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0112
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1421/50000 (2.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 262.8127s / 17942.5220 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0113
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0111
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1441/50000 (2.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 262.6673s / 18205.1893 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0112
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0113
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1461/50000 (2.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 262.3940s / 18467.5834 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0109
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0110
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 1481/50000 (2.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 263.6196s / 18731.2030 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0115
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0108
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1501/50000 (3.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 263.4885s / 18994.6914 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0115
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0109
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1521/50000 (3.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 264.7421s / 19259.4335 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0114
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0109
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 1541/50000 (3.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 264.4204s / 19523.8539 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0115
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0104
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1561/50000 (3.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 263.1052s / 19786.9590 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0114
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0103
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1581/50000 (3.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 261.7684s / 20048.7275 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0114
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0105
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 1601/50000 (3.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 262.0111s / 20310.7386 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0114
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0105
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1621/50000 (3.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 260.7605s / 20571.4991 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0112
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0104