pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 7]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=1024, bias=True)
      (1): Tanh()
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): Tanh()
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Tanh()
      (6): Linear(in_features=1024, out_features=6, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=1024, bias=True)
      (1): Tanh()
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): Tanh()
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Tanh()
      (6): Linear(in_features=1024, out_features=6, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 2, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 32, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [1024, 1024, 1024], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 4, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220114_0533/slimevolley_SlimeVolley-v0_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220114_0533/slimevolley_SlimeVolley-v0_selfplay2.
Episode: 1/10000 (0.0100%),                 avg. length: 633.0,                last time consumption/overall running time: 6.1875s / 6.1875 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0050
env0_second_0:                 episode reward: -1.0000,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 596.9,                last time consumption/overall running time: 83.3058s / 89.4933 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0075
env0_second_0:                 episode reward: -0.8500,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 591.3,                last time consumption/overall running time: 83.1211s / 172.6144 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0101
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 563.8,                last time consumption/overall running time: 80.2386s / 252.8530 s
env0_first_0:                 episode reward: 0.9000,                 loss: nan
env0_second_0:                 episode reward: -0.9000,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Score delta: 4.2, update the opponent.
Episode: 81/10000 (0.8100%),                 avg. length: 557.95,                last time consumption/overall running time: 79.8570s / 332.7099 s
env0_first_0:                 episode reward: -0.7000,                 loss: nan
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0115
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 587.5,                last time consumption/overall running time: 85.1726s / 417.8825 s
env0_first_0:                 episode reward: 0.6000,                 loss: nan
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0137
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 555.85,                last time consumption/overall running time: 81.0653s / 498.9479 s
env0_first_0:                 episode reward: 0.5000,                 loss: nan
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0142
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 536.95,                last time consumption/overall running time: 78.9080s / 577.8559 s
env0_first_0:                 episode reward: 1.1500,                 loss: nan
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0151
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 540.9,                last time consumption/overall running time: 79.4697s / 657.3256 s
env0_first_0:                 episode reward: -0.7500,                 loss: nan
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0159
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 593.55,                last time consumption/overall running time: 87.5280s / 744.8535 s
env0_first_0:                 episode reward: -0.0500,                 loss: nan
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0176
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 593.65,                last time consumption/overall running time: 87.8197s / 832.6732 s
env0_first_0:                 episode reward: -1.0000,                 loss: nan
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0181
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 601.5,                last time consumption/overall running time: 89.4480s / 922.1212 s
env0_first_0:                 episode reward: 0.8000,                 loss: nan
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0179
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 576.75,                last time consumption/overall running time: 85.5832s / 1007.7045 s
env0_first_0:                 episode reward: -1.5000,                 loss: nan
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0196
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 582.2,                last time consumption/overall running time: 84.9379s / 1092.6423 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Score delta: 4.2, update the opponent.
Episode: 281/10000 (2.8100%),                 avg. length: 577.6,                last time consumption/overall running time: 84.8645s / 1177.5069 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0215
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 619.45,                last time consumption/overall running time: 90.7490s / 1268.2558 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0224
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 590.05,                last time consumption/overall running time: 86.4806s / 1354.7365 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0227
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 583.65,                last time consumption/overall running time: 85.6050s / 1440.3414 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0209
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 572.55,                last time consumption/overall running time: 84.8104s / 1525.1519 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0188
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 563.75,                last time consumption/overall running time: 83.4068s / 1608.5587 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0195
env0_second_0:                 episode reward: 0.0500,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 597.75,                last time consumption/overall running time: 88.0936s / 1696.6522 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0189
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 571.0,                last time consumption/overall running time: 84.2244s / 1780.8767 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0198
env0_second_0:                 episode reward: -1.1000,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 596.15,                last time consumption/overall running time: 88.1193s / 1868.9960 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0202
env0_second_0:                 episode reward: -0.6000,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 571.15,                last time consumption/overall running time: 85.7325s / 1954.7285 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0230
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 556.1,                last time consumption/overall running time: 83.3134s / 2038.0419 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0233
env0_second_0:                 episode reward: -0.3000,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 563.65,                last time consumption/overall running time: 83.5052s / 2121.5471 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0210
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 564.5,                last time consumption/overall running time: 83.6030s / 2205.1500 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0203
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 586.55,                last time consumption/overall running time: 86.3561s / 2291.5061 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0201
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 618.95,                last time consumption/overall running time: 90.9305s / 2382.4366 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0207
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 612.05,                last time consumption/overall running time: 90.7284s / 2473.1650 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0198
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 559.15,                last time consumption/overall running time: 82.7650s / 2555.9300 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0216
env0_second_0:                 episode reward: -0.1000,                 loss: nan
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 572.75,                last time consumption/overall running time: 84.9967s / 2640.9267 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0213
env0_second_0:                 episode reward: -0.6500,                 loss: nan
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 578.75,                last time consumption/overall running time: 85.3688s / 2726.2955 s
env0_first_0:                 episode reward: 0.9500,                 loss: nan
env0_second_0:                 episode reward: -0.9500,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Score delta: 4.2, update the opponent.
Episode: 661/10000 (6.6100%),                 avg. length: 582.65,                last time consumption/overall running time: 85.0943s / 2811.3898 s
env0_first_0:                 episode reward: -0.2500,                 loss: nan
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0202
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 642.9,                last time consumption/overall running time: 93.9179s / 2905.3077 s
env0_first_0:                 episode reward: 0.0500,                 loss: nan
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0200
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 556.15,                last time consumption/overall running time: 81.3622s / 2986.6699 s
env0_first_0:                 episode reward: -0.3000,                 loss: nan
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0211
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 562.5,                last time consumption/overall running time: 81.9105s / 3068.5804 s
env0_first_0:                 episode reward: -0.6000,                 loss: nan
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Score delta: 4.6, update the opponent.
Episode: 741/10000 (7.4100%),                 avg. length: 629.45,                last time consumption/overall running time: 92.1562s / 3160.7366 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0195
env0_second_0:                 episode reward: -0.4000,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 621.9,                last time consumption/overall running time: 90.9095s / 3251.6461 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0220
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 626.2,                last time consumption/overall running time: 90.9811s / 3342.6272 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0235
env0_second_0:                 episode reward: -0.5000,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 605.1,                last time consumption/overall running time: 89.1891s / 3431.8163 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0224
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 641.65,                last time consumption/overall running time: 94.2960s / 3526.1123 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0279
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 645.8,                last time consumption/overall running time: 94.6934s / 3620.8058 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0291
env0_second_0:                 episode reward: -0.8500,                 loss: nan
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 642.45,                last time consumption/overall running time: 95.5725s / 3716.3783 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0267
env0_second_0:                 episode reward: -1.2500,                 loss: nan
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 742.1,                last time consumption/overall running time: 107.9463s / 3824.3246 s
env0_first_0:                 episode reward: 1.9500,                 loss: nan
env0_second_0:                 episode reward: -1.9500,                 loss: nan
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Score delta: 4.8, update the opponent.
Episode: 901/10000 (9.0100%),                 avg. length: 732.2,                last time consumption/overall running time: 106.7969s / 3931.1215 s
env0_first_0:                 episode reward: 1.4000,                 loss: nan
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0204
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 688.25,                last time consumption/overall running time: 101.8642s / 4032.9857 s
env0_first_0:                 episode reward: 2.1500,                 loss: nan
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0217
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 747.2,                last time consumption/overall running time: 110.2450s / 4143.2307 s
env0_first_0:                 episode reward: 0.5000,                 loss: nan
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0221
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 770.8,                last time consumption/overall running time: 113.4171s / 4256.6478 s
env0_first_0:                 episode reward: 0.9000,                 loss: nan
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0215
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 746.8,                last time consumption/overall running time: 110.8714s / 4367.5193 s
env0_first_0:                 episode reward: 1.1000,                 loss: nan
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0207
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 768.1,                last time consumption/overall running time: 114.3842s / 4481.9035 s
env0_first_0:                 episode reward: 1.4500,                 loss: nan
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0227
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 818.9,                last time consumption/overall running time: 122.1349s / 4604.0385 s
env0_first_0:                 episode reward: -0.1000,                 loss: nan
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0216
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 903.3,                last time consumption/overall running time: 133.2511s / 4737.2895 s
env0_first_0:                 episode reward: -1.0500,                 loss: nan
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0189
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 830.4,                last time consumption/overall running time: 122.4041s / 4859.6936 s
env0_first_0:                 episode reward: -1.5500,                 loss: nan
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Score delta: 4.2, update the opponent.
Episode: 1081/10000 (10.8100%),                 avg. length: 1103.2,                last time consumption/overall running time: 160.5513s / 5020.2449 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0129
env0_second_0:                 episode reward: 1.9500,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1153.65,                last time consumption/overall running time: 169.9190s / 5190.1639 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0132
env0_second_0:                 episode reward: 2.9000,                 loss: nan
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1094.2,                last time consumption/overall running time: 161.3151s / 5351.4790 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0118
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 973.0,                last time consumption/overall running time: 143.6984s / 5495.1774 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0124
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1120.6,                last time consumption/overall running time: 165.3432s / 5660.5206 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0139
env0_second_0:                 episode reward: 2.1000,                 loss: nan
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1178.85,                last time consumption/overall running time: 174.0191s / 5834.5397 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0142
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1237.5,                last time consumption/overall running time: 184.2499s / 6018.7896 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0144
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1252.9,                last time consumption/overall running time: 183.8337s / 6202.6233 s
env0_first_0:                 episode reward: 0.7500,                 loss: nan
env0_second_0:                 episode reward: -0.7500,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Score delta: 4.6, update the opponent.
Episode: 1241/10000 (12.4100%),                 avg. length: 1508.05,                last time consumption/overall running time: 219.7294s / 6422.3528 s
env0_first_0:                 episode reward: 0.5000,                 loss: nan
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0089
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1642.9,                last time consumption/overall running time: 241.7142s / 6664.0670 s
env0_first_0:                 episode reward: 0.4000,                 loss: nan
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0110
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1347.25,                last time consumption/overall running time: 199.5579s / 6863.6249 s
env0_first_0:                 episode reward: -0.5500,                 loss: nan
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Score delta: 4.4, update the opponent.
Episode: 1301/10000 (13.0100%),                 avg. length: 734.05,                last time consumption/overall running time: 106.7985s / 6970.4233 s
env0_first_0:                 episode reward: 2.1000,                 loss: nan
env0_second_0:                 episode reward: -2.1000,                 loss: nan
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Score delta: 5.0, update the opponent.
Episode: 1321/10000 (13.2100%),                 avg. length: 718.45,                last time consumption/overall running time: 101.4254s / 7071.8488 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0129
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 771.6,                last time consumption/overall running time: 111.2273s / 7183.0760 s
env0_first_0:                 episode reward: 1.5500,                 loss: nan
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0218
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 767.55,                last time consumption/overall running time: 111.4815s / 7294.5576 s
env0_first_0:                 episode reward: 1.2000,                 loss: nan
env0_second_0:                 episode reward: -1.2000,                 loss: 0.0339
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 864.8,                last time consumption/overall running time: 126.1190s / 7420.6766 s
env0_first_0:                 episode reward: 1.0000,                 loss: nan
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0409
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 783.15,                last time consumption/overall running time: 113.6508s / 7534.3275 s
env0_first_0:                 episode reward: 2.1000,                 loss: nan
env0_second_0:                 episode reward: -2.1000,                 loss: 0.0422
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 793.6,                last time consumption/overall running time: 116.9716s / 7651.2991 s
env0_first_0:                 episode reward: 1.8000,                 loss: nan
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0377
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 958.3,                last time consumption/overall running time: 142.5130s / 7793.8121 s
env0_first_0:                 episode reward: -0.9500,                 loss: nan
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0299
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 906.8,                last time consumption/overall running time: 133.7243s / 7927.5363 s
env0_first_0:                 episode reward: 0.4000,                 loss: nan
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0248
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 913.55,                last time consumption/overall running time: 134.8828s / 8062.4191 s
env0_first_0:                 episode reward: -0.9500,                 loss: nan
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0219
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 887.5,                last time consumption/overall running time: 132.9323s / 8195.3515 s
env0_first_0:                 episode reward: -0.9500,                 loss: nan
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0176
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 880.4,                last time consumption/overall running time: 130.2195s / 8325.5710 s
env0_first_0:                 episode reward: -0.6500,                 loss: nan
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0150
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 871.85,                last time consumption/overall running time: 129.8851s / 8455.4561 s
env0_first_0:                 episode reward: 0.0500,                 loss: nan
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0143
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 972.1,                last time consumption/overall running time: 143.4107s / 8598.8668 s
env0_first_0:                 episode reward: -1.0500,                 loss: nan
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0146
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 981.05,                last time consumption/overall running time: 146.1184s / 8744.9852 s
env0_first_0:                 episode reward: -0.1500,                 loss: nan
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0139
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 983.15,                last time consumption/overall running time: 146.6575s / 8891.6427 s
env0_first_0:                 episode reward: -0.2000,                 loss: nan
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0129
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 935.5,                last time consumption/overall running time: 139.7467s / 9031.3893 s
env0_first_0:                 episode reward: -0.7000,                 loss: nan
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0130
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 907.55,                last time consumption/overall running time: 134.8471s / 9166.2365 s
env0_first_0:                 episode reward: -0.8000,                 loss: nan
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0135
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 958.0,                last time consumption/overall running time: 142.9104s / 9309.1469 s
env0_first_0:                 episode reward: -1.4000,                 loss: nan
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0134
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 985.1,                last time consumption/overall running time: 146.7906s / 9455.9375 s
env0_first_0:                 episode reward: -0.4500,                 loss: nan
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0138
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 891.65,                last time consumption/overall running time: 131.5852s / 9587.5227 s
env0_first_0:                 episode reward: -1.9000,                 loss: nan
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Score delta: 4.8, update the opponent.
Episode: 1721/10000 (17.2100%),                 avg. length: 907.6,                last time consumption/overall running time: 131.9264s / 9719.4492 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0130
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 984.9,                last time consumption/overall running time: 143.4084s / 9862.8575 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0132
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 901.45,                last time consumption/overall running time: 132.1935s / 9995.0510 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0124
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 963.15,                last time consumption/overall running time: 142.7449s / 10137.7959 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0116
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1008.45,                last time consumption/overall running time: 148.3961s / 10286.1919 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0110
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1026.0,                last time consumption/overall running time: 152.4821s / 10438.6741 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0107
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1060.25,                last time consumption/overall running time: 157.9577s / 10596.6317 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0109
env0_second_0:                 episode reward: -0.8000,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1042.1,                last time consumption/overall running time: 154.1867s / 10750.8185 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0114
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1110.35,                last time consumption/overall running time: 164.9106s / 10915.7290 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0108
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1099.45,                last time consumption/overall running time: 163.0794s / 11078.8084 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0110
env0_second_0:                 episode reward: -0.7000,                 loss: nan
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 970.7,                last time consumption/overall running time: 144.9344s / 11223.7428 s
env0_first_0:                 episode reward: 1.2000,                 loss: 0.0116
env0_second_0:                 episode reward: -1.2000,                 loss: nan
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Score delta: 4.2, update the opponent.
Episode: 1941/10000 (19.4100%),                 avg. length: 763.3,                last time consumption/overall running time: 111.0769s / 11334.8197 s
env0_first_0:                 episode reward: -2.2500,                 loss: nan
env0_second_0:                 episode reward: 2.2500,                 loss: 0.0107
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 746.1,                last time consumption/overall running time: 106.7559s / 11441.5756 s
env0_first_0:                 episode reward: -2.4000,                 loss: nan
env0_second_0:                 episode reward: 2.4000,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Score delta: 4.6, update the opponent.
Episode: 1981/10000 (19.8100%),                 avg. length: 927.2,                last time consumption/overall running time: 132.8165s / 11574.3921 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0082
env0_second_0:                 episode reward: 2.6000,                 loss: nan
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1081.7,                last time consumption/overall running time: 158.1820s / 11732.5741 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0092
env0_second_0:                 episode reward: 2.5500,                 loss: nan
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 947.9,                last time consumption/overall running time: 140.4320s / 11873.0061 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0093
env0_second_0:                 episode reward: 3.0500,                 loss: nan
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1174.0,                last time consumption/overall running time: 174.7857s / 12047.7918 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0089
env0_second_0:                 episode reward: 2.1500,                 loss: nan
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1178.8,                last time consumption/overall running time: 176.7126s / 12224.5044 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0083
env0_second_0:                 episode reward: 2.4000,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 1452.8,                last time consumption/overall running time: 216.8789s / 12441.3833 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.0080
env0_second_0:                 episode reward: 2.8500,                 loss: nan
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 1437.15,                last time consumption/overall running time: 213.9260s / 12655.3093 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0074
env0_second_0:                 episode reward: 3.0500,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 1610.3,                last time consumption/overall running time: 239.4763s / 12894.7856 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0072
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 1463.1,                last time consumption/overall running time: 217.1971s / 13111.9827 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0069
env0_second_0:                 episode reward: 3.3000,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1961.85,                last time consumption/overall running time: 290.8397s / 13402.8223 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0072
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1948.55,                last time consumption/overall running time: 289.1529s / 13691.9752 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0065
env0_second_0:                 episode reward: -1.3000,                 loss: nan
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 2371.65,                last time consumption/overall running time: 351.1492s / 14043.1244 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0061
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1993.35,                last time consumption/overall running time: 294.1495s / 14337.2739 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0058
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 1372.85,                last time consumption/overall running time: 202.9326s / 14540.2065 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0063
env0_second_0:                 episode reward: 2.5500,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 2004.7,                last time consumption/overall running time: 299.0592s / 14839.2657 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0070
env0_second_0:                 episode reward: 0.5000,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1881.1,                last time consumption/overall running time: 280.4765s / 15119.7422 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0064
env0_second_0:                 episode reward: 1.2500,                 loss: nan
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 1887.9,                last time consumption/overall running time: 280.2620s / 15400.0042 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0061
env0_second_0:                 episode reward: 1.3500,                 loss: nan
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 2404.45,                last time consumption/overall running time: 359.0102s / 15759.0144 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0055
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 1413.95,                last time consumption/overall running time: 210.6536s / 15969.6680 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0056
env0_second_0:                 episode reward: 2.3500,                 loss: nan
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1824.4,                last time consumption/overall running time: 273.1239s / 16242.7918 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0068
env0_second_0:                 episode reward: 1.4500,                 loss: nan
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 2309.45,                last time consumption/overall running time: 343.4822s / 16586.2741 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0066
env0_second_0:                 episode reward: -0.1000,                 loss: nan
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 1957.55,                last time consumption/overall running time: 292.7097s / 16878.9838 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0060
env0_second_0:                 episode reward: 0.8500,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 2177.2,                last time consumption/overall running time: 324.1132s / 17203.0970 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0066
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 1659.65,                last time consumption/overall running time: 246.0221s / 17449.1191 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0062
env0_second_0:                 episode reward: 1.4000,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1859.55,                last time consumption/overall running time: 276.6382s / 17725.7572 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 2012.35,                last time consumption/overall running time: 300.6809s / 18026.4382 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0058
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 1935.05,                last time consumption/overall running time: 289.3646s / 18315.8028 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0056
env0_second_0:                 episode reward: 0.7500,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 2345.05,                last time consumption/overall running time: 348.0541s / 18663.8569 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0055
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 1484.5,                last time consumption/overall running time: 221.6528s / 18885.5097 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0054
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 1735.05,                last time consumption/overall running time: 257.3974s / 19142.9070 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0061
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1532.55,                last time consumption/overall running time: 227.9560s / 19370.8630 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0060
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 1583.15,                last time consumption/overall running time: 234.8211s / 19605.6841 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0065
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1626.3,                last time consumption/overall running time: 241.9028s / 19847.5869 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0066
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 2408.05,                last time consumption/overall running time: 359.3752s / 20206.9620 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0059
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 2075.1,                last time consumption/overall running time: 308.7366s / 20515.6986 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0049
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 1745.7,                last time consumption/overall running time: 258.4244s / 20774.1230 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0056
env0_second_0:                 episode reward: 0.7500,                 loss: nan
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 2090.4,                last time consumption/overall running time: 311.6896s / 21085.8126 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0062
env0_second_0:                 episode reward: 0.2000,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1954.85,                last time consumption/overall running time: 292.3652s / 21378.1778 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0057
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1574.65,                last time consumption/overall running time: 235.3967s / 21613.5745 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0062
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 2363.65,                last time consumption/overall running time: 349.4197s / 21962.9942 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 2085.65,                last time consumption/overall running time: 309.7729s / 22272.7671 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0051
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1302.85,                last time consumption/overall running time: 193.7993s / 22466.5664 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.0055
env0_second_0:                 episode reward: 2.8500,                 loss: nan
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 2371.75,                last time consumption/overall running time: 352.1031s / 22818.6694 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0058
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 1879.45,                last time consumption/overall running time: 281.4122s / 23100.0816 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0052
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 1687.15,                last time consumption/overall running time: 253.6291s / 23353.7107 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0061
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 2118.8,                last time consumption/overall running time: 314.9591s / 23668.6698 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0057
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 2028.3,                last time consumption/overall running time: 300.2289s / 23968.8987 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0057
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 1926.95,                last time consumption/overall running time: 285.3556s / 24254.2543 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0057
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 1825.65,                last time consumption/overall running time: 271.9632s / 24526.2175 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0051
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 1171.3,                last time consumption/overall running time: 172.6405s / 24698.8581 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0058
env0_second_0:                 episode reward: 2.6500,                 loss: nan
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 638.35,                last time consumption/overall running time: 88.2747s / 24787.1328 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0049
env0_second_0:                 episode reward: 3.6500,                 loss: nan
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 675.55,                last time consumption/overall running time: 96.1337s / 24883.2664 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0065
env0_second_0:                 episode reward: 3.6000,                 loss: nan
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 569.5,                last time consumption/overall running time: 82.4262s / 24965.6927 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0066
env0_second_0:                 episode reward: 3.7000,                 loss: nan
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 589.1,                last time consumption/overall running time: 86.7247s / 25052.4173 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.0066
env0_second_0:                 episode reward: 3.1500,                 loss: nan
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 650.2,                last time consumption/overall running time: 96.2360s / 25148.6533 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0089
env0_second_0:                 episode reward: 3.3000,                 loss: nan
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 612.85,                last time consumption/overall running time: 90.8495s / 25239.5028 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.0095
env0_second_0:                 episode reward: 3.9000,                 loss: nan
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 665.6,                last time consumption/overall running time: 98.6375s / 25338.1403 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0093
env0_second_0:                 episode reward: 3.3000,                 loss: nan
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 603.45,                last time consumption/overall running time: 88.8211s / 25426.9614 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0100
env0_second_0:                 episode reward: 3.6000,                 loss: nan
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 615.1,                last time consumption/overall running time: 91.0189s / 25517.9803 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0099
env0_second_0:                 episode reward: 3.4000,                 loss: nan
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 662.9,                last time consumption/overall running time: 98.8798s / 25616.8601 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0088
env0_second_0:                 episode reward: 3.2500,                 loss: nan
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 630.25,                last time consumption/overall running time: 94.9579s / 25711.8181 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0098
env0_second_0:                 episode reward: 3.3500,                 loss: nan
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 586.0,                last time consumption/overall running time: 87.3775s / 25799.1956 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0112
env0_second_0:                 episode reward: 2.9500,                 loss: nan
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 681.85,                last time consumption/overall running time: 101.4544s / 25900.6500 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0115
env0_second_0:                 episode reward: 3.2500,                 loss: nan
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 558.2,                last time consumption/overall running time: 83.4170s / 25984.0670 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0108
env0_second_0:                 episode reward: 3.2500,                 loss: nan
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 622.65,                last time consumption/overall running time: 93.3345s / 26077.4015 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0106
env0_second_0:                 episode reward: 2.7500,                 loss: nan
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 696.75,                last time consumption/overall running time: 104.3873s / 26181.7888 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0115
env0_second_0:                 episode reward: 2.9000,                 loss: nan
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 729.7,                last time consumption/overall running time: 108.8982s / 26290.6870 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0105
env0_second_0:                 episode reward: 3.0500,                 loss: nan
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 658.2,                last time consumption/overall running time: 98.1634s / 26388.8504 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0117
env0_second_0:                 episode reward: 3.8500,                 loss: nan
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 662.15,                last time consumption/overall running time: 99.9102s / 26488.7606 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0128
env0_second_0:                 episode reward: 3.7500,                 loss: nan
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 723.55,                last time consumption/overall running time: 108.8540s / 26597.6146 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0124
env0_second_0:                 episode reward: 3.7000,                 loss: nan
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 658.05,                last time consumption/overall running time: 99.2262s / 26696.8409 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0125
env0_second_0:                 episode reward: 4.0000,                 loss: nan
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 714.8,                last time consumption/overall running time: 108.2553s / 26805.0961 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0126
env0_second_0:                 episode reward: 3.2000,                 loss: nan
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 708.05,                last time consumption/overall running time: 105.6070s / 26910.7031 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0125
env0_second_0:                 episode reward: 4.1500,                 loss: nan
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 715.4,                last time consumption/overall running time: 106.6025s / 27017.3056 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0124
env0_second_0:                 episode reward: 3.6500,                 loss: nan
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 818.6,                last time consumption/overall running time: 122.1846s / 27139.4902 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0116
env0_second_0:                 episode reward: 2.9500,                 loss: nan
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 740.85,                last time consumption/overall running time: 110.1884s / 27249.6786 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0112
env0_second_0:                 episode reward: 2.8000,                 loss: nan
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 980.55,                last time consumption/overall running time: 145.6944s / 27395.3730 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0108
env0_second_0:                 episode reward: 2.9500,                 loss: nan
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 986.5,                last time consumption/overall running time: 146.7157s / 27542.0887 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0107
env0_second_0:                 episode reward: 3.1000,                 loss: nan
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1068.1,                last time consumption/overall running time: 160.2623s / 27702.3511 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.0114
env0_second_0:                 episode reward: 2.4500,                 loss: nan
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1066.05,                last time consumption/overall running time: 159.5293s / 27861.8804 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0116
env0_second_0:                 episode reward: 3.0000,                 loss: nan
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1904.2,                last time consumption/overall running time: 283.4608s / 28145.3412 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0102
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1605.8,                last time consumption/overall running time: 237.5811s / 28382.9223 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0063
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1388.6,                last time consumption/overall running time: 207.4031s / 28590.3254 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0070
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1576.3,                last time consumption/overall running time: 236.2010s / 28826.5264 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0071
env0_second_0:                 episode reward: 1.4000,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1353.15,                last time consumption/overall running time: 200.9615s / 29027.4879 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0071
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1466.65,                last time consumption/overall running time: 217.6393s / 29245.1272 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0075
env0_second_0:                 episode reward: 1.3500,                 loss: nan
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1491.9,                last time consumption/overall running time: 223.8908s / 29469.0180 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0071
env0_second_0:                 episode reward: 2.7000,                 loss: nan
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1382.7,                last time consumption/overall running time: 206.9065s / 29675.9245 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0071
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1477.9,                last time consumption/overall running time: 220.4642s / 29896.3888 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0076
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 1370.75,                last time consumption/overall running time: 205.4506s / 30101.8394 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0077
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 1814.95,                last time consumption/overall running time: 270.4390s / 30372.2784 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0070
env0_second_0:                 episode reward: 1.2500,                 loss: nan
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 1908.5,                last time consumption/overall running time: 284.4318s / 30656.7102 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0059
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 1952.9,                last time consumption/overall running time: 291.1871s / 30947.8974 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0057
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 1125.95,                last time consumption/overall running time: 168.4515s / 31116.3488 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.0060
env0_second_0:                 episode reward: 2.4500,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 1496.65,                last time consumption/overall running time: 224.5085s / 31340.8574 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0065
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1621.85,                last time consumption/overall running time: 240.9259s / 31581.7832 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0063
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1819.65,                last time consumption/overall running time: 267.7878s / 31849.5711 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0063
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1909.3,                last time consumption/overall running time: 282.4141s / 32131.9851 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0052
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 1598.6,                last time consumption/overall running time: 235.9410s / 32367.9261 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0054
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1142.6,                last time consumption/overall running time: 167.6569s / 32535.5830 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0055
env0_second_0:                 episode reward: 1.6000,                 loss: nan
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 554.15,                last time consumption/overall running time: 76.2922s / 32611.8753 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0048
env0_second_0:                 episode reward: 3.7500,                 loss: nan
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 643.8,                last time consumption/overall running time: 90.5012s / 32702.3765 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0064
env0_second_0:                 episode reward: 3.2500,                 loss: nan
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 682.95,                last time consumption/overall running time: 98.4787s / 32800.8552 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0071
env0_second_0:                 episode reward: 3.4500,                 loss: nan
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 733.7,                last time consumption/overall running time: 107.0826s / 32907.9378 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0088
env0_second_0:                 episode reward: 3.2500,                 loss: nan
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 569.15,                last time consumption/overall running time: 84.8161s / 32992.7539 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0088
env0_second_0:                 episode reward: 3.5000,                 loss: nan
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 615.1,                last time consumption/overall running time: 92.2110s / 33084.9649 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0082
env0_second_0:                 episode reward: 3.6500,                 loss: nan
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 613.3,                last time consumption/overall running time: 91.1886s / 33176.1535 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0096
env0_second_0:                 episode reward: 3.6000,                 loss: nan
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 650.7,                last time consumption/overall running time: 96.1758s / 33272.3294 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0102
env0_second_0:                 episode reward: 3.3000,                 loss: nan
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 675.4,                last time consumption/overall running time: 100.4066s / 33372.7360 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0108
env0_second_0:                 episode reward: 2.5000,                 loss: nan
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 654.95,                last time consumption/overall running time: 97.1829s / 33469.9188 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0113
env0_second_0:                 episode reward: 2.8000,                 loss: nan
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 682.6,                last time consumption/overall running time: 101.4191s / 33571.3379 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0111
env0_second_0:                 episode reward: 3.7500,                 loss: nan
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 632.55,                last time consumption/overall running time: 94.4799s / 33665.8178 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0099
env0_second_0:                 episode reward: 3.6500,                 loss: nan
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 742.0,                last time consumption/overall running time: 110.3924s / 33776.2102 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0107
env0_second_0:                 episode reward: 3.8500,                 loss: nan
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 669.9,                last time consumption/overall running time: 101.3005s / 33877.5107 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0095
env0_second_0:                 episode reward: 3.6000,                 loss: nan
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 769.15,                last time consumption/overall running time: 116.6822s / 33994.1929 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0116
env0_second_0:                 episode reward: 2.8000,                 loss: nan
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 701.6,                last time consumption/overall running time: 105.5728s / 34099.7657 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.0106
env0_second_0:                 episode reward: 3.1500,                 loss: nan
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 864.95,                last time consumption/overall running time: 127.4624s / 34227.2281 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0097
env0_second_0:                 episode reward: 3.1000,                 loss: nan
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 733.95,                last time consumption/overall running time: 110.6407s / 34337.8688 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0087
env0_second_0:                 episode reward: 3.0000,                 loss: nan
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 772.5,                last time consumption/overall running time: 115.1032s / 34452.9720 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0098
env0_second_0:                 episode reward: 3.1000,                 loss: nan
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 798.5,                last time consumption/overall running time: 120.2723s / 34573.2443 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0108
env0_second_0:                 episode reward: 3.4500,                 loss: nan
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 1070.0,                last time consumption/overall running time: 159.5034s / 34732.7477 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0097
env0_second_0:                 episode reward: 2.4000,                 loss: nan
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 1475.0,                last time consumption/overall running time: 221.6672s / 34954.4149 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0091
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 1598.05,                last time consumption/overall running time: 235.7478s / 35190.1627 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0063
env0_second_0:                 episode reward: 2.3000,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 1505.45,                last time consumption/overall running time: 224.0841s / 35414.2468 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0067
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 1921.5,                last time consumption/overall running time: 288.0514s / 35702.2982 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0054
env0_second_0:                 episode reward: 1.9500,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1317.6,                last time consumption/overall running time: 197.5556s / 35899.8538 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0046
env0_second_0:                 episode reward: 2.6000,                 loss: nan
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1184.7,                last time consumption/overall running time: 178.6428s / 36078.4966 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0067
env0_second_0:                 episode reward: 2.7000,                 loss: nan
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1929.55,                last time consumption/overall running time: 290.2032s / 36368.6998 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0073
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 2026.3,                last time consumption/overall running time: 303.8457s / 36672.5456 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0061
env0_second_0:                 episode reward: 2.0500,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 1512.75,                last time consumption/overall running time: 224.5732s / 36897.1188 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0062
env0_second_0:                 episode reward: 1.2500,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 1543.95,                last time consumption/overall running time: 231.6504s / 37128.7692 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0071
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 1712.25,                last time consumption/overall running time: 255.4399s / 37384.2091 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0073
env0_second_0:                 episode reward: 1.4000,                 loss: nan
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 2222.0,                last time consumption/overall running time: 333.3173s / 37717.5264 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0066
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 1841.25,                last time consumption/overall running time: 272.9631s / 37990.4895 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0063
env0_second_0:                 episode reward: -0.0500,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 2132.25,                last time consumption/overall running time: 317.3433s / 38307.8328 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0052
env0_second_0:                 episode reward: -0.9500,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 1495.95,                last time consumption/overall running time: 224.7019s / 38532.5346 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0045
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 1782.2,                last time consumption/overall running time: 267.4031s / 38799.9377 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0056
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 2359.3,                last time consumption/overall running time: 352.7432s / 39152.6809 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0052
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 1789.9,                last time consumption/overall running time: 269.4461s / 39422.1270 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0055
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 2073.05,                last time consumption/overall running time: 309.6990s / 39731.8259 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0056
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 1899.05,                last time consumption/overall running time: 282.4791s / 40014.3051 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0051
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 2179.35,                last time consumption/overall running time: 325.8180s / 40340.1231 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0050
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 1725.85,                last time consumption/overall running time: 260.5925s / 40600.7155 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0047
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 1807.35,                last time consumption/overall running time: 272.2310s / 40872.9465 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0052
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 1814.5,                last time consumption/overall running time: 272.7209s / 41145.6674 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0048
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 2286.05,                last time consumption/overall running time: 342.3247s / 41487.9921 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0047
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 1875.8,                last time consumption/overall running time: 282.2669s / 41770.2591 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0047
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 2066.35,                last time consumption/overall running time: 308.3570s / 42078.6160 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0052
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 1625.9,                last time consumption/overall running time: 243.2477s / 42321.8637 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0048
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1885.6,                last time consumption/overall running time: 279.0136s / 42600.8773 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0051
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 647.35,                last time consumption/overall running time: 89.0686s / 42689.9459 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0044
env0_second_0:                 episode reward: 3.4000,                 loss: nan
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 667.35,                last time consumption/overall running time: 95.1513s / 42785.0972 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0061
env0_second_0:                 episode reward: 4.0500,                 loss: nan
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 593.3,                last time consumption/overall running time: 86.7763s / 42871.8735 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0066
env0_second_0:                 episode reward: 3.9500,                 loss: nan
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 614.35,                last time consumption/overall running time: 91.4552s / 42963.3288 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0074
env0_second_0:                 episode reward: 4.0500,                 loss: nan
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 584.65,                last time consumption/overall running time: 88.1324s / 43051.4611 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0086
env0_second_0:                 episode reward: 3.3500,                 loss: nan
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 614.35,                last time consumption/overall running time: 92.2510s / 43143.7121 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0080
env0_second_0:                 episode reward: 3.5500,                 loss: nan
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 564.6,                last time consumption/overall running time: 84.7105s / 43228.4226 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0093
env0_second_0:                 episode reward: 3.8000,                 loss: nan
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 615.9,                last time consumption/overall running time: 91.2923s / 43319.7149 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.0102
env0_second_0:                 episode reward: 3.1500,                 loss: nan
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 677.05,                last time consumption/overall running time: 100.3627s / 43420.0776 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.0103
env0_second_0:                 episode reward: 3.9000,                 loss: nan
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 626.45,                last time consumption/overall running time: 92.3525s / 43512.4300 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0112
env0_second_0:                 episode reward: 3.4500,                 loss: nan
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 683.2,                last time consumption/overall running time: 101.7594s / 43614.1895 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0103
env0_second_0:                 episode reward: 3.4000,                 loss: nan
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 676.5,                last time consumption/overall running time: 102.3578s / 43716.5472 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0112
env0_second_0:                 episode reward: 3.8500,                 loss: nan
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 741.85,                last time consumption/overall running time: 111.2639s / 43827.8111 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0093
env0_second_0:                 episode reward: 3.5000,                 loss: nan
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 654.6,                last time consumption/overall running time: 98.0004s / 43925.8116 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0082
env0_second_0:                 episode reward: 3.8500,                 loss: nan
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 637.4,                last time consumption/overall running time: 94.7489s / 44020.5604 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0091
env0_second_0:                 episode reward: 3.6000,                 loss: nan
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 625.55,                last time consumption/overall running time: 93.7920s / 44114.3524 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0093
env0_second_0:                 episode reward: 3.2000,                 loss: nan
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 617.95,                last time consumption/overall running time: 91.8175s / 44206.1700 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0102
env0_second_0:                 episode reward: 2.9500,                 loss: nan
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 626.6,                last time consumption/overall running time: 93.7230s / 44299.8930 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0101
env0_second_0:                 episode reward: 3.3500,                 loss: nan
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 664.65,                last time consumption/overall running time: 99.3903s / 44399.2833 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0106
env0_second_0:                 episode reward: 3.1000,                 loss: nan
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 646.1,                last time consumption/overall running time: 95.3265s / 44494.6098 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0109
env0_second_0:                 episode reward: 3.6000,                 loss: nan
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 683.25,                last time consumption/overall running time: 102.2229s / 44596.8327 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0103
env0_second_0:                 episode reward: 2.8000,                 loss: nan
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 614.45,                last time consumption/overall running time: 92.0887s / 44688.9214 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0105
env0_second_0:                 episode reward: 3.6500,                 loss: nan
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 627.9,                last time consumption/overall running time: 94.5076s / 44783.4290 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0109
env0_second_0:                 episode reward: 3.4000,                 loss: nan
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 646.3,                last time consumption/overall running time: 96.9997s / 44880.4287 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0112
env0_second_0:                 episode reward: 3.5500,                 loss: nan
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 674.2,                last time consumption/overall running time: 99.6964s / 44980.1251 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0111
env0_second_0:                 episode reward: 3.3500,                 loss: nan
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 760.4,                last time consumption/overall running time: 114.8018s / 45094.9269 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0106
env0_second_0:                 episode reward: 3.6000,                 loss: nan
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 779.4,                last time consumption/overall running time: 117.2323s / 45212.1592 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0099
env0_second_0:                 episode reward: 3.2000,                 loss: nan
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 780.55,                last time consumption/overall running time: 117.4306s / 45329.5898 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0103
env0_second_0:                 episode reward: 3.5000,                 loss: nan
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 831.2,                last time consumption/overall running time: 125.3225s / 45454.9123 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0111
env0_second_0:                 episode reward: 3.0000,                 loss: nan
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 1237.8,                last time consumption/overall running time: 188.5371s / 45643.4494 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0099
env0_second_0:                 episode reward: 2.2500,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 1490.35,                last time consumption/overall running time: 223.1933s / 45866.6427 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0085
env0_second_0:                 episode reward: 0.7500,                 loss: nan
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 1695.8,                last time consumption/overall running time: 253.1501s / 46119.7928 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0067
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 1783.9,                last time consumption/overall running time: 265.4323s / 46385.2251 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0065
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1709.4,                last time consumption/overall running time: 254.7840s / 46640.0091 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0052
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 1736.8,                last time consumption/overall running time: 260.6547s / 46900.6637 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0053
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 1820.55,                last time consumption/overall running time: 269.0041s / 47169.6679 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0047
env0_second_0:                 episode reward: 0.9000,                 loss: nan
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 1233.2,                last time consumption/overall running time: 182.4722s / 47352.1401 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0054
env0_second_0:                 episode reward: 1.9500,                 loss: nan
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 1167.8,                last time consumption/overall running time: 176.8289s / 47528.9690 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0058
env0_second_0:                 episode reward: 3.0500,                 loss: nan
env1_first_0:                 episode reward: -2.6000,                 loss: nan