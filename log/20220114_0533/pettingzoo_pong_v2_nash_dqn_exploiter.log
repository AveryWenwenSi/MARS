pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [72, 91]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
      (3): ReLU()
      (4): Linear(in_features=64, out_features=64, bias=True)
      (5): ReLU()
      (6): Linear(in_features=64, out_features=64, bias=True)
      (7): ReLU()
      (8): Linear(in_features=64, out_features=6, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
      (3): ReLU()
      (4): Linear(in_features=64, out_features=64, bias=True)
      (5): ReLU()
      (6): Linear(in_features=64, out_features=64, bias=True)
      (7): ReLU()
      (8): Linear(in_features=64, out_features=6, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 1}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220114_0533/pettingzoo_pong_v2_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/20220114_0533/pettingzoo_pong_v2_nash_dqn_exploiter.
Episode: 1/10000 (0.0100%),                 avg. length: 1025.0,                last time consumption/overall running time: 8.2161s / 8.2161 s
env0_first_0:                 episode reward: -5.0000,                 loss: 0.0235
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0250
env1_first_0:                 episode reward: 18.0000,                 loss: nan
env1_second_0:                 episode reward: -18.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1172.35,                last time consumption/overall running time: 173.1568s / 181.3728 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0232
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0241
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1294.3,                last time consumption/overall running time: 227.9356s / 409.3084 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0167
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0200
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1245.65,                last time consumption/overall running time: 234.1098s / 643.4182 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0079
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0111
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1150.7,                last time consumption/overall running time: 219.6887s / 863.1069 s
env0_first_0:                 episode reward: 3.6000,                 loss: 0.0040
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0061
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 947.1,                last time consumption/overall running time: 182.8096s / 1045.9166 s
env0_first_0:                 episode reward: 10.8000,                 loss: 0.0030
env0_second_0:                 episode reward: -10.8000,                 loss: 0.0034
env1_first_0:                 episode reward: 13.8000,                 loss: nan
env1_second_0:                 episode reward: -13.8000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1060.85,                last time consumption/overall running time: 205.6634s / 1251.5799 s
env0_first_0:                 episode reward: 8.0000,                 loss: 0.0026
env0_second_0:                 episode reward: -8.0000,                 loss: 0.0026
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1016.4,                last time consumption/overall running time: 195.8884s / 1447.4683 s
env0_first_0:                 episode reward: 8.5500,                 loss: 0.0025
env0_second_0:                 episode reward: -8.5500,                 loss: 0.0022
env1_first_0:                 episode reward: 7.6000,                 loss: nan
env1_second_0:                 episode reward: -7.6000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 889.2,                last time consumption/overall running time: 172.0678s / 1619.5361 s
env0_first_0:                 episode reward: 14.0000,                 loss: 0.0025
env0_second_0:                 episode reward: -14.0000,                 loss: 0.0023
env1_first_0:                 episode reward: 13.0000,                 loss: nan
env1_second_0:                 episode reward: -13.0000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 976.55,                last time consumption/overall running time: 188.6906s / 1808.2267 s
env0_first_0:                 episode reward: 5.5500,                 loss: 0.0025
env0_second_0:                 episode reward: -5.5500,                 loss: 0.0022
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1151.05,                last time consumption/overall running time: 223.3535s / 2031.5802 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0031
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0028
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1204.3,                last time consumption/overall running time: 232.3304s / 2263.9107 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0049
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0041
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1146.75,                last time consumption/overall running time: 217.0564s / 2480.9671 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0064
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0049
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1221.95,                last time consumption/overall running time: 234.9876s / 2715.9547 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0066
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0054
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1325.15,                last time consumption/overall running time: 252.3681s / 2968.3228 s
env0_first_0:                 episode reward: -4.3000,                 loss: 0.0071
env0_second_0:                 episode reward: 4.3000,                 loss: 0.0060
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1189.55,                last time consumption/overall running time: 229.9987s / 3198.3215 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0085
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0069
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1251.2,                last time consumption/overall running time: 239.7101s / 3438.0316 s
env0_first_0:                 episode reward: -4.8500,                 loss: 0.0087
env0_second_0:                 episode reward: 4.8500,                 loss: 0.0075
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1191.95,                last time consumption/overall running time: 228.9864s / 3667.0180 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0088
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0075
env1_first_0:                 episode reward: -8.3000,                 loss: nan
env1_second_0:                 episode reward: 8.3000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1252.35,                last time consumption/overall running time: 240.8939s / 3907.9119 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0086
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0077
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1289.25,                last time consumption/overall running time: 247.0681s / 4154.9800 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0096
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0080
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1231.6,                last time consumption/overall running time: 233.8947s / 4388.8747 s
env0_first_0:                 episode reward: -8.9000,                 loss: 0.0104
env0_second_0:                 episode reward: 8.9000,                 loss: 0.0096
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1289.4,                last time consumption/overall running time: 250.5357s / 4639.4103 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0111
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0100
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1192.05,                last time consumption/overall running time: 230.0458s / 4869.4562 s
env0_first_0:                 episode reward: -8.9000,                 loss: 0.0109
env0_second_0:                 episode reward: 8.9000,                 loss: 0.0102
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1286.95,                last time consumption/overall running time: 246.7022s / 5116.1583 s
env0_first_0:                 episode reward: -7.2000,                 loss: 0.0114
env0_second_0:                 episode reward: 7.2000,                 loss: 0.0101
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1267.55,                last time consumption/overall running time: 242.0301s / 5358.1884 s
env0_first_0:                 episode reward: -7.4500,                 loss: 0.0125
env0_second_0:                 episode reward: 7.4500,                 loss: 0.0110
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1159.95,                last time consumption/overall running time: 223.3563s / 5581.5447 s
env0_first_0:                 episode reward: -9.2500,                 loss: 0.0120
env0_second_0:                 episode reward: 9.2500,                 loss: 0.0103
env1_first_0:                 episode reward: -8.7000,                 loss: nan
env1_second_0:                 episode reward: 8.7000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1240.25,                last time consumption/overall running time: 240.4693s / 5822.0140 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0115
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0105
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1227.85,                last time consumption/overall running time: 236.6968s / 6058.7108 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0113
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0112
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1171.6,                last time consumption/overall running time: 221.6526s / 6280.3634 s
env0_first_0:                 episode reward: -8.3500,                 loss: 0.0113
env0_second_0:                 episode reward: 8.3500,                 loss: 0.0105
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1080.4,                last time consumption/overall running time: 206.0274s / 6486.3908 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0116
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0112
env1_first_0:                 episode reward: -8.8500,                 loss: nan
env1_second_0:                 episode reward: 8.8500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1294.9,                last time consumption/overall running time: 248.9527s / 6735.3435 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0123
env0_second_0:                 episode reward: 4.4500,                 loss: 0.0119
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1196.75,                last time consumption/overall running time: 233.4986s / 6968.8421 s
env0_first_0:                 episode reward: -7.1000,                 loss: 0.0126
env0_second_0:                 episode reward: 7.1000,                 loss: 0.0125
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1191.7,                last time consumption/overall running time: 229.8461s / 7198.6882 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0129
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0121
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1140.05,                last time consumption/overall running time: 216.9306s / 7415.6188 s
env0_first_0:                 episode reward: -8.9500,                 loss: 0.0124
env0_second_0:                 episode reward: 8.9500,                 loss: 0.0115
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1205.85,                last time consumption/overall running time: 231.4002s / 7647.0190 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0135
env0_second_0:                 episode reward: 4.0500,                 loss: 0.0115
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1251.2,                last time consumption/overall running time: 242.3584s / 7889.3774 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0142
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0116
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1137.65,                last time consumption/overall running time: 217.5782s / 8106.9556 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0139
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0111
env1_first_0:                 episode reward: -7.9500,                 loss: nan
env1_second_0:                 episode reward: 7.9500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1099.95,                last time consumption/overall running time: 211.3129s / 8318.2685 s
env0_first_0:                 episode reward: -8.4000,                 loss: 0.0136
env0_second_0:                 episode reward: 8.4000,                 loss: 0.0109
env1_first_0:                 episode reward: -8.3500,                 loss: nan
env1_second_0:                 episode reward: 8.3500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 1214.4,                last time consumption/overall running time: 232.8763s / 8551.1448 s
env0_first_0:                 episode reward: -9.1500,                 loss: 0.0145
env0_second_0:                 episode reward: 9.1500,                 loss: 0.0122
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1195.0,                last time consumption/overall running time: 231.0197s / 8782.1645 s
env0_first_0:                 episode reward: -4.5500,                 loss: 0.0151
env0_second_0:                 episode reward: 4.5500,                 loss: 0.0117
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1182.9,                last time consumption/overall running time: 225.2771s / 9007.4416 s
env0_first_0:                 episode reward: -8.7500,                 loss: 0.0152
env0_second_0:                 episode reward: 8.7500,                 loss: 0.0119
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1142.4,                last time consumption/overall running time: 217.6321s / 9225.0737 s
env0_first_0:                 episode reward: -6.3500,                 loss: 0.0153
env0_second_0:                 episode reward: 6.3500,                 loss: 0.0131
env1_first_0:                 episode reward: -10.4500,                 loss: nan
env1_second_0:                 episode reward: 10.4500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1189.15,                last time consumption/overall running time: 225.0922s / 9450.1660 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0158
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0122
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1124.15,                last time consumption/overall running time: 215.6324s / 9665.7983 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0155
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0125
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1221.45,                last time consumption/overall running time: 230.5404s / 9896.3387 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0151
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0129
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1124.25,                last time consumption/overall running time: 215.5812s / 10111.9199 s
env0_first_0:                 episode reward: -9.0500,                 loss: 0.0151
env0_second_0:                 episode reward: 9.0500,                 loss: 0.0133
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1144.95,                last time consumption/overall running time: 219.0440s / 10330.9639 s
env0_first_0:                 episode reward: -9.9000,                 loss: 0.0164
env0_second_0:                 episode reward: 9.9000,                 loss: 0.0137
env1_first_0:                 episode reward: -9.7500,                 loss: nan
env1_second_0:                 episode reward: 9.7500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1097.45,                last time consumption/overall running time: 211.4756s / 10542.4394 s
env0_first_0:                 episode reward: -11.6500,                 loss: 0.0151
env0_second_0:                 episode reward: 11.6500,                 loss: 0.0137
env1_first_0:                 episode reward: -9.4500,                 loss: nan
env1_second_0:                 episode reward: 9.4500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1110.1,                last time consumption/overall running time: 212.1871s / 10754.6265 s
env0_first_0:                 episode reward: -9.0500,                 loss: 0.0153
env0_second_0:                 episode reward: 9.0500,                 loss: 0.0136
env1_first_0:                 episode reward: -10.5000,                 loss: nan
env1_second_0:                 episode reward: 10.5000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1117.5,                last time consumption/overall running time: 212.9913s / 10967.6178 s
env0_first_0:                 episode reward: -9.9500,                 loss: 0.0162
env0_second_0:                 episode reward: 9.9500,                 loss: 0.0142
env1_first_0:                 episode reward: -12.1500,                 loss: nan
env1_second_0:                 episode reward: 12.1500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1095.35,                last time consumption/overall running time: 211.9941s / 11179.6120 s
env0_first_0:                 episode reward: -10.9500,                 loss: 0.0167
env0_second_0:                 episode reward: 10.9500,                 loss: 0.0146
env1_first_0:                 episode reward: -11.2000,                 loss: nan
env1_second_0:                 episode reward: 11.2000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1111.35,                last time consumption/overall running time: 213.0006s / 11392.6126 s
env0_first_0:                 episode reward: -10.9000,                 loss: 0.0178
env0_second_0:                 episode reward: 10.9000,                 loss: 0.0151
env1_first_0:                 episode reward: -9.7500,                 loss: nan
env1_second_0:                 episode reward: 9.7500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1082.95,                last time consumption/overall running time: 208.2831s / 11600.8957 s
env0_first_0:                 episode reward: -10.0500,                 loss: 0.0185
env0_second_0:                 episode reward: 10.0500,                 loss: 0.0147
env1_first_0:                 episode reward: -11.3500,                 loss: nan
env1_second_0:                 episode reward: 11.3500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1001.85,                last time consumption/overall running time: 192.9100s / 11793.8057 s
env0_first_0:                 episode reward: -11.0500,                 loss: 0.0186
env0_second_0:                 episode reward: 11.0500,                 loss: 0.0147
env1_first_0:                 episode reward: -11.5500,                 loss: nan
env1_second_0:                 episode reward: 11.5500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1094.15,                last time consumption/overall running time: 206.2787s / 12000.0844 s
env0_first_0:                 episode reward: -11.7000,                 loss: 0.0179
env0_second_0:                 episode reward: 11.7000,                 loss: 0.0134
env1_first_0:                 episode reward: -12.9500,                 loss: nan
env1_second_0:                 episode reward: 12.9500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1135.6,                last time consumption/overall running time: 218.9137s / 12218.9981 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.0177
env0_second_0:                 episode reward: 7.1500,                 loss: 0.0144
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1197.25,                last time consumption/overall running time: 227.3491s / 12446.3472 s
env0_first_0:                 episode reward: -9.8500,                 loss: 0.0174
env0_second_0:                 episode reward: 9.8500,                 loss: 0.0173
env1_first_0:                 episode reward: -9.4500,                 loss: nan
env1_second_0:                 episode reward: 9.4500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1075.8,                last time consumption/overall running time: 205.7922s / 12652.1394 s
env0_first_0:                 episode reward: -13.1000,                 loss: 0.0184
env0_second_0:                 episode reward: 13.1000,                 loss: 0.0176
env1_first_0:                 episode reward: -13.5500,                 loss: nan
env1_second_0:                 episode reward: 13.5500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1075.95,                last time consumption/overall running time: 206.7838s / 12858.9232 s
env0_first_0:                 episode reward: -10.3000,                 loss: 0.0190
env0_second_0:                 episode reward: 10.3000,                 loss: 0.0171
env1_first_0:                 episode reward: -13.0000,                 loss: nan
env1_second_0:                 episode reward: 13.0000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1153.9,                last time consumption/overall running time: 224.6807s / 13083.6039 s
env0_first_0:                 episode reward: -11.4000,                 loss: 0.0177
env0_second_0:                 episode reward: 11.4000,                 loss: 0.0174
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1074.6,                last time consumption/overall running time: 206.5583s / 13290.1622 s
env0_first_0:                 episode reward: -10.8000,                 loss: 0.0171
env0_second_0:                 episode reward: 10.8000,                 loss: 0.0166
env1_first_0:                 episode reward: -11.0000,                 loss: nan
env1_second_0:                 episode reward: 11.0000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1020.45,                last time consumption/overall running time: 197.5693s / 13487.7315 s
env0_first_0:                 episode reward: -13.3000,                 loss: 0.0176
env0_second_0:                 episode reward: 13.3000,                 loss: 0.0169
env1_first_0:                 episode reward: -14.7000,                 loss: nan
env1_second_0:                 episode reward: 14.7000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1058.2,                last time consumption/overall running time: 205.6595s / 13693.3910 s
env0_first_0:                 episode reward: -15.8000,                 loss: 0.0175
env0_second_0:                 episode reward: 15.8000,                 loss: 0.0154
env1_first_0:                 episode reward: -10.5500,                 loss: nan
env1_second_0:                 episode reward: 10.5500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1042.55,                last time consumption/overall running time: 201.7769s / 13895.1678 s
env0_first_0:                 episode reward: -12.0500,                 loss: 0.0173
env0_second_0:                 episode reward: 12.0500,                 loss: 0.0164
env1_first_0:                 episode reward: -15.2500,                 loss: nan
env1_second_0:                 episode reward: 15.2500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1105.55,                last time consumption/overall running time: 217.9631s / 14113.1309 s
env0_first_0:                 episode reward: -13.0500,                 loss: 0.0196
env0_second_0:                 episode reward: 13.0500,                 loss: 0.0180
env1_first_0:                 episode reward: -13.4000,                 loss: nan
env1_second_0:                 episode reward: 13.4000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1118.75,                last time consumption/overall running time: 216.6084s / 14329.7393 s
env0_first_0:                 episode reward: -11.5000,                 loss: 0.0190
env0_second_0:                 episode reward: 11.5000,                 loss: 0.0184
env1_first_0:                 episode reward: -12.0500,                 loss: nan
env1_second_0:                 episode reward: 12.0500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1131.1,                last time consumption/overall running time: 219.8119s / 14549.5512 s
env0_first_0:                 episode reward: -12.9500,                 loss: 0.0190
env0_second_0:                 episode reward: 12.9500,                 loss: 0.0208
env1_first_0:                 episode reward: -13.8500,                 loss: nan
env1_second_0:                 episode reward: 13.8500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1089.6,                last time consumption/overall running time: 209.5913s / 14759.1425 s
env0_first_0:                 episode reward: -13.6000,                 loss: 0.0196
env0_second_0:                 episode reward: 13.6000,                 loss: 0.0202
env1_first_0:                 episode reward: -13.5500,                 loss: nan
env1_second_0:                 episode reward: 13.5500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1136.45,                last time consumption/overall running time: 221.1516s / 14980.2941 s
env0_first_0:                 episode reward: -12.7500,                 loss: 0.0200
env0_second_0:                 episode reward: 12.7500,                 loss: 0.0210
env1_first_0:                 episode reward: -11.0000,                 loss: nan
env1_second_0:                 episode reward: 11.0000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1159.15,                last time consumption/overall running time: 223.4277s / 15203.7218 s
env0_first_0:                 episode reward: -10.9500,                 loss: 0.0204
env0_second_0:                 episode reward: 10.9500,                 loss: 0.0217
env1_first_0:                 episode reward: -9.8500,                 loss: nan
env1_second_0:                 episode reward: 9.8500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1142.1,                last time consumption/overall running time: 223.3277s / 15427.0495 s
env0_first_0:                 episode reward: -13.4000,                 loss: 0.0209
env0_second_0:                 episode reward: 13.4000,                 loss: 0.0213
env1_first_0:                 episode reward: -12.3500,                 loss: nan
env1_second_0:                 episode reward: 12.3500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1098.9,                last time consumption/overall running time: 212.9494s / 15639.9989 s
env0_first_0:                 episode reward: -12.8000,                 loss: 0.0211
env0_second_0:                 episode reward: 12.8000,                 loss: 0.0206
env1_first_0:                 episode reward: -14.5500,                 loss: nan
env1_second_0:                 episode reward: 14.5500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 1046.1,                last time consumption/overall running time: 203.7249s / 15843.7238 s
env0_first_0:                 episode reward: -13.1000,                 loss: 0.0201
env0_second_0:                 episode reward: 13.1000,                 loss: 0.0203
env1_first_0:                 episode reward: -12.1000,                 loss: nan
env1_second_0:                 episode reward: 12.1000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1093.85,                last time consumption/overall running time: 208.0641s / 16051.7879 s
env0_first_0:                 episode reward: -14.0000,                 loss: 0.0213
env0_second_0:                 episode reward: 14.0000,                 loss: 0.0197
env1_first_0:                 episode reward: -13.4000,                 loss: nan
env1_second_0:                 episode reward: 13.4000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1116.75,                last time consumption/overall running time: 214.3042s / 16266.0920 s
env0_first_0:                 episode reward: -13.1000,                 loss: 0.0224
env0_second_0:                 episode reward: 13.1000,                 loss: 0.0183
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1148.9,                last time consumption/overall running time: 221.2304s / 16487.3224 s
env0_first_0:                 episode reward: -12.2000,                 loss: 0.0215
env0_second_0:                 episode reward: 12.2000,                 loss: 0.0201
env1_first_0:                 episode reward: -14.2000,                 loss: nan
env1_second_0:                 episode reward: 14.2000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1120.75,                last time consumption/overall running time: 215.8010s / 16703.1234 s
env0_first_0:                 episode reward: -14.1000,                 loss: 0.0214
env0_second_0:                 episode reward: 14.1000,                 loss: 0.0220
env1_first_0:                 episode reward: -13.8000,                 loss: nan
env1_second_0:                 episode reward: 13.8000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1122.3,                last time consumption/overall running time: 216.8637s / 16919.9871 s
env0_first_0:                 episode reward: -12.4000,                 loss: 0.0217
env0_second_0:                 episode reward: 12.4000,                 loss: 0.0220
env1_first_0:                 episode reward: -13.5000,                 loss: nan
env1_second_0:                 episode reward: 13.5000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1048.15,                last time consumption/overall running time: 200.6005s / 17120.5876 s
env0_first_0:                 episode reward: -14.1000,                 loss: 0.0248
env0_second_0:                 episode reward: 14.1000,                 loss: 0.0227
env1_first_0:                 episode reward: -13.7000,                 loss: nan
env1_second_0:                 episode reward: 13.7000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1007.85,                last time consumption/overall running time: 195.4582s / 17316.0458 s
env0_first_0:                 episode reward: -15.3000,                 loss: 0.0204
env0_second_0:                 episode reward: 15.3000,                 loss: 0.0229
env1_first_0:                 episode reward: -14.6500,                 loss: nan
env1_second_0:                 episode reward: 14.6500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1134.3,                last time consumption/overall running time: 218.5303s / 17534.5761 s
env0_first_0:                 episode reward: -13.4000,                 loss: 0.0207
env0_second_0:                 episode reward: 13.4000,                 loss: 0.0191
env1_first_0:                 episode reward: -14.2500,                 loss: nan
env1_second_0:                 episode reward: 14.2500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1073.4,                last time consumption/overall running time: 205.2953s / 17739.8714 s
env0_first_0:                 episode reward: -12.0500,                 loss: 0.0203
env0_second_0:                 episode reward: 12.0500,                 loss: 0.0198
env1_first_0:                 episode reward: -16.1500,                 loss: nan
env1_second_0:                 episode reward: 16.1500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1105.85,                last time consumption/overall running time: 213.4286s / 17953.3000 s
env0_first_0:                 episode reward: -13.7000,                 loss: 0.0196
env0_second_0:                 episode reward: 13.7000,                 loss: 0.0221
env1_first_0:                 episode reward: -15.5500,                 loss: nan
env1_second_0:                 episode reward: 15.5500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1105.1,                last time consumption/overall running time: 211.2416s / 18164.5417 s
env0_first_0:                 episode reward: -14.4000,                 loss: 0.0215
env0_second_0:                 episode reward: 14.4000,                 loss: 0.0212
env1_first_0:                 episode reward: -13.8000,                 loss: nan
env1_second_0:                 episode reward: 13.8000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1339.55,                last time consumption/overall running time: 257.2588s / 18421.8004 s
env0_first_0:                 episode reward: -12.6000,                 loss: 0.0212
env0_second_0:                 episode reward: 12.6000,                 loss: 0.0206
env1_first_0:                 episode reward: -11.7000,                 loss: nan
env1_second_0:                 episode reward: 11.7000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1324.95,                last time consumption/overall running time: 253.3147s / 18675.1151 s
env0_first_0:                 episode reward: -8.9000,                 loss: 0.0206
env0_second_0:                 episode reward: 8.9000,                 loss: 0.0212
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1209.15,                last time consumption/overall running time: 236.2820s / 18911.3971 s
env0_first_0:                 episode reward: -10.9500,                 loss: 0.0219
env0_second_0:                 episode reward: 10.9500,                 loss: 0.0209
env1_first_0:                 episode reward: -13.1000,                 loss: nan
env1_second_0:                 episode reward: 13.1000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1248.2,                last time consumption/overall running time: 243.9480s / 19155.3451 s
env0_first_0:                 episode reward: -11.6500,                 loss: 0.0205
env0_second_0:                 episode reward: 11.6500,                 loss: 0.0205
env1_first_0:                 episode reward: -13.0000,                 loss: nan
env1_second_0:                 episode reward: 13.0000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1327.7,                last time consumption/overall running time: 255.8848s / 19411.2299 s
env0_first_0:                 episode reward: -11.4000,                 loss: 0.0193
env0_second_0:                 episode reward: 11.4000,                 loss: 0.0208
env1_first_0:                 episode reward: -11.2000,                 loss: nan
env1_second_0:                 episode reward: 11.2000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1276.15,                last time consumption/overall running time: 248.1442s / 19659.3741 s
env0_first_0:                 episode reward: -9.5500,                 loss: 0.0198
env0_second_0:                 episode reward: 9.5500,                 loss: 0.0209
env1_first_0:                 episode reward: -10.7000,                 loss: nan
env1_second_0:                 episode reward: 10.7000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1166.35,                last time consumption/overall running time: 229.0214s / 19888.3955 s
env0_first_0:                 episode reward: -13.6500,                 loss: 0.0190
env0_second_0:                 episode reward: 13.6500,                 loss: 0.0237
env1_first_0:                 episode reward: -12.1500,                 loss: nan
env1_second_0:                 episode reward: 12.1500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1263.6,                last time consumption/overall running time: 244.7982s / 20133.1937 s
env0_first_0:                 episode reward: -12.0500,                 loss: 0.0225
env0_second_0:                 episode reward: 12.0500,                 loss: 0.0238
env1_first_0:                 episode reward: -13.6000,                 loss: nan
env1_second_0:                 episode reward: 13.6000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1221.3,                last time consumption/overall running time: 236.5311s / 20369.7247 s
env0_first_0:                 episode reward: -13.4500,                 loss: 0.0195
env0_second_0:                 episode reward: 13.4500,                 loss: 0.0231
env1_first_0:                 episode reward: -13.9500,                 loss: nan
env1_second_0:                 episode reward: 13.9500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1237.7,                last time consumption/overall running time: 237.9967s / 20607.7215 s
env0_first_0:                 episode reward: -12.1000,                 loss: 0.0194
env0_second_0:                 episode reward: 12.1000,                 loss: 0.0228
env1_first_0:                 episode reward: -14.6500,                 loss: nan
env1_second_0:                 episode reward: 14.6500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1259.35,                last time consumption/overall running time: 242.8586s / 20850.5801 s
env0_first_0:                 episode reward: -13.3500,                 loss: 0.0205
env0_second_0:                 episode reward: 13.3500,                 loss: 0.0222
env1_first_0:                 episode reward: -12.6500,                 loss: nan
env1_second_0:                 episode reward: 12.6500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1384.95,                last time consumption/overall running time: 264.5118s / 21115.0919 s
env0_first_0:                 episode reward: -12.8000,                 loss: 0.0215
env0_second_0:                 episode reward: 12.8000,                 loss: 0.0248
env1_first_0:                 episode reward: -11.3500,                 loss: nan
env1_second_0:                 episode reward: 11.3500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1322.3,                last time consumption/overall running time: 256.8155s / 21371.9073 s
env0_first_0:                 episode reward: -12.9000,                 loss: 0.0214
env0_second_0:                 episode reward: 12.9000,                 loss: 0.0257
env1_first_0:                 episode reward: -11.5500,                 loss: nan
env1_second_0:                 episode reward: 11.5500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 1227.5,                last time consumption/overall running time: 237.3631s / 21609.2704 s
env0_first_0:                 episode reward: -13.1000,                 loss: 0.0222
env0_second_0:                 episode reward: 13.1000,                 loss: 0.0243
env1_first_0:                 episode reward: -12.4500,                 loss: nan
env1_second_0:                 episode reward: 12.4500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 1313.35,                last time consumption/overall running time: 254.7892s / 21864.0596 s
env0_first_0:                 episode reward: -11.7500,                 loss: 0.0226
env0_second_0:                 episode reward: 11.7500,                 loss: 0.0234
env1_first_0:                 episode reward: -13.1000,                 loss: nan
env1_second_0:                 episode reward: 13.1000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 1344.3,                last time consumption/overall running time: 255.1694s / 22119.2290 s
env0_first_0:                 episode reward: -12.6500,                 loss: 0.0222
env0_second_0:                 episode reward: 12.6500,                 loss: 0.0202
env1_first_0:                 episode reward: -11.2000,                 loss: nan
env1_second_0:                 episode reward: 11.2000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1288.55,                last time consumption/overall running time: 249.9154s / 22369.1444 s
env0_first_0:                 episode reward: -12.9000,                 loss: 0.0210
env0_second_0:                 episode reward: 12.9000,                 loss: 0.0188
env1_first_0:                 episode reward: -11.7000,                 loss: nan
env1_second_0:                 episode reward: 11.7000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1324.2,                last time consumption/overall running time: 256.5897s / 22625.7341 s
env0_first_0:                 episode reward: -11.4500,                 loss: 0.0210
env0_second_0:                 episode reward: 11.4500,                 loss: 0.0174
env1_first_0:                 episode reward: -13.1000,                 loss: nan
env1_second_0:                 episode reward: 13.1000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1457.45,                last time consumption/overall running time: 281.2519s / 22906.9860 s
env0_first_0:                 episode reward: -7.8500,                 loss: 0.0192
env0_second_0:                 episode reward: 7.8500,                 loss: 0.0182
env1_first_0:                 episode reward: -10.9000,                 loss: nan
env1_second_0:                 episode reward: 10.9000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1185.45,                last time consumption/overall running time: 235.5468s / 23142.5328 s
env0_first_0:                 episode reward: -12.2000,                 loss: 0.0203
env0_second_0:                 episode reward: 12.2000,                 loss: 0.0188
env1_first_0:                 episode reward: -14.6000,                 loss: nan
env1_second_0:                 episode reward: 14.6000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 1311.8,                last time consumption/overall running time: 254.2225s / 23396.7553 s
env0_first_0:                 episode reward: -13.6000,                 loss: 0.0191
env0_second_0:                 episode reward: 13.6000,                 loss: 0.0195
env1_first_0:                 episode reward: -13.1000,                 loss: nan
env1_second_0:                 episode reward: 13.1000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 1316.35,                last time consumption/overall running time: 256.3251s / 23653.0804 s
env0_first_0:                 episode reward: -14.2500,                 loss: 0.0198
env0_second_0:                 episode reward: 14.2500,                 loss: 0.0173
env1_first_0:                 episode reward: -12.3000,                 loss: nan
env1_second_0:                 episode reward: 12.3000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 1380.75,                last time consumption/overall running time: 266.4440s / 23919.5244 s
env0_first_0:                 episode reward: -12.5000,                 loss: 0.0207
env0_second_0:                 episode reward: 12.5000,                 loss: 0.0190
env1_first_0:                 episode reward: -12.8000,                 loss: nan
env1_second_0:                 episode reward: 12.8000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 1357.15,                last time consumption/overall running time: 260.8854s / 24180.4098 s
env0_first_0:                 episode reward: -12.5000,                 loss: 0.0206
env0_second_0:                 episode reward: 12.5000,                 loss: 0.0164
env1_first_0:                 episode reward: -11.7500,                 loss: nan
env1_second_0:                 episode reward: 11.7500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1367.05,                last time consumption/overall running time: 267.6234s / 24448.0332 s
env0_first_0:                 episode reward: -14.1500,                 loss: 0.0201
env0_second_0:                 episode reward: 14.1500,                 loss: 0.0155
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1379.15,                last time consumption/overall running time: 268.8379s / 24716.8711 s
env0_first_0:                 episode reward: -12.2000,                 loss: 0.0202
env0_second_0:                 episode reward: 12.2000,                 loss: 0.0169
env1_first_0:                 episode reward: -10.2500,                 loss: nan
env1_second_0:                 episode reward: 10.2500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 1358.5,                last time consumption/overall running time: 263.7363s / 24980.6074 s
env0_first_0:                 episode reward: -10.8000,                 loss: 0.0209
env0_second_0:                 episode reward: 10.8000,                 loss: 0.0184
env1_first_0:                 episode reward: -13.0000,                 loss: nan
env1_second_0:                 episode reward: 13.0000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1352.55,                last time consumption/overall running time: 265.3005s / 25245.9079 s
env0_first_0:                 episode reward: -11.5500,                 loss: 0.0228
env0_second_0:                 episode reward: 11.5500,                 loss: 0.0172
env1_first_0:                 episode reward: -12.1500,                 loss: nan
env1_second_0:                 episode reward: 12.1500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 1427.6,                last time consumption/overall running time: 277.9306s / 25523.8386 s
env0_first_0:                 episode reward: -11.1000,                 loss: 0.0228
env0_second_0:                 episode reward: 11.1000,                 loss: 0.0185
env1_first_0:                 episode reward: -11.8000,                 loss: nan
env1_second_0:                 episode reward: 11.8000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 1425.4,                last time consumption/overall running time: 271.5452s / 25795.3837 s
env0_first_0:                 episode reward: -9.9500,                 loss: 0.0211
env0_second_0:                 episode reward: 9.9500,                 loss: 0.0198
env1_first_0:                 episode reward: -11.0000,                 loss: nan
env1_second_0:                 episode reward: 11.0000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1343.4,                last time consumption/overall running time: 260.4236s / 26055.8073 s
env0_first_0:                 episode reward: -12.0000,                 loss: 0.0184
env0_second_0:                 episode reward: 12.0000,                 loss: 0.0197
env1_first_0:                 episode reward: -14.1000,                 loss: nan
env1_second_0:                 episode reward: 14.1000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 1241.75,                last time consumption/overall running time: 241.7995s / 26297.6069 s
env0_first_0:                 episode reward: -15.3500,                 loss: 0.0174
env0_second_0:                 episode reward: 15.3500,                 loss: 0.0186
env1_first_0:                 episode reward: -13.2000,                 loss: nan
env1_second_0:                 episode reward: 13.2000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 1445.75,                last time consumption/overall running time: 282.8643s / 26580.4712 s
env0_first_0:                 episode reward: -10.6000,                 loss: 0.0211
env0_second_0:                 episode reward: 10.6000,                 loss: 0.0214
env1_first_0:                 episode reward: -12.7500,                 loss: nan
env1_second_0:                 episode reward: 12.7500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 1343.1,                last time consumption/overall running time: 259.0701s / 26839.5413 s
env0_first_0:                 episode reward: -12.3500,                 loss: 0.0249
env0_second_0:                 episode reward: 12.3500,                 loss: 0.0197
env1_first_0:                 episode reward: -13.0000,                 loss: nan
env1_second_0:                 episode reward: 13.0000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1422.5,                last time consumption/overall running time: 273.8530s / 27113.3943 s
env0_first_0:                 episode reward: -13.7000,                 loss: 0.0227
env0_second_0:                 episode reward: 13.7000,                 loss: 0.0202
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 1359.35,                last time consumption/overall running time: 261.4065s / 27374.8008 s
env0_first_0:                 episode reward: -11.3000,                 loss: 0.0203
env0_second_0:                 episode reward: 11.3000,                 loss: 0.0195
env1_first_0:                 episode reward: -13.7500,                 loss: nan
env1_second_0:                 episode reward: 13.7500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 1368.3,                last time consumption/overall running time: 264.6182s / 27639.4190 s
env0_first_0:                 episode reward: -13.0000,                 loss: 0.0210
env0_second_0:                 episode reward: 13.0000,                 loss: 0.0182
env1_first_0:                 episode reward: -13.0000,                 loss: nan
env1_second_0:                 episode reward: 13.0000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 1315.65,                last time consumption/overall running time: 254.2904s / 27893.7094 s
env0_first_0:                 episode reward: -14.1000,                 loss: 0.0231
env0_second_0:                 episode reward: 14.1000,                 loss: 0.0199
env1_first_0:                 episode reward: -12.2000,                 loss: nan
env1_second_0:                 episode reward: 12.2000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 1317.85,                last time consumption/overall running time: 255.2170s / 28148.9264 s
env0_first_0:                 episode reward: -13.0500,                 loss: 0.0224
env0_second_0:                 episode reward: 13.0500,                 loss: 0.0177
env1_first_0:                 episode reward: -12.1000,                 loss: nan
env1_second_0:                 episode reward: 12.1000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1225.55,                last time consumption/overall running time: 237.8894s / 28386.8158 s
env0_first_0:                 episode reward: -12.5000,                 loss: 0.0221
env0_second_0:                 episode reward: 12.5000,                 loss: 0.0167
env1_first_0:                 episode reward: -12.0500,                 loss: nan
env1_second_0:                 episode reward: 12.0500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 1290.1,                last time consumption/overall running time: 248.0766s / 28634.8924 s
env0_first_0:                 episode reward: -13.1500,                 loss: 0.0249
env0_second_0:                 episode reward: 13.1500,                 loss: 0.0152
env1_first_0:                 episode reward: -14.2000,                 loss: nan
env1_second_0:                 episode reward: 14.2000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 1280.05,                last time consumption/overall running time: 248.3370s / 28883.2294 s
env0_first_0:                 episode reward: -13.4000,                 loss: 0.0249
env0_second_0:                 episode reward: 13.4000,                 loss: 0.0181
env1_first_0:                 episode reward: -15.5000,                 loss: nan
env1_second_0:                 episode reward: 15.5000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 1465.6,                last time consumption/overall running time: 285.9431s / 29169.1725 s
env0_first_0:                 episode reward: -11.9000,                 loss: 0.0229
env0_second_0:                 episode reward: 11.9000,                 loss: 0.0159
env1_first_0:                 episode reward: -12.7500,                 loss: nan
env1_second_0:                 episode reward: 12.7500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 1420.55,                last time consumption/overall running time: 276.9424s / 29446.1150 s
env0_first_0:                 episode reward: -9.2500,                 loss: 0.0254
env0_second_0:                 episode reward: 9.2500,                 loss: 0.0153
env1_first_0:                 episode reward: -11.5000,                 loss: nan
env1_second_0:                 episode reward: 11.5000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 1525.45,                last time consumption/overall running time: 294.1410s / 29740.2559 s
env0_first_0:                 episode reward: -9.4500,                 loss: 0.0236
env0_second_0:                 episode reward: 9.4500,                 loss: 0.0158
env1_first_0:                 episode reward: -10.6000,                 loss: nan
env1_second_0:                 episode reward: 10.6000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1573.15,                last time consumption/overall running time: 305.5760s / 30045.8319 s
env0_first_0:                 episode reward: -11.8000,                 loss: 0.0225
env0_second_0:                 episode reward: 11.8000,                 loss: 0.0165
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 1451.7,                last time consumption/overall running time: 280.5458s / 30326.3777 s
env0_first_0:                 episode reward: -9.9500,                 loss: 0.0240
env0_second_0:                 episode reward: 9.9500,                 loss: 0.0163
env1_first_0:                 episode reward: -10.2000,                 loss: nan
env1_second_0:                 episode reward: 10.2000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1716.35,                last time consumption/overall running time: 334.1634s / 30660.5411 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0243
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0154
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 1902.0,                last time consumption/overall running time: 370.1966s / 31030.7378 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0241
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0167
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 1777.8,                last time consumption/overall running time: 344.2726s / 31375.0104 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0229
env0_second_0:                 episode reward: 3.6000,                 loss: 0.0168
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 1568.65,                last time consumption/overall running time: 304.1308s / 31679.1411 s
env0_first_0:                 episode reward: -8.8500,                 loss: 0.0247
env0_second_0:                 episode reward: 8.8500,                 loss: 0.0167
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 1616.95,                last time consumption/overall running time: 312.2648s / 31991.4059 s
env0_first_0:                 episode reward: -7.4500,                 loss: 0.0251
env0_second_0:                 episode reward: 7.4500,                 loss: 0.0158
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1536.3,                last time consumption/overall running time: 298.5880s / 32289.9939 s
env0_first_0:                 episode reward: -11.4500,                 loss: 0.0237
env0_second_0:                 episode reward: 11.4500,                 loss: 0.0143
env1_first_0:                 episode reward: -12.0000,                 loss: nan
env1_second_0:                 episode reward: 12.0000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1493.5,                last time consumption/overall running time: 288.8231s / 32578.8170 s
env0_first_0:                 episode reward: -10.7000,                 loss: 0.0196
env0_second_0:                 episode reward: 10.7000,                 loss: 0.0128
env1_first_0:                 episode reward: -12.9000,                 loss: nan
env1_second_0:                 episode reward: 12.9000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 1574.3,                last time consumption/overall running time: 304.4949s / 32883.3119 s
env0_first_0:                 episode reward: -13.2000,                 loss: 0.0192
env0_second_0:                 episode reward: 13.2000,                 loss: 0.0131
env1_first_0:                 episode reward: -10.6000,                 loss: nan
env1_second_0:                 episode reward: 10.6000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1561.3,                last time consumption/overall running time: 301.8397s / 33185.1516 s
env0_first_0:                 episode reward: -11.7500,                 loss: 0.0188
env0_second_0:                 episode reward: 11.7500,                 loss: 0.0131
env1_first_0:                 episode reward: -12.4500,                 loss: nan
env1_second_0:                 episode reward: 12.4500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1439.9,                last time consumption/overall running time: 275.3081s / 33460.4597 s
env0_first_0:                 episode reward: -14.7000,                 loss: 0.0187
env0_second_0:                 episode reward: 14.7000,                 loss: 0.0118
env1_first_0:                 episode reward: -12.3500,                 loss: nan
env1_second_0:                 episode reward: 12.3500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 1355.15,                last time consumption/overall running time: 264.5763s / 33725.0360 s
env0_first_0:                 episode reward: -12.4500,                 loss: 0.0209
env0_second_0:                 episode reward: 12.4500,                 loss: 0.0120
env1_first_0:                 episode reward: -13.7000,                 loss: nan
env1_second_0:                 episode reward: 13.7000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 1497.0,                last time consumption/overall running time: 291.8429s / 34016.8789 s
env0_first_0:                 episode reward: -12.9000,                 loss: 0.0193
env0_second_0:                 episode reward: 12.9000,                 loss: 0.0119
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 1524.05,                last time consumption/overall running time: 289.8817s / 34306.7606 s
env0_first_0:                 episode reward: -10.3500,                 loss: 0.0196
env0_second_0:                 episode reward: 10.3500,                 loss: 0.0122
env1_first_0:                 episode reward: -11.6000,                 loss: nan
env1_second_0:                 episode reward: 11.6000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 1400.5,                last time consumption/overall running time: 270.2506s / 34577.0112 s
env0_first_0:                 episode reward: -11.1000,                 loss: 0.0196
env0_second_0:                 episode reward: 11.1000,                 loss: 0.0102
env1_first_0:                 episode reward: -13.5500,                 loss: nan
env1_second_0:                 episode reward: 13.5500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 1423.65,                last time consumption/overall running time: 276.7064s / 34853.7175 s
env0_first_0:                 episode reward: -14.8500,                 loss: 0.0193
env0_second_0:                 episode reward: 14.8500,                 loss: 0.0100
env1_first_0:                 episode reward: -14.0500,                 loss: nan
env1_second_0:                 episode reward: 14.0500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 1465.45,                last time consumption/overall running time: 281.5558s / 35135.2733 s
env0_first_0:                 episode reward: -13.2500,                 loss: 0.0208
env0_second_0:                 episode reward: 13.2500,                 loss: 0.0096
env1_first_0:                 episode reward: -13.6000,                 loss: nan
env1_second_0:                 episode reward: 13.6000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 1467.45,                last time consumption/overall running time: 288.6158s / 35423.8891 s
env0_first_0:                 episode reward: -14.8000,                 loss: 0.0184
env0_second_0:                 episode reward: 14.8000,                 loss: 0.0098
env1_first_0:                 episode reward: -11.1000,                 loss: nan
env1_second_0:                 episode reward: 11.1000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 1494.5,                last time consumption/overall running time: 294.3158s / 35718.2049 s
env0_first_0:                 episode reward: -12.0500,                 loss: 0.0187
env0_second_0:                 episode reward: 12.0500,                 loss: 0.0096
env1_first_0:                 episode reward: -14.7500,                 loss: nan
env1_second_0:                 episode reward: 14.7500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 1544.0,                last time consumption/overall running time: 300.0971s / 36018.3020 s
env0_first_0:                 episode reward: -11.2500,                 loss: 0.0182
env0_second_0:                 episode reward: 11.2500,                 loss: 0.0104
env1_first_0:                 episode reward: -13.3500,                 loss: nan
env1_second_0:                 episode reward: 13.3500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 1453.75,                last time consumption/overall running time: 282.4551s / 36300.7571 s
env0_first_0:                 episode reward: -12.2500,                 loss: 0.0199
env0_second_0:                 episode reward: 12.2500,                 loss: 0.0102
env1_first_0:                 episode reward: -14.7000,                 loss: nan
env1_second_0:                 episode reward: 14.7000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 1590.5,                last time consumption/overall running time: 303.9818s / 36604.7389 s
env0_first_0:                 episode reward: -11.6500,                 loss: 0.0192
env0_second_0:                 episode reward: 11.6500,                 loss: 0.0109
env1_first_0:                 episode reward: -9.7500,                 loss: nan
env1_second_0:                 episode reward: 9.7500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 1544.8,                last time consumption/overall running time: 301.9319s / 36906.6708 s
env0_first_0:                 episode reward: -10.1500,                 loss: 0.0193
env0_second_0:                 episode reward: 10.1500,                 loss: 0.0113
env1_first_0:                 episode reward: -12.0500,                 loss: nan
env1_second_0:                 episode reward: 12.0500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 1647.9,                last time consumption/overall running time: 318.2263s / 37224.8971 s
env0_first_0:                 episode reward: -10.9500,                 loss: 0.0192
env0_second_0:                 episode reward: 10.9500,                 loss: 0.0128
env1_first_0:                 episode reward: -11.2000,                 loss: nan
env1_second_0:                 episode reward: 11.2000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 1434.0,                last time consumption/overall running time: 267.1936s / 37492.0907 s
env0_first_0:                 episode reward: -11.4500,                 loss: 0.0188
env0_second_0:                 episode reward: 11.4500,                 loss: 0.0141
env1_first_0:                 episode reward: -14.3000,                 loss: nan
env1_second_0:                 episode reward: 14.3000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 1493.1,                last time consumption/overall running time: 285.9955s / 37778.0862 s
env0_first_0:                 episode reward: -13.3000,                 loss: 0.0185
env0_second_0:                 episode reward: 13.3000,                 loss: 0.0128
env1_first_0:                 episode reward: -13.5500,                 loss: nan
env1_second_0:                 episode reward: 13.5500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 1438.55,                last time consumption/overall running time: 274.5871s / 38052.6733 s
env0_first_0:                 episode reward: -14.9000,                 loss: 0.0195
env0_second_0:                 episode reward: 14.9000,                 loss: 0.0130
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 1390.9,                last time consumption/overall running time: 268.2959s / 38320.9691 s
env0_first_0:                 episode reward: -13.2000,                 loss: 0.0177
env0_second_0:                 episode reward: 13.2000,                 loss: 0.0119
env1_first_0:                 episode reward: -13.7000,                 loss: nan
env1_second_0:                 episode reward: 13.7000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 1437.45,                last time consumption/overall running time: 275.1756s / 38596.1447 s
env0_first_0:                 episode reward: -12.3500,                 loss: 0.0176
env0_second_0:                 episode reward: 12.3500,                 loss: 0.0141
env1_first_0:                 episode reward: -14.0500,                 loss: nan
env1_second_0:                 episode reward: 14.0500,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 1461.35,                last time consumption/overall running time: 281.8304s / 38877.9751 s
env0_first_0:                 episode reward: -13.4000,                 loss: 0.0181
env0_second_0:                 episode reward: 13.4000,                 loss: 0.0137
env1_first_0:                 episode reward: -15.0000,                 loss: nan
env1_second_0:                 episode reward: 15.0000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 1473.0,                last time consumption/overall running time: 279.7380s / 39157.7131 s
env0_first_0:                 episode reward: -12.8500,                 loss: 0.0207
env0_second_0:                 episode reward: 12.8500,                 loss: 0.0124
env1_first_0:                 episode reward: -14.7500,                 loss: nan
env1_second_0:                 episode reward: 14.7500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 1451.2,                last time consumption/overall running time: 279.0241s / 39436.7372 s
env0_first_0:                 episode reward: -15.6000,                 loss: 0.0191
env0_second_0:                 episode reward: 15.6000,                 loss: 0.0135
env1_first_0:                 episode reward: -12.8000,                 loss: nan
env1_second_0:                 episode reward: 12.8000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 1514.3,                last time consumption/overall running time: 294.4657s / 39731.2030 s
env0_first_0:                 episode reward: -13.5500,                 loss: 0.0186
env0_second_0:                 episode reward: 13.5500,                 loss: 0.0122
env1_first_0:                 episode reward: -14.0500,                 loss: nan
env1_second_0:                 episode reward: 14.0500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 1533.1,                last time consumption/overall running time: 296.5639s / 40027.7668 s
env0_first_0:                 episode reward: -13.5500,                 loss: 0.0164
env0_second_0:                 episode reward: 13.5500,                 loss: 0.0119
env1_first_0:                 episode reward: -13.3500,                 loss: nan
env1_second_0:                 episode reward: 13.3500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 1502.2,                last time consumption/overall running time: 290.0954s / 40317.8622 s
env0_first_0:                 episode reward: -13.6000,                 loss: 0.0165
env0_second_0:                 episode reward: 13.6000,                 loss: 0.0119
env1_first_0:                 episode reward: -13.9500,                 loss: nan
env1_second_0:                 episode reward: 13.9500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 1609.25,                last time consumption/overall running time: 307.9990s / 40625.8613 s
env0_first_0:                 episode reward: -13.0500,                 loss: 0.0178
env0_second_0:                 episode reward: 13.0500,                 loss: 0.0119
env1_first_0:                 episode reward: -13.4000,                 loss: nan
env1_second_0:                 episode reward: 13.4000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 1448.6,                last time consumption/overall running time: 277.2792s / 40903.1405 s
env0_first_0:                 episode reward: -11.5500,                 loss: 0.0163
env0_second_0:                 episode reward: 11.5500,                 loss: 0.0122
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 1592.15,                last time consumption/overall running time: 303.2230s / 41206.3635 s
env0_first_0:                 episode reward: -12.3000,                 loss: 0.0170
env0_second_0:                 episode reward: 12.3000,                 loss: 0.0106
env1_first_0:                 episode reward: -14.7500,                 loss: nan
env1_second_0:                 episode reward: 14.7500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1447.45,                last time consumption/overall running time: 277.2316s / 41483.5951 s
env0_first_0:                 episode reward: -14.4500,                 loss: 0.0148
env0_second_0:                 episode reward: 14.4500,                 loss: 0.0108
env1_first_0:                 episode reward: -12.9500,                 loss: nan
env1_second_0:                 episode reward: 12.9500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1501.45,                last time consumption/overall running time: 281.9821s / 41765.5772 s
env0_first_0:                 episode reward: -13.2500,                 loss: 0.0138
env0_second_0:                 episode reward: 13.2500,                 loss: 0.0109
env1_first_0:                 episode reward: -12.8500,                 loss: nan
env1_second_0:                 episode reward: 12.8500,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 1632.55,                last time consumption/overall running time: 307.8937s / 42073.4708 s
env0_first_0:                 episode reward: -12.1500,                 loss: 0.0162
env0_second_0:                 episode reward: 12.1500,                 loss: 0.0104
env1_first_0:                 episode reward: -12.5500,                 loss: nan
env1_second_0:                 episode reward: 12.5500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 1592.7,                last time consumption/overall running time: 306.4231s / 42379.8939 s
env0_first_0:                 episode reward: -13.7000,                 loss: 0.0160
env0_second_0:                 episode reward: 13.7000,                 loss: 0.0102
env1_first_0:                 episode reward: -13.4000,                 loss: nan
env1_second_0:                 episode reward: 13.4000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 1576.5,                last time consumption/overall running time: 304.8757s / 42684.7697 s
env0_first_0:                 episode reward: -12.0000,                 loss: 0.0166
env0_second_0:                 episode reward: 12.0000,                 loss: 0.0105
env1_first_0:                 episode reward: -11.4500,                 loss: nan
env1_second_0:                 episode reward: 11.4500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1637.95,                last time consumption/overall running time: 313.9800s / 42998.7496 s
env0_first_0:                 episode reward: -11.5500,                 loss: 0.0151
env0_second_0:                 episode reward: 11.5500,                 loss: 0.0110
env1_first_0:                 episode reward: -12.9000,                 loss: nan
env1_second_0:                 episode reward: 12.9000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1570.05,                last time consumption/overall running time: 303.9298s / 43302.6795 s
env0_first_0:                 episode reward: -13.4000,                 loss: 0.0153
env0_second_0:                 episode reward: 13.4000,                 loss: 0.0119
env1_first_0:                 episode reward: -14.4000,                 loss: nan
env1_second_0:                 episode reward: 14.4000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1496.05,                last time consumption/overall running time: 287.5711s / 43590.2506 s
env0_first_0:                 episode reward: -12.6000,                 loss: 0.0150
env0_second_0:                 episode reward: 12.6000,                 loss: 0.0121
env1_first_0:                 episode reward: -14.5500,                 loss: nan
env1_second_0:                 episode reward: 14.5500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1537.5,                last time consumption/overall running time: 295.9549s / 43886.2054 s
env0_first_0:                 episode reward: -12.2500,                 loss: 0.0155
env0_second_0:                 episode reward: 12.2500,                 loss: 0.0119
env1_first_0:                 episode reward: -12.4000,                 loss: nan
env1_second_0:                 episode reward: 12.4000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1496.25,                last time consumption/overall running time: 289.0367s / 44175.2421 s
env0_first_0:                 episode reward: -13.0500,                 loss: 0.0133
env0_second_0:                 episode reward: 13.0500,                 loss: 0.0107
env1_first_0:                 episode reward: -13.1000,                 loss: nan
env1_second_0:                 episode reward: 13.1000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1479.7,                last time consumption/overall running time: 287.2145s / 44462.4567 s
env0_first_0:                 episode reward: -12.3500,                 loss: 0.0133
env0_second_0:                 episode reward: 12.3500,                 loss: 0.0095
env1_first_0:                 episode reward: -15.0500,                 loss: nan
env1_second_0:                 episode reward: 15.0500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1527.8,                last time consumption/overall running time: 290.4505s / 44752.9072 s
env0_first_0:                 episode reward: -13.1000,                 loss: 0.0150
env0_second_0:                 episode reward: 13.1000,                 loss: 0.0101
env1_first_0:                 episode reward: -13.1000,                 loss: nan
env1_second_0:                 episode reward: 13.1000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1583.1,                last time consumption/overall running time: 298.9836s / 45051.8908 s
env0_first_0:                 episode reward: -11.9500,                 loss: 0.0146
env0_second_0:                 episode reward: 11.9500,                 loss: 0.0106
env1_first_0:                 episode reward: -15.5500,                 loss: nan
env1_second_0:                 episode reward: 15.5500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1668.9,                last time consumption/overall running time: 311.4860s / 45363.3768 s
env0_first_0:                 episode reward: -13.5000,                 loss: 0.0149
env0_second_0:                 episode reward: 13.5000,                 loss: 0.0098
env1_first_0:                 episode reward: -11.8000,                 loss: nan
env1_second_0:                 episode reward: 11.8000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1431.75,                last time consumption/overall running time: 276.3054s / 45639.6822 s