pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 51
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7fc22b8d5b00>
Agents No. [1] (index starting from 0) are not learnable.
{'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 16, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7fc22a9e89e8>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 14
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 16, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f2a9125b278>}}
Save models to : /home/zihan/research/MARS/data/model/0/pettingzoo_surround_v1_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/0/pettingzoo_surround_v1_fictitious_selfplay2.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 1511.6,                    last time consumption/overall running time: 161.8705s / 161.8705 s
first_0:                     episode reward: -4.0500
second_0:                     episode reward: 4.0500
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 1508.0,                    last time consumption/overall running time: 184.2378s / 346.1083 s
first_0:                     episode reward: -0.2000
second_0:                     episode reward: 0.2000
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 1452.25,                    last time consumption/overall running time: 185.1252s / 531.2335 s
first_0:                     episode reward: -5.0000
second_0:                     episode reward: 5.0000
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 1024.1,                    last time consumption/overall running time: 132.1819s / 663.4154 s
first_0:                     episode reward: -9.0500
second_0:                     episode reward: 9.0500
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 1050.3,                    last time consumption/overall running time: 137.1981s / 800.6135 s
first_0:                     episode reward: -6.7500
second_0:                     episode reward: 6.7500
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 1057.4,                    last time consumption/overall running time: 136.8000s / 937.4136 s
first_0:                     episode reward: 9.7500
second_0:                     episode reward: -9.7500
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 1128.25,                    last time consumption/overall running time: 146.5705s / 1083.9841 s
first_0:                     episode reward: 9.4000
second_0:                     episode reward: -9.4000
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 1064.75,                    last time consumption/overall running time: 138.9465s / 1222.9306 s
first_0:                     episode reward: 9.9500
second_0:                     episode reward: -9.9500
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 1078.95,                    last time consumption/overall running time: 140.3392s / 1363.2698 s
first_0:                     episode reward: 9.9500
second_0:                     episode reward: -9.9500
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 1096.05,                    last time consumption/overall running time: 143.0351s / 1506.3049 s
first_0:                     episode reward: 9.9500
second_0:                     episode reward: -9.9500
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 1071.9,                    last time consumption/overall running time: 140.2682s / 1646.5730 s
first_0:                     episode reward: 9.9500
second_0:                     episode reward: -9.9500
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 1095.95,                    last time consumption/overall running time: 143.4123s / 1789.9853 s
first_0:                     episode reward: 9.5500
second_0:                     episode reward: -9.5500
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 1038.4,                    last time consumption/overall running time: 135.7369s / 1925.7222 s
first_0:                     episode reward: 8.2000
second_0:                     episode reward: -8.2000
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 1043.4,                    last time consumption/overall running time: 135.8630s / 2061.5852 s
first_0:                     episode reward: 6.6000
second_0:                     episode reward: -6.6000
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 1319.85,                    last time consumption/overall running time: 172.8075s / 2234.3926 s
first_0:                     episode reward: 8.0500
second_0:                     episode reward: -8.0500
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 1351.8,                    last time consumption/overall running time: 175.9739s / 2410.3665 s
first_0:                     episode reward: 8.9500
second_0:                     episode reward: -8.9500
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 1339.2,                    last time consumption/overall running time: 173.8800s / 2584.2466 s
first_0:                     episode reward: 8.8000
second_0:                     episode reward: -8.8000
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 1297.3,                    last time consumption/overall running time: 169.1355s / 2753.3820 s
first_0:                     episode reward: 9.1500
second_0:                     episode reward: -9.1500
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 1176.5,                    last time consumption/overall running time: 153.5578s / 2906.9399 s
first_0:                     episode reward: 9.8500
second_0:                     episode reward: -9.8500
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 1204.35,                    last time consumption/overall running time: 155.6153s / 3062.5552 s
first_0:                     episode reward: 7.0000
second_0:                     episode reward: -7.0000
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 1047.25,                    last time consumption/overall running time: 134.1905s / 3196.7457 s
first_0:                     episode reward: 3.2500
second_0:                     episode reward: -3.2500
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 1248.5,                    last time consumption/overall running time: 161.7419s / 3358.4875 s
first_0:                     episode reward: -7.2000
second_0:                     episode reward: 7.2000
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 1991.8,                    last time consumption/overall running time: 259.0372s / 3617.5247 s
first_0:                     episode reward: 1.0000
second_0:                     episode reward: -1.0000
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 1581.45,                    last time consumption/overall running time: 205.5422s / 3823.0669 spygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 91
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 16, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f2a9125b278>}}
Save models to : /home/zihan/research/MARS/data/model/1/pettingzoo_surround_v1_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/1/pettingzoo_surround_v1_fictitious_selfplay2.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 1540.95,                    last time consumption/overall running time: 165.3744s / 165.3744 s
first_0:                     episode reward: -2.8000
second_0:                     episode reward: 2.8000
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 1446.6,                    last time consumption/overall running time: 177.0375s / 342.4119 s
first_0:                     episode reward: 0.6500
second_0:                     episode reward: -0.6500
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 1385.0,                    last time consumption/overall running time: 176.7502s / 519.1622 s
first_0:                     episode reward: -6.2000
second_0:                     episode reward: 6.2000
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 1069.4,                    last time consumption/overall running time: 138.2703s / 657.4325 s
first_0:                     episode reward: -8.4500
second_0:                     episode reward: 8.4500
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 1403.55,                    last time consumption/overall running time: 183.2005s / 840.6330 s
first_0:                     episode reward: -1.3500
second_0:                     episode reward: 1.3500
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 1381.95,                    last time consumption/overall running time: 179.4014s / 1020.0344 s
first_0:                     episode reward: 5.2000
second_0:                     episode reward: -5.2000
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 1091.6,                    last time consumption/overall running time: 142.2209s / 1162.2553 s
first_0:                     episode reward: 9.9000
second_0:                     episode reward: -9.9000
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 1068.6,                    last time consumption/overall running time: 139.8354s / 1302.0908 s
first_0:                     episode reward: 9.9000
second_0:                     episode reward: -9.9000
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 1093.2,                    last time consumption/overall running time: 142.5645s / 1444.6553 s
first_0:                     episode reward: 9.8500
second_0:                     episode reward: -9.8500
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 1078.15,                    last time consumption/overall running time: 141.4644s / 1586.1197 s
first_0:                     episode reward: 9.9500
second_0:                     episode reward: -9.9500
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 1093.8,                    last time consumption/overall running time: 143.3386s / 1729.4583 s
first_0:                     episode reward: 9.8500
second_0:                     episode reward: -9.8500
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 1074.5,                    last time consumption/overall running time: 141.2471s / 1870.7054 s
first_0:                     episode reward: 10.0000
second_0:                     episode reward: -10.0000
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 1108.4,                    last time consumption/overall running time: 145.0782s / 2015.7836 s
first_0:                     episode reward: 5.2500
second_0:                     episode reward: -5.2500
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 1186.55,                    last time consumption/overall running time: 155.8112s / 2171.5947 s
first_0:                     episode reward: 9.7000
second_0:                     episode reward: -9.7000
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 1298.15,                    last time consumption/overall running time: 169.8161s / 2341.4108 s
first_0:                     episode reward: 4.4000
second_0:                     episode reward: -4.4000
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 1288.85,                    last time consumption/overall running time: 168.0930s / 2509.5039 s
first_0:                     episode reward: 9.6000
second_0:                     episode reward: -9.6000
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 1390.9,                    last time consumption/overall running time: 181.1103s / 2690.6141 s
first_0:                     episode reward: 8.2500
second_0:                     episode reward: -8.2500
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 1134.65,                    last time consumption/overall running time: 148.2737s / 2838.8878 s
first_0:                     episode reward: 9.3500
second_0:                     episode reward: -9.3500
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 1291.65,                    last time consumption/overall running time: 168.2490s / 3007.1368 s
first_0:                     episode reward: 9.6500
second_0:                     episode reward: -9.6500
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 1577.75,                    last time consumption/overall running time: 202.4242s / 3209.5610 s
first_0:                     episode reward: 6.5500
second_0:                     episode reward: -6.5500
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 1437.4,                    last time consumption/overall running time: 186.8726s / 3396.4337 s
first_0:                     episode reward: 8.4000
second_0:                     episode reward: -8.4000
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 1580.1,                    last time consumption/overall running time: 205.7295s / 3602.1632 s
first_0:                     episode reward: 0.0500
second_0:                     episode reward: -0.0500
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 1703.5,                    last time consumption/overall running time: 222.3321s / 3824.4953 s
first_0:                     episode reward: 5.7500
second_0:                     episode reward: -5.7500
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 1454.7,                    last time consumption/overall running time: 190.4962s / 4014.9914 s