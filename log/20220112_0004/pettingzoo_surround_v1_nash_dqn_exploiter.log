pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 15
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7f61c54fad68>
No agent are not learnable.
{'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 1}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f61c549afd0>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 3
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 1}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f27aecf9128>}}
Save models to : /home/zihan/research/MARS/data/model/1/pettingzoo_surround_v1_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/1/pettingzoo_surround_v1_nash_dqn_exploiter.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 1604.35,                    last time consumption/overall running time: 142.8294s / 142.8294 s
first_0:                     episode reward: -1.1000
second_0:                     episode reward: 1.1000
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 1379.2,                    last time consumption/overall running time: 176.3524s / 319.1818 s
first_0:                     episode reward: -6.3000
second_0:                     episode reward: 6.3000
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 1351.55,                    last time consumption/overall running time: 190.9855s / 510.1674 s
first_0:                     episode reward: -6.1000
second_0:                     episode reward: 6.1000
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 1333.0,                    last time consumption/overall running time: 193.0975s / 703.2649 s
first_0:                     episode reward: -6.8500
second_0:                     episode reward: 6.8500
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 1347.6,                    last time consumption/overall running time: 197.2597s / 900.5245 s
first_0:                     episode reward: -6.7000
second_0:                     episode reward: 6.7000
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 1346.7,                    last time consumption/overall running time: 201.5642s / 1102.0887 s
first_0:                     episode reward: -7.2500
second_0:                     episode reward: 7.2500
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 1254.7,                    last time consumption/overall running time: 186.6407s / 1288.7294 s
first_0:                     episode reward: -8.2500
second_0:                     episode reward: 8.2500
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 1239.55,                    last time consumption/overall running time: 183.7286s / 1472.4580 s
first_0:                     episode reward: -8.1500
second_0:                     episode reward: 8.1500
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 1170.7,                    last time consumption/overall running time: 176.4538s / 1648.9118 s
first_0:                     episode reward: -8.5000
second_0:                     episode reward: 8.5000
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 1323.35,                    last time consumption/overall running time: 196.9034s / 1845.8152 s
first_0:                     episode reward: -7.1500
second_0:                     episode reward: 7.1500
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 1328.45,                    last time consumption/overall running time: 200.3964s / 2046.2116 s
first_0:                     episode reward: -6.9500
second_0:                     episode reward: 6.9500
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 1240.3,                    last time consumption/overall running time: 183.3157s / 2229.5273 s
first_0:                     episode reward: -8.3500
second_0:                     episode reward: 8.3500
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 1246.45,                    last time consumption/overall running time: 186.1229s / 2415.6503 s
first_0:                     episode reward: -8.2500
second_0:                     episode reward: 8.2500
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 1236.7,                    last time consumption/overall running time: 183.7453s / 2599.3956 s
first_0:                     episode reward: -8.2500
second_0:                     episode reward: 8.2500
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 1204.3,                    last time consumption/overall running time: 179.4907s / 2778.8863 s
first_0:                     episode reward: -8.4000
second_0:                     episode reward: 8.4000
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 1443.4,                    last time consumption/overall running time: 214.7284s / 2993.6147 s
first_0:                     episode reward: -5.8500
second_0:                     episode reward: 5.8500
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 1255.25,                    last time consumption/overall running time: 188.0275s / 3181.6421 s
first_0:                     episode reward: -8.1000
second_0:                     episode reward: 8.1000
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 1192.25,                    last time consumption/overall running time: 178.8083s / 3360.4504 s
first_0:                     episode reward: -8.7500
second_0:                     episode reward: 8.7500
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 1219.6,                    last time consumption/overall running time: 182.8682s / 3543.3186 s
first_0:                     episode reward: -8.6500
second_0:                     episode reward: 8.6500
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 1414.25,                    last time consumption/overall running time: 210.7170s / 3754.0356 s
first_0:                     episode reward: -6.1000
second_0:                     episode reward: 6.1000
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 1447.85,                    last time consumption/overall running time: 217.1675s / 3971.2032 s
first_0:                     episode reward: -4.6500
second_0:                     episode reward: 4.6500
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 1287.4,                    last time consumption/overall running time: 189.1436s / 4160.3468 s
first_0:                     episode reward: -7.0000
second_0:                     episode reward: 7.0000
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 1369.6,                    last time consumption/overall running time: 203.0252s / 4363.3720 s
first_0:                     episode reward: -6.9000
second_0:                     episode reward: 6.9000
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 1194.95,                    last time consumption/overall running time: 178.6119s / 4541.9839 spygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 99
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 1}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f27aecf9128>}}
Save models to : /home/zihan/research/MARS/data/model/0/pettingzoo_surround_v1_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/0/pettingzoo_surround_v1_nash_dqn_exploiter.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 1546.8,                    last time consumption/overall running time: 138.1607s / 138.1607 s
first_0:                     episode reward: -0.8000
second_0:                     episode reward: 0.8000
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 1562.6,                    last time consumption/overall running time: 197.0090s / 335.1697 s
first_0:                     episode reward: -4.2000
second_0:                     episode reward: 4.2000
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 1418.75,                    last time consumption/overall running time: 200.0877s / 535.2575 s
first_0:                     episode reward: -6.1000
second_0:                     episode reward: 6.1000
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 1337.0,                    last time consumption/overall running time: 195.0679s / 730.3253 s
first_0:                     episode reward: -6.9000
second_0:                     episode reward: 6.9000
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 1335.85,                    last time consumption/overall running time: 200.0801s / 930.4054 s
first_0:                     episode reward: -6.9500
second_0:                     episode reward: 6.9500
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 1284.35,                    last time consumption/overall running time: 192.1806s / 1122.5861 s
first_0:                     episode reward: -7.4000
second_0:                     episode reward: 7.4000
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 1281.85,                    last time consumption/overall running time: 190.9039s / 1313.4900 s
first_0:                     episode reward: -8.1000
second_0:                     episode reward: 8.1000
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 1259.0,                    last time consumption/overall running time: 187.3421s / 1500.8321 s
first_0:                     episode reward: -8.0000
second_0:                     episode reward: 8.0000
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 1282.2,                    last time consumption/overall running time: 194.1044s / 1694.9365 s
first_0:                     episode reward: -7.7000
second_0:                     episode reward: 7.7000
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 1272.85,                    last time consumption/overall running time: 192.5856s / 1887.5221 s
first_0:                     episode reward: -6.9500
second_0:                     episode reward: 6.9500
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 1408.75,                    last time consumption/overall running time: 210.9508s / 2098.4730 s
first_0:                     episode reward: -6.1500
second_0:                     episode reward: 6.1500
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 1325.65,                    last time consumption/overall running time: 200.1577s / 2298.6307 s
first_0:                     episode reward: -7.1500
second_0:                     episode reward: 7.1500
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 1233.8,                    last time consumption/overall running time: 185.5582s / 2484.1889 s
first_0:                     episode reward: -7.8500
second_0:                     episode reward: 7.8500
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 1282.65,                    last time consumption/overall running time: 194.2623s / 2678.4512 s
first_0:                     episode reward: -8.3500
second_0:                     episode reward: 8.3500
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 1280.85,                    last time consumption/overall running time: 194.5331s / 2872.9843 s
first_0:                     episode reward: -7.7500
second_0:                     episode reward: 7.7500
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 1339.45,                    last time consumption/overall running time: 202.1989s / 3075.1832 s
first_0:                     episode reward: -6.3000
second_0:                     episode reward: 6.3000
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 1312.95,                    last time consumption/overall running time: 199.0200s / 3274.2032 s
first_0:                     episode reward: -7.7000
second_0:                     episode reward: 7.7000
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 1128.6,                    last time consumption/overall running time: 168.6702s / 3442.8734 s
first_0:                     episode reward: -9.1000
second_0:                     episode reward: 9.1000
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 1175.05,                    last time consumption/overall running time: 176.9688s / 3619.8422 s
first_0:                     episode reward: -8.6000
second_0:                     episode reward: 8.6000
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 1576.05,                    last time consumption/overall running time: 237.8502s / 3857.6924 s
first_0:                     episode reward: -3.6500
second_0:                     episode reward: 3.6500
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 1389.85,                    last time consumption/overall running time: 210.3500s / 4068.0424 s
first_0:                     episode reward: -6.8500
second_0:                     episode reward: 6.8500
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 1312.9,                    last time consumption/overall running time: 199.8575s / 4267.8999 s
first_0:                     episode reward: -8.1000
second_0:                     episode reward: 8.1000
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 1281.35,                    last time consumption/overall running time: 191.9876s / 4459.8875 s
first_0:                     episode reward: -7.9500
second_0:                     episode reward: 7.9500
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 1131.25,                    last time consumption/overall running time: 170.9485s / 4630.8360 s