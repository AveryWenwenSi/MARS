pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 37
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7fc88f60ee48>
No agent are not learnable.
{'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 10000, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nfsp', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'eta': 0.1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7fc88ff133c8>, 'reservoir_buffer': <AutoProxy[reservoir_buffer] object, typeid 'reservoir_buffer' at 0x7fc88f5b1c18>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 70
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 10000, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nfsp', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'eta': 0.1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7fa3d91d4eb8>, 'reservoir_buffer': <AutoProxy[reservoir_buffer] object, typeid 'reservoir_buffer' at 0x7fa3d82e4d30>}}
Save models to : /home/zihan/research/MARS/data/model/1/pettingzoo_surround_v1_nfsp. 
 Save logs to: /home/zihan/research/MARS/data/log/1/pettingzoo_surround_v1_nfsp.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 1505.25,                    last time consumption/overall running time: 145.4008s / 145.4008 s
first_0:                     episode reward: 3.0000
second_0:                     episode reward: -3.0000
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 1427.1,                    last time consumption/overall running time: 172.2474s / 317.6482 s
first_0:                     episode reward: 5.5000
second_0:                     episode reward: -5.5000
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 1659.7,                    last time consumption/overall running time: 216.8903s / 534.5385 s
first_0:                     episode reward: -1.8500
second_0:                     episode reward: 1.8500
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 1535.05,                    last time consumption/overall running time: 204.8153s / 739.3538 s
first_0:                     episode reward: -2.4000
second_0:                     episode reward: 2.4000
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 1802.9,                    last time consumption/overall running time: 242.1158s / 981.4695 s
first_0:                     episode reward: 1.4000
second_0:                     episode reward: -1.4000
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 1809.5,                    last time consumption/overall running time: 243.6793s / 1225.1489 s
first_0:                     episode reward: 2.2500
second_0:                     episode reward: -2.2500
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 1614.2,                    last time consumption/overall running time: 217.7622s / 1442.9111 s
first_0:                     episode reward: 3.1000
second_0:                     episode reward: -3.1000
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 1699.45,                    last time consumption/overall running time: 230.1198s / 1673.0309 s
first_0:                     episode reward: -4.4000
second_0:                     episode reward: 4.4000
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 1607.95,                    last time consumption/overall running time: 217.4587s / 1890.4896 s
first_0:                     episode reward: 0.5000
second_0:                     episode reward: -0.5000
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 1568.1,                    last time consumption/overall running time: 212.1455s / 2102.6351 s
first_0:                     episode reward: 4.8000
second_0:                     episode reward: -4.8000
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 1637.0,                    last time consumption/overall running time: 220.0821s / 2322.7172 s
first_0:                     episode reward: 3.2500
second_0:                     episode reward: -3.2500
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 1768.45,                    last time consumption/overall running time: 238.1076s / 2560.8248 s
first_0:                     episode reward: 1.3000
second_0:                     episode reward: -1.3000
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 1257.3,                    last time consumption/overall running time: 169.6560s / 2730.4809 s
first_0:                     episode reward: 8.3500
second_0:                     episode reward: -8.3500
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 1214.45,                    last time consumption/overall running time: 164.0625s / 2894.5434 s
first_0:                     episode reward: 8.6500
second_0:                     episode reward: -8.6500
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 1211.4,                    last time consumption/overall running time: 163.8290s / 3058.3724 s
first_0:                     episode reward: 8.8500
second_0:                     episode reward: -8.8500
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 1119.45,                    last time consumption/overall running time: 151.9006s / 3210.2729 s
first_0:                     episode reward: 9.4500
second_0:                     episode reward: -9.4500
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 1156.35,                    last time consumption/overall running time: 156.6029s / 3366.8759 s
first_0:                     episode reward: 9.2500
second_0:                     episode reward: -9.2500
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 1129.15,                    last time consumption/overall running time: 152.2861s / 3519.1620 s
first_0:                     episode reward: 9.3000
second_0:                     episode reward: -9.3000
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 1191.5,                    last time consumption/overall running time: 161.1339s / 3680.2959 s
first_0:                     episode reward: 8.9000
second_0:                     episode reward: -8.9000
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 1456.9,                    last time consumption/overall running time: 196.2931s / 3876.5889 s
first_0:                     episode reward: 6.6500
second_0:                     episode reward: -6.6500
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 1857.05,                    last time consumption/overall running time: 250.5327s / 4127.1217 s
first_0:                     episode reward: 4.7000
second_0:                     episode reward: -4.7000
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 1934.35,                    last time consumption/overall running time: 260.7506s / 4387.8723 s
first_0:                     episode reward: 1.8000
second_0:                     episode reward: -1.8000
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 1825.3,                    last time consumption/overall running time: 246.6320s / 4634.5043 s
first_0:                     episode reward: 0.9500
second_0:                     episode reward: -0.9500
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 1582.9,                    last time consumption/overall running time: 212.0650s / 4846.5693 spygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 13
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 10000, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nfsp', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'eta': 0.1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7fa3d91d4eb8>, 'reservoir_buffer': <AutoProxy[reservoir_buffer] object, typeid 'reservoir_buffer' at 0x7fa3d82e4d30>}}
Save models to : /home/zihan/research/MARS/data/model/0/pettingzoo_surround_v1_nfsp. 
 Save logs to: /home/zihan/research/MARS/data/log/0/pettingzoo_surround_v1_nfsp.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 1475.85,                    last time consumption/overall running time: 142.6573s / 142.6573 s
first_0:                     episode reward: 2.8000
second_0:                     episode reward: -2.8000
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 1432.15,                    last time consumption/overall running time: 171.7871s / 314.4443 s
first_0:                     episode reward: 5.4000
second_0:                     episode reward: -5.4000
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 1720.05,                    last time consumption/overall running time: 223.4799s / 537.9242 s
first_0:                     episode reward: -1.7000
second_0:                     episode reward: 1.7000
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 1505.0,                    last time consumption/overall running time: 200.2021s / 738.1264 s
first_0:                     episode reward: -2.4500
second_0:                     episode reward: 2.4500
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 1929.55,                    last time consumption/overall running time: 259.2371s / 997.3635 s
first_0:                     episode reward: -0.7000
second_0:                     episode reward: 0.7000
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 1930.45,                    last time consumption/overall running time: 259.5647s / 1256.9282 s
first_0:                     episode reward: 2.6000
second_0:                     episode reward: -2.6000
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 1513.3,                    last time consumption/overall running time: 203.9589s / 1460.8871 s
first_0:                     episode reward: 1.2500
second_0:                     episode reward: -1.2500
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 1799.15,                    last time consumption/overall running time: 244.0092s / 1704.8963 s
first_0:                     episode reward: -1.4000
second_0:                     episode reward: 1.4000
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 1793.05,                    last time consumption/overall running time: 242.3637s / 1947.2600 s
first_0:                     episode reward: 1.3000
second_0:                     episode reward: -1.3000
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 1674.35,                    last time consumption/overall running time: 225.7627s / 2173.0226 s
first_0:                     episode reward: 0.3500
second_0:                     episode reward: -0.3500
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 1746.5,                    last time consumption/overall running time: 235.6933s / 2408.7160 s
first_0:                     episode reward: 4.7500
second_0:                     episode reward: -4.7500
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 1540.2,                    last time consumption/overall running time: 207.4490s / 2616.1650 s
first_0:                     episode reward: 3.8500
second_0:                     episode reward: -3.8500
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 1195.7,                    last time consumption/overall running time: 161.4889s / 2777.6539 s
first_0:                     episode reward: 8.9000
second_0:                     episode reward: -8.9000
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 1211.85,                    last time consumption/overall running time: 164.6965s / 2942.3504 s
first_0:                     episode reward: 8.8500
second_0:                     episode reward: -8.8500
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 1172.35,                    last time consumption/overall running time: 159.1933s / 3101.5436 s
first_0:                     episode reward: 9.0500
second_0:                     episode reward: -9.0500
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 1155.55,                    last time consumption/overall running time: 156.5652s / 3258.1088 s
first_0:                     episode reward: 9.2000
second_0:                     episode reward: -9.2000
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 1157.8,                    last time consumption/overall running time: 156.5717s / 3414.6805 s
first_0:                     episode reward: 9.1500
second_0:                     episode reward: -9.1500
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 1112.0,                    last time consumption/overall running time: 150.3027s / 3564.9832 s
first_0:                     episode reward: 9.4500
second_0:                     episode reward: -9.4500
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 1292.85,                    last time consumption/overall running time: 174.6034s / 3739.5866 s
first_0:                     episode reward: 8.2500
second_0:                     episode reward: -8.2500
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 1464.2,                    last time consumption/overall running time: 198.3082s / 3937.8949 s
first_0:                     episode reward: 6.9000
second_0:                     episode reward: -6.9000
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 2075.3,                    last time consumption/overall running time: 280.3836s / 4218.2785 s
first_0:                     episode reward: 1.0000
second_0:                     episode reward: -1.0000
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 1768.8,                    last time consumption/overall running time: 239.0059s / 4457.2844 s
first_0:                     episode reward: 3.0500
second_0:                     episode reward: -3.0500
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 1989.2,                    last time consumption/overall running time: 268.6662s / 4725.9506 s
first_0:                     episode reward: -0.2000
second_0:                     episode reward: 0.2000
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 1545.05,                    last time consumption/overall running time: 208.3457s / 4934.2963 s