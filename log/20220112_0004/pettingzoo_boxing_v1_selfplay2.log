pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
random seed: 54
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7fb70f0d1f60>
Agents No. [1] (index starting from 0) are not learnable.
{'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 60, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7fb70fa31470>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
random seed: 77
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 60, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7fdcbcb87cf8>}}
Save models to : /home/zihan/research/MARS/data/model/1/pettingzoo_boxing_v1_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/1/pettingzoo_boxing_v1_selfplay2.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 189.9281s / 189.9281 s
first_0:                     episode reward: 4.5500
second_0:                     episode reward: -4.5500
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 210.8528s / 400.7809 s
first_0:                     episode reward: 0.8500
second_0:                     episode reward: -0.8500
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 217.1408s / 617.9217 s
first_0:                     episode reward: 0.7000
second_0:                     episode reward: -0.7000
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.3054s / 837.2271 s
first_0:                     episode reward: 0.6500
second_0:                     episode reward: -0.6500
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 218.1577s / 1055.3848 s
first_0:                     episode reward: 0.2500
second_0:                     episode reward: -0.2500
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 218.6993s / 1274.0841 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.1186s / 1493.2027 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.0715s / 1712.2742 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.0007s / 1931.2749 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.8510s / 2151.1259 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.3466s / 2370.4725 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.4433s / 2589.9158 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.1684s / 2809.0843 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.8115s / 3028.8958 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.4285s / 3249.3243 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 218.5288s / 3467.8531 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.5978s / 3687.4509 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 218.7131s / 3906.1641 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.0764s / 4125.2404 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 218.7133s / 4343.9537 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.4577s / 4563.4115 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.2674s / 4782.6788 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.6182s / 5002.2971 s
first_0:                     episode reward: 0.5000
second_0:                     episode reward: -0.5000
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.4815s / 5222.7786 s
first_0:                     episode reward: 0.0000pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
random seed: 99
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 60, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7fdcbcb87cf8>}}
Save models to : /home/zihan/research/MARS/data/model/0/pettingzoo_boxing_v1_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/0/pettingzoo_boxing_v1_selfplay2.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 190.2542s / 190.2542 s
first_0:                     episode reward: 3.4500
second_0:                     episode reward: -3.4500
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 211.1106s / 401.3648 s
first_0:                     episode reward: 1.9500
second_0:                     episode reward: -1.9500
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 217.7163s / 619.0811 s
first_0:                     episode reward: 0.5500
second_0:                     episode reward: -0.5500
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.2767s / 838.3577 s
first_0:                     episode reward: 0.6500
second_0:                     episode reward: -0.6500
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 218.6738s / 1057.0315 s
first_0:                     episode reward: 0.7500
second_0:                     episode reward: -0.7500
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.4232s / 1276.4547 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.6396s / 1497.0943 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.4384s / 1717.5326 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.4239s / 1937.9565 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.7153s / 2158.6718 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.3881s / 2379.0599 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.2410s / 2599.3009 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.3507s / 2819.6516 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.8012s / 3040.4528 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.5147s / 3260.9675 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.3122s / 3481.2797 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.6086s / 3700.8882 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.0824s / 3920.9706 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.8243s / 4140.7949 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.5871s / 4360.3819 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 219.5702s / 4579.9521 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.0249s / 4799.9770 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.6076s / 5020.5846 s
first_0:                     episode reward: 0.3500
second_0:                     episode reward: -0.3500
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.2523s / 5241.8369 s
first_0:                     episode reward: 0.5000