pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 38
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7f93435e5e48>
Agents No. [1] (index starting from 0) are not learnable.
{'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 20, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f9343603dd8>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 39
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 20, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7fe1137b4240>}}
Save models to : /home/zihan/research/MARS/data/model/0/pettingzoo_pong_v2_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/0/pettingzoo_pong_v2_fictitious_selfplay2.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 2150.2,                    last time consumption/overall running time: 190.0357s / 190.0357 s
first_0:                     episode reward: 66.7500
second_0:                     episode reward: -66.7500
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 1599.65,                    last time consumption/overall running time: 188.4325s / 378.4682 s
first_0:                     episode reward: 36.4000
second_0:                     episode reward: -36.4000
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 1278.8,                    last time consumption/overall running time: 153.6167s / 532.0849 s
first_0:                     episode reward: -6.6500
second_0:                     episode reward: 6.6500
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 1377.55,                    last time consumption/overall running time: 168.1434s / 700.2282 s
first_0:                     episode reward: -10.1500
second_0:                     episode reward: 10.1500
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 1199.35,                    last time consumption/overall running time: 148.0969s / 848.3252 s
first_0:                     episode reward: -20.2500
second_0:                     episode reward: 20.2500
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 890.05,                    last time consumption/overall running time: 111.1580s / 959.4831 s
first_0:                     episode reward: -20.4500
second_0:                     episode reward: 20.4500
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 1030.05,                    last time consumption/overall running time: 126.7362s / 1086.2193 s
first_0:                     episode reward: -2.2000
second_0:                     episode reward: 2.2000
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 859.15,                    last time consumption/overall running time: 104.7606s / 1190.9799 s
first_0:                     episode reward: 0.3500
second_0:                     episode reward: -0.3500
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 751.9,                    last time consumption/overall running time: 92.8210s / 1283.8009 s
first_0:                     episode reward: 20.2500
second_0:                     episode reward: -20.2500
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 728.0,                    last time consumption/overall running time: 89.7048s / 1373.5057 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 832.4,                    last time consumption/overall running time: 103.1715s / 1476.6772 s
first_0:                     episode reward: 11.1000
second_0:                     episode reward: -11.1000
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 1165.15,                    last time consumption/overall running time: 144.8603s / 1621.5375 s
first_0:                     episode reward: -14.1000
second_0:                     episode reward: 14.1000
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 881.7,                    last time consumption/overall running time: 109.4332s / 1730.9707 s
first_0:                     episode reward: 13.4000
second_0:                     episode reward: -13.4000
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 737.4,                    last time consumption/overall running time: 91.9897s / 1822.9604 s
first_0:                     episode reward: 20.7500
second_0:                     episode reward: -20.7500
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 728.0,                    last time consumption/overall running time: 90.4582s / 1913.4186 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 728.0,                    last time consumption/overall running time: 90.6422s / 2004.0608 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 728.05,                    last time consumption/overall running time: 90.6137s / 2094.6745 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 756.9,                    last time consumption/overall running time: 94.0287s / 2188.7032 s
first_0:                     episode reward: 20.1000
second_0:                     episode reward: -20.1000
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 2591.3,                    last time consumption/overall running time: 320.7549s / 2509.4581 s
first_0:                     episode reward: -41.4000
second_0:                     episode reward: 41.4000
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 757.75,                    last time consumption/overall running time: 94.3288s / 2603.7869 s
first_0:                     episode reward: 20.0000
second_0:                     episode reward: -20.0000
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 972.1,                    last time consumption/overall running time: 121.0344s / 2724.8212 s
first_0:                     episode reward: 12.7500
second_0:                     episode reward: -12.7500
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 2506.6,                    last time consumption/overall running time: 310.8998s / 3035.7210 s
first_0:                     episode reward: -42.5000
second_0:                     episode reward: 42.5000
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 728.0,                    last time consumption/overall running time: 90.4187s / 3126.1397 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 1158.6,                    last time consumption/overall running time: 141.6684s / 3267.8081 spygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 52
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 20, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7fe1137b4240>}}
Save models to : /home/zihan/research/MARS/data/model/1/pettingzoo_pong_v2_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/1/pettingzoo_pong_v2_fictitious_selfplay2.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 2600.75,                    last time consumption/overall running time: 244.4678s / 244.4678 s
first_0:                     episode reward: 82.1000
second_0:                     episode reward: -82.1000
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 2043.4,                    last time consumption/overall running time: 241.9922s / 486.4600 s
first_0:                     episode reward: 28.4000
second_0:                     episode reward: -28.4000
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 1332.05,                    last time consumption/overall running time: 162.1905s / 648.6504 s
first_0:                     episode reward: -7.6000
second_0:                     episode reward: 7.6000
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 1307.2,                    last time consumption/overall running time: 160.7786s / 809.4290 s
first_0:                     episode reward: -21.0000
second_0:                     episode reward: 21.0000
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 954.3,                    last time consumption/overall running time: 118.5468s / 927.9758 s
first_0:                     episode reward: -20.1000
second_0:                     episode reward: 20.1000
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 1072.1,                    last time consumption/overall running time: 132.4140s / 1060.3898 s
first_0:                     episode reward: -15.4000
second_0:                     episode reward: 15.4000
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 848.25,                    last time consumption/overall running time: 103.7162s / 1164.1060 s
first_0:                     episode reward: 2.3000
second_0:                     episode reward: -2.3000
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 738.95,                    last time consumption/overall running time: 91.0451s / 1255.1511 s
first_0:                     episode reward: 20.6500
second_0:                     episode reward: -20.6500
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 728.3,                    last time consumption/overall running time: 89.7763s / 1344.9274 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 728.1,                    last time consumption/overall running time: 89.8661s / 1434.7935 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 965.85,                    last time consumption/overall running time: 119.8207s / 1554.6143 s
first_0:                     episode reward: -2.1000
second_0:                     episode reward: 2.1000
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 1048.2,                    last time consumption/overall running time: 129.7033s / 1684.3176 s
first_0:                     episode reward: -7.1000
second_0:                     episode reward: 7.1000
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 760.25,                    last time consumption/overall running time: 94.0762s / 1778.3938 s
first_0:                     episode reward: 20.0500
second_0:                     episode reward: -20.0500
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 728.3,                    last time consumption/overall running time: 90.2611s / 1868.6550 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 728.0,                    last time consumption/overall running time: 90.0775s / 1958.7324 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 728.0,                    last time consumption/overall running time: 90.4380s / 2049.1704 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 728.0,                    last time consumption/overall running time: 89.6095s / 2138.7799 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 1628.25,                    last time consumption/overall running time: 200.8419s / 2339.6218 s
first_0:                     episode reward: -9.0000
second_0:                     episode reward: 9.0000
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 1777.35,                    last time consumption/overall running time: 219.1006s / 2558.7224 s
first_0:                     episode reward: -14.1500
second_0:                     episode reward: 14.1500
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 764.6,                    last time consumption/overall running time: 94.8997s / 2653.6222 s
first_0:                     episode reward: 19.9000
second_0:                     episode reward: -19.9000
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 1918.55,                    last time consumption/overall running time: 237.1206s / 2890.7428 s
first_0:                     episode reward: -23.5500
second_0:                     episode reward: 23.5500
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 1534.75,                    last time consumption/overall running time: 189.8727s / 3080.6155 s
first_0:                     episode reward: -4.8500
second_0:                     episode reward: 4.8500
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 728.0,                    last time consumption/overall running time: 89.6451s / 3170.2606 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 1346.5,                    last time consumption/overall running time: 163.5275s / 3333.7881 s