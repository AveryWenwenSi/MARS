pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
random seed: 14
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7f9fa84e7fd0>
No agent are not learnable.
{'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 10000, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nfsp', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'eta': 0.1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f9fa8deb160>, 'reservoir_buffer': <AutoProxy[reservoir_buffer] object, typeid 'reservoir_buffer' at 0x7f9fa848acf8>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
random seed: 70
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 10000, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nfsp', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'eta': 0.1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f54d1c71f28>, 'reservoir_buffer': <AutoProxy[reservoir_buffer] object, typeid 'reservoir_buffer' at 0x7f54d1329b38>}}
Save models to : /home/zihan/research/MARS/data/model/0/pettingzoo_boxing_v1_nfsp. 
 Save logs to: /home/zihan/research/MARS/data/log/0/pettingzoo_boxing_v1_nfsp.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 166.6135s / 166.6135 s
first_0:                     episode reward: -1.9000
second_0:                     episode reward: 1.9000
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 207.9173s / 374.5309 s
first_0:                     episode reward: 0.1500
second_0:                     episode reward: -0.1500
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 216.9805s / 591.5114 s
first_0:                     episode reward: -0.3500
second_0:                     episode reward: 0.3500
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.3026s / 811.8140 s
first_0:                     episode reward: -0.1000
second_0:                     episode reward: 0.1000
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.4184s / 1032.2323 s
first_0:                     episode reward: 2.8000
second_0:                     episode reward: -2.8000
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.0571s / 1252.2894 s
first_0:                     episode reward: 0.7000
second_0:                     episode reward: -0.7000
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.7185s / 1474.0079 s
first_0:                     episode reward: 0.0500
second_0:                     episode reward: -0.0500
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 222.4948s / 1696.5027 s
first_0:                     episode reward: 0.4500
second_0:                     episode reward: -0.4500
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 222.3455s / 1918.8482 s
first_0:                     episode reward: -2.7500
second_0:                     episode reward: 2.7500
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.7612s / 2140.6093 s
first_0:                     episode reward: 0.3500
second_0:                     episode reward: -0.3500
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 222.1218s / 2362.7311 s
first_0:                     episode reward: 0.9500
second_0:                     episode reward: -0.9500
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 222.4165s / 2585.1476 s
first_0:                     episode reward: 0.1000
second_0:                     episode reward: -0.1000
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.8802s / 2807.0278 s
first_0:                     episode reward: 2.5500
second_0:                     episode reward: -2.5500
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.8212s / 3027.8490 s
first_0:                     episode reward: 2.1000
second_0:                     episode reward: -2.1000
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.5430s / 3249.3920 s
first_0:                     episode reward: -1.8500
second_0:                     episode reward: 1.8500
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 222.0757s / 3471.4677 s
first_0:                     episode reward: 2.8500
second_0:                     episode reward: -2.8500
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 222.9475s / 3694.4152 s
first_0:                     episode reward: -0.7500
second_0:                     episode reward: 0.7500
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.6088s / 3916.0240 s
first_0:                     episode reward: 0.1500
second_0:                     episode reward: -0.1500
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 224.0144s / 4140.0384 s
first_0:                     episode reward: -0.6000
second_0:                     episode reward: 0.6000
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 223.4956s / 4363.5339 s
first_0:                     episode reward: -3.3000
second_0:                     episode reward: 3.3000
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 1771.7,                    last time consumption/overall running time: 220.6564s / 4584.1903 s
first_0:                     episode reward: -3.5000
second_0:                     episode reward: 3.5000
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 222.3892s / 4806.5796 s
first_0:                     episode reward: -2.9500
second_0:                     episode reward: 2.9500
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.8413s / 5028.4209 s
first_0:                     episode reward: -9.8000
second_0:                     episode reward: 9.8000
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.0265s / 5249.4474 spygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
random seed: 18
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 10000, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nfsp', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'eta': 0.1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f54d1c71f28>, 'reservoir_buffer': <AutoProxy[reservoir_buffer] object, typeid 'reservoir_buffer' at 0x7f54d1329b38>}}
Save models to : /home/zihan/research/MARS/data/model/1/pettingzoo_boxing_v1_nfsp. 
 Save logs to: /home/zihan/research/MARS/data/log/1/pettingzoo_boxing_v1_nfsp.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 171.0667s / 171.0667 s
first_0:                     episode reward: -0.5000
second_0:                     episode reward: 0.5000
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 211.0007s / 382.0674 s
first_0:                     episode reward: 1.4500
second_0:                     episode reward: -1.4500
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.3936s / 603.4610 s
first_0:                     episode reward: 1.2000
second_0:                     episode reward: -1.2000
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 223.9266s / 827.3876 s
first_0:                     episode reward: 0.2500
second_0:                     episode reward: -0.2500
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 223.2604s / 1050.6480 s
first_0:                     episode reward: -0.4500
second_0:                     episode reward: 0.4500
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 222.9343s / 1273.5824 s
first_0:                     episode reward: -0.5500
second_0:                     episode reward: 0.5500
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 223.5973s / 1497.1797 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 224.5274s / 1721.7071 s
first_0:                     episode reward: 0.5000
second_0:                     episode reward: -0.5000
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 223.0189s / 1944.7260 s
first_0:                     episode reward: 0.1000
second_0:                     episode reward: -0.1000
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 222.5786s / 2167.3047 s
first_0:                     episode reward: 3.9000
second_0:                     episode reward: -3.9000
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.6003s / 2388.9050 s
first_0:                     episode reward: 0.7500
second_0:                     episode reward: -0.7500
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.8032s / 2610.7082 s
first_0:                     episode reward: -0.9000
second_0:                     episode reward: 0.9000
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.5623s / 2832.2706 s
first_0:                     episode reward: 1.4500
second_0:                     episode reward: -1.4500
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.7918s / 3054.0624 s
first_0:                     episode reward: -0.7500
second_0:                     episode reward: 0.7500
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.3048s / 3275.3672 s
first_0:                     episode reward: 0.7000
second_0:                     episode reward: -0.7000
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 222.7442s / 3498.1114 s
first_0:                     episode reward: 1.3500
second_0:                     episode reward: -1.3500
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.8250s / 3719.9364 s
first_0:                     episode reward: -0.8000
second_0:                     episode reward: 0.8000
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.2712s / 3941.2076 s
first_0:                     episode reward: -4.0500
second_0:                     episode reward: 4.0500
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 220.7239s / 4161.9315 s
first_0:                     episode reward: 0.0000
second_0:                     episode reward: 0.0000
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 223.2994s / 4385.2308 s
first_0:                     episode reward: -3.6000
second_0:                     episode reward: 3.6000
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.2118s / 4606.4426 s
first_0:                     episode reward: -1.2000
second_0:                     episode reward: 1.2000
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.2314s / 4827.6740 s
first_0:                     episode reward: 1.0500
second_0:                     episode reward: -1.0500
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 1753.35,                    last time consumption/overall running time: 217.3010s / 5044.9750 s
first_0:                     episode reward: -8.6000
second_0:                     episode reward: 8.6000
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 1784.0,                    last time consumption/overall running time: 221.2190s / 5266.1940 s