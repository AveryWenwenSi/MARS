pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 53
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7f863c562ef0>
Agents No. [1] (index starting from 0) are not learnable.
{'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 16, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f863ce66080>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 58
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 16, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7ff0bec11fd0>}}
Save models to : /home/zihan/research/MARS/data/model/0/pettingzoo_surround_v1_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/0/pettingzoo_surround_v1_nxdo2.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 1618.55,                    last time consumption/overall running time: 162.4104s / 162.4104 s
first_0:                     episode reward: -2.0000
second_0:                     episode reward: 2.0000
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 1368.55,                    last time consumption/overall running time: 167.0898s / 329.5002 s
first_0:                     episode reward: 4.7500
second_0:                     episode reward: -4.7500
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 1314.4,                    last time consumption/overall running time: 167.6381s / 497.1383 s
first_0:                     episode reward: 6.6500
second_0:                     episode reward: -6.6500
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 1175.7,                    last time consumption/overall running time: 151.0241s / 648.1623 s
first_0:                     episode reward: 9.4000
second_0:                     episode reward: -9.4000
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 1105.85,                    last time consumption/overall running time: 143.9260s / 792.0884 s
first_0:                     episode reward: 9.9000
second_0:                     episode reward: -9.9000
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 1054.85,                    last time consumption/overall running time: 138.1812s / 930.2695 s
first_0:                     episode reward: 9.9000
second_0:                     episode reward: -9.9000
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 1063.15,                    last time consumption/overall running time: 139.5351s / 1069.8046 s
first_0:                     episode reward: 10.0000
second_0:                     episode reward: -10.0000
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 1050.15,                    last time consumption/overall running time: 137.4295s / 1207.2341 s
first_0:                     episode reward: 9.6500
second_0:                     episode reward: -9.6500
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 1084.6,                    last time consumption/overall running time: 141.3870s / 1348.6211 s
first_0:                     episode reward: 9.5500
second_0:                     episode reward: -9.5500
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 1111.3,                    last time consumption/overall running time: 145.5864s / 1494.2075 s
first_0:                     episode reward: 9.7500
second_0:                     episode reward: -9.7500
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 1163.15,                    last time consumption/overall running time: 153.1630s / 1647.3705 s
first_0:                     episode reward: 4.0000
second_0:                     episode reward: -4.0000
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 1271.05,                    last time consumption/overall running time: 164.8215s / 1812.1920 s
first_0:                     episode reward: 4.7500
second_0:                     episode reward: -4.7500
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 1306.1,                    last time consumption/overall running time: 167.8543s / 1980.0464 s
first_0:                     episode reward: 0.4500
second_0:                     episode reward: -0.4500
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 1165.95,                    last time consumption/overall running time: 152.0673s / 2132.1136 s
first_0:                     episode reward: -5.4500
second_0:                     episode reward: 5.4500
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 1202.05,                    last time consumption/overall running time: 157.0936s / 2289.2072 s
first_0:                     episode reward: -8.3500
second_0:                     episode reward: 8.3500
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 1434.05,                    last time consumption/overall running time: 188.6191s / 2477.8263 s
first_0:                     episode reward: -6.6500
second_0:                     episode reward: 6.6500
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 1357.0,                    last time consumption/overall running time: 177.7635s / 2655.5898 s
first_0:                     episode reward: 6.6000
second_0:                     episode reward: -6.6000
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 1532.3,                    last time consumption/overall running time: 200.6525s / 2856.2423 s
first_0:                     episode reward: 2.1000
second_0:                     episode reward: -2.1000
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 1220.2,                    last time consumption/overall running time: 159.8864s / 3016.1286 s
first_0:                     episode reward: -6.8500
second_0:                     episode reward: 6.8500
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 1211.15,                    last time consumption/overall running time: 159.8504s / 3175.9791 s
first_0:                     episode reward: 8.0000
second_0:                     episode reward: -8.0000
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 1197.4,                    last time consumption/overall running time: 156.8476s / 3332.8266 s
first_0:                     episode reward: 9.8500
second_0:                     episode reward: -9.8500
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 1144.05,                    last time consumption/overall running time: 149.9026s / 3482.7292 s
first_0:                     episode reward: 9.7000
second_0:                     episode reward: -9.7000
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 1377.25,                    last time consumption/overall running time: 180.7035s / 3663.4327 s
first_0:                     episode reward: 8.9500
second_0:                     episode reward: -8.9500
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 1552.5,                    last time consumption/overall running time: 203.1547s / 3866.5874 spygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 7
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 16, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7ff0bec11fd0>}}
Save models to : /home/zihan/research/MARS/data/model/1/pettingzoo_surround_v1_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/1/pettingzoo_surround_v1_nxdo2.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 1554.9,                    last time consumption/overall running time: 159.9679s / 159.9679 s
first_0:                     episode reward: -1.2500
second_0:                     episode reward: 1.2500
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 1328.8,                    last time consumption/overall running time: 161.9395s / 321.9075 s
first_0:                     episode reward: 5.0000
second_0:                     episode reward: -5.0000
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 1356.7,                    last time consumption/overall running time: 173.0614s / 494.9689 s
first_0:                     episode reward: 6.2500
second_0:                     episode reward: -6.2500
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 1182.95,                    last time consumption/overall running time: 152.6183s / 647.5872 s
first_0:                     episode reward: 9.5500
second_0:                     episode reward: -9.5500
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 1130.85,                    last time consumption/overall running time: 146.7781s / 794.3653 s
first_0:                     episode reward: 9.7500
second_0:                     episode reward: -9.7500
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 1052.2,                    last time consumption/overall running time: 137.2309s / 931.5962 s
first_0:                     episode reward: 9.9500
second_0:                     episode reward: -9.9500
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 1066.5,                    last time consumption/overall running time: 139.7262s / 1071.3224 s
first_0:                     episode reward: 9.9500
second_0:                     episode reward: -9.9500
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 1072.4,                    last time consumption/overall running time: 140.5359s / 1211.8583 s
first_0:                     episode reward: 9.7500
second_0:                     episode reward: -9.7500
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 1089.65,                    last time consumption/overall running time: 142.0531s / 1353.9114 s
first_0:                     episode reward: 9.8000
second_0:                     episode reward: -9.8000
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 1121.0,                    last time consumption/overall running time: 147.0391s / 1500.9505 s
first_0:                     episode reward: 9.3000
second_0:                     episode reward: -9.3000
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 1077.8,                    last time consumption/overall running time: 141.5918s / 1642.5422 s
first_0:                     episode reward: 9.8500
second_0:                     episode reward: -9.8500
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 1317.7,                    last time consumption/overall running time: 171.1303s / 1813.6725 s
first_0:                     episode reward: 7.9000
second_0:                     episode reward: -7.9000
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 1481.65,                    last time consumption/overall running time: 190.7864s / 2004.4589 s
first_0:                     episode reward: 7.5500
second_0:                     episode reward: -7.5500
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 1489.75,                    last time consumption/overall running time: 194.6415s / 2199.1004 s
first_0:                     episode reward: 7.3000
second_0:                     episode reward: -7.3000
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 1908.35,                    last time consumption/overall running time: 249.9038s / 2449.0042 s
first_0:                     episode reward: 5.2000
second_0:                     episode reward: -5.2000
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 1556.7,                    last time consumption/overall running time: 203.4961s / 2652.5003 s
first_0:                     episode reward: 7.1500
second_0:                     episode reward: -7.1500
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 1448.55,                    last time consumption/overall running time: 189.0812s / 2841.5815 s
first_0:                     episode reward: 7.7000
second_0:                     episode reward: -7.7000
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 1626.3,                    last time consumption/overall running time: 212.6425s / 3054.2240 s
first_0:                     episode reward: 6.1500
second_0:                     episode reward: -6.1500
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 1790.8,                    last time consumption/overall running time: 235.7439s / 3289.9680 s
first_0:                     episode reward: 4.2000
second_0:                     episode reward: -4.2000
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 1456.35,                    last time consumption/overall running time: 190.5814s / 3480.5494 s
first_0:                     episode reward: 8.2000
second_0:                     episode reward: -8.2000
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 1218.8,                    last time consumption/overall running time: 159.3604s / 3639.9098 s
first_0:                     episode reward: 9.6000
second_0:                     episode reward: -9.6000
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 1204.25,                    last time consumption/overall running time: 156.9670s / 3796.8769 s
first_0:                     episode reward: 9.5500
second_0:                     episode reward: -9.5500
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 1394.3,                    last time consumption/overall running time: 182.1064s / 3978.9833 s
first_0:                     episode reward: 8.1500
second_0:                     episode reward: -8.1500
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 1676.55,                    last time consumption/overall running time: 219.6291s / 4198.6124 s