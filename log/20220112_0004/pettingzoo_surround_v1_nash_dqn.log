pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 2
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7fae17619e10>
No agent are not learnable.
{'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7fae18670da0>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 70
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f7a5cb21908>}}
Save models to : /home/zihan/research/MARS/data/model/1/pettingzoo_surround_v1_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/1/pettingzoo_surround_v1_nash_dqn.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 1518.75,                    last time consumption/overall running time: 141.2098s / 141.2098 s
first_0:                     episode reward: 0.5000
second_0:                     episode reward: -0.5000
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 1537.95,                    last time consumption/overall running time: 199.8130s / 341.0228 s
first_0:                     episode reward: -1.2000
second_0:                     episode reward: 1.2000
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 1456.35,                    last time consumption/overall running time: 211.7680s / 552.7908 s
first_0:                     episode reward: -1.5500
second_0:                     episode reward: 1.5500
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 1586.6,                    last time consumption/overall running time: 241.7406s / 794.5314 s
first_0:                     episode reward: -0.7000
second_0:                     episode reward: 0.7000
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 1577.7,                    last time consumption/overall running time: 241.0615s / 1035.5929 s
first_0:                     episode reward: -1.2000
second_0:                     episode reward: 1.2000
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 1618.5,                    last time consumption/overall running time: 250.9009s / 1286.4938 s
first_0:                     episode reward: 1.3500
second_0:                     episode reward: -1.3500
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 1649.15,                    last time consumption/overall running time: 254.4447s / 1540.9385 s
first_0:                     episode reward: 0.2000
second_0:                     episode reward: -0.2000
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 1588.2,                    last time consumption/overall running time: 247.7570s / 1788.6956 s
first_0:                     episode reward: -0.5000
second_0:                     episode reward: 0.5000
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 1502.5,                    last time consumption/overall running time: 230.7788s / 2019.4744 s
first_0:                     episode reward: -0.7500
second_0:                     episode reward: 0.7500
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 1668.15,                    last time consumption/overall running time: 257.4541s / 2276.9284 s
first_0:                     episode reward: 1.3000
second_0:                     episode reward: -1.3000
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 1534.65,                    last time consumption/overall running time: 235.8739s / 2512.8023 s
first_0:                     episode reward: -1.2500
second_0:                     episode reward: 1.2500
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 1507.4,                    last time consumption/overall running time: 232.1960s / 2744.9984 s
first_0:                     episode reward: -0.8500
second_0:                     episode reward: 0.8500
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 1503.65,                    last time consumption/overall running time: 229.9450s / 2974.9434 s
first_0:                     episode reward: -0.9500
second_0:                     episode reward: 0.9500
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 1558.0,                    last time consumption/overall running time: 241.9758s / 3216.9191 s
first_0:                     episode reward: 0.1500
second_0:                     episode reward: -0.1500
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 1510.95,                    last time consumption/overall running time: 233.1036s / 3450.0227 s
first_0:                     episode reward: 0.5500
second_0:                     episode reward: -0.5500
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 1554.8,                    last time consumption/overall running time: 237.8192s / 3687.8419 s
first_0:                     episode reward: 0.1500
second_0:                     episode reward: -0.1500
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 1578.6,                    last time consumption/overall running time: 242.8077s / 3930.6496 s
first_0:                     episode reward: -0.1000
second_0:                     episode reward: 0.1000
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 1548.5,                    last time consumption/overall running time: 237.2147s / 4167.8643 s
first_0:                     episode reward: 1.6000
second_0:                     episode reward: -1.6000
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 1596.3,                    last time consumption/overall running time: 246.0646s / 4413.9289 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 1557.35,                    last time consumption/overall running time: 238.6553s / 4652.5841 s
first_0:                     episode reward: -1.3500
second_0:                     episode reward: 1.3500
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 1587.85,                    last time consumption/overall running time: 246.0797s / 4898.6638 s
first_0:                     episode reward: 0.2500
second_0:                     episode reward: -0.2500
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 1453.2,                    last time consumption/overall running time: 224.0338s / 5122.6976 s
first_0:                     episode reward: 1.0500
second_0:                     episode reward: -1.0500
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 1623.75,                    last time consumption/overall running time: 247.6827s / 5370.3803 s
first_0:                     episode reward: -1.3000
second_0:                     episode reward: 1.3000
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 1576.6,                    last time consumption/overall running time: 242.7115s / 5613.0918 s
first_0:                     episode reward: 0.5000pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 71
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f7a5cb21908>}}
Save models to : /home/zihan/research/MARS/data/model/0/pettingzoo_surround_v1_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/0/pettingzoo_surround_v1_nash_dqn.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 1594.4,                    last time consumption/overall running time: 147.5964s / 147.5964 s
first_0:                     episode reward: 0.7000
second_0:                     episode reward: -0.7000
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 1517.3,                    last time consumption/overall running time: 199.5555s / 347.1519 s
first_0:                     episode reward: 0.4000
second_0:                     episode reward: -0.4000
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 1530.95,                    last time consumption/overall running time: 222.7298s / 569.8817 s
first_0:                     episode reward: 0.5500
second_0:                     episode reward: -0.5500
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 1503.35,                    last time consumption/overall running time: 224.9486s / 794.8303 s
first_0:                     episode reward: 1.8000
second_0:                     episode reward: -1.8000
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 1585.25,                    last time consumption/overall running time: 241.3129s / 1036.1432 s
first_0:                     episode reward: -0.9000
second_0:                     episode reward: 0.9000
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 1543.15,                    last time consumption/overall running time: 236.7192s / 1272.8624 s
first_0:                     episode reward: -0.2000
second_0:                     episode reward: 0.2000
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 1590.95,                    last time consumption/overall running time: 242.0334s / 1514.8958 s
first_0:                     episode reward: 1.7500
second_0:                     episode reward: -1.7500
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 1613.3,                    last time consumption/overall running time: 248.6435s / 1763.5393 s
first_0:                     episode reward: -0.6500
second_0:                     episode reward: 0.6500
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 1604.25,                    last time consumption/overall running time: 247.9715s / 2011.5108 s
first_0:                     episode reward: -0.7500
second_0:                     episode reward: 0.7500
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 1539.85,                    last time consumption/overall running time: 237.8814s / 2249.3922 s
first_0:                     episode reward: -0.6500
second_0:                     episode reward: 0.6500
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 1527.9,                    last time consumption/overall running time: 233.7688s / 2483.1610 s
first_0:                     episode reward: 0.2500
second_0:                     episode reward: -0.2500
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 1574.05,                    last time consumption/overall running time: 241.7487s / 2724.9097 s
first_0:                     episode reward: 1.2000
second_0:                     episode reward: -1.2000
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 1647.9,                    last time consumption/overall running time: 252.8608s / 2977.7704 s
first_0:                     episode reward: -1.2000
second_0:                     episode reward: 1.2000
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 1657.95,                    last time consumption/overall running time: 257.1446s / 3234.9150 s
first_0:                     episode reward: -0.5000
second_0:                     episode reward: 0.5000
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 1683.95,                    last time consumption/overall running time: 258.1389s / 3493.0539 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 1545.55,                    last time consumption/overall running time: 237.7900s / 3730.8439 s
first_0:                     episode reward: -0.1500
second_0:                     episode reward: 0.1500
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 1613.3,                    last time consumption/overall running time: 247.8732s / 3978.7171 s
first_0:                     episode reward: -1.2500
second_0:                     episode reward: 1.2500
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 1579.5,                    last time consumption/overall running time: 242.0037s / 4220.7209 s
first_0:                     episode reward: 0.2500
second_0:                     episode reward: -0.2500
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 1544.1,                    last time consumption/overall running time: 238.1660s / 4458.8869 s
first_0:                     episode reward: -0.9000
second_0:                     episode reward: 0.9000
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 1594.55,                    last time consumption/overall running time: 245.7140s / 4704.6008 s
first_0:                     episode reward: 0.2000
second_0:                     episode reward: -0.2000
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 1499.5,                    last time consumption/overall running time: 232.5544s / 4937.1553 s
first_0:                     episode reward: 0.0500
second_0:                     episode reward: -0.0500
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 1536.85,                    last time consumption/overall running time: 234.5945s / 5171.7498 s
first_0:                     episode reward: -0.1000
second_0:                     episode reward: 0.1000
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 1547.25,                    last time consumption/overall running time: 237.0109s / 5408.7607 s
first_0:                     episode reward: 0.3000
second_0:                     episode reward: -0.3000
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 1555.0,                    last time consumption/overall running time: 237.7647s / 5646.5255 s
first_0:                     episode reward: 0.7500