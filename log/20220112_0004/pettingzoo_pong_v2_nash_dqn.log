pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 72
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7f27290d7eb8>
No agent are not learnable.
{'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f272996f780>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 27
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f5224996be0>}}
Save models to : /home/zihan/research/MARS/data/model/0/pettingzoo_pong_v2_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/0/pettingzoo_pong_v2_nash_dqn.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 1273.7,                    last time consumption/overall running time: 111.7643s / 111.7643 s
first_0:                     episode reward: 3.0500
second_0:                     episode reward: -3.0500
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 1273.05,                    last time consumption/overall running time: 155.9798s / 267.7441 s
first_0:                     episode reward: 1.2000
second_0:                     episode reward: -1.2000
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 1354.15,                    last time consumption/overall running time: 190.4555s / 458.1996 s
first_0:                     episode reward: -1.7000
second_0:                     episode reward: 1.7000
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 1329.95,                    last time consumption/overall running time: 195.5188s / 653.7184 s
first_0:                     episode reward: 3.8500
second_0:                     episode reward: -3.8500
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 1275.35,                    last time consumption/overall running time: 192.5494s / 846.2678 s
first_0:                     episode reward: 7.0000
second_0:                     episode reward: -7.0000
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 1356.25,                    last time consumption/overall running time: 207.2554s / 1053.5232 s
first_0:                     episode reward: 4.4000
second_0:                     episode reward: -4.4000
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 1300.75,                    last time consumption/overall running time: 195.6299s / 1249.1531 s
first_0:                     episode reward: -0.2500
second_0:                     episode reward: 0.2500
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 1347.8,                    last time consumption/overall running time: 203.7287s / 1452.8818 s
first_0:                     episode reward: 3.6000
second_0:                     episode reward: -3.6000
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 1242.05,                    last time consumption/overall running time: 189.5144s / 1642.3962 s
first_0:                     episode reward: 4.6000
second_0:                     episode reward: -4.6000
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 1385.6,                    last time consumption/overall running time: 209.6924s / 1852.0886 s
first_0:                     episode reward: 2.5000
second_0:                     episode reward: -2.5000
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 1424.8,                    last time consumption/overall running time: 215.7978s / 2067.8864 s
first_0:                     episode reward: 1.0000
second_0:                     episode reward: -1.0000
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 1469.85,                    last time consumption/overall running time: 222.3569s / 2290.2433 s
first_0:                     episode reward: 2.5000
second_0:                     episode reward: -2.5000
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 1370.8,                    last time consumption/overall running time: 212.1707s / 2502.4140 s
first_0:                     episode reward: -1.3000
second_0:                     episode reward: 1.3000
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 1477.85,                    last time consumption/overall running time: 225.4149s / 2727.8289 s
first_0:                     episode reward: 3.2000
second_0:                     episode reward: -3.2000
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 1327.15,                    last time consumption/overall running time: 202.8963s / 2930.7251 s
first_0:                     episode reward: 1.8500
second_0:                     episode reward: -1.8500
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 1415.5,                    last time consumption/overall running time: 215.6658s / 3146.3910 s
first_0:                     episode reward: -1.0500
second_0:                     episode reward: 1.0500
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 1437.35,                    last time consumption/overall running time: 219.6336s / 3366.0245 s
first_0:                     episode reward: -3.9500
second_0:                     episode reward: 3.9500
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 1431.4,                    last time consumption/overall running time: 221.1557s / 3587.1803 s
first_0:                     episode reward: -2.6500
second_0:                     episode reward: 2.6500
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 1450.85,                    last time consumption/overall running time: 223.6098s / 3810.7900 s
first_0:                     episode reward: -4.7500
second_0:                     episode reward: 4.7500
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 1485.95,                    last time consumption/overall running time: 228.3607s / 4039.1508 s
first_0:                     episode reward: -4.3000
second_0:                     episode reward: 4.3000
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 1427.55,                    last time consumption/overall running time: 219.2953s / 4258.4460 s
first_0:                     episode reward: -4.9000
second_0:                     episode reward: 4.9000
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 1430.4,                    last time consumption/overall running time: 218.8290s / 4477.2750 s
first_0:                     episode reward: -4.4000
second_0:                     episode reward: 4.4000
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 1447.75,                    last time consumption/overall running time: 220.0331s / 4697.3081 s
first_0:                     episode reward: -8.1000
second_0:                     episode reward: 8.1000
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 1447.15,                    last time consumption/overall running time: 221.0770s / 4918.3852 s
first_0:                     episode reward: -9.3500
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 36
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f5224996be0>}}
Save models to : /home/zihan/research/MARS/data/model/1/pettingzoo_pong_v2_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/1/pettingzoo_pong_v2_nash_dqn.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 1319.2,                    last time consumption/overall running time: 116.8927s / 116.8927 s
first_0:                     episode reward: 1.2000
second_0:                     episode reward: -1.2000
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 1383.05,                    last time consumption/overall running time: 169.3966s / 286.2892 s
first_0:                     episode reward: 0.5000
second_0:                     episode reward: -0.5000
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 1463.7,                    last time consumption/overall running time: 207.3257s / 493.6150 s
first_0:                     episode reward: 2.2500
second_0:                     episode reward: -2.2500
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 1358.45,                    last time consumption/overall running time: 198.8213s / 692.4363 s
first_0:                     episode reward: 3.1000
second_0:                     episode reward: -3.1000
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 1431.25,                    last time consumption/overall running time: 215.5258s / 907.9621 s
first_0:                     episode reward: 1.4000
second_0:                     episode reward: -1.4000
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 1413.4,                    last time consumption/overall running time: 213.7232s / 1121.6853 s
first_0:                     episode reward: 2.5500
second_0:                     episode reward: -2.5500
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 1402.25,                    last time consumption/overall running time: 211.6087s / 1333.2940 s
first_0:                     episode reward: 1.6000
second_0:                     episode reward: -1.6000
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 1316.45,                    last time consumption/overall running time: 198.6415s / 1531.9355 s
first_0:                     episode reward: 3.8500
second_0:                     episode reward: -3.8500
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 1414.25,                    last time consumption/overall running time: 213.1075s / 1745.0430 s
first_0:                     episode reward: 1.7000
second_0:                     episode reward: -1.7000
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 1482.65,                    last time consumption/overall running time: 222.2052s / 1967.2482 s
first_0:                     episode reward: 0.5000
second_0:                     episode reward: -0.5000
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 1390.85,                    last time consumption/overall running time: 209.4105s / 2176.6587 s
first_0:                     episode reward: 3.6000
second_0:                     episode reward: -3.6000
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 1377.75,                    last time consumption/overall running time: 208.0942s / 2384.7530 s
first_0:                     episode reward: 0.2500
second_0:                     episode reward: -0.2500
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 1477.25,                    last time consumption/overall running time: 222.0184s / 2606.7713 s
first_0:                     episode reward: -3.6500
second_0:                     episode reward: 3.6500
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 1407.7,                    last time consumption/overall running time: 212.2476s / 2819.0189 s
first_0:                     episode reward: 4.3000
second_0:                     episode reward: -4.3000
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 1357.95,                    last time consumption/overall running time: 204.9820s / 3024.0009 s
first_0:                     episode reward: 3.4500
second_0:                     episode reward: -3.4500
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 1237.2,                    last time consumption/overall running time: 186.4177s / 3210.4186 s
first_0:                     episode reward: 2.1500
second_0:                     episode reward: -2.1500
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 1250.0,                    last time consumption/overall running time: 188.9818s / 3399.4004 s
first_0:                     episode reward: -3.8000
second_0:                     episode reward: 3.8000
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 1543.35,                    last time consumption/overall running time: 232.6158s / 3632.0162 s
first_0:                     episode reward: 0.1000
second_0:                     episode reward: -0.1000
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 1523.25,                    last time consumption/overall running time: 231.4461s / 3863.4623 s
first_0:                     episode reward: -1.1500
second_0:                     episode reward: 1.1500
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 1428.65,                    last time consumption/overall running time: 214.0908s / 4077.5531 s
first_0:                     episode reward: -5.0500
second_0:                     episode reward: 5.0500
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 1427.35,                    last time consumption/overall running time: 214.8303s / 4292.3834 s
first_0:                     episode reward: -6.8500
second_0:                     episode reward: 6.8500
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 1416.8,                    last time consumption/overall running time: 214.5186s / 4506.9020 s
first_0:                     episode reward: -7.6500
second_0:                     episode reward: 7.6500
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 1486.4,                    last time consumption/overall running time: 222.4486s / 4729.3506 s
first_0:                     episode reward: -9.5500
second_0:                     episode reward: 9.5500
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 1456.5,                    last time consumption/overall running time: 219.9603s / 4949.3109 s
first_0:                     episode reward: -7.5500
second_0:                     episode reward: 7.5500