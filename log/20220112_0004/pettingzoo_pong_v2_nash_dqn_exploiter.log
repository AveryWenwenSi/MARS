pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 96
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7f23cccd2fd0>
No agent are not learnable.
{'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 1}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f23ccc72eb8>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 68
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 1}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f9f300b0198>}}
Save models to : /home/zihan/research/MARS/data/model/0/pettingzoo_pong_v2_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/0/pettingzoo_pong_v2_nash_dqn_exploiter.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 1286.2,                    last time consumption/overall running time: 108.4793s / 108.4793 s
first_0:                     episode reward: -0.4500
second_0:                     episode reward: 0.4500
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 1365.9,                    last time consumption/overall running time: 167.9998s / 276.4791 s
first_0:                     episode reward: 1.1500
second_0:                     episode reward: -1.1500
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 1383.2,                    last time consumption/overall running time: 192.2018s / 468.6809 s
first_0:                     episode reward: 1.2000
second_0:                     episode reward: -1.2000
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 1352.55,                    last time consumption/overall running time: 197.2559s / 665.9368 s
first_0:                     episode reward: 3.6000
second_0:                     episode reward: -3.6000
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 1159.2,                    last time consumption/overall running time: 171.9226s / 837.8594 s
first_0:                     episode reward: 9.7000
second_0:                     episode reward: -9.7000
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 1117.2,                    last time consumption/overall running time: 166.1572s / 1004.0166 s
first_0:                     episode reward: 4.5500
second_0:                     episode reward: -4.5500
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 1326.1,                    last time consumption/overall running time: 199.4048s / 1203.4214 s
first_0:                     episode reward: 5.4000
second_0:                     episode reward: -5.4000
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 1329.5,                    last time consumption/overall running time: 199.2234s / 1402.6448 s
first_0:                     episode reward: -0.5500
second_0:                     episode reward: 0.5500
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 1251.1,                    last time consumption/overall running time: 187.7366s / 1590.3814 s
first_0:                     episode reward: -0.8500
second_0:                     episode reward: 0.8500
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 1251.0,                    last time consumption/overall running time: 187.7656s / 1778.1469 s
first_0:                     episode reward: 0.7000
second_0:                     episode reward: -0.7000
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 1275.3,                    last time consumption/overall running time: 192.5375s / 1970.6844 s
first_0:                     episode reward: 2.8000
second_0:                     episode reward: -2.8000
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 1237.85,                    last time consumption/overall running time: 184.9360s / 2155.6204 s
first_0:                     episode reward: 1.2000
second_0:                     episode reward: -1.2000
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 1327.75,                    last time consumption/overall running time: 199.4385s / 2355.0589 s
first_0:                     episode reward: -5.6000
second_0:                     episode reward: 5.6000
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 1376.45,                    last time consumption/overall running time: 210.9885s / 2566.0474 s
first_0:                     episode reward: -3.7500
second_0:                     episode reward: 3.7500
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 1349.3,                    last time consumption/overall running time: 204.5323s / 2770.5797 s
first_0:                     episode reward: -3.2000
second_0:                     episode reward: 3.2000
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 1275.4,                    last time consumption/overall running time: 193.2233s / 2963.8030 s
first_0:                     episode reward: 2.2000
second_0:                     episode reward: -2.2000
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 1275.0,                    last time consumption/overall running time: 190.8067s / 3154.6097 s
first_0:                     episode reward: -5.2500
second_0:                     episode reward: 5.2500
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 1318.7,                    last time consumption/overall running time: 203.1427s / 3357.7524 s
first_0:                     episode reward: -3.9000
second_0:                     episode reward: 3.9000
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 1288.15,                    last time consumption/overall running time: 196.1989s / 3553.9513 s
first_0:                     episode reward: -5.3000
second_0:                     episode reward: 5.3000
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 1299.35,                    last time consumption/overall running time: 197.6088s / 3751.5601 s
first_0:                     episode reward: -5.0000
second_0:                     episode reward: 5.0000
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 1372.95,                    last time consumption/overall running time: 208.0227s / 3959.5828 s
first_0:                     episode reward: -4.6500
second_0:                     episode reward: 4.6500
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 1328.3,                    last time consumption/overall running time: 203.1970s / 4162.7798 s
first_0:                     episode reward: -0.2500
second_0:                     episode reward: 0.2500
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 1358.95,                    last time consumption/overall running time: 207.3654s / 4370.1452 s
first_0:                     episode reward: -5.7000
second_0:                     episode reward: 5.7000
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 1336.75,                    last time consumption/overall running time: 201.2872s / 4571.4324 spygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 93
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 1}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f9f300b0198>}}
Save models to : /home/zihan/research/MARS/data/model/1/pettingzoo_pong_v2_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/1/pettingzoo_pong_v2_nash_dqn_exploiter.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 1465.45,                    last time consumption/overall running time: 132.8679s / 132.8679 s
first_0:                     episode reward: 3.1000
second_0:                     episode reward: -3.1000
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 1355.1,                    last time consumption/overall running time: 169.2358s / 302.1037 s
first_0:                     episode reward: -2.9500
second_0:                     episode reward: 2.9500
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 1355.1,                    last time consumption/overall running time: 191.3392s / 493.4429 s
first_0:                     episode reward: 2.0500
second_0:                     episode reward: -2.0500
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 1271.4,                    last time consumption/overall running time: 183.7452s / 677.1880 s
first_0:                     episode reward: 3.5500
second_0:                     episode reward: -3.5500
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 1074.95,                    last time consumption/overall running time: 159.1347s / 836.3228 s
first_0:                     episode reward: 12.0000
second_0:                     episode reward: -12.0000
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 1205.3,                    last time consumption/overall running time: 181.9472s / 1018.2700 s
first_0:                     episode reward: 6.3000
second_0:                     episode reward: -6.3000
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 1337.55,                    last time consumption/overall running time: 199.2000s / 1217.4700 s
first_0:                     episode reward: -0.8000
second_0:                     episode reward: 0.8000
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 1211.4,                    last time consumption/overall running time: 182.5262s / 1399.9961 s
first_0:                     episode reward: -2.7000
second_0:                     episode reward: 2.7000
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 1182.0,                    last time consumption/overall running time: 176.6937s / 1576.6898 s
first_0:                     episode reward: -2.4500
second_0:                     episode reward: 2.4500
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 1311.5,                    last time consumption/overall running time: 194.6597s / 1771.3496 s
first_0:                     episode reward: 0.9000
second_0:                     episode reward: -0.9000
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 1339.0,                    last time consumption/overall running time: 200.9626s / 1972.3122 s
first_0:                     episode reward: 0.6500
second_0:                     episode reward: -0.6500
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 1272.45,                    last time consumption/overall running time: 191.7004s / 2164.0125 s
first_0:                     episode reward: -2.1500
second_0:                     episode reward: 2.1500
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 1293.9,                    last time consumption/overall running time: 194.3669s / 2358.3795 s
first_0:                     episode reward: -7.8000
second_0:                     episode reward: 7.8000
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 1373.25,                    last time consumption/overall running time: 208.3291s / 2566.7086 s
first_0:                     episode reward: -0.7000
second_0:                     episode reward: 0.7000
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 1341.25,                    last time consumption/overall running time: 201.2235s / 2767.9320 s
first_0:                     episode reward: -5.4500
second_0:                     episode reward: 5.4500
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 1300.4,                    last time consumption/overall running time: 196.2265s / 2964.1585 s
first_0:                     episode reward: -2.0000
second_0:                     episode reward: 2.0000
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 1315.2,                    last time consumption/overall running time: 198.4092s / 3162.5678 s
first_0:                     episode reward: -2.3000
second_0:                     episode reward: 2.3000
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 1389.6,                    last time consumption/overall running time: 210.5854s / 3373.1532 s
first_0:                     episode reward: -6.3500
second_0:                     episode reward: 6.3500
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 1292.55,                    last time consumption/overall running time: 196.5825s / 3569.7357 s
first_0:                     episode reward: -5.2000
second_0:                     episode reward: 5.2000
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 1301.15,                    last time consumption/overall running time: 196.1673s / 3765.9031 s
first_0:                     episode reward: -2.3000
second_0:                     episode reward: 2.3000
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 1268.6,                    last time consumption/overall running time: 190.4195s / 3956.3226 s
first_0:                     episode reward: -4.1000
second_0:                     episode reward: 4.1000
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 1343.85,                    last time consumption/overall running time: 204.1874s / 4160.5100 s
first_0:                     episode reward: -5.3000
second_0:                     episode reward: 5.3000
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 1432.9,                    last time consumption/overall running time: 216.0070s / 4376.5170 s
first_0:                     episode reward: -0.3000
second_0:                     episode reward: 0.3000
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 1329.15,                    last time consumption/overall running time: 198.2455s / 4574.7625 s