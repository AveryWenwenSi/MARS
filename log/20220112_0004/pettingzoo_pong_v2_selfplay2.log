pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 81
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7f374a4328d0>
Agents No. [1] (index starting from 0) are not learnable.
{'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 20, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f374ad363c8>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 64
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 20, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f48e0509048>}}
Save models to : /home/zihan/research/MARS/data/model/0/pettingzoo_pong_v2_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/0/pettingzoo_pong_v2_selfplay2.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 749.75,                    last time consumption/overall running time: 78.1985s / 78.1985 s
first_0:                     episode reward: 20.9500
second_0:                     episode reward: -20.9500
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 770.05,                    last time consumption/overall running time: 83.0916s / 161.2901 s
first_0:                     episode reward: 20.6500
second_0:                     episode reward: -20.6500
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 763.15,                    last time consumption/overall running time: 87.1757s / 248.4658 s
first_0:                     episode reward: 20.7500
second_0:                     episode reward: -20.7500
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 1145.25,                    last time consumption/overall running time: 133.6186s / 382.0844 s
first_0:                     episode reward: 32.1000
second_0:                     episode reward: -32.1000
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 1456.1,                    last time consumption/overall running time: 176.0988s / 558.1831 s
first_0:                     episode reward: 39.8500
second_0:                     episode reward: -39.8500
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 2450.25,                    last time consumption/overall running time: 302.2463s / 860.4294 s
first_0:                     episode reward: 51.7500
second_0:                     episode reward: -51.7500
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 3466.1,                    last time consumption/overall running time: 431.6069s / 1292.0364 s
first_0:                     episode reward: 82.8500
second_0:                     episode reward: -82.8500
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 1367.5,                    last time consumption/overall running time: 169.4108s / 1461.4472 s
first_0:                     episode reward: -11.2000
second_0:                     episode reward: 11.2000
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 2858.35,                    last time consumption/overall running time: 354.2975s / 1815.7447 s
first_0:                     episode reward: 71.2500
second_0:                     episode reward: -71.2500
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 1956.75,                    last time consumption/overall running time: 242.6637s / 2058.4084 s
first_0:                     episode reward: 37.5000
second_0:                     episode reward: -37.5000
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 1999.35,                    last time consumption/overall running time: 248.7451s / 2307.1535 s
first_0:                     episode reward: 38.1000
second_0:                     episode reward: -38.1000
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 1012.0,                    last time consumption/overall running time: 125.5037s / 2432.6572 s
first_0:                     episode reward: -8.1500
second_0:                     episode reward: 8.1500
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 885.6,                    last time consumption/overall running time: 109.8398s / 2542.4970 s
first_0:                     episode reward: 5.2000
second_0:                     episode reward: -5.2000
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 2335.05,                    last time consumption/overall running time: 288.7778s / 2831.2748 s
first_0:                     episode reward: 58.6500
second_0:                     episode reward: -58.6500
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 4493.0,                    last time consumption/overall running time: 556.8496s / 3388.1244 s
first_0:                     episode reward: 116.7000
second_0:                     episode reward: -116.7000
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 3106.25,                    last time consumption/overall running time: 385.0231s / 3773.1475 s
first_0:                     episode reward: 76.1000
second_0:                     episode reward: -76.1000
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 729.45,                    last time consumption/overall running time: 89.7091s / 3862.8566 s
first_0:                     episode reward: 20.9500
second_0:                     episode reward: -20.9500
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 2021.0,                    last time consumption/overall running time: 247.2200s / 4110.0765 s
first_0:                     episode reward: 35.0500
second_0:                     episode reward: -35.0500
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 1152.7,                    last time consumption/overall running time: 142.6874s / 4252.7639 s
first_0:                     episode reward: 32.6000
second_0:                     episode reward: -32.6000
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 1045.75,                    last time consumption/overall running time: 129.9923s / 4382.7563 s
first_0:                     episode reward: 24.0500
second_0:                     episode reward: -24.0500
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 883.95,                    last time consumption/overall running time: 109.5881s / 4492.3444 s
first_0:                     episode reward: 12.4500
second_0:                     episode reward: -12.4500
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 818.4,                    last time consumption/overall running time: 101.8759s / 4594.2202 s
first_0:                     episode reward: 16.9500
second_0:                     episode reward: -16.9500
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 1433.4,                    last time consumption/overall running time: 179.8442s / 4774.0645 s
first_0:                     episode reward: 29.8500
second_0:                     episode reward: -29.8500
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 1917.75,                    last time consumption/overall running time: 239.0395s / 5013.1040 spygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 57
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 20, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f48e0509048>}}
Save models to : /home/zihan/research/MARS/data/model/1/pettingzoo_pong_v2_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/1/pettingzoo_pong_v2_selfplay2.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 750.85,                    last time consumption/overall running time: 75.1175s / 75.1175 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 766.55,                    last time consumption/overall running time: 82.6958s / 157.8133 s
first_0:                     episode reward: 20.8000
second_0:                     episode reward: -20.8000
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 744.05,                    last time consumption/overall running time: 85.1060s / 242.9193 s
first_0:                     episode reward: 20.7000
second_0:                     episode reward: -20.7000
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 1125.05,                    last time consumption/overall running time: 131.3503s / 374.2696 s
first_0:                     episode reward: 30.5000
second_0:                     episode reward: -30.5000
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 929.65,                    last time consumption/overall running time: 111.7787s / 486.0483 s
first_0:                     episode reward: 24.3000
second_0:                     episode reward: -24.3000
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 2826.3,                    last time consumption/overall running time: 347.0062s / 833.0545 s
first_0:                     episode reward: 69.9500
second_0:                     episode reward: -69.9500
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 3670.25,                    last time consumption/overall running time: 456.9095s / 1289.9640 s
first_0:                     episode reward: 91.2500
second_0:                     episode reward: -91.2500
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 1506.75,                    last time consumption/overall running time: 186.7948s / 1476.7588 s
first_0:                     episode reward: 0.4000
second_0:                     episode reward: -0.4000
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 3575.6,                    last time consumption/overall running time: 442.5711s / 1919.3299 s
first_0:                     episode reward: 86.4500
second_0:                     episode reward: -86.4500
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 1138.2,                    last time consumption/overall running time: 141.4058s / 2060.7357 s
first_0:                     episode reward: 3.6000
second_0:                     episode reward: -3.6000
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 1897.55,                    last time consumption/overall running time: 235.6631s / 2296.3987 s
first_0:                     episode reward: 41.0500
second_0:                     episode reward: -41.0500
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 913.05,                    last time consumption/overall running time: 113.2665s / 2409.6652 s
first_0:                     episode reward: 2.0000
second_0:                     episode reward: -2.0000
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 917.1,                    last time consumption/overall running time: 113.7226s / 2523.3878 s
first_0:                     episode reward: 5.0000
second_0:                     episode reward: -5.0000
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 2348.85,                    last time consumption/overall running time: 290.0307s / 2813.4185 s
first_0:                     episode reward: 55.4000
second_0:                     episode reward: -55.4000
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 6145.4,                    last time consumption/overall running time: 761.6047s / 3575.0232 s
first_0:                     episode reward: 171.0500
second_0:                     episode reward: -171.0500
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 1785.85,                    last time consumption/overall running time: 220.4782s / 3795.5014 s
first_0:                     episode reward: 38.9000
second_0:                     episode reward: -38.9000
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 728.0,                    last time consumption/overall running time: 89.4107s / 3884.9122 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 2518.35,                    last time consumption/overall running time: 308.6400s / 4193.5522 s
first_0:                     episode reward: 55.8500
second_0:                     episode reward: -55.8500
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 1060.6,                    last time consumption/overall running time: 131.4305s / 4324.9827 s
first_0:                     episode reward: 31.3500
second_0:                     episode reward: -31.3500
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 837.65,                    last time consumption/overall running time: 103.9033s / 4428.8860 s
first_0:                     episode reward: 8.0000
second_0:                     episode reward: -8.0000
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 795.6,                    last time consumption/overall running time: 98.8935s / 4527.7795 s
first_0:                     episode reward: 18.9500
second_0:                     episode reward: -18.9500
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 1795.25,                    last time consumption/overall running time: 225.1044s / 4752.8839 s
first_0:                     episode reward: 40.6000
second_0:                     episode reward: -40.6000
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 1735.0,                    last time consumption/overall running time: 215.9588s / 4968.8428 s
first_0:                     episode reward: 11.2000
second_0:                     episode reward: -11.2000
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 1087.7,                    last time consumption/overall running time: 134.8617s / 5103.7045 s