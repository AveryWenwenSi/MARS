pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 83
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7fac408dbd68>
Agents No. [1] (index starting from 0) are not learnable.
{'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 20, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7fac408fad30>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 43
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 20, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f8386128128>}}
Save models to : /home/zihan/research/MARS/data/model/1/pettingzoo_pong_v2_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/1/pettingzoo_pong_v2_nxdo2.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 1294.3,                    last time consumption/overall running time: 128.1875s / 128.1875 s
first_0:                     episode reward: -1.2000
second_0:                     episode reward: 1.2000
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 958.4,                    last time consumption/overall running time: 109.7510s / 237.9385 s
first_0:                     episode reward: 6.9000
second_0:                     episode reward: -6.9000
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 802.7,                    last time consumption/overall running time: 97.3064s / 335.2449 s
first_0:                     episode reward: 19.3000
second_0:                     episode reward: -19.3000
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 742.9,                    last time consumption/overall running time: 91.5635s / 426.8083 s
first_0:                     episode reward: 20.7000
second_0:                     episode reward: -20.7000
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 730.35,                    last time consumption/overall running time: 91.4540s / 518.2623 s
first_0:                     episode reward: 20.9500
second_0:                     episode reward: -20.9500
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 729.8,                    last time consumption/overall running time: 90.0977s / 608.3600 s
first_0:                     episode reward: 20.9500
second_0:                     episode reward: -20.9500
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 779.65,                    last time consumption/overall running time: 95.7349s / 704.0948 s
first_0:                     episode reward: 19.5000
second_0:                     episode reward: -19.5000
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 1716.5,                    last time consumption/overall running time: 208.2432s / 912.3381 s
first_0:                     episode reward: -2.0000
second_0:                     episode reward: 2.0000
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 1285.55,                    last time consumption/overall running time: 158.5345s / 1070.8726 s
first_0:                     episode reward: -32.1500
second_0:                     episode reward: 32.1500
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 1755.55,                    last time consumption/overall running time: 218.0551s / 1288.9276 s
first_0:                     episode reward: -43.1000
second_0:                     episode reward: 43.1000
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 1786.9,                    last time consumption/overall running time: 221.5201s / 1510.4477 s
first_0:                     episode reward: -44.3000
second_0:                     episode reward: 44.3000
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 1403.7,                    last time consumption/overall running time: 173.8510s / 1684.2987 s
first_0:                     episode reward: -35.6000
second_0:                     episode reward: 35.6000
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 1424.35,                    last time consumption/overall running time: 177.0870s / 1861.3857 s
first_0:                     episode reward: -35.9500
second_0:                     episode reward: 35.9500
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 2270.6,                    last time consumption/overall running time: 281.5528s / 2142.9384 s
first_0:                     episode reward: -56.9500
second_0:                     episode reward: 56.9500
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 2664.85,                    last time consumption/overall running time: 330.1194s / 2473.0578 s
first_0:                     episode reward: -64.6000
second_0:                     episode reward: 64.6000
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 2431.3,                    last time consumption/overall running time: 300.3309s / 2773.3887 s
first_0:                     episode reward: -58.5000
second_0:                     episode reward: 58.5000
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 2331.05,                    last time consumption/overall running time: 288.3923s / 3061.7810 s
first_0:                     episode reward: -58.6000
second_0:                     episode reward: 58.6000
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 3039.5,                    last time consumption/overall running time: 378.7949s / 3440.5759 s
first_0:                     episode reward: -73.2000
second_0:                     episode reward: 73.2000
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 2688.65,                    last time consumption/overall running time: 337.0845s / 3777.6603 s
first_0:                     episode reward: -64.4000
second_0:                     episode reward: 64.4000
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 3031.3,                    last time consumption/overall running time: 364.9636s / 4142.6240 s
first_0:                     episode reward: -73.1500
second_0:                     episode reward: 73.1500
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 1023.65,                    last time consumption/overall running time: 125.4829s / 4268.1069 s
first_0:                     episode reward: 5.7500
second_0:                     episode reward: -5.7500
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 1082.45,                    last time consumption/overall running time: 132.8826s / 4400.9895 s
first_0:                     episode reward: 2.8000
second_0:                     episode reward: -2.8000
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 1108.75,                    last time consumption/overall running time: 138.2888s / 4539.2783 s
first_0:                     episode reward: 6.0000
second_0:                     episode reward: -6.0000
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 1104.55,                    last time consumption/overall running time: 138.2405s / 4677.5188 spygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 28
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 20, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f8386128128>}}
Save models to : /home/zihan/research/MARS/data/model/0/pettingzoo_pong_v2_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/0/pettingzoo_pong_v2_nxdo2.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 1308.85,                    last time consumption/overall running time: 132.7008s / 132.7008 s
first_0:                     episode reward: -0.5000
second_0:                     episode reward: 0.5000
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 1007.85,                    last time consumption/overall running time: 116.1039s / 248.8048 s
first_0:                     episode reward: 9.1500
second_0:                     episode reward: -9.1500
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 799.65,                    last time consumption/overall running time: 97.1423s / 345.9471 s
first_0:                     episode reward: 17.7500
second_0:                     episode reward: -17.7500
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 729.25,                    last time consumption/overall running time: 90.4559s / 436.4030 s
first_0:                     episode reward: 21.0000
second_0:                     episode reward: -21.0000
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 729.9,                    last time consumption/overall running time: 90.6921s / 527.0951 s
first_0:                     episode reward: 20.9500
second_0:                     episode reward: -20.9500
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 735.25,                    last time consumption/overall running time: 91.3659s / 618.4610 s
first_0:                     episode reward: 20.8500
second_0:                     episode reward: -20.8500
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 795.8,                    last time consumption/overall running time: 98.6269s / 717.0879 s
first_0:                     episode reward: 18.9000
second_0:                     episode reward: -18.9000
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 1836.0,                    last time consumption/overall running time: 224.9117s / 941.9996 s
first_0:                     episode reward: -4.0500
second_0:                     episode reward: 4.0500
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 1952.95,                    last time consumption/overall running time: 244.1399s / 1186.1395 s
first_0:                     episode reward: -45.1000
second_0:                     episode reward: 45.1000
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 1611.9,                    last time consumption/overall running time: 200.9270s / 1387.0665 s
first_0:                     episode reward: -38.9000
second_0:                     episode reward: 38.9000
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 1912.15,                    last time consumption/overall running time: 238.1560s / 1625.2224 s
first_0:                     episode reward: -47.8500
second_0:                     episode reward: 47.8500
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 1583.9,                    last time consumption/overall running time: 197.7664s / 1822.9889 s
first_0:                     episode reward: -39.2000
second_0:                     episode reward: 39.2000
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 2653.35,                    last time consumption/overall running time: 331.3269s / 2154.3158 s
first_0:                     episode reward: -63.8000
second_0:                     episode reward: 63.8000
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 3202.8,                    last time consumption/overall running time: 398.3894s / 2552.7052 s
first_0:                     episode reward: -78.7000
second_0:                     episode reward: 78.7000
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 2566.75,                    last time consumption/overall running time: 319.2288s / 2871.9340 s
first_0:                     episode reward: -62.4000
second_0:                     episode reward: 62.4000
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 2737.35,                    last time consumption/overall running time: 340.0159s / 3211.9499 s
first_0:                     episode reward: -65.1000
second_0:                     episode reward: 65.1000
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 2988.8,                    last time consumption/overall running time: 375.2052s / 3587.1551 s
first_0:                     episode reward: -72.1000
second_0:                     episode reward: 72.1000
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 2699.3,                    last time consumption/overall running time: 340.0675s / 3927.2226 s
first_0:                     episode reward: -64.3000
second_0:                     episode reward: 64.3000
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 1973.95,                    last time consumption/overall running time: 233.7269s / 4160.9494 s
first_0:                     episode reward: -42.2000
second_0:                     episode reward: 42.2000
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 1153.95,                    last time consumption/overall running time: 141.8034s / 4302.7529 s
first_0:                     episode reward: -1.1500
second_0:                     episode reward: 1.1500
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 1144.5,                    last time consumption/overall running time: 141.8838s / 4444.6367 s
first_0:                     episode reward: 1.3500
second_0:                     episode reward: -1.3500
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 1087.7,                    last time consumption/overall running time: 136.7364s / 4581.3731 s
first_0:                     episode reward: 6.2000
second_0:                     episode reward: -6.2000
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 1214.3,                    last time consumption/overall running time: 153.0280s / 4734.4011 s
first_0:                     episode reward: 8.4000
second_0:                     episode reward: -8.4000
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 1076.85,                    last time consumption/overall running time: 135.1663s / 4869.5674 s