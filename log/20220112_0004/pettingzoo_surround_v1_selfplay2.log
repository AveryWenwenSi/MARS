pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 89
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7fa8ab595e80>
Agents No. [1] (index starting from 0) are not learnable.
{'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 16, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7fa8ac4deda0>}}
pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 44
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 16, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7fcd452b2d68>}}
Save models to : /home/zihan/research/MARS/data/model/0/pettingzoo_surround_v1_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/0/pettingzoo_surround_v1_selfplay2.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 1492.9,                    last time consumption/overall running time: 163.2589s / 163.2589 s
first_0:                     episode reward: -3.8500
second_0:                     episode reward: 3.8500
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 1320.2,                    last time consumption/overall running time: 160.9097s / 324.1686 s
first_0:                     episode reward: 1.9500
second_0:                     episode reward: -1.9500
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 1162.85,                    last time consumption/overall running time: 149.0356s / 473.2042 s
first_0:                     episode reward: 7.0000
second_0:                     episode reward: -7.0000
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 1049.8,                    last time consumption/overall running time: 136.1348s / 609.3390 s
first_0:                     episode reward: 9.5000
second_0:                     episode reward: -9.5000
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 1076.0,                    last time consumption/overall running time: 140.9589s / 750.2979 s
first_0:                     episode reward: 9.4000
second_0:                     episode reward: -9.4000
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 1072.0,                    last time consumption/overall running time: 140.8121s / 891.1101 s
first_0:                     episode reward: 9.4000
second_0:                     episode reward: -9.4000
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 1150.8,                    last time consumption/overall running time: 151.6272s / 1042.7373 s
first_0:                     episode reward: 8.8500
second_0:                     episode reward: -8.8500
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 987.25,                    last time consumption/overall running time: 129.7512s / 1172.4885 s
first_0:                     episode reward: 9.0500
second_0:                     episode reward: -9.0500
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 993.15,                    last time consumption/overall running time: 131.2519s / 1303.7404 s
first_0:                     episode reward: 9.8500
second_0:                     episode reward: -9.8500
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 991.25,                    last time consumption/overall running time: 131.0835s / 1434.8239 s
first_0:                     episode reward: 9.9500
second_0:                     episode reward: -9.9500
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 1474.95,                    last time consumption/overall running time: 190.5255s / 1625.3494 s
first_0:                     episode reward: 5.6000
second_0:                     episode reward: -5.6000
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 1623.6,                    last time consumption/overall running time: 213.2386s / 1838.5880 s
first_0:                     episode reward: 3.9500
second_0:                     episode reward: -3.9500
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 1077.05,                    last time consumption/overall running time: 142.2377s / 1980.8257 s
first_0:                     episode reward: 9.3500
second_0:                     episode reward: -9.3500
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 1027.4,                    last time consumption/overall running time: 135.7614s / 2116.5871 s
first_0:                     episode reward: 9.7000
second_0:                     episode reward: -9.7000
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 1156.25,                    last time consumption/overall running time: 152.5814s / 2269.1685 s
first_0:                     episode reward: 7.6000
second_0:                     episode reward: -7.6000
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 1370.55,                    last time consumption/overall running time: 180.1972s / 2449.3658 s
first_0:                     episode reward: 3.8500
second_0:                     episode reward: -3.8500
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 1419.1,                    last time consumption/overall running time: 187.2986s / 2636.6644 s
first_0:                     episode reward: -5.8500
second_0:                     episode reward: 5.8500
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 1522.15,                    last time consumption/overall running time: 200.5926s / 2837.2570 s
first_0:                     episode reward: -0.2000
second_0:                     episode reward: 0.2000
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 1216.0,                    last time consumption/overall running time: 160.1948s / 2997.4518 s
first_0:                     episode reward: 8.3000
second_0:                     episode reward: -8.3000
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 1137.55,                    last time consumption/overall running time: 150.4538s / 3147.9056 s
first_0:                     episode reward: 9.4000
second_0:                     episode reward: -9.4000
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 1265.8,                    last time consumption/overall running time: 166.6379s / 3314.5434 s
first_0:                     episode reward: 8.6000
second_0:                     episode reward: -8.6000
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 1308.7,                    last time consumption/overall running time: 172.8006s / 3487.3440 s
first_0:                     episode reward: 7.4500
second_0:                     episode reward: -7.4500
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 1146.55,                    last time consumption/overall running time: 151.0670s / 3638.4110 s
first_0:                     episode reward: 8.3500
second_0:                     episode reward: -8.3500
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 1192.1,                    last time consumption/overall running time: 157.3660s / 3795.7770 spygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 73
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 16, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7fcd452b2d68>}}
Save models to : /home/zihan/research/MARS/data/model/1/pettingzoo_surround_v1_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/1/pettingzoo_surround_v1_selfplay2.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 1465.55,                    last time consumption/overall running time: 159.2608s / 159.2608 s
first_0:                     episode reward: -4.2000
second_0:                     episode reward: 4.2000
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 1342.3,                    last time consumption/overall running time: 163.6715s / 322.9324 s
first_0:                     episode reward: 2.1000
second_0:                     episode reward: -2.1000
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 1271.85,                    last time consumption/overall running time: 162.1228s / 485.0552 s
first_0:                     episode reward: 5.6500
second_0:                     episode reward: -5.6500
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 1053.7,                    last time consumption/overall running time: 137.2644s / 622.3196 s
first_0:                     episode reward: 8.9000
second_0:                     episode reward: -8.9000
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 1055.7,                    last time consumption/overall running time: 137.9663s / 760.2859 s
first_0:                     episode reward: 8.2000
second_0:                     episode reward: -8.2000
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 1501.2,                    last time consumption/overall running time: 196.9236s / 957.2094 s
first_0:                     episode reward: 2.8000
second_0:                     episode reward: -2.8000
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 1504.7,                    last time consumption/overall running time: 197.0783s / 1154.2877 s
first_0:                     episode reward: 2.8500
second_0:                     episode reward: -2.8500
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 1268.2,                    last time consumption/overall running time: 167.2088s / 1321.4965 s
first_0:                     episode reward: 7.5500
second_0:                     episode reward: -7.5500
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 1075.6,                    last time consumption/overall running time: 141.1414s / 1462.6379 s
first_0:                     episode reward: 9.4000
second_0:                     episode reward: -9.4000
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 1359.9,                    last time consumption/overall running time: 175.2437s / 1637.8816 s
first_0:                     episode reward: 6.2500
second_0:                     episode reward: -6.2500
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 1134.7,                    last time consumption/overall running time: 148.6639s / 1786.5455 s
first_0:                     episode reward: 9.7500
second_0:                     episode reward: -9.7500
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 951.2,                    last time consumption/overall running time: 125.8511s / 1912.3966 s
first_0:                     episode reward: 9.9000
second_0:                     episode reward: -9.9000
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 1127.95,                    last time consumption/overall running time: 148.2875s / 2060.6841 s
first_0:                     episode reward: 9.3500
second_0:                     episode reward: -9.3500
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 1892.35,                    last time consumption/overall running time: 249.1623s / 2309.8464 s
first_0:                     episode reward: 3.3500
second_0:                     episode reward: -3.3500
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 1678.35,                    last time consumption/overall running time: 220.3887s / 2530.2351 s
first_0:                     episode reward: 3.5500
second_0:                     episode reward: -3.5500
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 1102.1,                    last time consumption/overall running time: 145.2571s / 2675.4921 s
first_0:                     episode reward: 9.5000
second_0:                     episode reward: -9.5000
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 1147.6,                    last time consumption/overall running time: 150.3035s / 2825.7956 s
first_0:                     episode reward: 9.0000
second_0:                     episode reward: -9.0000
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 1629.1,                    last time consumption/overall running time: 213.8084s / 3039.6040 s
first_0:                     episode reward: 5.3000
second_0:                     episode reward: -5.3000
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 1538.9,                    last time consumption/overall running time: 202.9023s / 3242.5063 s
first_0:                     episode reward: 6.3500
second_0:                     episode reward: -6.3500
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 1310.85,                    last time consumption/overall running time: 172.1690s / 3414.6753 s
first_0:                     episode reward: 7.6500
second_0:                     episode reward: -7.6500
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 1594.25,                    last time consumption/overall running time: 209.1637s / 3623.8390 s
first_0:                     episode reward: 4.8500
second_0:                     episode reward: -4.8500
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 1771.5,                    last time consumption/overall running time: 232.7155s / 3856.5545 s
first_0:                     episode reward: 5.0500
second_0:                     episode reward: -5.0500
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 1331.1,                    last time consumption/overall running time: 175.4854s / 4032.0399 s
first_0:                     episode reward: 8.1000
second_0:                     episode reward: -8.1000
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 1791.3,                    last time consumption/overall running time: 234.9113s / 4266.9512 s