pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
Load tennis_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
random seed: 13
<mars.env.wrappers.mars_wrappers.Dict2TupleWrapper object at 0x7f958ff99950>
No agent are not learnable.
{'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f959110a050>}}
pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
Load tennis_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
random seed: 75
Arguments:  {'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f2732d8f650>}}
Save models to : /home/zihan/research/MARS/data/model/0/pettingzoo_tennis_v2_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/0/pettingzoo_tennis_v2_nash_dqn.
Process ID: 0, episode: 20/10000 (0.2000%),                     avg. length: 9623.65,                    last time consumption/overall running time: 2541.8247s / 2541.8247 s
first_0:                     episode reward: 2.8500
second_0:                     episode reward: -2.8500
Process ID: 0, episode: 40/10000 (0.4000%),                     avg. length: 9999.0,                    last time consumption/overall running time: 2970.3813s / 5512.2060 s
first_0:                     episode reward: 4.1500
second_0:                     episode reward: -4.1500
Process ID: 0, episode: 60/10000 (0.6000%),                     avg. length: 9999.0,                    last time consumption/overall running time: 2973.3068s / 8485.5128 s
first_0:                     episode reward: 1.0500
second_0:                     episode reward: -1.0500
Process ID: 0, episode: 80/10000 (0.8000%),                     avg. length: 9255.35,                    last time consumption/overall running time: 2751.4012s / 11236.9140 s
first_0:                     episode reward: -4.2000
second_0:                     episode reward: 4.2000
Process ID: 0, episode: 100/10000 (1.0000%),                     avg. length: 9968.1,                    last time consumption/overall running time: 2960.0710s / 14196.9850 s
first_0:                     episode reward: 5.7000
second_0:                     episode reward: -5.7000
Process ID: 0, episode: 120/10000 (1.2000%),                     avg. length: 9999.0,                    last time consumption/overall running time: 2976.3018s / 17173.2868 s
first_0:                     episode reward: 1.5000
second_0:                     episode reward: -1.5000
Process ID: 0, episode: 140/10000 (1.4000%),                     avg. length: 9260.15,                    last time consumption/overall running time: 2756.0101s / 19929.2969 s
first_0:                     episode reward: 0.1000
second_0:                     episode reward: -0.1000
Process ID: 0, episode: 160/10000 (1.6000%),                     avg. length: 8707.8,                    last time consumption/overall running time: 2585.5669s / 22514.8638 s
first_0:                     episode reward: 1.2500
second_0:                     episode reward: -1.2500
Process ID: 0, episode: 180/10000 (1.8000%),                     avg. length: 8516.1,                    last time consumption/overall running time: 2536.1073s / 25050.9712 s
first_0:                     episode reward: -3.9000
second_0:                     episode reward: 3.9000
Process ID: 0, episode: 200/10000 (2.0000%),                     avg. length: 7688.15,                    last time consumption/overall running time: 2289.0177s / 27339.9889 s
first_0:                     episode reward: -4.3500
second_0:                     episode reward: 4.3500
Process ID: 0, episode: 220/10000 (2.2000%),                     avg. length: 7634.3,                    last time consumption/overall running time: 2269.7569s / 29609.7458 s
first_0:                     episode reward: -4.3500
second_0:                     episode reward: 4.3500
Process ID: 0, episode: 240/10000 (2.4000%),                     avg. length: 8072.95,                    last time consumption/overall running time: 2402.2436s / 32011.9894 s
first_0:                     episode reward: -6.4000
second_0:                     episode reward: 6.4000
Process ID: 0, episode: 260/10000 (2.6000%),                     avg. length: 7824.45,                    last time consumption/overall running time: 2324.2730s / 34336.2624 s
first_0:                     episode reward: -4.8000
second_0:                     episode reward: 4.8000
Process ID: 0, episode: 280/10000 (2.8000%),                     avg. length: 5651.05,                    last time consumption/overall running time: 1673.9305s / 36010.1929 s
first_0:                     episode reward: -9.0500
second_0:                     episode reward: 9.0500
Process ID: 0, episode: 300/10000 (3.0000%),                     avg. length: 6025.0,                    last time consumption/overall running time: 1790.7376s / 37800.9304 s
first_0:                     episode reward: -3.7500
second_0:                     episode reward: 3.7500
Process ID: 0, episode: 320/10000 (3.2000%),                     avg. length: 5252.0,                    last time consumption/overall running time: 1561.4574s / 39362.3879 s
first_0:                     episode reward: -5.3000
second_0:                     episode reward: 5.3000
Process ID: 0, episode: 340/10000 (3.4000%),                     avg. length: 4638.75,                    last time consumption/overall running time: 1380.5794s / 40742.9673 s
first_0:                     episode reward: -4.5000
second_0:                     episode reward: 4.5000
Process ID: 0, episode: 360/10000 (3.6000%),                     avg. length: 4462.7,                    last time consumption/overall running time: 1328.2148s / 42071.1820 s
first_0:                     episode reward: -5.3000
second_0:                     episode reward: 5.3000
Process ID: 0, episode: 380/10000 (3.8000%),                     avg. length: 4574.9,                    last time consumption/overall running time: 1370.4677s / 43441.6497 s
first_0:                     episode reward: -8.5500
second_0:                     episode reward: 8.5500
Process ID: 0, episode: 400/10000 (4.0000%),                     avg. length: 4974.6,                    last time consumption/overall running time: 1485.5623s / 44927.2120 s
first_0:                     episode reward: -9.1000
second_0:                     episode reward: 9.1000
Process ID: 0, episode: 420/10000 (4.2000%),                     avg. length: 4483.95,                    last time consumption/overall running time: 1341.2990s / 46268.5110 s
first_0:                     episode reward: -9.2000
second_0:                     episode reward: 9.2000
Process ID: 0, episode: 440/10000 (4.4000%),                     avg. length: 4151.9,                    last time consumption/overall running time: 1238.1566s / 47506.6676 s
first_0:                     episode reward: -8.7500
second_0:                     episode reward: 8.7500
Process ID: 0, episode: 460/10000 (4.6000%),                     avg. length: 3319.1,                    last time consumption/overall running time: 995.2526s / 48501.9201 s
first_0:                     episode reward: -10.8500
second_0:                     episode reward: 10.8500
Process ID: 0, episode: 480/10000 (4.8000%),                     avg. length: 4149.95,                    last time consumption/overall running time: 1242.5610s / 49744.4812 spygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
Load tennis_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
random seed: 35
Arguments:  {'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': True, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}, 'add_components': {'replay_buffer': <AutoProxy[replay_buffer] object, typeid 'replay_buffer' at 0x7f2732d8f610>}}
Save models to : /home/zihan/research/MARS/data/model/1/pettingzoo_tennis_v2_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/1/pettingzoo_tennis_v2_nash_dqn.
Process ID: 1, episode: 20/10000 (0.2000%),                     avg. length: 9226.3,                    last time consumption/overall running time: 2415.3015s / 2415.3015 s
first_0:                     episode reward: 2.5500
second_0:                     episode reward: -2.5500
Process ID: 1, episode: 40/10000 (0.4000%),                     avg. length: 9665.15,                    last time consumption/overall running time: 2868.4778s / 5283.7793 s
first_0:                     episode reward: 3.4000
second_0:                     episode reward: -3.4000
Process ID: 1, episode: 60/10000 (0.6000%),                     avg. length: 9639.95,                    last time consumption/overall running time: 2864.9610s / 8148.7403 s
first_0:                     episode reward: 2.6500
second_0:                     episode reward: -2.6500
Process ID: 1, episode: 80/10000 (0.8000%),                     avg. length: 8803.05,                    last time consumption/overall running time: 2619.0419s / 10767.7822 s
first_0:                     episode reward: 3.7000
second_0:                     episode reward: -3.7000
Process ID: 1, episode: 100/10000 (1.0000%),                     avg. length: 9365.5,                    last time consumption/overall running time: 2781.9447s / 13549.7269 s
first_0:                     episode reward: 1.5500
second_0:                     episode reward: -1.5500
Process ID: 1, episode: 120/10000 (1.2000%),                     avg. length: 9014.05,                    last time consumption/overall running time: 2678.7641s / 16228.4910 s
first_0:                     episode reward: 0.9500
second_0:                     episode reward: -0.9500
Process ID: 1, episode: 140/10000 (1.4000%),                     avg. length: 9542.0,                    last time consumption/overall running time: 2831.2735s / 19059.7645 s
first_0:                     episode reward: -1.5000
second_0:                     episode reward: 1.5000
Process ID: 1, episode: 160/10000 (1.6000%),                     avg. length: 9287.05,                    last time consumption/overall running time: 2753.7670s / 21813.5315 s
first_0:                     episode reward: -3.8500
second_0:                     episode reward: 3.8500
Process ID: 1, episode: 180/10000 (1.8000%),                     avg. length: 9645.8,                    last time consumption/overall running time: 2879.3743s / 24692.9058 s
first_0:                     episode reward: -0.6000
second_0:                     episode reward: 0.6000
Process ID: 1, episode: 200/10000 (2.0000%),                     avg. length: 8173.5,                    last time consumption/overall running time: 2439.7483s / 27132.6541 s
first_0:                     episode reward: -0.8000
second_0:                     episode reward: 0.8000
Process ID: 1, episode: 220/10000 (2.2000%),                     avg. length: 7902.3,                    last time consumption/overall running time: 2349.4936s / 29482.1477 s
first_0:                     episode reward: -4.4500
second_0:                     episode reward: 4.4500
Process ID: 1, episode: 240/10000 (2.4000%),                     avg. length: 7386.2,                    last time consumption/overall running time: 2201.2531s / 31683.4008 s
first_0:                     episode reward: -5.5000
second_0:                     episode reward: 5.5000
Process ID: 1, episode: 260/10000 (2.6000%),                     avg. length: 5924.6,                    last time consumption/overall running time: 1764.3648s / 33447.7655 s
first_0:                     episode reward: -2.0500
second_0:                     episode reward: 2.0500
Process ID: 1, episode: 280/10000 (2.8000%),                     avg. length: 7214.2,                    last time consumption/overall running time: 2150.5942s / 35598.3597 s
first_0:                     episode reward: -4.9000
second_0:                     episode reward: 4.9000
Process ID: 1, episode: 300/10000 (3.0000%),                     avg. length: 6058.55,                    last time consumption/overall running time: 1808.5230s / 37406.8827 s
first_0:                     episode reward: -3.8000
second_0:                     episode reward: 3.8000
Process ID: 1, episode: 320/10000 (3.2000%),                     avg. length: 6398.1,                    last time consumption/overall running time: 1896.9498s / 39303.8324 s
first_0:                     episode reward: -2.7500
second_0:                     episode reward: 2.7500
Process ID: 1, episode: 340/10000 (3.4000%),                     avg. length: 5230.25,                    last time consumption/overall running time: 1553.3249s / 40857.1574 s
first_0:                     episode reward: -7.0500
second_0:                     episode reward: 7.0500
Process ID: 1, episode: 360/10000 (3.6000%),                     avg. length: 5506.6,                    last time consumption/overall running time: 1631.4094s / 42488.5667 s
first_0:                     episode reward: -8.5500
second_0:                     episode reward: 8.5500
Process ID: 1, episode: 380/10000 (3.8000%),                     avg. length: 5756.75,                    last time consumption/overall running time: 1714.8152s / 44203.3819 s
first_0:                     episode reward: -7.9500
second_0:                     episode reward: 7.9500
Process ID: 1, episode: 400/10000 (4.0000%),                     avg. length: 4205.65,                    last time consumption/overall running time: 1248.1264s / 45451.5083 s
first_0:                     episode reward: -5.9500
second_0:                     episode reward: 5.9500
Process ID: 1, episode: 420/10000 (4.2000%),                     avg. length: 4200.6,                    last time consumption/overall running time: 1250.2493s / 46701.7576 s
first_0:                     episode reward: -9.8000
second_0:                     episode reward: 9.8000
Process ID: 1, episode: 440/10000 (4.4000%),                     avg. length: 4360.05,                    last time consumption/overall running time: 1305.6907s / 48007.4483 s
first_0:                     episode reward: -10.2500
second_0:                     episode reward: 10.2500
Process ID: 1, episode: 460/10000 (4.6000%),                     avg. length: 4552.7,                    last time consumption/overall running time: 1365.5557s / 49373.0040 s
first_0:                     episode reward: -7.7000
second_0:                     episode reward: 7.7000
Process ID: 1, episode: 480/10000 (4.8000%),                     avg. length: 4650.45,                    last time consumption/overall running time: 1390.8257s / 50763.8297 s