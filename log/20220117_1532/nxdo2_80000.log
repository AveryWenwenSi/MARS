pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
random seed: 91
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7f8e844645d0>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.224 0.093 0.114 ... 0.    0.    0.   ]
 [0.072 0.011 0.062 ... 0.    0.    0.   ]]
Load checkpoints (policy family):  [['83' '5753' '6419' ... '78946' '79127' '79375']
 ['121' '6342' '6627' ... '78967' '79225' '79437']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220116204408/epi_80000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220116204408_exploit_80000/mdp_arbitrary_mdp_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220116204408_exploit_80000/mdp_arbitrary_mdp_nxdo2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2623s / 1.2623 s
agent0:                 episode reward: -0.8110,                 loss: nan
agent1:                 episode reward: 0.8110,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0958s / 1.3581 s
agent0:                 episode reward: -0.2403,                 loss: nan
agent1:                 episode reward: 0.2403,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0920s / 1.4500 s
agent0:                 episode reward: 0.0023,                 loss: nan
agent1:                 episode reward: -0.0023,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0802s / 1.5303 s
agent0:                 episode reward: 0.0403,                 loss: nan
agent1:                 episode reward: -0.0403,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2734s / 1.8037 s
agent0:                 episode reward: -0.0268,                 loss: nan
agent1:                 episode reward: 0.0268,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3429s / 2.1465 s
agent0:                 episode reward: -0.2879,                 loss: nan
agent1:                 episode reward: 0.2879,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1608s / 2.3073 s
agent0:                 episode reward: 0.6034,                 loss: nan
agent1:                 episode reward: -0.6034,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.4378s / 2.7451 s
agent0:                 episode reward: 0.0402,                 loss: nan
agent1:                 episode reward: -0.0402,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1665s / 2.9116 s
agent0:                 episode reward: -0.1931,                 loss: nan
agent1:                 episode reward: 0.1931,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0840s / 2.9956 s
agent0:                 episode reward: -0.1548,                 loss: nan
agent1:                 episode reward: 0.1548,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0793s / 3.0749 s
agent0:                 episode reward: 0.0755,                 loss: nan
agent1:                 episode reward: -0.0755,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 32.7350s / 35.8099 s
agent0:                 episode reward: 0.1891,                 loss: nan
agent1:                 episode reward: -0.1891,                 loss: 0.2633
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 101.3912s / 137.2011 s
agent0:                 episode reward: -0.1234,                 loss: nan
agent1:                 episode reward: 0.1234,                 loss: 0.2258
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 99.9232s / 237.1243 s
agent0:                 episode reward: 0.2509,                 loss: nan
agent1:                 episode reward: -0.2509,                 loss: 0.1905
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 98.1993s / 335.3236 s
agent0:                 episode reward: -0.2298,                 loss: nan
agent1:                 episode reward: 0.2298,                 loss: 0.1797
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 99.8120s / 435.1356 s
agent0:                 episode reward: -0.1772,                 loss: nan
agent1:                 episode reward: 0.1772,                 loss: 0.1756
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 100.5754s / 535.7110 s
agent0:                 episode reward: -0.1029,                 loss: nan
agent1:                 episode reward: 0.1029,                 loss: 0.1745
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 97.8732s / 633.5843 s
agent0:                 episode reward: 0.2432,                 loss: nan
agent1:                 episode reward: -0.2432,                 loss: 0.1711
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 100.9615s / 734.5457 s
agent0:                 episode reward: 0.2357,                 loss: nan
agent1:                 episode reward: -0.2357,                 loss: 0.1685
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 103.6364s / 838.1822 s
agent0:                 episode reward: -0.2723,                 loss: nan
agent1:                 episode reward: 0.2723,                 loss: 0.1663
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 99.6174s / 937.7996 s
agent0:                 episode reward: 0.1019,                 loss: nan
agent1:                 episode reward: -0.1019,                 loss: 0.1687
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 99.3439s / 1037.1434 s
agent0:                 episode reward: -0.4457,                 loss: nan
agent1:                 episode reward: 0.4457,                 loss: 0.1682
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 100.0481s / 1137.1915 s
agent0:                 episode reward: -0.0023,                 loss: nan
agent1:                 episode reward: 0.0023,                 loss: 0.1677
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 101.7134s / 1238.9049 s
agent0:                 episode reward: 0.1139,                 loss: nan
agent1:                 episode reward: -0.1139,                 loss: 0.1692
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 98.8079s / 1337.7127 s
agent0:                 episode reward: -0.2678,                 loss: nan
agent1:                 episode reward: 0.2678,                 loss: 0.1650
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 100.4384s / 1438.1512 s
agent0:                 episode reward: 0.0707,                 loss: nan
agent1:                 episode reward: -0.0707,                 loss: 0.1641
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 98.8230s / 1536.9742 s
agent0:                 episode reward: -0.0090,                 loss: nan
agent1:                 episode reward: 0.0090,                 loss: 0.1642
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 102.9633s / 1639.9374 s
agent0:                 episode reward: -0.3463,                 loss: nan
agent1:                 episode reward: 0.3463,                 loss: 0.1636
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 99.6860s / 1739.6234 s
agent0:                 episode reward: 0.4656,                 loss: nan
agent1:                 episode reward: -0.4656,                 loss: 0.1778
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 102.8332s / 1842.4566 s
agent0:                 episode reward: -0.0611,                 loss: nan
agent1:                 episode reward: 0.0611,                 loss: 0.1648
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 98.4545s / 1940.9111 s
agent0:                 episode reward: -0.3047,                 loss: nan
agent1:                 episode reward: 0.3047,                 loss: 0.1608
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 98.4809s / 2039.3920 s
agent0:                 episode reward: -0.3637,                 loss: nan
agent1:                 episode reward: 0.3637,                 loss: 0.1617
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 99.3158s / 2138.7078 s
agent0:                 episode reward: 0.0405,                 loss: nan
agent1:                 episode reward: -0.0405,                 loss: 0.1609
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 97.6552s / 2236.3631 s
agent0:                 episode reward: 0.3003,                 loss: nan
agent1:                 episode reward: -0.3003,                 loss: 0.1603
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 100.9149s / 2337.2779 s
agent0:                 episode reward: -0.0979,                 loss: nan
agent1:                 episode reward: 0.0979,                 loss: 0.1586
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 99.3500s / 2436.6279 s
agent0:                 episode reward: 0.2024,                 loss: nan
agent1:                 episode reward: -0.2024,                 loss: 0.1589
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 99.2194s / 2535.8474 s
agent0:                 episode reward: 0.0461,                 loss: nan
agent1:                 episode reward: -0.0461,                 loss: 0.1588
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 102.2639s / 2638.1113 s
agent0:                 episode reward: 0.0255,                 loss: nan
agent1:                 episode reward: -0.0255,                 loss: 0.1580
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 107.0498s / 2745.1611 s
agent0:                 episode reward: -0.3496,                 loss: nan
agent1:                 episode reward: 0.3496,                 loss: 0.1600
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 132.1200s / 2877.2811 s
agent0:                 episode reward: 0.0164,                 loss: nan
agent1:                 episode reward: -0.0164,                 loss: 0.1587
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.1643s / 3014.4454 s
agent0:                 episode reward: -0.0705,                 loss: nan
agent1:                 episode reward: 0.0705,                 loss: 0.1591
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.7636s / 3155.2090 s
agent0:                 episode reward: 0.4471,                 loss: nan
agent1:                 episode reward: -0.4471,                 loss: 0.1574
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 136.1283s / 3291.3373 s
agent0:                 episode reward: 0.2113,                 loss: nan
agent1:                 episode reward: -0.2113,                 loss: 0.1577
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.9270s / 3427.2643 s
agent0:                 episode reward: 0.1748,                 loss: nan
agent1:                 episode reward: -0.1748,                 loss: 0.1591
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 137.4231s / 3564.6874 s
agent0:                 episode reward: 0.6433,                 loss: nan
agent1:                 episode reward: -0.6433,                 loss: 0.1601
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 135.8231s / 3700.5105 s
agent0:                 episode reward: -0.0522,                 loss: nan
agent1:                 episode reward: 0.0522,                 loss: 0.1575
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.0739s / 3838.5844 s
agent0:                 episode reward: 0.2151,                 loss: nan
agent1:                 episode reward: -0.2151,                 loss: 0.1554
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 136.2312s / 3974.8156 s
agent0:                 episode reward: 0.0566,                 loss: nan
agent1:                 episode reward: -0.0566,                 loss: 0.1553
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.0916s / 4112.9072 s
agent0:                 episode reward: 0.1705,                 loss: nan
agent1:                 episode reward: -0.1705,                 loss: 0.1547
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.9753s / 4248.8826 s
agent0:                 episode reward: 0.3714,                 loss: nan
agent1:                 episode reward: -0.3714,                 loss: 0.1556
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 134.9638s / 4383.8464 s
agent0:                 episode reward: 0.0200,                 loss: nan
agent1:                 episode reward: -0.0200,                 loss: 0.1551
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.1799s / 4524.0263 s
agent0:                 episode reward: -0.1849,                 loss: nan
agent1:                 episode reward: 0.1849,                 loss: 0.1550
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 136.5223s / 4660.5485 s
agent0:                 episode reward: -0.3219,                 loss: nan
agent1:                 episode reward: 0.3219,                 loss: 0.1547
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.9011s / 4801.4496 s
agent0:                 episode reward: 0.0735,                 loss: nan
agent1:                 episode reward: -0.0735,                 loss: 0.1566
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.4467s / 4941.8963 s
agent0:                 episode reward: 0.2577,                 loss: nan
agent1:                 episode reward: -0.2577,                 loss: 0.1557
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 142.8400s / 5084.7364 s
agent0:                 episode reward: 0.2590,                 loss: nan
agent1:                 episode reward: -0.2590,                 loss: 0.1538
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 137.8344s / 5222.5708 s
agent0:                 episode reward: -0.3514,                 loss: nan
agent1:                 episode reward: 0.3514,                 loss: 0.1553
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.3605s / 5363.9313 s
agent0:                 episode reward: 0.0925,                 loss: nan
agent1:                 episode reward: -0.0925,                 loss: 0.1550
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.6210s / 5503.5523 s
agent0:                 episode reward: -0.0872,                 loss: nan
agent1:                 episode reward: 0.0872,                 loss: 0.1555
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.1853s / 5643.7377 s
agent0:                 episode reward: 0.3472,                 loss: nan
agent1:                 episode reward: -0.3472,                 loss: 0.1552
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 136.1376s / 5779.8753 s
agent0:                 episode reward: 0.0258,                 loss: nan
agent1:                 episode reward: -0.0258,                 loss: 0.1543
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.2087s / 5921.0840 s
agent0:                 episode reward: 0.5104,                 loss: nan
agent1:                 episode reward: -0.5104,                 loss: 0.1550
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.1914s / 6062.2753 s
agent0:                 episode reward: -0.1925,                 loss: nan
agent1:                 episode reward: 0.1925,                 loss: 0.1582
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.2215s / 6203.4969 s
agent0:                 episode reward: -0.3069,                 loss: nan
agent1:                 episode reward: 0.3069,                 loss: 0.1587
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.2150s / 6344.7118 s
agent0:                 episode reward: -0.0112,                 loss: nan
agent1:                 episode reward: 0.0112,                 loss: 0.1577
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 134.8406s / 6479.5524 s
agent0:                 episode reward: 0.2824,                 loss: nan
agent1:                 episode reward: -0.2824,                 loss: 0.1571
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.4638s / 6617.0163 s
agent0:                 episode reward: -0.3693,                 loss: nan
agent1:                 episode reward: 0.3693,                 loss: 0.1584
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 136.2967s / 6753.3130 s
agent0:                 episode reward: -0.0148,                 loss: nan
agent1:                 episode reward: 0.0148,                 loss: 0.1569
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 135.9106s / 6889.2236 s
agent0:                 episode reward: 0.0676,                 loss: nan
agent1:                 episode reward: -0.0676,                 loss: 0.1561
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.7665s / 7028.9901 s
agent0:                 episode reward: 0.3831,                 loss: nan
agent1:                 episode reward: -0.3831,                 loss: 0.1551
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.8583s / 7167.8484 s
agent0:                 episode reward: -0.0937,                 loss: nan
agent1:                 episode reward: 0.0937,                 loss: 0.1566
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.5423s / 7306.3907 s
agent0:                 episode reward: 0.5515,                 loss: nan
agent1:                 episode reward: -0.5515,                 loss: 0.1563
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.4207s / 7446.8114 s
agent0:                 episode reward: -0.0675,                 loss: nan
agent1:                 episode reward: 0.0675,                 loss: 0.1554
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.9848s / 7586.7963 s
agent0:                 episode reward: 0.1032,                 loss: nan
agent1:                 episode reward: -0.1032,                 loss: 0.1555
Episode: 1481/30000 (4.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 137.9227s / 7724.7190 s
agent0:                 episode reward: -0.1184,                 loss: nan
agent1:                 episode reward: 0.1184,                 loss: 0.1544
Episode: 1501/30000 (5.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.7916s / 7864.5106 s
agent0:                 episode reward: -0.2449,                 loss: nan
agent1:                 episode reward: 0.2449,                 loss: 0.1564
Episode: 1521/30000 (5.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.3128s / 8002.8234 s
agent0:                 episode reward: -0.0158,                 loss: nan
agent1:                 episode reward: 0.0158,                 loss: 0.1558
Episode: 1541/30000 (5.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.7066s / 8141.5300 s
agent0:                 episode reward: 0.1401,                 loss: nan
agent1:                 episode reward: -0.1401,                 loss: 0.1553
Episode: 1561/30000 (5.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.5208s / 8279.0509 s
agent0:                 episode reward: -0.4139,                 loss: nan
agent1:                 episode reward: 0.4139,                 loss: 0.1541
Episode: 1581/30000 (5.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.3755s / 8418.4263 s
agent0:                 episode reward: -0.0588,                 loss: nan
agent1:                 episode reward: 0.0588,                 loss: 0.1550
Episode: 1601/30000 (5.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.4209s / 8557.8472 s
agent0:                 episode reward: 0.2396,                 loss: nan
agent1:                 episode reward: -0.2396,                 loss: 0.1529
Episode: 1621/30000 (5.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.2754s / 8698.1226 s
agent0:                 episode reward: 0.3384,                 loss: nan
agent1:                 episode reward: -0.3384,                 loss: 0.1554
Episode: 1641/30000 (5.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.1241s / 8838.2467 s
agent0:                 episode reward: 0.0784,                 loss: nan
agent1:                 episode reward: -0.0784,                 loss: 0.1558
Episode: 1661/30000 (5.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.6978s / 8976.9445 s
agent0:                 episode reward: -0.3902,                 loss: nan
agent1:                 episode reward: 0.3902,                 loss: 0.1547
Episode: 1681/30000 (5.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.8373s / 9115.7818 s
agent0:                 episode reward: -0.0904,                 loss: nan
agent1:                 episode reward: 0.0904,                 loss: 0.1549
Episode: 1701/30000 (5.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.6970s / 9255.4789 s
agent0:                 episode reward: 0.1272,                 loss: nan
agent1:                 episode reward: -0.1272,                 loss: 0.1552
Episode: 1721/30000 (5.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.2494s / 9395.7283 s
agent0:                 episode reward: 0.5316,                 loss: nan
agent1:                 episode reward: -0.5316,                 loss: 0.1545
Episode: 1741/30000 (5.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.5246s / 9536.2529 s
agent0:                 episode reward: -0.1989,                 loss: nan
agent1:                 episode reward: 0.1989,                 loss: 0.1545
Episode: 1761/30000 (5.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.9415s / 9672.1944 s
agent0:                 episode reward: 0.2394,                 loss: nan
agent1:                 episode reward: -0.2394,                 loss: 0.1548
Episode: 1781/30000 (5.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.8307s / 9812.0250 s
agent0:                 episode reward: -0.2134,                 loss: nan
agent1:                 episode reward: 0.2134,                 loss: 0.1526
Episode: 1801/30000 (6.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.9565s / 9950.9815 s
agent0:                 episode reward: -0.0984,                 loss: nan
agent1:                 episode reward: 0.0984,                 loss: 0.1535
Episode: 1821/30000 (6.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.5987s / 10086.5802 s
agent0:                 episode reward: 0.3813,                 loss: nan
agent1:                 episode reward: -0.3813,                 loss: 0.1540
Episode: 1841/30000 (6.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.9523s / 10227.5325 s
agent0:                 episode reward: -0.3066,                 loss: nan
agent1:                 episode reward: 0.3066,                 loss: 0.1542
Episode: 1861/30000 (6.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.0073s / 10364.5398 s
agent0:                 episode reward: -0.0718,                 loss: nan
agent1:                 episode reward: 0.0718,                 loss: 0.1548
Episode: 1881/30000 (6.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.3929s / 10504.9328 s
agent0:                 episode reward: 0.1701,                 loss: nan
agent1:                 episode reward: -0.1701,                 loss: 0.1542
Episode: 1901/30000 (6.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.5801s / 10643.5128 s
agent0:                 episode reward: -0.1960,                 loss: nan
agent1:                 episode reward: 0.1960,                 loss: 0.1575
Episode: 1921/30000 (6.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 142.7147s / 10786.2275 s
agent0:                 episode reward: -0.1049,                 loss: nan
agent1:                 episode reward: 0.1049,                 loss: 0.1556
Episode: 1941/30000 (6.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.1971s / 10921.4246 s
agent0:                 episode reward: 0.0181,                 loss: nan
agent1:                 episode reward: -0.0181,                 loss: 0.1559
Episode: 1961/30000 (6.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.9869s / 11061.4115 s
agent0:                 episode reward: -0.2236,                 loss: nan
agent1:                 episode reward: 0.2236,                 loss: 0.1553
Episode: 1981/30000 (6.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.4067s / 11202.8182 s
agent0:                 episode reward: 0.0879,                 loss: nan
agent1:                 episode reward: -0.0879,                 loss: 0.1554
Episode: 2001/30000 (6.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.8600s / 11340.6782 s
agent0:                 episode reward: 0.2026,                 loss: nan
agent1:                 episode reward: -0.2026,                 loss: 0.1566
Episode: 2021/30000 (6.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 136.2734s / 11476.9516 s
agent0:                 episode reward: 0.0273,                 loss: nan
agent1:                 episode reward: -0.0273,                 loss: 0.1555
Episode: 2041/30000 (6.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.0386s / 11615.9902 s
agent0:                 episode reward: 0.0375,                 loss: nan
agent1:                 episode reward: -0.0375,                 loss: 0.1551
Episode: 2061/30000 (6.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.2825s / 11754.2727 s
agent0:                 episode reward: -0.2787,                 loss: nan
agent1:                 episode reward: 0.2787,                 loss: 0.1554
Episode: 2081/30000 (6.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.8666s / 11894.1394 s
agent0:                 episode reward: -0.1035,                 loss: nan
agent1:                 episode reward: 0.1035,                 loss: 0.1557
Episode: 2101/30000 (7.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.0211s / 12031.1605 s
agent0:                 episode reward: 0.1471,                 loss: nan
agent1:                 episode reward: -0.1471,                 loss: 0.1552
Episode: 2121/30000 (7.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.1912s / 12169.3517 s
agent0:                 episode reward: 0.4215,                 loss: nan
agent1:                 episode reward: -0.4215,                 loss: 0.1562
Episode: 2141/30000 (7.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 144.5664s / 12313.9181 s
agent0:                 episode reward: 0.1878,                 loss: nan
agent1:                 episode reward: -0.1878,                 loss: 0.1538
Episode: 2161/30000 (7.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 142.1062s / 12456.0243 s
agent0:                 episode reward: -0.2748,                 loss: nan
agent1:                 episode reward: 0.2748,                 loss: 0.1566
Episode: 2181/30000 (7.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.1807s / 12597.2051 s
agent0:                 episode reward: -0.0084,                 loss: nan
agent1:                 episode reward: 0.0084,                 loss: 0.1550
Episode: 2201/30000 (7.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 143.4865s / 12740.6915 s
agent0:                 episode reward: -0.1482,                 loss: nan
agent1:                 episode reward: 0.1482,                 loss: 0.1549
Episode: 2221/30000 (7.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 144.8800s / 12885.5716 s
agent0:                 episode reward: 0.0521,                 loss: nan
agent1:                 episode reward: -0.0521,                 loss: 0.1546
Episode: 2241/30000 (7.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.6468s / 13026.2183 s
agent0:                 episode reward: -0.1179,                 loss: nan
agent1:                 episode reward: 0.1179,                 loss: 0.1552
Episode: 2261/30000 (7.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 144.7570s / 13170.9753 s
agent0:                 episode reward: 0.1989,                 loss: nan
agent1:                 episode reward: -0.1989,                 loss: 0.1565
Episode: 2281/30000 (7.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.3191s / 13310.2944 s
agent0:                 episode reward: 0.1515,                 loss: nan
agent1:                 episode reward: -0.1515,                 loss: 0.1566
Episode: 2301/30000 (7.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.7910s / 13451.0854 s
agent0:                 episode reward: -0.3844,                 loss: nan
agent1:                 episode reward: 0.3844,                 loss: 0.1558
Episode: 2321/30000 (7.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 143.6087s / 13594.6941 s
agent0:                 episode reward: -0.1693,                 loss: nan
agent1:                 episode reward: 0.1693,                 loss: 0.1546
Episode: 2341/30000 (7.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 142.2350s / 13736.9291 s
agent0:                 episode reward: 0.0628,                 loss: nan
agent1:                 episode reward: -0.0628,                 loss: 0.1548
Episode: 2361/30000 (7.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.8072s / 13877.7364 s
agent0:                 episode reward: 0.1088,                 loss: nan
agent1:                 episode reward: -0.1088,                 loss: 0.1565
Episode: 2381/30000 (7.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.6253s / 14019.3616 s
agent0:                 episode reward: -0.1725,                 loss: nan
agent1:                 episode reward: 0.1725,                 loss: 0.1561
Episode: 2401/30000 (8.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 143.5895s / 14162.9511 s
agent0:                 episode reward: -0.0365,                 loss: nan
agent1:                 episode reward: 0.0365,                 loss: 0.1536
Episode: 2421/30000 (8.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.3600s / 14302.3110 s
agent0:                 episode reward: -0.2536,                 loss: nan
agent1:                 episode reward: 0.2536,                 loss: 0.1540
Episode: 2441/30000 (8.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.2825s / 14441.5936 s
agent0:                 episode reward: 0.2520,                 loss: nan
agent1:                 episode reward: -0.2520,                 loss: 0.1554
Episode: 2461/30000 (8.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 142.5353s / 14584.1289 s
agent0:                 episode reward: -0.1002,                 loss: nan
agent1:                 episode reward: 0.1002,                 loss: 0.1548
Episode: 2481/30000 (8.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.8744s / 14725.0033 s
agent0:                 episode reward: 0.1881,                 loss: nan
agent1:                 episode reward: -0.1881,                 loss: 0.1557
Episode: 2501/30000 (8.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.8738s / 14866.8772 s
agent0:                 episode reward: -0.1822,                 loss: nan
agent1:                 episode reward: 0.1822,                 loss: 0.1565
Episode: 2521/30000 (8.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 142.8436s / 15009.7208 s
agent0:                 episode reward: -0.1718,                 loss: nan
agent1:                 episode reward: 0.1718,                 loss: 0.1554
Episode: 2541/30000 (8.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.9270s / 15150.6477 s
agent0:                 episode reward: -0.1398,                 loss: nan
agent1:                 episode reward: 0.1398,                 loss: 0.1569
Episode: 2561/30000 (8.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.5589s / 15291.2067 s
agent0:                 episode reward: -0.3986,                 loss: nan
agent1:                 episode reward: 0.3986,                 loss: 0.1530
Episode: 2581/30000 (8.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.7033s / 15431.9100 s
agent0:                 episode reward: -0.0739,                 loss: nan
agent1:                 episode reward: 0.0739,                 loss: 0.1537
Episode: 2601/30000 (8.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.6905s / 15573.6005 s
agent0:                 episode reward: -0.0488,                 loss: nan
agent1:                 episode reward: 0.0488,                 loss: 0.1545
Episode: 2621/30000 (8.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 145.3699s / 15718.9704 s
agent0:                 episode reward: 0.4547,                 loss: nan
agent1:                 episode reward: -0.4547,                 loss: 0.1524
Episode: 2641/30000 (8.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.0470s / 15860.0174 s
agent0:                 episode reward: -0.1557,                 loss: nan
agent1:                 episode reward: 0.1557,                 loss: 0.1515
Episode: 2661/30000 (8.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 142.5550s / 16002.5724 s
agent0:                 episode reward: -0.1095,                 loss: nan
agent1:                 episode reward: 0.1095,                 loss: 0.1530
Episode: 2681/30000 (8.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 143.5780s / 16146.1505 s
agent0:                 episode reward: -0.2231,                 loss: nan
agent1:                 episode reward: 0.2231,                 loss: 0.1510
Episode: 2701/30000 (9.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 142.1661s / 16288.3165 s
agent0:                 episode reward: 0.2604,                 loss: nan
agent1:                 episode reward: -0.2604,                 loss: 0.1537
Episode: 2721/30000 (9.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 143.2542s / 16431.5707 s
agent0:                 episode reward: 0.0966,                 loss: nan
agent1:                 episode reward: -0.0966,                 loss: 0.1506
Episode: 2741/30000 (9.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 143.2951s / 16574.8659 s
agent0:                 episode reward: 0.2478,                 loss: nan
agent1:                 episode reward: -0.2478,                 loss: 0.1542
Episode: 2761/30000 (9.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.0984s / 16711.9642 s
agent0:                 episode reward: -0.0864,                 loss: nan
agent1:                 episode reward: 0.0864,                 loss: 0.1528
Episode: 2781/30000 (9.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 143.6383s / 16855.6025 s
agent0:                 episode reward: -0.5659,                 loss: nan
agent1:                 episode reward: 0.5659,                 loss: 0.1528
Episode: 2801/30000 (9.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.3476s / 16993.9501 s
agent0:                 episode reward: 0.0114,                 loss: nan
agent1:                 episode reward: -0.0114,                 loss: 0.1519
Episode: 2821/30000 (9.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.6321s / 17133.5822 s
agent0:                 episode reward: 0.0581,                 loss: nan
agent1:                 episode reward: -0.0581,                 loss: 0.1528
Episode: 2841/30000 (9.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.3004s / 17272.8826 s
agent0:                 episode reward: -0.5344,                 loss: nan
agent1:                 episode reward: 0.5344,                 loss: 0.1517
Episode: 2861/30000 (9.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.2090s / 17413.0916 s
agent0:                 episode reward: -0.2071,                 loss: nan
agent1:                 episode reward: 0.2071,                 loss: 0.1531
Episode: 2881/30000 (9.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 142.5455s / 17555.6372 s
agent0:                 episode reward: 0.1977,                 loss: nan
agent1:                 episode reward: -0.1977,                 loss: 0.1538
Episode: 2901/30000 (9.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.6764s / 17694.3136 s
agent0:                 episode reward: -0.2791,                 loss: nan
agent1:                 episode reward: 0.2791,                 loss: 0.1542
Episode: 2921/30000 (9.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 142.0932s / 17836.4068 s
agent0:                 episode reward: -0.1453,                 loss: nan
agent1:                 episode reward: 0.1453,                 loss: 0.1526
Episode: 2941/30000 (9.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.4097s / 17975.8165 s
agent0:                 episode reward: 0.4605,                 loss: nan
agent1:                 episode reward: -0.4605,                 loss: 0.1540
Episode: 2961/30000 (9.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.8637s / 18114.6802 s
agent0:                 episode reward: 0.3426,                 loss: nan
agent1:                 episode reward: -0.3426,                 loss: 0.1530
Episode: 2981/30000 (9.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.2822s / 18254.9624 s
agent0:                 episode reward: -0.3548,                 loss: nan
agent1:                 episode reward: 0.3548,                 loss: 0.1532
Episode: 3001/30000 (10.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 142.2006s / 18397.1629 s
agent0:                 episode reward: 0.3596,                 loss: nan
agent1:                 episode reward: -0.3596,                 loss: 0.1544
Episode: 3021/30000 (10.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.0707s / 18537.2337 s
agent0:                 episode reward: 0.3981,                 loss: nan
agent1:                 episode reward: -0.3981,                 loss: 0.1519
Episode: 3041/30000 (10.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.9886s / 18676.2223 s
agent0:                 episode reward: 0.1323,                 loss: nan
agent1:                 episode reward: -0.1323,                 loss: 0.1543
Episode: 3061/30000 (10.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 147.2140s / 18823.4363 s
agent0:                 episode reward: -0.0940,                 loss: nan
agent1:                 episode reward: 0.0940,                 loss: 0.1529
Episode: 3081/30000 (10.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.2581s / 18964.6944 s
agent0:                 episode reward: -0.0283,                 loss: nan
agent1:                 episode reward: 0.0283,                 loss: 0.1529
Episode: 3101/30000 (10.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.3487s / 19104.0430 s
agent0:                 episode reward: -0.1975,                 loss: nan
agent1:                 episode reward: 0.1975,                 loss: 0.1547