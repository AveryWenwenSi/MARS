pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
random seed: 91
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7fa367a58550>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.048 0.733 0.    0.    0.219 0.   ]
 [0.179 0.    0.    0.687 0.    0.133]]
Load checkpoints (policy family):  [['83' '5753' '6419' '9691' '12712' '16446']
 ['121' '6342' '6627' '9768' '12785' '16467']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220116204408/epi_20000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220116204408_exploit_20000/mdp_arbitrary_mdp_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220116204408_exploit_20000/mdp_arbitrary_mdp_nxdo2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2844s / 1.2844 s
agent0:                 episode reward: 1.2564,                 loss: nan
agent1:                 episode reward: -1.2564,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1995s / 1.4839 s
agent0:                 episode reward: 0.5473,                 loss: nan
agent1:                 episode reward: -0.5473,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1595s / 1.6434 s
agent0:                 episode reward: -0.2140,                 loss: nan
agent1:                 episode reward: 0.2140,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3573s / 2.0007 s
agent0:                 episode reward: 0.2408,                 loss: nan
agent1:                 episode reward: -0.2408,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3627s / 2.3634 s
agent0:                 episode reward: -0.2240,                 loss: nan
agent1:                 episode reward: 0.2240,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.4069s / 2.7703 s
agent0:                 episode reward: 0.1530,                 loss: nan
agent1:                 episode reward: -0.1530,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3960s / 3.1663 s
agent0:                 episode reward: 0.3199,                 loss: nan
agent1:                 episode reward: -0.3199,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.4177s / 3.5841 s
agent0:                 episode reward: 0.0559,                 loss: nan
agent1:                 episode reward: -0.0559,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.4208s / 4.0049 s
agent0:                 episode reward: 0.5743,                 loss: nan
agent1:                 episode reward: -0.5743,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3420s / 4.3469 s
agent0:                 episode reward: 0.0286,                 loss: nan
agent1:                 episode reward: -0.0286,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.5658s / 4.9127 s
agent0:                 episode reward: -0.1794,                 loss: nan
agent1:                 episode reward: 0.1794,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 38.6921s / 43.6048 s
agent0:                 episode reward: 0.5047,                 loss: nan
agent1:                 episode reward: -0.5047,                 loss: 0.1814
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 102.5629s / 146.1678 s
agent0:                 episode reward: 0.3272,                 loss: nan
agent1:                 episode reward: -0.3272,                 loss: 0.1694
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 99.7547s / 245.9225 s
agent0:                 episode reward: 0.1490,                 loss: nan
agent1:                 episode reward: -0.1490,                 loss: 0.1615
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 101.0168s / 346.9393 s
agent0:                 episode reward: 0.2807,                 loss: nan
agent1:                 episode reward: -0.2807,                 loss: 0.1537
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 100.4120s / 447.3513 s
agent0:                 episode reward: -0.4284,                 loss: nan
agent1:                 episode reward: 0.4284,                 loss: 0.1507
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 97.2180s / 544.5693 s
agent0:                 episode reward: 0.0005,                 loss: nan
agent1:                 episode reward: -0.0005,                 loss: 0.1472
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 100.4401s / 645.0094 s
agent0:                 episode reward: -0.1891,                 loss: nan
agent1:                 episode reward: 0.1891,                 loss: 0.1441
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 98.3600s / 743.3694 s
agent0:                 episode reward: 0.0823,                 loss: nan
agent1:                 episode reward: -0.0823,                 loss: 0.1443
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 94.9492s / 838.3185 s
agent0:                 episode reward: -0.0703,                 loss: nan
agent1:                 episode reward: 0.0703,                 loss: 0.1443
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 97.5814s / 935.8999 s
agent0:                 episode reward: 0.0710,                 loss: nan
agent1:                 episode reward: -0.0710,                 loss: 0.1412
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 100.5324s / 1036.4323 s
agent0:                 episode reward: -0.1867,                 loss: nan
agent1:                 episode reward: 0.1867,                 loss: 0.1421
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 104.1415s / 1140.5738 s
agent0:                 episode reward: 0.5205,                 loss: nan
agent1:                 episode reward: -0.5205,                 loss: 0.1396
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 106.7833s / 1247.3570 s
agent0:                 episode reward: 0.0260,                 loss: nan
agent1:                 episode reward: -0.0260,                 loss: 0.1403
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 111.4667s / 1358.8237 s
agent0:                 episode reward: 0.0968,                 loss: nan
agent1:                 episode reward: -0.0968,                 loss: 0.1393
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 109.3317s / 1468.1554 s
agent0:                 episode reward: 0.3578,                 loss: nan
agent1:                 episode reward: -0.3578,                 loss: 0.1387
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 106.4559s / 1574.6114 s
agent0:                 episode reward: 0.2196,                 loss: nan
agent1:                 episode reward: -0.2196,                 loss: 0.1374
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 106.0004s / 1680.6117 s
agent0:                 episode reward: 0.4737,                 loss: nan
agent1:                 episode reward: -0.4737,                 loss: 0.1350
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 107.0155s / 1787.6272 s
agent0:                 episode reward: 0.7448,                 loss: nan
agent1:                 episode reward: -0.7448,                 loss: 0.1410
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 108.9555s / 1896.5827 s
agent0:                 episode reward: 0.3864,                 loss: nan
agent1:                 episode reward: -0.3864,                 loss: 0.1413
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 106.5813s / 2003.1640 s
agent0:                 episode reward: 0.2169,                 loss: nan
agent1:                 episode reward: -0.2169,                 loss: 0.1409
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 109.8144s / 2112.9785 s
agent0:                 episode reward: -0.1721,                 loss: nan
agent1:                 episode reward: 0.1721,                 loss: 0.1386
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 110.1358s / 2223.1143 s
agent0:                 episode reward: 0.0146,                 loss: nan
agent1:                 episode reward: -0.0146,                 loss: 0.1383
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 108.1625s / 2331.2768 s
agent0:                 episode reward: -0.0176,                 loss: nan
agent1:                 episode reward: 0.0176,                 loss: 0.1368
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 108.0395s / 2439.3163 s
agent0:                 episode reward: 0.1908,                 loss: nan
agent1:                 episode reward: -0.1908,                 loss: 0.1360
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 108.0515s / 2547.3677 s
agent0:                 episode reward: 0.3719,                 loss: nan
agent1:                 episode reward: -0.3719,                 loss: 0.1357
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 109.0526s / 2656.4203 s
agent0:                 episode reward: 0.1578,                 loss: nan
agent1:                 episode reward: -0.1578,                 loss: 0.1338
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 146.9581s / 2803.3785 s
agent0:                 episode reward: 0.1725,                 loss: nan
agent1:                 episode reward: -0.1725,                 loss: 0.1357
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 241.9980s / 3045.3765 s
agent0:                 episode reward: -0.4192,                 loss: nan
agent1:                 episode reward: 0.4192,                 loss: 0.1336
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 243.2856s / 3288.6621 s
agent0:                 episode reward: 0.1460,                 loss: nan
agent1:                 episode reward: -0.1460,                 loss: 0.1327
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 238.6525s / 3527.3145 s
agent0:                 episode reward: 0.4593,                 loss: nan
agent1:                 episode reward: -0.4593,                 loss: 0.1308
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 228.6829s / 3755.9974 s
agent0:                 episode reward: 0.0600,                 loss: nan
agent1:                 episode reward: -0.0600,                 loss: 0.1320
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.2873s / 4002.2847 s
agent0:                 episode reward: 0.0404,                 loss: nan
agent1:                 episode reward: -0.0404,                 loss: 0.1318
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 236.4143s / 4238.6990 s
agent0:                 episode reward: 0.2938,                 loss: nan
agent1:                 episode reward: -0.2938,                 loss: 0.1297
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 240.6152s / 4479.3142 s
agent0:                 episode reward: 0.2405,                 loss: nan
agent1:                 episode reward: -0.2405,                 loss: 0.1297
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 233.4805s / 4712.7947 s
agent0:                 episode reward: 0.1508,                 loss: nan
agent1:                 episode reward: -0.1508,                 loss: 0.1278
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 243.3726s / 4956.1673 s
agent0:                 episode reward: 0.5130,                 loss: nan
agent1:                 episode reward: -0.5130,                 loss: 0.1252
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.3035s / 5199.4707 s
agent0:                 episode reward: 0.3092,                 loss: nan
agent1:                 episode reward: -0.3092,                 loss: 0.1264
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.1992s / 5441.6699 s
agent0:                 episode reward: -0.2961,                 loss: nan
agent1:                 episode reward: 0.2961,                 loss: 0.1264
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.6862s / 5683.3561 s
agent0:                 episode reward: 0.1228,                 loss: nan
agent1:                 episode reward: -0.1228,                 loss: 0.1254
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 237.6353s / 5920.9914 s
agent0:                 episode reward: 0.0044,                 loss: nan
agent1:                 episode reward: -0.0044,                 loss: 0.1249
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 236.5540s / 6157.5455 s
agent0:                 episode reward: -0.0374,                 loss: nan
agent1:                 episode reward: 0.0374,                 loss: 0.1250
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.6460s / 6404.1915 s
agent0:                 episode reward: 0.0150,                 loss: nan
agent1:                 episode reward: -0.0150,                 loss: 0.1254
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.7815s / 6650.9730 s
agent0:                 episode reward: 0.1955,                 loss: nan
agent1:                 episode reward: -0.1955,                 loss: 0.1247
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 253.9348s / 6904.9078 s
agent0:                 episode reward: 0.1104,                 loss: nan
agent1:                 episode reward: -0.1104,                 loss: 0.1257
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 229.8490s / 7134.7568 s
agent0:                 episode reward: -0.0291,                 loss: nan
agent1:                 episode reward: 0.0291,                 loss: 0.1255
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 237.4891s / 7372.2458 s
agent0:                 episode reward: 0.3935,                 loss: nan
agent1:                 episode reward: -0.3935,                 loss: 0.1251
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.2060s / 7612.4519 s
agent0:                 episode reward: 0.4549,                 loss: nan
agent1:                 episode reward: -0.4549,                 loss: 0.1235
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.6397s / 7855.0916 s
agent0:                 episode reward: 0.0740,                 loss: nan
agent1:                 episode reward: -0.0740,                 loss: 0.1239
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 239.6206s / 8094.7121 s
agent0:                 episode reward: -0.0461,                 loss: nan
agent1:                 episode reward: 0.0461,                 loss: 0.1222
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 245.2404s / 8339.9525 s
agent0:                 episode reward: -0.0481,                 loss: nan
agent1:                 episode reward: 0.0481,                 loss: 0.1237
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 238.1341s / 8578.0866 s
agent0:                 episode reward: -0.1465,                 loss: nan
agent1:                 episode reward: 0.1465,                 loss: 0.1240
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.0593s / 8821.1459 s
agent0:                 episode reward: 0.1606,                 loss: nan
agent1:                 episode reward: -0.1606,                 loss: 0.1258
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.1930s / 9061.3389 s
agent0:                 episode reward: 0.0001,                 loss: nan
agent1:                 episode reward: -0.0001,                 loss: 0.1259
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.7222s / 9304.0611 s
agent0:                 episode reward: 0.6683,                 loss: nan
agent1:                 episode reward: -0.6683,                 loss: 0.1256
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.0168s / 9548.0779 s
agent0:                 episode reward: 0.4949,                 loss: nan
agent1:                 episode reward: -0.4949,                 loss: 0.1243
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.8048s / 9790.8828 s
agent0:                 episode reward: 0.0050,                 loss: nan
agent1:                 episode reward: -0.0050,                 loss: 0.1257
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 238.6365s / 10029.5192 s
agent0:                 episode reward: -0.1813,                 loss: nan
agent1:                 episode reward: 0.1813,                 loss: 0.1246
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 236.0161s / 10265.5354 s
agent0:                 episode reward: 0.1711,                 loss: nan
agent1:                 episode reward: -0.1711,                 loss: 0.1250
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 234.1626s / 10499.6979 s
agent0:                 episode reward: 0.3281,                 loss: nan
agent1:                 episode reward: -0.3281,                 loss: 0.1244
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.1849s / 10743.8828 s
agent0:                 episode reward: 0.3630,                 loss: nan
agent1:                 episode reward: -0.3630,                 loss: 0.1245
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.9497s / 10988.8325 s
agent0:                 episode reward: 0.0984,                 loss: nan
agent1:                 episode reward: -0.0984,                 loss: 0.1252
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.5783s / 11230.4108 s
agent0:                 episode reward: 0.4013,                 loss: nan
agent1:                 episode reward: -0.4013,                 loss: 0.1252
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 248.3095s / 11478.7203 s
agent0:                 episode reward: -0.0656,                 loss: nan
agent1:                 episode reward: 0.0656,                 loss: 0.1241
Episode: 1481/30000 (4.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 236.5322s / 11715.2525 s
agent0:                 episode reward: 0.2501,                 loss: nan
agent1:                 episode reward: -0.2501,                 loss: 0.1264
Episode: 1501/30000 (5.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 243.6357s / 11958.8882 s
agent0:                 episode reward: 0.2516,                 loss: nan
agent1:                 episode reward: -0.2516,                 loss: 0.1252
Episode: 1521/30000 (5.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 243.8942s / 12202.7824 s
agent0:                 episode reward: -0.2305,                 loss: nan
agent1:                 episode reward: 0.2305,                 loss: 0.1265
Episode: 1541/30000 (5.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.7801s / 12449.5625 s
agent0:                 episode reward: -0.1381,                 loss: nan
agent1:                 episode reward: 0.1381,                 loss: 0.1255
Episode: 1561/30000 (5.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.1805s / 12697.7430 s
agent0:                 episode reward: 0.1615,                 loss: nan
agent1:                 episode reward: -0.1615,                 loss: 0.1258
Episode: 1581/30000 (5.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 249.7333s / 12947.4763 s
agent0:                 episode reward: -0.2072,                 loss: nan
agent1:                 episode reward: 0.2072,                 loss: 0.1237
Episode: 1601/30000 (5.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 241.0706s / 13188.5469 s
agent0:                 episode reward: 0.3343,                 loss: nan
agent1:                 episode reward: -0.3343,                 loss: 0.1245
Episode: 1621/30000 (5.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.7493s / 13436.2962 s
agent0:                 episode reward: -0.3295,                 loss: nan
agent1:                 episode reward: 0.3295,                 loss: 0.1236
Episode: 1641/30000 (5.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.9192s / 13681.2154 s
agent0:                 episode reward: -0.1938,                 loss: nan
agent1:                 episode reward: 0.1938,                 loss: 0.1234
Episode: 1661/30000 (5.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.6978s / 13924.9132 s
agent0:                 episode reward: 0.1126,                 loss: nan
agent1:                 episode reward: -0.1126,                 loss: 0.1238
Episode: 1681/30000 (5.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.4910s / 14171.4042 s
agent0:                 episode reward: -0.1636,                 loss: nan
agent1:                 episode reward: 0.1636,                 loss: 0.1236
Episode: 1701/30000 (5.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.1731s / 14417.5773 s
agent0:                 episode reward: -0.0483,                 loss: nan
agent1:                 episode reward: 0.0483,                 loss: 0.1245
Episode: 1721/30000 (5.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.1498s / 14663.7271 s
agent0:                 episode reward: -0.3472,                 loss: nan
agent1:                 episode reward: 0.3472,                 loss: 0.1229
Episode: 1741/30000 (5.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.2586s / 14903.9857 s
agent0:                 episode reward: -0.0424,                 loss: nan
agent1:                 episode reward: 0.0424,                 loss: 0.1236
Episode: 1761/30000 (5.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.8611s / 15150.8468 s
agent0:                 episode reward: -0.3895,                 loss: nan
agent1:                 episode reward: 0.3895,                 loss: 0.1246
Episode: 1781/30000 (5.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.4593s / 15397.3060 s
agent0:                 episode reward: -0.0027,                 loss: nan
agent1:                 episode reward: 0.0027,                 loss: 0.1238
Episode: 1801/30000 (6.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 239.6644s / 15636.9704 s
agent0:                 episode reward: 0.4410,                 loss: nan
agent1:                 episode reward: -0.4410,                 loss: 0.1236
Episode: 1821/30000 (6.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 251.0348s / 15888.0053 s
agent0:                 episode reward: 0.3375,                 loss: nan
agent1:                 episode reward: -0.3375,                 loss: 0.1229
Episode: 1841/30000 (6.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.0802s / 16134.0854 s
agent0:                 episode reward: 0.3652,                 loss: nan
agent1:                 episode reward: -0.3652,                 loss: 0.1236
Episode: 1861/30000 (6.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.0528s / 16382.1382 s
agent0:                 episode reward: 0.0718,                 loss: nan
agent1:                 episode reward: -0.0718,                 loss: 0.1218
Episode: 1881/30000 (6.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.4367s / 16628.5749 s
agent0:                 episode reward: -0.7560,                 loss: nan
agent1:                 episode reward: 0.7560,                 loss: 0.1226
Episode: 1901/30000 (6.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 253.5605s / 16882.1354 s
agent0:                 episode reward: -0.0513,                 loss: nan
agent1:                 episode reward: 0.0513,                 loss: 0.1244
Episode: 1921/30000 (6.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.4573s / 17129.5926 s
agent0:                 episode reward: -0.0409,                 loss: nan
agent1:                 episode reward: 0.0409,                 loss: 0.1212
Episode: 1941/30000 (6.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.5666s / 17376.1593 s
agent0:                 episode reward: -0.1164,                 loss: nan
agent1:                 episode reward: 0.1164,                 loss: 0.1225
Episode: 1961/30000 (6.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.3229s / 17622.4822 s
agent0:                 episode reward: 0.0872,                 loss: nan
agent1:                 episode reward: -0.0872,                 loss: 0.1232
Episode: 1981/30000 (6.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 239.8084s / 17862.2905 s
agent0:                 episode reward: 0.2029,                 loss: nan
agent1:                 episode reward: -0.2029,                 loss: 0.1223
Episode: 2001/30000 (6.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.6886s / 18108.9791 s
agent0:                 episode reward: -0.4153,                 loss: nan
agent1:                 episode reward: 0.4153,                 loss: 0.1235
Episode: 2021/30000 (6.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 238.7757s / 18347.7548 s