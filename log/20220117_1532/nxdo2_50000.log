pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
random seed: 91
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7fad47b999d0>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [array([0.241, 0.082, 0.128, ..., 0.015, 0.185, 0.061]) array([0.059, 0.   , 0.135, ..., 0.   , 0.   , 0.011])]
Load checkpoints (policy family):  [list(['83', '5753', '6419', '9691', '12712', '16446', '20191', '20772', '24729', '28587', '35631', '38431', '38946', '39041', '39808', '40118', '40412', '41100', '41478', '41778', '41983', '42076', '42433', '42768', '43151', '43229', '43949', '44117', '44258', '44846', '45166', '45962', '46596', '46667', '47327', '47599', '47724', '48094', '48455', '48649', '48828', '49089', '49310', '49568'])
 list(['121', '6342', '6627', '9768', '12785', '16467', '20231', '20802', '24751', '28619', '35652', '38452', '38973', '39078', '39831', '40164', '40433', '41156', '41501', '41819', '42011', '42097', '42458', '42797', '43176', '43250', '44010', '44146', '44297', '44888', '45313', '45997', '46620', '46694', '47431', '47654', '47771', '48131', '48485', '48670', '48949', '49156', '49349'])]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220116204408/epi_50000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220116204408_exploit_50000/mdp_arbitrary_mdp_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220116204408_exploit_50000/mdp_arbitrary_mdp_nxdo2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4024s / 1.4024 s
agent0:                 episode reward: -1.1778,                 loss: nan
agent1:                 episode reward: 1.1778,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3351s / 1.7375 s
agent0:                 episode reward: -0.3436,                 loss: nan
agent1:                 episode reward: 0.3436,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2593s / 1.9968 s
agent0:                 episode reward: -0.0865,                 loss: nan
agent1:                 episode reward: 0.0865,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.5830s / 2.5798 s
agent0:                 episode reward: 0.0463,                 loss: nan
agent1:                 episode reward: -0.0463,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.6286s / 3.2084 s
agent0:                 episode reward: 0.4358,                 loss: nan
agent1:                 episode reward: -0.4358,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.5781s / 3.7865 s
agent0:                 episode reward: 0.0265,                 loss: nan
agent1:                 episode reward: -0.0265,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.5671s / 4.3535 s
agent0:                 episode reward: -0.3469,                 loss: nan
agent1:                 episode reward: 0.3469,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.4948s / 4.8483 s
agent0:                 episode reward: 0.2239,                 loss: nan
agent1:                 episode reward: -0.2239,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1210s / 5.9693 s
agent0:                 episode reward: -0.0502,                 loss: nan
agent1:                 episode reward: 0.0502,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1916s / 7.1609 s
agent0:                 episode reward: 0.1800,                 loss: nan
agent1:                 episode reward: -0.1800,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5921s / 9.7531 s
agent0:                 episode reward: -0.0115,                 loss: nan
agent1:                 episode reward: 0.0115,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 39.5858s / 49.3389 s
agent0:                 episode reward: -0.1703,                 loss: nan
agent1:                 episode reward: 0.1703,                 loss: 0.2371
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 97.3310s / 146.6699 s
agent0:                 episode reward: -0.1343,                 loss: nan
agent1:                 episode reward: 0.1343,                 loss: 0.2064
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 100.1885s / 246.8584 s
agent0:                 episode reward: -0.2983,                 loss: nan
agent1:                 episode reward: 0.2983,                 loss: 0.1665
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 101.4226s / 348.2810 s
agent0:                 episode reward: 0.0313,                 loss: nan
agent1:                 episode reward: -0.0313,                 loss: 0.1576
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 101.7278s / 450.0089 s
agent0:                 episode reward: 0.1651,                 loss: nan
agent1:                 episode reward: -0.1651,                 loss: 0.1561
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 99.2116s / 549.2205 s
agent0:                 episode reward: -0.0397,                 loss: nan
agent1:                 episode reward: 0.0397,                 loss: 0.1544
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 101.1972s / 650.4177 s
agent0:                 episode reward: -0.1015,                 loss: nan
agent1:                 episode reward: 0.1015,                 loss: 0.1526
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 98.7647s / 749.1824 s
agent0:                 episode reward: 0.2505,                 loss: nan
agent1:                 episode reward: -0.2505,                 loss: 0.1516
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 92.3106s / 841.4929 s
agent0:                 episode reward: 0.1335,                 loss: nan
agent1:                 episode reward: -0.1335,                 loss: 0.1500
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 99.1707s / 940.6636 s
agent0:                 episode reward: 0.0477,                 loss: nan
agent1:                 episode reward: -0.0477,                 loss: 0.1512
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 103.2721s / 1043.9357 s
agent0:                 episode reward: 0.0740,                 loss: nan
agent1:                 episode reward: -0.0740,                 loss: 0.1508
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 101.7447s / 1145.6804 s
agent0:                 episode reward: -0.0588,                 loss: nan
agent1:                 episode reward: 0.0588,                 loss: 0.1496
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 107.1487s / 1252.8291 s
agent0:                 episode reward: 0.2256,                 loss: nan
agent1:                 episode reward: -0.2256,                 loss: 0.1502
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 112.7069s / 1365.5360 s
agent0:                 episode reward: -0.0336,                 loss: nan
agent1:                 episode reward: 0.0336,                 loss: 0.1498
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 110.1338s / 1475.6698 s
agent0:                 episode reward: 0.3375,                 loss: nan
agent1:                 episode reward: -0.3375,                 loss: 0.1494
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 108.0043s / 1583.6741 s
agent0:                 episode reward: -0.1477,                 loss: nan
agent1:                 episode reward: 0.1477,                 loss: 0.1487
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 106.4389s / 1690.1129 s
agent0:                 episode reward: 0.1565,                 loss: nan
agent1:                 episode reward: -0.1565,                 loss: 0.1488
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 107.3535s / 1797.4665 s
agent0:                 episode reward: 0.0716,                 loss: nan
agent1:                 episode reward: -0.0716,                 loss: 0.1708
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 110.7413s / 1908.2078 s
agent0:                 episode reward: -0.3726,                 loss: nan
agent1:                 episode reward: 0.3726,                 loss: 0.1644
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 108.3504s / 2016.5582 s
agent0:                 episode reward: 0.2499,                 loss: nan
agent1:                 episode reward: -0.2499,                 loss: 0.1625
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 108.5462s / 2125.1044 s
agent0:                 episode reward: 0.1083,                 loss: nan
agent1:                 episode reward: -0.1083,                 loss: 0.1599
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 107.4276s / 2232.5320 s
agent0:                 episode reward: 0.0251,                 loss: nan
agent1:                 episode reward: -0.0251,                 loss: 0.1596
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 108.0672s / 2340.5992 s
agent0:                 episode reward: -0.3295,                 loss: nan
agent1:                 episode reward: 0.3295,                 loss: 0.1596
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 109.4122s / 2450.0114 s
agent0:                 episode reward: -0.3419,                 loss: nan
agent1:                 episode reward: 0.3419,                 loss: 0.1593
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 108.1691s / 2558.1805 s
agent0:                 episode reward: -0.0095,                 loss: nan
agent1:                 episode reward: 0.0095,                 loss: 0.1615
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 109.9478s / 2668.1283 s
agent0:                 episode reward: -0.0949,                 loss: nan
agent1:                 episode reward: 0.0949,                 loss: 0.1593
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 162.7601s / 2830.8884 s
agent0:                 episode reward: -0.5710,                 loss: nan
agent1:                 episode reward: 0.5710,                 loss: 0.1612
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 236.4185s / 3067.3069 s
agent0:                 episode reward: 0.3184,                 loss: nan
agent1:                 episode reward: -0.3184,                 loss: 0.1596
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 235.4596s / 3302.7666 s
agent0:                 episode reward: 0.2327,                 loss: nan
agent1:                 episode reward: -0.2327,                 loss: 0.1601
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 238.3849s / 3541.1515 s
agent0:                 episode reward: 0.1113,                 loss: nan
agent1:                 episode reward: -0.1113,                 loss: 0.1620
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 239.7796s / 3780.9311 s
agent0:                 episode reward: 0.0002,                 loss: nan
agent1:                 episode reward: -0.0002,                 loss: 0.1614
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 234.9784s / 4015.9095 s
agent0:                 episode reward: 0.1662,                 loss: nan
agent1:                 episode reward: -0.1662,                 loss: 0.1597
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 239.4611s / 4255.3706 s
agent0:                 episode reward: -0.1411,                 loss: nan
agent1:                 episode reward: 0.1411,                 loss: 0.1611
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 235.5219s / 4490.8926 s
agent0:                 episode reward: 0.1748,                 loss: nan
agent1:                 episode reward: -0.1748,                 loss: 0.1610
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 232.7814s / 4723.6739 s
agent0:                 episode reward: 0.0482,                 loss: nan
agent1:                 episode reward: -0.0482,                 loss: 0.1561
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.6606s / 4965.3346 s
agent0:                 episode reward: 0.2333,                 loss: nan
agent1:                 episode reward: -0.2333,                 loss: 0.1544
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 240.6308s / 5205.9654 s
agent0:                 episode reward: 0.2503,                 loss: nan
agent1:                 episode reward: -0.2503,                 loss: 0.1556
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 237.7147s / 5443.6801 s
agent0:                 episode reward: -0.0227,                 loss: nan
agent1:                 episode reward: 0.0227,                 loss: 0.1557
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.2940s / 5684.9741 s
agent0:                 episode reward: -0.0288,                 loss: nan
agent1:                 episode reward: 0.0288,                 loss: 0.1548
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 239.8587s / 5924.8328 s
agent0:                 episode reward: -0.1700,                 loss: nan
agent1:                 episode reward: 0.1700,                 loss: 0.1551
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.6409s / 6173.4736 s
agent0:                 episode reward: -0.0976,                 loss: nan
agent1:                 episode reward: 0.0976,                 loss: 0.1538
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 239.4786s / 6412.9522 s
agent0:                 episode reward: -0.2066,                 loss: nan
agent1:                 episode reward: 0.2066,                 loss: 0.1551
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.2821s / 6659.2343 s
agent0:                 episode reward: 0.0009,                 loss: nan
agent1:                 episode reward: -0.0009,                 loss: 0.1546
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 237.4427s / 6896.6770 s
agent0:                 episode reward: 0.3022,                 loss: nan
agent1:                 episode reward: -0.3022,                 loss: 0.1545
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.6317s / 7141.3087 s
agent0:                 episode reward: 0.2276,                 loss: nan
agent1:                 episode reward: -0.2276,                 loss: 0.1547
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.1247s / 7384.4334 s
agent0:                 episode reward: 0.1497,                 loss: nan
agent1:                 episode reward: -0.1497,                 loss: 0.1531
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.5623s / 7624.9956 s
agent0:                 episode reward: -0.1272,                 loss: nan
agent1:                 episode reward: 0.1272,                 loss: 0.1543
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 234.6991s / 7859.6947 s
agent0:                 episode reward: -0.0839,                 loss: nan
agent1:                 episode reward: 0.0839,                 loss: 0.1537
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 250.5848s / 8110.2795 s
agent0:                 episode reward: -0.0334,                 loss: nan
agent1:                 episode reward: 0.0334,                 loss: 0.1550
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 237.2136s / 8347.4931 s
agent0:                 episode reward: 0.1108,                 loss: nan
agent1:                 episode reward: -0.1108,                 loss: 0.1537
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 243.7252s / 8591.2183 s
agent0:                 episode reward: -0.1656,                 loss: nan
agent1:                 episode reward: 0.1656,                 loss: 0.1531
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.3599s / 8836.5782 s
agent0:                 episode reward: -0.1100,                 loss: nan
agent1:                 episode reward: 0.1100,                 loss: 0.1547
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 239.0566s / 9075.6349 s
agent0:                 episode reward: 0.1736,                 loss: nan
agent1:                 episode reward: -0.1736,                 loss: 0.1544
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.3313s / 9315.9662 s
agent0:                 episode reward: -0.0653,                 loss: nan
agent1:                 episode reward: 0.0653,                 loss: 0.1557
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.6282s / 9559.5944 s
agent0:                 episode reward: 0.2361,                 loss: nan
agent1:                 episode reward: -0.2361,                 loss: 0.1539
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.2935s / 9800.8879 s
agent0:                 episode reward: -0.2536,                 loss: nan
agent1:                 episode reward: 0.2536,                 loss: 0.1529
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.5741s / 10048.4620 s
agent0:                 episode reward: -0.2056,                 loss: nan
agent1:                 episode reward: 0.2056,                 loss: 0.1533
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 234.9452s / 10283.4072 s
agent0:                 episode reward: 0.2097,                 loss: nan
agent1:                 episode reward: -0.2097,                 loss: 0.1511
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 239.8173s / 10523.2245 s
agent0:                 episode reward: -0.4008,                 loss: nan
agent1:                 episode reward: 0.4008,                 loss: 0.1532
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 248.3949s / 10771.6194 s
agent0:                 episode reward: -0.2228,                 loss: nan
agent1:                 episode reward: 0.2228,                 loss: 0.1522
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 249.2701s / 11020.8895 s
agent0:                 episode reward: -0.1907,                 loss: nan
agent1:                 episode reward: 0.1907,                 loss: 0.1529
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.0923s / 11266.9818 s
agent0:                 episode reward: 0.1543,                 loss: nan
agent1:                 episode reward: -0.1543,                 loss: 0.1510
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.4522s / 11512.4340 s
agent0:                 episode reward: -0.4018,                 loss: nan
agent1:                 episode reward: 0.4018,                 loss: 0.1521
Episode: 1481/30000 (4.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.9217s / 11756.3557 s
agent0:                 episode reward: 0.0013,                 loss: nan
agent1:                 episode reward: -0.0013,                 loss: 0.1517
Episode: 1501/30000 (5.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.8007s / 12005.1564 s
agent0:                 episode reward: 0.2395,                 loss: nan
agent1:                 episode reward: -0.2395,                 loss: 0.1521
Episode: 1521/30000 (5.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 239.7014s / 12244.8579 s
agent0:                 episode reward: -0.0780,                 loss: nan
agent1:                 episode reward: 0.0780,                 loss: 0.1519
Episode: 1541/30000 (5.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 238.6697s / 12483.5276 s
agent0:                 episode reward: -0.2639,                 loss: nan
agent1:                 episode reward: 0.2639,                 loss: 0.1522
Episode: 1561/30000 (5.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.9843s / 12728.5119 s
agent0:                 episode reward: 0.1668,                 loss: nan
agent1:                 episode reward: -0.1668,                 loss: 0.1523
Episode: 1581/30000 (5.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.7419s / 12973.2537 s
agent0:                 episode reward: -0.2464,                 loss: nan
agent1:                 episode reward: 0.2464,                 loss: 0.1529
Episode: 1601/30000 (5.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.6330s / 13215.8868 s
agent0:                 episode reward: 0.0606,                 loss: nan
agent1:                 episode reward: -0.0606,                 loss: 0.1521
Episode: 1621/30000 (5.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.9520s / 13458.8388 s
agent0:                 episode reward: -0.2363,                 loss: nan
agent1:                 episode reward: 0.2363,                 loss: 0.1520
Episode: 1641/30000 (5.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.4165s / 13700.2553 s
agent0:                 episode reward: -0.1241,                 loss: nan
agent1:                 episode reward: 0.1241,                 loss: 0.1530
Episode: 1661/30000 (5.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 236.0906s / 13936.3459 s
agent0:                 episode reward: -0.3776,                 loss: nan
agent1:                 episode reward: 0.3776,                 loss: 0.1523
Episode: 1681/30000 (5.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.0464s / 14178.3923 s
agent0:                 episode reward: -0.0716,                 loss: nan
agent1:                 episode reward: 0.0716,                 loss: 0.1530
Episode: 1701/30000 (5.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.5533s / 14420.9456 s
agent0:                 episode reward: -0.1497,                 loss: nan
agent1:                 episode reward: 0.1497,                 loss: 0.1514
Episode: 1721/30000 (5.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.2120s / 14667.1576 s
agent0:                 episode reward: 0.0445,                 loss: nan
agent1:                 episode reward: -0.0445,                 loss: 0.1514
Episode: 1741/30000 (5.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.4828s / 14907.6404 s
agent0:                 episode reward: 0.0903,                 loss: nan
agent1:                 episode reward: -0.0903,                 loss: 0.1515
Episode: 1761/30000 (5.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.4060s / 15150.0464 s
agent0:                 episode reward: 0.1097,                 loss: nan
agent1:                 episode reward: -0.1097,                 loss: 0.1511
Episode: 1781/30000 (5.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.9619s / 15395.0083 s
agent0:                 episode reward: 0.0072,                 loss: nan
agent1:                 episode reward: -0.0072,                 loss: 0.1501
Episode: 1801/30000 (6.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.6447s / 15639.6529 s
agent0:                 episode reward: 0.0507,                 loss: nan
agent1:                 episode reward: -0.0507,                 loss: 0.1503
Episode: 1821/30000 (6.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.2912s / 15880.9442 s
agent0:                 episode reward: -0.0110,                 loss: nan
agent1:                 episode reward: 0.0110,                 loss: 0.1496
Episode: 1841/30000 (6.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 237.3858s / 16118.3300 s
agent0:                 episode reward: 0.3606,                 loss: nan
agent1:                 episode reward: -0.3606,                 loss: 0.1494
Episode: 1861/30000 (6.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.9683s / 16360.2982 s
agent0:                 episode reward: -0.1710,                 loss: nan
agent1:                 episode reward: 0.1710,                 loss: 0.1507
Episode: 1881/30000 (6.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.2467s / 16605.5449 s
agent0:                 episode reward: -0.1274,                 loss: nan
agent1:                 episode reward: 0.1274,                 loss: 0.1497
Episode: 1901/30000 (6.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.7962s / 16852.3411 s
agent0:                 episode reward: 0.2715,                 loss: nan
agent1:                 episode reward: -0.2715,                 loss: 0.1522
Episode: 1921/30000 (6.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 245.8902s / 17098.2314 s
agent0:                 episode reward: 0.0879,                 loss: nan
agent1:                 episode reward: -0.0879,                 loss: 0.1514
Episode: 1941/30000 (6.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.0063s / 17339.2377 s
agent0:                 episode reward: 0.0037,                 loss: nan
agent1:                 episode reward: -0.0037,                 loss: 0.1521
Episode: 1961/30000 (6.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 237.9175s / 17577.1551 s
agent0:                 episode reward: 0.1820,                 loss: nan