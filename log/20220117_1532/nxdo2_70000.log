pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
random seed: 91
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7fd171ff1210>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.256 0.046 0.123 ... 0.    0.    0.   ]
 [0.064 0.03  0.054 ... 0.    0.    0.   ]]
Load checkpoints (policy family):  [['83' '5753' '6419' ... '68870' '69079' '69130']
 ['121' '6342' '6627' ... '68960' '69100' '69268']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220116204408/epi_70000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220116204408_exploit_70000/mdp_arbitrary_mdp_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220116204408_exploit_70000/mdp_arbitrary_mdp_nxdo2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3115s / 1.3115 s
agent0:                 episode reward: -0.9838,                 loss: nan
agent1:                 episode reward: 0.9838,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3395s / 1.6510 s
agent0:                 episode reward: -0.0334,                 loss: nan
agent1:                 episode reward: 0.0334,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1797s / 1.8308 s
agent0:                 episode reward: -0.0240,                 loss: nan
agent1:                 episode reward: 0.0240,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.6582s / 2.4890 s
agent0:                 episode reward: -0.1039,                 loss: nan
agent1:                 episode reward: 0.1039,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1926s / 2.6816 s
agent0:                 episode reward: -0.1303,                 loss: nan
agent1:                 episode reward: 0.1303,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.5156s / 3.1972 s
agent0:                 episode reward: 0.0094,                 loss: nan
agent1:                 episode reward: -0.0094,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.6620s / 3.8591 s
agent0:                 episode reward: -0.0581,                 loss: nan
agent1:                 episode reward: 0.0581,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.6664s / 4.5255 s
agent0:                 episode reward: 0.1345,                 loss: nan
agent1:                 episode reward: -0.1345,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0200s / 5.5455 s
agent0:                 episode reward: 0.2691,                 loss: nan
agent1:                 episode reward: -0.2691,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.9764s / 6.5219 s
agent0:                 episode reward: -0.0095,                 loss: nan
agent1:                 episode reward: 0.0095,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4493s / 7.9712 s
agent0:                 episode reward: -0.1414,                 loss: nan
agent1:                 episode reward: 0.1414,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 36.8655s / 44.8367 s
agent0:                 episode reward: -0.0599,                 loss: nan
agent1:                 episode reward: 0.0599,                 loss: 0.2360
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 96.9845s / 141.8212 s
agent0:                 episode reward: -0.0865,                 loss: nan
agent1:                 episode reward: 0.0865,                 loss: 0.2069
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 98.8858s / 240.7070 s
agent0:                 episode reward: 0.0167,                 loss: nan
agent1:                 episode reward: -0.0167,                 loss: 0.1750
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 102.6325s / 343.3395 s
agent0:                 episode reward: -0.0298,                 loss: nan
agent1:                 episode reward: 0.0298,                 loss: 0.1640
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 100.8768s / 444.2163 s
agent0:                 episode reward: 0.0982,                 loss: nan
agent1:                 episode reward: -0.0982,                 loss: 0.1595
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 97.2802s / 541.4965 s
agent0:                 episode reward: -0.1773,                 loss: nan
agent1:                 episode reward: 0.1773,                 loss: 0.1536
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 103.2875s / 644.7841 s
agent0:                 episode reward: 0.4195,                 loss: nan
agent1:                 episode reward: -0.4195,                 loss: 0.1502
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 97.4815s / 742.2656 s
agent0:                 episode reward: 0.1911,                 loss: nan
agent1:                 episode reward: -0.1911,                 loss: 0.1500
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 97.1911s / 839.4567 s
agent0:                 episode reward: 0.1843,                 loss: nan
agent1:                 episode reward: -0.1843,                 loss: 0.1471
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 100.5327s / 939.9894 s
agent0:                 episode reward: 0.1016,                 loss: nan
agent1:                 episode reward: -0.1016,                 loss: 0.1479
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 102.5673s / 1042.5568 s
agent0:                 episode reward: 0.1409,                 loss: nan
agent1:                 episode reward: -0.1409,                 loss: 0.1480
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 103.0046s / 1145.5614 s
agent0:                 episode reward: 0.0724,                 loss: nan
agent1:                 episode reward: -0.0724,                 loss: 0.1472
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 107.1823s / 1252.7437 s
agent0:                 episode reward: 0.1209,                 loss: nan
agent1:                 episode reward: -0.1209,                 loss: 0.1472
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 109.4592s / 1362.2028 s
agent0:                 episode reward: 0.2851,                 loss: nan
agent1:                 episode reward: -0.2851,                 loss: 0.1459
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 110.7749s / 1472.9777 s
agent0:                 episode reward: 0.0457,                 loss: nan
agent1:                 episode reward: -0.0457,                 loss: 0.1459
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 104.5691s / 1577.5468 s
agent0:                 episode reward: 0.2460,                 loss: nan
agent1:                 episode reward: -0.2460,                 loss: 0.1453
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 105.3673s / 1682.9141 s
agent0:                 episode reward: -0.1547,                 loss: nan
agent1:                 episode reward: 0.1547,                 loss: 0.1462
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 107.9729s / 1790.8870 s
agent0:                 episode reward: -0.0585,                 loss: nan
agent1:                 episode reward: 0.0585,                 loss: 0.1699
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 108.4215s / 1899.3085 s
agent0:                 episode reward: -0.0337,                 loss: nan
agent1:                 episode reward: 0.0337,                 loss: 0.1581
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 108.6688s / 2007.9772 s
agent0:                 episode reward: 0.5495,                 loss: nan
agent1:                 episode reward: -0.5495,                 loss: 0.1590
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 109.8096s / 2117.7869 s
agent0:                 episode reward: 0.0580,                 loss: nan
agent1:                 episode reward: -0.0580,                 loss: 0.1581
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 109.1439s / 2226.9308 s
agent0:                 episode reward: 0.0745,                 loss: nan
agent1:                 episode reward: -0.0745,                 loss: 0.1571
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 110.3353s / 2337.2662 s
agent0:                 episode reward: -0.3499,                 loss: nan
agent1:                 episode reward: 0.3499,                 loss: 0.1567
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 108.7620s / 2446.0282 s
agent0:                 episode reward: -0.0352,                 loss: nan
agent1:                 episode reward: 0.0352,                 loss: 0.1563
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 107.7134s / 2553.7416 s
agent0:                 episode reward: 0.5338,                 loss: nan
agent1:                 episode reward: -0.5338,                 loss: 0.1550
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 111.1749s / 2664.9165 s
agent0:                 episode reward: -0.1817,                 loss: nan
agent1:                 episode reward: 0.1817,                 loss: 0.1568
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 152.1677s / 2817.0842 s
agent0:                 episode reward: -0.0112,                 loss: nan
agent1:                 episode reward: 0.0112,                 loss: 0.1565
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 240.6101s / 3057.6942 s
agent0:                 episode reward: -0.3013,                 loss: nan
agent1:                 episode reward: 0.3013,                 loss: 0.1564
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.3562s / 3298.0504 s
agent0:                 episode reward: -0.2701,                 loss: nan
agent1:                 episode reward: 0.2701,                 loss: 0.1546
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.0419s / 3543.0923 s
agent0:                 episode reward: 0.2048,                 loss: nan
agent1:                 episode reward: -0.2048,                 loss: 0.1568
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.3995s / 3785.4919 s
agent0:                 episode reward: 0.0972,                 loss: nan
agent1:                 episode reward: -0.0972,                 loss: 0.1555
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.3742s / 4029.8661 s
agent0:                 episode reward: -0.1831,                 loss: nan
agent1:                 episode reward: 0.1831,                 loss: 0.1547
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 236.9194s / 4266.7855 s
agent0:                 episode reward: 0.0741,                 loss: nan
agent1:                 episode reward: -0.0741,                 loss: 0.1561
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 236.4141s / 4503.1996 s
agent0:                 episode reward: 0.1008,                 loss: nan
agent1:                 episode reward: -0.1008,                 loss: 0.1556
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 238.2939s / 4741.4935 s
agent0:                 episode reward: 0.0306,                 loss: nan
agent1:                 episode reward: -0.0306,                 loss: 0.1499
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 238.2651s / 4979.7586 s
agent0:                 episode reward: -0.2116,                 loss: nan
agent1:                 episode reward: 0.2116,                 loss: 0.1459
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.7555s / 5222.5141 s
agent0:                 episode reward: 0.0442,                 loss: nan
agent1:                 episode reward: -0.0442,                 loss: 0.1470
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.2061s / 5464.7202 s
agent0:                 episode reward: 0.0267,                 loss: nan
agent1:                 episode reward: -0.0267,                 loss: 0.1450
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.8362s / 5706.5564 s
agent0:                 episode reward: 0.0788,                 loss: nan
agent1:                 episode reward: -0.0788,                 loss: 0.1458
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.2558s / 5948.8122 s
agent0:                 episode reward: 0.3245,                 loss: nan
agent1:                 episode reward: -0.3245,                 loss: 0.1459
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.0050s / 6190.8172 s
agent0:                 episode reward: 0.0411,                 loss: nan
agent1:                 episode reward: -0.0411,                 loss: 0.1453
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 243.9005s / 6434.7177 s
agent0:                 episode reward: 0.1670,                 loss: nan
agent1:                 episode reward: -0.1670,                 loss: 0.1447
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 247.0425s / 6681.7602 s
agent0:                 episode reward: -0.0203,                 loss: nan
agent1:                 episode reward: 0.0203,                 loss: 0.1445
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 251.7148s / 6933.4749 s
agent0:                 episode reward: -0.0387,                 loss: nan
agent1:                 episode reward: 0.0387,                 loss: 0.1443
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 249.1930s / 7182.6679 s
agent0:                 episode reward: -0.2309,                 loss: nan
agent1:                 episode reward: 0.2309,                 loss: 0.1436
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 237.9700s / 7420.6379 s
agent0:                 episode reward: -0.3500,                 loss: nan
agent1:                 episode reward: 0.3500,                 loss: 0.1438
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.7565s / 7662.3943 s
agent0:                 episode reward: -0.3136,                 loss: nan
agent1:                 episode reward: 0.3136,                 loss: 0.1434
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.8560s / 7908.2504 s
agent0:                 episode reward: -0.1989,                 loss: nan
agent1:                 episode reward: 0.1989,                 loss: 0.1436
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 248.7809s / 8157.0312 s
agent0:                 episode reward: -0.0950,                 loss: nan
agent1:                 episode reward: 0.0950,                 loss: 0.1425
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.5586s / 8403.5898 s
agent0:                 episode reward: 0.2202,                 loss: nan
agent1:                 episode reward: -0.2202,                 loss: 0.1435
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.1961s / 8643.7859 s
agent0:                 episode reward: -0.0359,                 loss: nan
agent1:                 episode reward: 0.0359,                 loss: 0.1446
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 239.8970s / 8883.6828 s
agent0:                 episode reward: -0.1562,                 loss: nan
agent1:                 episode reward: 0.1562,                 loss: 0.1454
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 245.1039s / 9128.7867 s
agent0:                 episode reward: -0.1277,                 loss: nan
agent1:                 episode reward: 0.1277,                 loss: 0.1458
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.3892s / 9374.1760 s
agent0:                 episode reward: 0.4767,                 loss: nan
agent1:                 episode reward: -0.4767,                 loss: 0.1460
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.3359s / 9619.5118 s
agent0:                 episode reward: 0.2874,                 loss: nan
agent1:                 episode reward: -0.2874,                 loss: 0.1465
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.0856s / 9861.5974 s
agent0:                 episode reward: 0.3516,                 loss: nan
agent1:                 episode reward: -0.3516,                 loss: 0.1466
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.7922s / 10107.3896 s
agent0:                 episode reward: 0.1388,                 loss: nan
agent1:                 episode reward: -0.1388,                 loss: 0.1467
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 247.4012s / 10354.7908 s
agent0:                 episode reward: -0.5154,                 loss: nan
agent1:                 episode reward: 0.5154,                 loss: 0.1471
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 239.7229s / 10594.5137 s
agent0:                 episode reward: 0.1647,                 loss: nan
agent1:                 episode reward: -0.1647,                 loss: 0.1467
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.1323s / 10840.6461 s
agent0:                 episode reward: 0.1606,                 loss: nan
agent1:                 episode reward: -0.1606,                 loss: 0.1458
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 248.1893s / 11088.8354 s
agent0:                 episode reward: 0.0304,                 loss: nan
agent1:                 episode reward: -0.0304,                 loss: 0.1443
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 237.1224s / 11325.9578 s
agent0:                 episode reward: -0.0772,                 loss: nan
agent1:                 episode reward: 0.0772,                 loss: 0.1468
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.7307s / 11567.6885 s
agent0:                 episode reward: -0.2149,                 loss: nan
agent1:                 episode reward: 0.2149,                 loss: 0.1459
Episode: 1481/30000 (4.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.6217s / 11811.3101 s
agent0:                 episode reward: -0.2624,                 loss: nan
agent1:                 episode reward: 0.2624,                 loss: 0.1451
Episode: 1501/30000 (5.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 252.1226s / 12063.4327 s
agent0:                 episode reward: -0.3115,                 loss: nan
agent1:                 episode reward: 0.3115,                 loss: 0.1476
Episode: 1521/30000 (5.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.0027s / 12309.4354 s
agent0:                 episode reward: -0.5347,                 loss: nan
agent1:                 episode reward: 0.5347,                 loss: 0.1483
Episode: 1541/30000 (5.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.4308s / 12555.8662 s
agent0:                 episode reward: -0.3073,                 loss: nan
agent1:                 episode reward: 0.3073,                 loss: 0.1469
Episode: 1561/30000 (5.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.7255s / 12800.5917 s
agent0:                 episode reward: -0.2296,                 loss: nan
agent1:                 episode reward: 0.2296,                 loss: 0.1484
Episode: 1581/30000 (5.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 238.0104s / 13038.6021 s
agent0:                 episode reward: 0.0929,                 loss: nan
agent1:                 episode reward: -0.0929,                 loss: 0.1501
Episode: 1601/30000 (5.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 247.1156s / 13285.7177 s
agent0:                 episode reward: 0.3247,                 loss: nan
agent1:                 episode reward: -0.3247,                 loss: 0.1487
Episode: 1621/30000 (5.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.6472s / 13533.3649 s
agent0:                 episode reward: -0.2811,                 loss: nan
agent1:                 episode reward: 0.2811,                 loss: 0.1492
Episode: 1641/30000 (5.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.8293s / 13774.1941 s
agent0:                 episode reward: -0.0823,                 loss: nan
agent1:                 episode reward: 0.0823,                 loss: 0.1495
Episode: 1661/30000 (5.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 254.2975s / 14028.4916 s
agent0:                 episode reward: -0.0701,                 loss: nan
agent1:                 episode reward: 0.0701,                 loss: 0.1483
Episode: 1681/30000 (5.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 250.1930s / 14278.6845 s
agent0:                 episode reward: 0.0817,                 loss: nan
agent1:                 episode reward: -0.0817,                 loss: 0.1508
Episode: 1701/30000 (5.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 255.9902s / 14534.6747 s
agent0:                 episode reward: -0.1322,                 loss: nan
agent1:                 episode reward: 0.1322,                 loss: 0.1497
Episode: 1721/30000 (5.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 250.6876s / 14785.3624 s
agent0:                 episode reward: 0.0523,                 loss: nan
agent1:                 episode reward: -0.0523,                 loss: 0.1498
Episode: 1741/30000 (5.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.2330s / 15032.5953 s
agent0:                 episode reward: 0.2765,                 loss: nan
agent1:                 episode reward: -0.2765,                 loss: 0.1503
Episode: 1761/30000 (5.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.2048s / 15273.8001 s
agent0:                 episode reward: -0.1551,                 loss: nan
agent1:                 episode reward: 0.1551,                 loss: 0.1490
Episode: 1781/30000 (5.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 250.4127s / 15524.2128 s
agent0:                 episode reward: -0.1293,                 loss: nan
agent1:                 episode reward: 0.1293,                 loss: 0.1496
Episode: 1801/30000 (6.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 239.0292s / 15763.2419 s
agent0:                 episode reward: 0.2195,                 loss: nan
agent1:                 episode reward: -0.2195,                 loss: 0.1494
Episode: 1821/30000 (6.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.1004s / 16010.3423 s
agent0:                 episode reward: -0.0106,                 loss: nan
agent1:                 episode reward: 0.0106,                 loss: 0.1497
Episode: 1841/30000 (6.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.6367s / 16253.9790 s
agent0:                 episode reward: -0.1071,                 loss: nan
agent1:                 episode reward: 0.1071,                 loss: 0.1499
Episode: 1861/30000 (6.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.8662s / 16501.8452 s
agent0:                 episode reward: 0.0554,                 loss: nan
agent1:                 episode reward: -0.0554,                 loss: 0.1500
Episode: 1881/30000 (6.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 248.9452s / 16750.7904 s
agent0:                 episode reward: -0.1843,                 loss: nan
agent1:                 episode reward: 0.1843,                 loss: 0.1497
Episode: 1901/30000 (6.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.1552s / 16992.9456 s
agent0:                 episode reward: -0.3683,                 loss: nan
agent1:                 episode reward: 0.3683,                 loss: 0.1558
Episode: 1921/30000 (6.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.9786s / 17241.9243 s
agent0:                 episode reward: -0.5642,                 loss: nan
agent1:                 episode reward: 0.5642,                 loss: 0.1560
Episode: 1941/30000 (6.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 249.6080s / 17491.5322 s
agent0:                 episode reward: 0.2237,                 loss: nan
agent1:                 episode reward: -0.2237,                 loss: 0.1555
Episode: 1961/30000 (6.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 255.0718s / 17746.6041 s
agent0:                 episode reward: -0.3557,                 loss: nan
agent1:                 episode reward: 0.3557,                 loss: 0.1561
Episode: 1981/30000 (6.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 251.3418s / 17997.9459 s
agent0:                 episode reward: 0.0230,                 loss: nan
agent1:                 episode reward: -0.0230,                 loss: 0.1550
Episode: 2001/30000 (6.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.4517s / 18244.3976 s
agent0:                 episode reward: 0.0727,                 loss: nan
agent1:                 episode reward: -0.0727,                 loss: 0.1556
Episode: 2021/30000 (6.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 240.8268s / 18485.2243 s