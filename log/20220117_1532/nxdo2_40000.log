pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
random seed: 91
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7f786ad541d0>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.483 0.092 0.2   ... 0.    0.121 0.   ]
 [0.    0.    0.    ... 0.    0.    0.   ]]
Load checkpoints (policy family):  [['83' '5753' '6419' ... '38431' '38946' '39041']
 ['121' '6342' '6627' ... '38452' '38973' '39078']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220116204408/epi_40000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220116204408_exploit_40000/mdp_arbitrary_mdp_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220116204408_exploit_40000/mdp_arbitrary_mdp_nxdo2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2606s / 1.2606 s
agent0:                 episode reward: -0.4744,                 loss: nan
agent1:                 episode reward: 0.4744,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.4537s / 1.7143 s
agent0:                 episode reward: 0.4723,                 loss: nan
agent1:                 episode reward: -0.4723,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1577s / 1.8720 s
agent0:                 episode reward: 0.3058,                 loss: nan
agent1:                 episode reward: -0.3058,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3066s / 2.1786 s
agent0:                 episode reward: 0.2827,                 loss: nan
agent1:                 episode reward: -0.2827,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3034s / 2.4820 s
agent0:                 episode reward: -0.0356,                 loss: nan
agent1:                 episode reward: 0.0356,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3069s / 2.7889 s
agent0:                 episode reward: -0.1243,                 loss: nan
agent1:                 episode reward: 0.1243,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.4922s / 3.2811 s
agent0:                 episode reward: 0.0441,                 loss: nan
agent1:                 episode reward: -0.0441,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.6996s / 3.9806 s
agent0:                 episode reward: 0.3907,                 loss: nan
agent1:                 episode reward: -0.3907,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.7728s / 4.7535 s
agent0:                 episode reward: 0.0765,                 loss: nan
agent1:                 episode reward: -0.0765,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.6616s / 5.4151 s
agent0:                 episode reward: -0.1751,                 loss: nan
agent1:                 episode reward: 0.1751,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0432s / 7.4583 s
agent0:                 episode reward: -0.1439,                 loss: nan
agent1:                 episode reward: 0.1439,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 32.7733s / 40.2315 s
agent0:                 episode reward: -0.1264,                 loss: nan
agent1:                 episode reward: 0.1264,                 loss: 0.2253
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 100.3257s / 140.5572 s
agent0:                 episode reward: 0.2718,                 loss: nan
agent1:                 episode reward: -0.2718,                 loss: 0.2020
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 100.3537s / 240.9109 s
agent0:                 episode reward: -0.1710,                 loss: nan
agent1:                 episode reward: 0.1710,                 loss: 0.1853
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 98.4512s / 339.3621 s
agent0:                 episode reward: 0.1013,                 loss: nan
agent1:                 episode reward: -0.1013,                 loss: 0.1780
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 98.2227s / 437.5847 s
agent0:                 episode reward: -0.0881,                 loss: nan
agent1:                 episode reward: 0.0881,                 loss: 0.1711
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 101.1085s / 538.6933 s
agent0:                 episode reward: 0.0601,                 loss: nan
agent1:                 episode reward: -0.0601,                 loss: 0.1657
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 96.5466s / 635.2399 s
agent0:                 episode reward: 0.0663,                 loss: nan
agent1:                 episode reward: -0.0663,                 loss: 0.1607
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 102.0470s / 737.2869 s
agent0:                 episode reward: -0.1020,                 loss: nan
agent1:                 episode reward: 0.1020,                 loss: 0.1581
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 102.4601s / 839.7470 s
agent0:                 episode reward: 0.0852,                 loss: nan
agent1:                 episode reward: -0.0852,                 loss: 0.1558
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 101.3740s / 941.1210 s
agent0:                 episode reward: 0.1563,                 loss: nan
agent1:                 episode reward: -0.1563,                 loss: 0.1546
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 97.6998s / 1038.8209 s
agent0:                 episode reward: 0.0233,                 loss: nan
agent1:                 episode reward: -0.0233,                 loss: 0.1530
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 98.9604s / 1137.7813 s
agent0:                 episode reward: -0.2651,                 loss: nan
agent1:                 episode reward: 0.2651,                 loss: 0.1490
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 100.6011s / 1238.3824 s
agent0:                 episode reward: 0.2732,                 loss: nan
agent1:                 episode reward: -0.2732,                 loss: 0.1504
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 90.4735s / 1328.8559 s
agent0:                 episode reward: -0.0417,                 loss: nan
agent1:                 episode reward: 0.0417,                 loss: 0.1514
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 98.8533s / 1427.7092 s
agent0:                 episode reward: -0.1633,                 loss: nan
agent1:                 episode reward: 0.1633,                 loss: 0.1503
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 98.2024s / 1525.9116 s
agent0:                 episode reward: -0.0213,                 loss: nan
agent1:                 episode reward: 0.0213,                 loss: 0.1496
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 102.6870s / 1628.5987 s
agent0:                 episode reward: -0.0520,                 loss: nan
agent1:                 episode reward: 0.0520,                 loss: 0.1504
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 101.3885s / 1729.9872 s
agent0:                 episode reward: 0.1141,                 loss: nan
agent1:                 episode reward: -0.1141,                 loss: 0.1566
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 100.6578s / 1830.6450 s
agent0:                 episode reward: -0.0704,                 loss: nan
agent1:                 episode reward: 0.0704,                 loss: 0.1496
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 95.5175s / 1926.1626 s
agent0:                 episode reward: -0.6455,                 loss: nan
agent1:                 episode reward: 0.6455,                 loss: 0.1501
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 95.8068s / 2021.9694 s
agent0:                 episode reward: -0.2227,                 loss: nan
agent1:                 episode reward: 0.2227,                 loss: 0.1510
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 97.9008s / 2119.8702 s
agent0:                 episode reward: 0.3572,                 loss: nan
agent1:                 episode reward: -0.3572,                 loss: 0.1490
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 99.6646s / 2219.5347 s
agent0:                 episode reward: -0.0591,                 loss: nan
agent1:                 episode reward: 0.0591,                 loss: 0.1492
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 100.0831s / 2319.6178 s
agent0:                 episode reward: -0.4149,                 loss: nan
agent1:                 episode reward: 0.4149,                 loss: 0.1504
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 97.4473s / 2417.0652 s
agent0:                 episode reward: -0.3580,                 loss: nan
agent1:                 episode reward: 0.3580,                 loss: 0.1485
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 98.9558s / 2516.0210 s
agent0:                 episode reward: 0.3449,                 loss: nan
agent1:                 episode reward: -0.3449,                 loss: 0.1478
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 101.9243s / 2617.9453 s
agent0:                 episode reward: 0.5168,                 loss: nan
agent1:                 episode reward: -0.5168,                 loss: 0.1487
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 96.8572s / 2714.8025 s
agent0:                 episode reward: -0.0517,                 loss: nan
agent1:                 episode reward: 0.0517,                 loss: 0.1488
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 127.0475s / 2841.8500 s
agent0:                 episode reward: -0.0404,                 loss: nan
agent1:                 episode reward: 0.0404,                 loss: 0.1493
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.0840s / 2978.9340 s
agent0:                 episode reward: -0.0035,                 loss: nan
agent1:                 episode reward: 0.0035,                 loss: 0.1479
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.0799s / 3119.0140 s
agent0:                 episode reward: -0.2973,                 loss: nan
agent1:                 episode reward: 0.2973,                 loss: 0.1476
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 134.7535s / 3253.7674 s
agent0:                 episode reward: -0.1834,                 loss: nan
agent1:                 episode reward: 0.1834,                 loss: 0.1473
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.7802s / 3391.5477 s
agent0:                 episode reward: 0.1340,                 loss: nan
agent1:                 episode reward: -0.1340,                 loss: 0.1457
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 136.1960s / 3527.7437 s
agent0:                 episode reward: 0.3304,                 loss: nan
agent1:                 episode reward: -0.3304,                 loss: 0.1470
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.3942s / 3665.1379 s
agent0:                 episode reward: -0.0160,                 loss: nan
agent1:                 episode reward: 0.0160,                 loss: 0.1435
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 136.9382s / 3802.0761 s
agent0:                 episode reward: 0.1101,                 loss: nan
agent1:                 episode reward: -0.1101,                 loss: 0.1422
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.4870s / 3940.5631 s
agent0:                 episode reward: -0.1944,                 loss: nan
agent1:                 episode reward: 0.1944,                 loss: 0.1437
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 136.2063s / 4076.7694 s
agent0:                 episode reward: 0.0844,                 loss: nan
agent1:                 episode reward: -0.0844,                 loss: 0.1426
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.4777s / 4215.2471 s
agent0:                 episode reward: -0.1242,                 loss: nan
agent1:                 episode reward: 0.1242,                 loss: 0.1419
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 134.1641s / 4349.4112 s
agent0:                 episode reward: -0.2368,                 loss: nan
agent1:                 episode reward: 0.2368,                 loss: 0.1411
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 136.5882s / 4485.9994 s
agent0:                 episode reward: 0.1322,                 loss: nan
agent1:                 episode reward: -0.1322,                 loss: 0.1397
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.6358s / 4621.6352 s
agent0:                 episode reward: 0.2073,                 loss: nan
agent1:                 episode reward: -0.2073,                 loss: 0.1392
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 142.0687s / 4763.7039 s
agent0:                 episode reward: -0.2163,                 loss: nan
agent1:                 episode reward: 0.2163,                 loss: 0.1396
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.3771s / 4903.0810 s
agent0:                 episode reward: 0.1440,                 loss: nan
agent1:                 episode reward: -0.1440,                 loss: 0.1389
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.6831s / 5044.7641 s
agent0:                 episode reward: -0.1825,                 loss: nan
agent1:                 episode reward: 0.1825,                 loss: 0.1398
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 137.7750s / 5182.5391 s
agent0:                 episode reward: 0.2475,                 loss: nan
agent1:                 episode reward: -0.2475,                 loss: 0.1392
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 136.0698s / 5318.6089 s
agent0:                 episode reward: 0.1094,                 loss: nan
agent1:                 episode reward: -0.1094,                 loss: 0.1397
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.0273s / 5457.6362 s
agent0:                 episode reward: 0.0210,                 loss: nan
agent1:                 episode reward: -0.0210,                 loss: 0.1385
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.5571s / 5599.1933 s
agent0:                 episode reward: 0.0945,                 loss: nan
agent1:                 episode reward: -0.0945,                 loss: 0.1378
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 136.9185s / 5736.1117 s
agent0:                 episode reward: 0.1307,                 loss: nan
agent1:                 episode reward: -0.1307,                 loss: 0.1395
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.7632s / 5873.8749 s
agent0:                 episode reward: 0.4958,                 loss: nan
agent1:                 episode reward: -0.4958,                 loss: 0.1410
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.5859s / 6013.4608 s
agent0:                 episode reward: 0.2157,                 loss: nan
agent1:                 episode reward: -0.2157,                 loss: 0.1438
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.6280s / 6153.0888 s
agent0:                 episode reward: -0.2363,                 loss: nan
agent1:                 episode reward: 0.2363,                 loss: 0.1442
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.4871s / 6290.5759 s
agent0:                 episode reward: -0.2053,                 loss: nan
agent1:                 episode reward: 0.2053,                 loss: 0.1435
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 136.7766s / 6427.3525 s
agent0:                 episode reward: 0.0129,                 loss: nan
agent1:                 episode reward: -0.0129,                 loss: 0.1423
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 136.8214s / 6564.1739 s
agent0:                 episode reward: 0.2820,                 loss: nan
agent1:                 episode reward: -0.2820,                 loss: 0.1429
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.9588s / 6700.1327 s
agent0:                 episode reward: -0.1430,                 loss: nan
agent1:                 episode reward: 0.1430,                 loss: 0.1442
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 136.3321s / 6836.4648 s
agent0:                 episode reward: -0.0504,                 loss: nan
agent1:                 episode reward: 0.0504,                 loss: 0.1446
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.3064s / 6975.7712 s
agent0:                 episode reward: 0.0889,                 loss: nan
agent1:                 episode reward: -0.0889,                 loss: 0.1437
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 142.7725s / 7118.5438 s
agent0:                 episode reward: 0.0239,                 loss: nan
agent1:                 episode reward: -0.0239,                 loss: 0.1421
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 136.4283s / 7254.9721 s
agent0:                 episode reward: 0.5522,                 loss: nan
agent1:                 episode reward: -0.5522,                 loss: 0.1420
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.5248s / 7394.4969 s
agent0:                 episode reward: 0.0471,                 loss: nan
agent1:                 episode reward: -0.0471,                 loss: 0.1424
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.2242s / 7534.7210 s
agent0:                 episode reward: 0.1819,                 loss: nan
agent1:                 episode reward: -0.1819,                 loss: 0.1427
Episode: 1481/30000 (4.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.8026s / 7673.5237 s
agent0:                 episode reward: 0.1441,                 loss: nan
agent1:                 episode reward: -0.1441,                 loss: 0.1426
Episode: 1501/30000 (5.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 136.8436s / 7810.3673 s
agent0:                 episode reward: 0.0101,                 loss: nan
agent1:                 episode reward: -0.0101,                 loss: 0.1427
Episode: 1521/30000 (5.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.4045s / 7948.7718 s
agent0:                 episode reward: -0.2297,                 loss: nan
agent1:                 episode reward: 0.2297,                 loss: 0.1431
Episode: 1541/30000 (5.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.8477s / 8087.6195 s
agent0:                 episode reward: 0.0708,                 loss: nan
agent1:                 episode reward: -0.0708,                 loss: 0.1426
Episode: 1561/30000 (5.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.3778s / 8224.9972 s
agent0:                 episode reward: -0.0194,                 loss: nan
agent1:                 episode reward: 0.0194,                 loss: 0.1421
Episode: 1581/30000 (5.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.1808s / 8363.1781 s
agent0:                 episode reward: 0.2516,                 loss: nan
agent1:                 episode reward: -0.2516,                 loss: 0.1422
Episode: 1601/30000 (5.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.8517s / 8504.0298 s
agent0:                 episode reward: 0.2493,                 loss: nan
agent1:                 episode reward: -0.2493,                 loss: 0.1424
Episode: 1621/30000 (5.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.8314s / 8643.8611 s
agent0:                 episode reward: 0.0271,                 loss: nan
agent1:                 episode reward: -0.0271,                 loss: 0.1422
Episode: 1641/30000 (5.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.8852s / 8782.7463 s
agent0:                 episode reward: 0.2161,                 loss: nan
agent1:                 episode reward: -0.2161,                 loss: 0.1417
Episode: 1661/30000 (5.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 135.8527s / 8918.5991 s
agent0:                 episode reward: -0.2808,                 loss: nan
agent1:                 episode reward: 0.2808,                 loss: 0.1415
Episode: 1681/30000 (5.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.8273s / 9058.4264 s
agent0:                 episode reward: 0.1190,                 loss: nan
agent1:                 episode reward: -0.1190,                 loss: 0.1407
Episode: 1701/30000 (5.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.8669s / 9198.2932 s
agent0:                 episode reward: -0.1030,                 loss: nan
agent1:                 episode reward: 0.1030,                 loss: 0.1427
Episode: 1721/30000 (5.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.3388s / 9336.6320 s
agent0:                 episode reward: 0.2109,                 loss: nan
agent1:                 episode reward: -0.2109,                 loss: 0.1428
Episode: 1741/30000 (5.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.4837s / 9476.1158 s
agent0:                 episode reward: -0.1460,                 loss: nan
agent1:                 episode reward: 0.1460,                 loss: 0.1406
Episode: 1761/30000 (5.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 136.7802s / 9612.8960 s
agent0:                 episode reward: -0.2177,                 loss: nan
agent1:                 episode reward: 0.2177,                 loss: 0.1422
Episode: 1781/30000 (5.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.3034s / 9751.1994 s
agent0:                 episode reward: -0.1702,                 loss: nan
agent1:                 episode reward: 0.1702,                 loss: 0.1426
Episode: 1801/30000 (6.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.3202s / 9892.5196 s
agent0:                 episode reward: 0.0480,                 loss: nan
agent1:                 episode reward: -0.0480,                 loss: 0.1421
Episode: 1821/30000 (6.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.8483s / 10028.3679 s
agent0:                 episode reward: -0.2842,                 loss: nan
agent1:                 episode reward: 0.2842,                 loss: 0.1421
Episode: 1841/30000 (6.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.8901s / 10170.2580 s
agent0:                 episode reward: -0.2154,                 loss: nan
agent1:                 episode reward: 0.2154,                 loss: 0.1406
Episode: 1861/30000 (6.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.8962s / 10309.1543 s
agent0:                 episode reward: 0.0199,                 loss: nan
agent1:                 episode reward: -0.0199,                 loss: 0.1399
Episode: 1881/30000 (6.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.2652s / 10444.4195 s
agent0:                 episode reward: -0.3171,                 loss: nan
agent1:                 episode reward: 0.3171,                 loss: 0.1405
Episode: 1901/30000 (6.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.8254s / 10584.2449 s
agent0:                 episode reward: -0.1555,                 loss: nan
agent1:                 episode reward: 0.1555,                 loss: 0.1427
Episode: 1921/30000 (6.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.7472s / 10724.9921 s
agent0:                 episode reward: 0.0254,                 loss: nan
agent1:                 episode reward: -0.0254,                 loss: 0.1412
Episode: 1941/30000 (6.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.9298s / 10862.9220 s
agent0:                 episode reward: -0.1548,                 loss: nan
agent1:                 episode reward: 0.1548,                 loss: 0.1429
Episode: 1961/30000 (6.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 135.5980s / 10998.5199 s
agent0:                 episode reward: -0.3941,                 loss: nan
agent1:                 episode reward: 0.3941,                 loss: 0.1407
Episode: 1981/30000 (6.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.3557s / 11137.8757 s
agent0:                 episode reward: 0.0451,                 loss: nan
agent1:                 episode reward: -0.0451,                 loss: 0.1423
Episode: 2001/30000 (6.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.8717s / 11275.7474 s
agent0:                 episode reward: 0.0931,                 loss: nan
agent1:                 episode reward: -0.0931,                 loss: 0.1427
Episode: 2021/30000 (6.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.9976s / 11415.7450 s
agent0:                 episode reward: -0.0381,                 loss: nan
agent1:                 episode reward: 0.0381,                 loss: 0.1418
Episode: 2041/30000 (6.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.4842s / 11555.2292 s
agent0:                 episode reward: -0.1643,                 loss: nan
agent1:                 episode reward: 0.1643,                 loss: 0.1434
Episode: 2061/30000 (6.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.0588s / 11692.2879 s
agent0:                 episode reward: -0.1634,                 loss: nan
agent1:                 episode reward: 0.1634,                 loss: 0.1445
Episode: 2081/30000 (6.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.9520s / 11832.2399 s
agent0:                 episode reward: -0.1224,                 loss: nan
agent1:                 episode reward: 0.1224,                 loss: 0.1424
Episode: 2101/30000 (7.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 136.9255s / 11969.1654 s
agent0:                 episode reward: 0.0609,                 loss: nan
agent1:                 episode reward: -0.0609,                 loss: 0.1438
Episode: 2121/30000 (7.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.8430s / 12107.0084 s
agent0:                 episode reward: -0.1047,                 loss: nan
agent1:                 episode reward: 0.1047,                 loss: 0.1438
Episode: 2141/30000 (7.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.9461s / 12246.9545 s
agent0:                 episode reward: -0.1705,                 loss: nan
agent1:                 episode reward: 0.1705,                 loss: 0.1430
Episode: 2161/30000 (7.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.7838s / 12388.7383 s
agent0:                 episode reward: 0.1207,                 loss: nan
agent1:                 episode reward: -0.1207,                 loss: 0.1442
Episode: 2181/30000 (7.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.8095s / 12530.5478 s
agent0:                 episode reward: -0.1082,                 loss: nan
agent1:                 episode reward: 0.1082,                 loss: 0.1427
Episode: 2201/30000 (7.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.7123s / 12672.2601 s
agent0:                 episode reward: -0.0634,                 loss: nan
agent1:                 episode reward: 0.0634,                 loss: 0.1414
Episode: 2221/30000 (7.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 143.3919s / 12815.6521 s
agent0:                 episode reward: -0.0889,                 loss: nan
agent1:                 episode reward: 0.0889,                 loss: 0.1414
Episode: 2241/30000 (7.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.9657s / 12956.6178 s
agent0:                 episode reward: -0.8223,                 loss: nan
agent1:                 episode reward: 0.8223,                 loss: 0.1393
Episode: 2261/30000 (7.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.5196s / 13097.1373 s
agent0:                 episode reward: -0.3911,                 loss: nan
agent1:                 episode reward: 0.3911,                 loss: 0.1400
Episode: 2281/30000 (7.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 143.3227s / 13240.4600 s
agent0:                 episode reward: -0.2392,                 loss: nan
agent1:                 episode reward: 0.2392,                 loss: 0.1401
Episode: 2301/30000 (7.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.1607s / 13380.6208 s
agent0:                 episode reward: 0.1150,                 loss: nan
agent1:                 episode reward: -0.1150,                 loss: 0.1399
Episode: 2321/30000 (7.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.2474s / 13520.8682 s
agent0:                 episode reward: -0.2833,                 loss: nan
agent1:                 episode reward: 0.2833,                 loss: 0.1390
Episode: 2341/30000 (7.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.7463s / 13662.6145 s
agent0:                 episode reward: 0.0957,                 loss: nan
agent1:                 episode reward: -0.0957,                 loss: 0.1402
Episode: 2361/30000 (7.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.9104s / 13803.5250 s
agent0:                 episode reward: -0.3256,                 loss: nan
agent1:                 episode reward: 0.3256,                 loss: 0.1389
Episode: 2381/30000 (7.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.0541s / 13944.5790 s
agent0:                 episode reward: -0.1259,                 loss: nan
agent1:                 episode reward: 0.1259,                 loss: 0.1384
Episode: 2401/30000 (8.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 143.5130s / 14088.0920 s
agent0:                 episode reward: -0.5052,                 loss: nan
agent1:                 episode reward: 0.5052,                 loss: 0.1388
Episode: 2421/30000 (8.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.4218s / 14229.5138 s
agent0:                 episode reward: -0.1786,                 loss: nan
agent1:                 episode reward: 0.1786,                 loss: 0.1381
Episode: 2441/30000 (8.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.3255s / 14367.8392 s
agent0:                 episode reward: -0.1610,                 loss: nan
agent1:                 episode reward: 0.1610,                 loss: 0.1386
Episode: 2461/30000 (8.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.2667s / 14508.1059 s
agent0:                 episode reward: -0.0382,                 loss: nan
agent1:                 episode reward: 0.0382,                 loss: 0.1389
Episode: 2481/30000 (8.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 142.7257s / 14650.8316 s
agent0:                 episode reward: -0.2641,                 loss: nan
agent1:                 episode reward: 0.2641,                 loss: 0.1387
Episode: 2501/30000 (8.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 142.8305s / 14793.6622 s
agent0:                 episode reward: -0.3102,                 loss: nan
agent1:                 episode reward: 0.3102,                 loss: 0.1393
Episode: 2521/30000 (8.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.5932s / 14935.2553 s
agent0:                 episode reward: -0.4424,                 loss: nan
agent1:                 episode reward: 0.4424,                 loss: 0.1391
Episode: 2541/30000 (8.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 143.0372s / 15078.2925 s
agent0:                 episode reward: 0.1692,                 loss: nan
agent1:                 episode reward: -0.1692,                 loss: 0.1396
Episode: 2561/30000 (8.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 142.4964s / 15220.7890 s
agent0:                 episode reward: -0.2110,                 loss: nan
agent1:                 episode reward: 0.2110,                 loss: 0.1401
Episode: 2581/30000 (8.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.3340s / 15361.1230 s
agent0:                 episode reward: 0.0893,                 loss: nan
agent1:                 episode reward: -0.0893,                 loss: 0.1397
Episode: 2601/30000 (8.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.7437s / 15501.8667 s
agent0:                 episode reward: -0.1355,                 loss: nan
agent1:                 episode reward: 0.1355,                 loss: 0.1406
Episode: 2621/30000 (8.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 144.1844s / 15646.0511 s
agent0:                 episode reward: -0.1760,                 loss: nan
agent1:                 episode reward: 0.1760,                 loss: 0.1398
Episode: 2641/30000 (8.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.5094s / 15787.5604 s
agent0:                 episode reward: -0.2573,                 loss: nan
agent1:                 episode reward: 0.2573,                 loss: 0.1410
Episode: 2661/30000 (8.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 142.7714s / 15930.3319 s
agent0:                 episode reward: -0.2481,                 loss: nan
agent1:                 episode reward: 0.2481,                 loss: 0.1392
Episode: 2681/30000 (8.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 143.5899s / 16073.9218 s
agent0:                 episode reward: 0.0633,                 loss: nan
agent1:                 episode reward: -0.0633,                 loss: 0.1400
Episode: 2701/30000 (9.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 144.5057s / 16218.4274 s
agent0:                 episode reward: -0.1846,                 loss: nan
agent1:                 episode reward: 0.1846,                 loss: 0.1402
Episode: 2721/30000 (9.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.6743s / 16359.1017 s
agent0:                 episode reward: 0.3819,                 loss: nan
agent1:                 episode reward: -0.3819,                 loss: 0.1404
Episode: 2741/30000 (9.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.5554s / 16497.6572 s
agent0:                 episode reward: -0.3221,                 loss: nan
agent1:                 episode reward: 0.3221,                 loss: 0.1397
Episode: 2761/30000 (9.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 144.0902s / 16641.7474 s
agent0:                 episode reward: -0.0726,                 loss: nan
agent1:                 episode reward: 0.0726,                 loss: 0.1413
Episode: 2781/30000 (9.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.2789s / 16780.0263 s
agent0:                 episode reward: 0.0244,                 loss: nan
agent1:                 episode reward: -0.0244,                 loss: 0.1422
Episode: 2801/30000 (9.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.8074s / 16921.8337 s
agent0:                 episode reward: -0.1235,                 loss: nan
agent1:                 episode reward: 0.1235,                 loss: 0.1396
Episode: 2821/30000 (9.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.1738s / 17059.0075 s
agent0:                 episode reward: -0.0485,                 loss: nan
agent1:                 episode reward: 0.0485,                 loss: 0.1386
Episode: 2841/30000 (9.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.9356s / 17198.9432 s
agent0:                 episode reward: -0.0470,                 loss: nan
agent1:                 episode reward: 0.0470,                 loss: 0.1391
Episode: 2861/30000 (9.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 142.7899s / 17341.7331 s
agent0:                 episode reward: -0.0379,                 loss: nan
agent1:                 episode reward: 0.0379,                 loss: 0.1403
Episode: 2881/30000 (9.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.7824s / 17481.5155 s
agent0:                 episode reward: 0.0784,                 loss: nan
agent1:                 episode reward: -0.0784,                 loss: 0.1385
Episode: 2901/30000 (9.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.4866s / 17622.0021 s
agent0:                 episode reward: -0.0151,                 loss: nan
agent1:                 episode reward: 0.0151,                 loss: 0.1428
Episode: 2921/30000 (9.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.4742s / 17762.4762 s
agent0:                 episode reward: -0.1397,                 loss: nan
agent1:                 episode reward: 0.1397,                 loss: 0.1431
Episode: 2941/30000 (9.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.6792s / 17902.1554 s
agent0:                 episode reward: -0.0818,                 loss: nan
agent1:                 episode reward: 0.0818,                 loss: 0.1426
Episode: 2961/30000 (9.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.4318s / 18041.5873 s
agent0:                 episode reward: -0.1968,                 loss: nan
agent1:                 episode reward: 0.1968,                 loss: 0.1431
Episode: 2981/30000 (9.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.1863s / 18181.7736 s
agent0:                 episode reward: -0.2119,                 loss: nan
agent1:                 episode reward: 0.2119,                 loss: 0.1425
Episode: 3001/30000 (10.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.7687s / 18322.5423 s
agent0:                 episode reward: 0.0727,                 loss: nan
agent1:                 episode reward: -0.0727,                 loss: 0.1432
Episode: 3021/30000 (10.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 143.2092s / 18465.7515 s
agent0:                 episode reward: -0.1617,                 loss: nan
agent1:                 episode reward: 0.1617,                 loss: 0.1423
Episode: 3041/30000 (10.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 137.3450s / 18603.0965 s
agent0:                 episode reward: -0.1736,                 loss: nan
agent1:                 episode reward: 0.1736,                 loss: 0.1412
Episode: 3061/30000 (10.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.7102s / 18743.8067 s
agent0:                 episode reward: -0.0628,                 loss: nan
agent1:                 episode reward: 0.0628,                 loss: 0.1420
Episode: 3081/30000 (10.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 145.0335s / 18888.8402 s
agent0:                 episode reward: 0.1986,                 loss: nan
agent1:                 episode reward: -0.1986,                 loss: 0.1431
Episode: 3101/30000 (10.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 137.9759s / 19026.8161 s
agent0:                 episode reward: -0.1860,                 loss: nan
agent1:                 episode reward: 0.1860,                 loss: 0.1405