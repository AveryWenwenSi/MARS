pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
random seed: 91
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7fb490256250>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.284 0.369 0.348]
 [0.355 0.408 0.236]]
Load checkpoints (policy family):  [['83' '5753' '6419']
 ['121' '6342' '6627']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220116204408/epi_10000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220116204408_exploit_10000/mdp_arbitrary_mdp_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220116204408_exploit_10000/mdp_arbitrary_mdp_nxdo2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2763s / 1.2763 s
agent0:                 episode reward: 1.1516,                 loss: nan
agent1:                 episode reward: -1.1516,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1887s / 1.4650 s
agent0:                 episode reward: -0.0587,                 loss: nan
agent1:                 episode reward: 0.0587,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2285s / 1.6935 s
agent0:                 episode reward: -0.1900,                 loss: nan
agent1:                 episode reward: 0.1900,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3286s / 2.0220 s
agent0:                 episode reward: 0.0623,                 loss: nan
agent1:                 episode reward: -0.0623,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0865s / 2.1085 s
agent0:                 episode reward: 0.1206,                 loss: nan
agent1:                 episode reward: -0.1206,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0807s / 2.1892 s
agent0:                 episode reward: 0.3041,                 loss: nan
agent1:                 episode reward: -0.3041,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0919s / 2.2811 s
agent0:                 episode reward: 0.2781,                 loss: nan
agent1:                 episode reward: -0.2781,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1529s / 2.4340 s
agent0:                 episode reward: 0.3576,                 loss: nan
agent1:                 episode reward: -0.3576,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2048s / 2.6388 s
agent0:                 episode reward: 0.0128,                 loss: nan
agent1:                 episode reward: -0.0128,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1931s / 2.8319 s
agent0:                 episode reward: 0.2631,                 loss: nan
agent1:                 episode reward: -0.2631,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1964s / 3.0283 s
agent0:                 episode reward: -0.0921,                 loss: nan
agent1:                 episode reward: 0.0921,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 27.5073s / 30.5356 s
agent0:                 episode reward: 0.0764,                 loss: nan
agent1:                 episode reward: -0.0764,                 loss: 0.1838
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 102.8260s / 133.3616 s
agent0:                 episode reward: 0.0852,                 loss: nan
agent1:                 episode reward: -0.0852,                 loss: 0.1753
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 96.6351s / 229.9967 s
agent0:                 episode reward: -0.1071,                 loss: nan
agent1:                 episode reward: 0.1071,                 loss: 0.1705
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 96.1527s / 326.1494 s
agent0:                 episode reward: -0.0984,                 loss: nan
agent1:                 episode reward: 0.0984,                 loss: 0.1672
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 97.2173s / 423.3667 s
agent0:                 episode reward: -0.0253,                 loss: nan
agent1:                 episode reward: 0.0253,                 loss: 0.1624
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 99.0320s / 522.3987 s
agent0:                 episode reward: 0.0326,                 loss: nan
agent1:                 episode reward: -0.0326,                 loss: 0.1585
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 98.5462s / 620.9449 s
agent0:                 episode reward: 0.5321,                 loss: nan
agent1:                 episode reward: -0.5321,                 loss: 0.1564
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 100.0778s / 721.0227 s
agent0:                 episode reward: -0.3230,                 loss: nan
agent1:                 episode reward: 0.3230,                 loss: 0.1552
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 102.0341s / 823.0567 s
agent0:                 episode reward: 0.2010,                 loss: nan
agent1:                 episode reward: -0.2010,                 loss: 0.1522
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 99.9927s / 923.0495 s
agent0:                 episode reward: 0.2431,                 loss: nan
agent1:                 episode reward: -0.2431,                 loss: 0.1520
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 99.8111s / 1022.8606 s
agent0:                 episode reward: 0.0709,                 loss: nan
agent1:                 episode reward: -0.0709,                 loss: 0.1503
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 96.3217s / 1119.1822 s
agent0:                 episode reward: 0.2799,                 loss: nan
agent1:                 episode reward: -0.2799,                 loss: 0.1475
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 99.1141s / 1218.2964 s
agent0:                 episode reward: 0.2573,                 loss: nan
agent1:                 episode reward: -0.2573,                 loss: 0.1485
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 90.3888s / 1308.6851 s
agent0:                 episode reward: -0.2670,                 loss: nan
agent1:                 episode reward: 0.2670,                 loss: 0.1494
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 98.1984s / 1406.8836 s
agent0:                 episode reward: -0.0495,                 loss: nan
agent1:                 episode reward: 0.0495,                 loss: 0.1473
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 98.1895s / 1505.0731 s
agent0:                 episode reward: -0.2892,                 loss: nan
agent1:                 episode reward: 0.2892,                 loss: 0.1460
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 100.6652s / 1605.7383 s
agent0:                 episode reward: 0.0704,                 loss: nan
agent1:                 episode reward: -0.0704,                 loss: 0.1457
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 102.1674s / 1707.9057 s
agent0:                 episode reward: -0.0285,                 loss: nan
agent1:                 episode reward: 0.0285,                 loss: 0.1454
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 102.0417s / 1809.9474 s
agent0:                 episode reward: 0.4083,                 loss: nan
agent1:                 episode reward: -0.4083,                 loss: 0.1440
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 96.8298s / 1906.7772 s
agent0:                 episode reward: 0.2390,                 loss: nan
agent1:                 episode reward: -0.2390,                 loss: 0.1441
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 99.0020s / 2005.7792 s
agent0:                 episode reward: 0.2483,                 loss: nan
agent1:                 episode reward: -0.2483,                 loss: 0.1417
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 96.9261s / 2102.7053 s
agent0:                 episode reward: 0.0660,                 loss: nan
agent1:                 episode reward: -0.0660,                 loss: 0.1413
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 98.3126s / 2201.0179 s
agent0:                 episode reward: 0.1697,                 loss: nan
agent1:                 episode reward: -0.1697,                 loss: 0.1404
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 100.1367s / 2301.1546 s
agent0:                 episode reward: 0.0415,                 loss: nan
agent1:                 episode reward: -0.0415,                 loss: 0.1414
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 98.8887s / 2400.0433 s
agent0:                 episode reward: -0.2196,                 loss: nan
agent1:                 episode reward: 0.2196,                 loss: 0.1413
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 98.4073s / 2498.4506 s
agent0:                 episode reward: -0.1829,                 loss: nan
agent1:                 episode reward: 0.1829,                 loss: 0.1414
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 101.4970s / 2599.9477 s
agent0:                 episode reward: -0.7450,                 loss: nan
agent1:                 episode reward: 0.7450,                 loss: 0.1423
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 96.1026s / 2696.0503 s
agent0:                 episode reward: 0.2218,                 loss: nan
agent1:                 episode reward: -0.2218,                 loss: 0.1410
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 122.6964s / 2818.7467 s
agent0:                 episode reward: 0.0595,                 loss: nan
agent1:                 episode reward: -0.0595,                 loss: 0.1414
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.0893s / 2957.8360 s
agent0:                 episode reward: 0.2981,                 loss: nan
agent1:                 episode reward: -0.2981,                 loss: 0.1407
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.8277s / 3098.6637 s
agent0:                 episode reward: -0.1877,                 loss: nan
agent1:                 episode reward: 0.1877,                 loss: 0.1418
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 136.4011s / 3235.0648 s
agent0:                 episode reward: 0.3700,                 loss: nan
agent1:                 episode reward: -0.3700,                 loss: 0.1399
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.7926s / 3370.8574 s
agent0:                 episode reward: 0.2242,                 loss: nan
agent1:                 episode reward: -0.2242,                 loss: 0.1400
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 136.5618s / 3507.4191 s
agent0:                 episode reward: -0.1475,                 loss: nan
agent1:                 episode reward: 0.1475,                 loss: 0.1402
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.8553s / 3645.2744 s
agent0:                 episode reward: -0.2379,                 loss: nan
agent1:                 episode reward: 0.2379,                 loss: 0.1482
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.6196s / 3780.8940 s
agent0:                 episode reward: -0.0323,                 loss: nan
agent1:                 episode reward: 0.0323,                 loss: 0.1455
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 136.8961s / 3917.7901 s
agent0:                 episode reward: 0.3953,                 loss: nan
agent1:                 episode reward: -0.3953,                 loss: 0.1441
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 136.4602s / 4054.2503 s
agent0:                 episode reward: -0.1554,                 loss: nan
agent1:                 episode reward: 0.1554,                 loss: 0.1447
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 136.9609s / 4191.2112 s
agent0:                 episode reward: -0.0588,                 loss: nan
agent1:                 episode reward: 0.0588,                 loss: 0.1454
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 136.0816s / 4327.2929 s
agent0:                 episode reward: 0.0824,                 loss: nan
agent1:                 episode reward: -0.0824,                 loss: 0.1440
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.0432s / 4465.3360 s
agent0:                 episode reward: -0.0518,                 loss: nan
agent1:                 episode reward: 0.0518,                 loss: 0.1430
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.3493s / 4602.6854 s
agent0:                 episode reward: 0.1926,                 loss: nan
agent1:                 episode reward: -0.1926,                 loss: 0.1434
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 143.7163s / 4746.4017 s
agent0:                 episode reward: 0.2443,                 loss: nan
agent1:                 episode reward: -0.2443,                 loss: 0.1433
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.4604s / 4883.8621 s
agent0:                 episode reward: 0.0548,                 loss: nan
agent1:                 episode reward: -0.0548,                 loss: 0.1429
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.9214s / 5024.7835 s
agent0:                 episode reward: 0.0853,                 loss: nan
agent1:                 episode reward: -0.0853,                 loss: 0.1408
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 142.0322s / 5166.8157 s
agent0:                 episode reward: 0.2971,                 loss: nan
agent1:                 episode reward: -0.2971,                 loss: 0.1429
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.8937s / 5305.7094 s
agent0:                 episode reward: -0.0308,                 loss: nan
agent1:                 episode reward: 0.0308,                 loss: 0.1431
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.1299s / 5444.8392 s
agent0:                 episode reward: -0.0454,                 loss: nan
agent1:                 episode reward: 0.0454,                 loss: 0.1419
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.5445s / 5585.3837 s
agent0:                 episode reward: -0.1361,                 loss: nan
agent1:                 episode reward: 0.1361,                 loss: 0.1433
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.4164s / 5722.8002 s
agent0:                 episode reward: -0.3703,                 loss: nan
agent1:                 episode reward: 0.3703,                 loss: 0.1419
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.8175s / 5858.6176 s
agent0:                 episode reward: 0.0556,                 loss: nan
agent1:                 episode reward: -0.0556,                 loss: 0.1414
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.8636s / 5998.4812 s
agent0:                 episode reward: 0.3238,                 loss: nan
agent1:                 episode reward: -0.3238,                 loss: 0.1372
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.4451s / 6138.9263 s
agent0:                 episode reward: 0.2208,                 loss: nan
agent1:                 episode reward: -0.2208,                 loss: 0.1358
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.3564s / 6277.2827 s
agent0:                 episode reward: 0.0491,                 loss: nan
agent1:                 episode reward: -0.0491,                 loss: 0.1371
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.9302s / 6416.2129 s
agent0:                 episode reward: -0.2484,                 loss: nan
agent1:                 episode reward: 0.2484,                 loss: 0.1364
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 135.9438s / 6552.1567 s
agent0:                 episode reward: 0.0632,                 loss: nan
agent1:                 episode reward: -0.0632,                 loss: 0.1350
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 136.6903s / 6688.8470 s
agent0:                 episode reward: 0.3496,                 loss: nan
agent1:                 episode reward: -0.3496,                 loss: 0.1346
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.2340s / 6827.0810 s
agent0:                 episode reward: -0.1582,                 loss: nan
agent1:                 episode reward: 0.1582,                 loss: 0.1351
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.3348s / 6967.4158 s
agent0:                 episode reward: 0.0474,                 loss: nan
agent1:                 episode reward: -0.0474,                 loss: 0.1353
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.5580s / 7108.9738 s
agent0:                 episode reward: -0.1664,                 loss: nan
agent1:                 episode reward: 0.1664,                 loss: 0.1363
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 136.2517s / 7245.2255 s
agent0:                 episode reward: 0.1314,                 loss: nan
agent1:                 episode reward: -0.1314,                 loss: 0.1360
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.5011s / 7383.7266 s
agent0:                 episode reward: -0.0362,                 loss: nan
agent1:                 episode reward: 0.0362,                 loss: 0.1358
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.8654s / 7525.5920 s
agent0:                 episode reward: 0.0676,                 loss: nan
agent1:                 episode reward: -0.0676,                 loss: 0.1374
Episode: 1481/30000 (4.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.7120s / 7665.3040 s
agent0:                 episode reward: -0.0759,                 loss: nan
agent1:                 episode reward: 0.0759,                 loss: 0.1359
Episode: 1501/30000 (5.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.1676s / 7802.4715 s
agent0:                 episode reward: -0.1373,                 loss: nan
agent1:                 episode reward: 0.1373,                 loss: 0.1350
Episode: 1521/30000 (5.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.9855s / 7940.4570 s
agent0:                 episode reward: -0.3518,                 loss: nan
agent1:                 episode reward: 0.3518,                 loss: 0.1358
Episode: 1541/30000 (5.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.4864s / 8079.9434 s
agent0:                 episode reward: -0.1035,                 loss: nan
agent1:                 episode reward: 0.1035,                 loss: 0.1379
Episode: 1561/30000 (5.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.3508s / 8217.2942 s
agent0:                 episode reward: 0.1942,                 loss: nan
agent1:                 episode reward: -0.1942,                 loss: 0.1358
Episode: 1581/30000 (5.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 136.8840s / 8354.1782 s
agent0:                 episode reward: 0.0778,                 loss: nan
agent1:                 episode reward: -0.0778,                 loss: 0.1359
Episode: 1601/30000 (5.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.6596s / 8494.8378 s
agent0:                 episode reward: 0.0030,                 loss: nan
agent1:                 episode reward: -0.0030,                 loss: 0.1349
Episode: 1621/30000 (5.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.4286s / 8634.2664 s
agent0:                 episode reward: 0.1143,                 loss: nan
agent1:                 episode reward: -0.1143,                 loss: 0.1361
Episode: 1641/30000 (5.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.1171s / 8773.3835 s
agent0:                 episode reward: -0.4018,                 loss: nan
agent1:                 episode reward: 0.4018,                 loss: 0.1370
Episode: 1661/30000 (5.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 136.7428s / 8910.1263 s
agent0:                 episode reward: 0.4249,                 loss: nan
agent1:                 episode reward: -0.4249,                 loss: 0.1355
Episode: 1681/30000 (5.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.5059s / 9048.6322 s
agent0:                 episode reward: -0.1752,                 loss: nan
agent1:                 episode reward: 0.1752,                 loss: 0.1350
Episode: 1701/30000 (5.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.9565s / 9190.5887 s
agent0:                 episode reward: 0.0493,                 loss: nan
agent1:                 episode reward: -0.0493,                 loss: 0.1366
Episode: 1721/30000 (5.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.1972s / 9328.7859 s
agent0:                 episode reward: -0.1531,                 loss: nan
agent1:                 episode reward: 0.1531,                 loss: 0.1365
Episode: 1741/30000 (5.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 142.3660s / 9471.1519 s
agent0:                 episode reward: 0.2764,                 loss: nan
agent1:                 episode reward: -0.2764,                 loss: 0.1350
Episode: 1761/30000 (5.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 136.1277s / 9607.2796 s
agent0:                 episode reward: 0.2820,                 loss: nan
agent1:                 episode reward: -0.2820,                 loss: 0.1362
Episode: 1781/30000 (5.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.6787s / 9746.9583 s
agent0:                 episode reward: -0.2909,                 loss: nan
agent1:                 episode reward: 0.2909,                 loss: 0.1350
Episode: 1801/30000 (6.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.3350s / 9887.2933 s
agent0:                 episode reward: 0.0142,                 loss: nan
agent1:                 episode reward: -0.0142,                 loss: 0.1345
Episode: 1821/30000 (6.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.0391s / 10024.3323 s
agent0:                 episode reward: -0.3929,                 loss: nan
agent1:                 episode reward: 0.3929,                 loss: 0.1349
Episode: 1841/30000 (6.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.3983s / 10165.7307 s
agent0:                 episode reward: 0.0534,                 loss: nan
agent1:                 episode reward: -0.0534,                 loss: 0.1354
Episode: 1861/30000 (6.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.6652s / 10307.3959 s
agent0:                 episode reward: -0.1943,                 loss: nan
agent1:                 episode reward: 0.1943,                 loss: 0.1355
Episode: 1881/30000 (6.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.8526s / 10445.2485 s
agent0:                 episode reward: 0.0842,                 loss: nan
agent1:                 episode reward: -0.0842,                 loss: 0.1361
Episode: 1901/30000 (6.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.8236s / 10584.0722 s
agent0:                 episode reward: -0.1555,                 loss: nan
agent1:                 episode reward: 0.1555,                 loss: 0.1412
Episode: 1921/30000 (6.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.1328s / 10723.2050 s
agent0:                 episode reward: 0.1643,                 loss: nan
agent1:                 episode reward: -0.1643,                 loss: 0.1411
Episode: 1941/30000 (6.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.6233s / 10861.8283 s
agent0:                 episode reward: -0.0636,                 loss: nan
agent1:                 episode reward: 0.0636,                 loss: 0.1416
Episode: 1961/30000 (6.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 135.2081s / 10997.0364 s
agent0:                 episode reward: -0.2461,                 loss: nan
agent1:                 episode reward: 0.2461,                 loss: 0.1421
Episode: 1981/30000 (6.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.9934s / 11136.0297 s
agent0:                 episode reward: 0.0169,                 loss: nan
agent1:                 episode reward: -0.0169,                 loss: 0.1424
Episode: 2001/30000 (6.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.7848s / 11274.8146 s
agent0:                 episode reward: -0.3753,                 loss: nan
agent1:                 episode reward: 0.3753,                 loss: 0.1407
Episode: 2021/30000 (6.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.4937s / 11413.3083 s
agent0:                 episode reward: -0.5091,                 loss: nan
agent1:                 episode reward: 0.5091,                 loss: 0.1398
Episode: 2041/30000 (6.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 142.1808s / 11555.4890 s
agent0:                 episode reward: -0.2313,                 loss: nan
agent1:                 episode reward: 0.2313,                 loss: 0.1390
Episode: 2061/30000 (6.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 136.2312s / 11691.7203 s
agent0:                 episode reward: 0.1618,                 loss: nan
agent1:                 episode reward: -0.1618,                 loss: 0.1415
Episode: 2081/30000 (6.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.2082s / 11829.9284 s
agent0:                 episode reward: -0.1479,                 loss: nan
agent1:                 episode reward: 0.1479,                 loss: 0.1389
Episode: 2101/30000 (7.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 136.0945s / 11966.0229 s
agent0:                 episode reward: -0.0864,                 loss: nan
agent1:                 episode reward: 0.0864,                 loss: 0.1402
Episode: 2121/30000 (7.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.1479s / 12101.1708 s
agent0:                 episode reward: -0.3281,                 loss: nan
agent1:                 episode reward: 0.3281,                 loss: 0.1419
Episode: 2141/30000 (7.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 142.1269s / 12243.2977 s
agent0:                 episode reward: 0.1014,                 loss: nan
agent1:                 episode reward: -0.1014,                 loss: 0.1399
Episode: 2161/30000 (7.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.1531s / 12383.4508 s
agent0:                 episode reward: -0.2802,                 loss: nan
agent1:                 episode reward: 0.2802,                 loss: 0.1402
Episode: 2181/30000 (7.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 143.0629s / 12526.5137 s
agent0:                 episode reward: -0.1604,                 loss: nan
agent1:                 episode reward: 0.1604,                 loss: 0.1404
Episode: 2201/30000 (7.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.5926s / 12668.1064 s
agent0:                 episode reward: 0.4303,                 loss: nan
agent1:                 episode reward: -0.4303,                 loss: 0.1406
Episode: 2221/30000 (7.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 143.0710s / 12811.1774 s
agent0:                 episode reward: 0.4934,                 loss: nan
agent1:                 episode reward: -0.4934,                 loss: 0.1401
Episode: 2241/30000 (7.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 142.5073s / 12953.6847 s
agent0:                 episode reward: 0.0228,                 loss: nan
agent1:                 episode reward: -0.0228,                 loss: 0.1375
Episode: 2261/30000 (7.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 142.8524s / 13096.5370 s
agent0:                 episode reward: 0.0271,                 loss: nan
agent1:                 episode reward: -0.0271,                 loss: 0.1379
Episode: 2281/30000 (7.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 142.1058s / 13238.6428 s
agent0:                 episode reward: 0.0845,                 loss: nan
agent1:                 episode reward: -0.0845,                 loss: 0.1376
Episode: 2301/30000 (7.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.6810s / 13379.3238 s
agent0:                 episode reward: -0.0700,                 loss: nan
agent1:                 episode reward: 0.0700,                 loss: 0.1380
Episode: 2321/30000 (7.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.3381s / 13520.6619 s
agent0:                 episode reward: 0.0017,                 loss: nan
agent1:                 episode reward: -0.0017,                 loss: 0.1372
Episode: 2341/30000 (7.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 143.5486s / 13664.2106 s
agent0:                 episode reward: 0.0857,                 loss: nan
agent1:                 episode reward: -0.0857,                 loss: 0.1387
Episode: 2361/30000 (7.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.2647s / 13805.4753 s
agent0:                 episode reward: 0.1433,                 loss: nan
agent1:                 episode reward: -0.1433,                 loss: 0.1380
Episode: 2381/30000 (7.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.2622s / 13945.7375 s
agent0:                 episode reward: 0.1089,                 loss: nan
agent1:                 episode reward: -0.1089,                 loss: 0.1377
Episode: 2401/30000 (8.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 143.4702s / 14089.2077 s
agent0:                 episode reward: 0.3015,                 loss: nan
agent1:                 episode reward: -0.3015,                 loss: 0.1391
Episode: 2421/30000 (8.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.4524s / 14230.6601 s
agent0:                 episode reward: 0.4189,                 loss: nan
agent1:                 episode reward: -0.4189,                 loss: 0.1362
Episode: 2441/30000 (8.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.5031s / 14370.1632 s
agent0:                 episode reward: 0.0409,                 loss: nan
agent1:                 episode reward: -0.0409,                 loss: 0.1375
Episode: 2461/30000 (8.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.1106s / 14510.2737 s
agent0:                 episode reward: 0.1106,                 loss: nan
agent1:                 episode reward: -0.1106,                 loss: 0.1372
Episode: 2481/30000 (8.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 143.2369s / 14653.5107 s
agent0:                 episode reward: 0.0973,                 loss: nan
agent1:                 episode reward: -0.0973,                 loss: 0.1378
Episode: 2501/30000 (8.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 142.0784s / 14795.5890 s
agent0:                 episode reward: -0.3443,                 loss: nan
agent1:                 episode reward: 0.3443,                 loss: 0.1385
Episode: 2521/30000 (8.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.9674s / 14936.5564 s
agent0:                 episode reward: -0.1706,                 loss: nan
agent1:                 episode reward: 0.1706,                 loss: 0.1386
Episode: 2541/30000 (8.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 143.3995s / 15079.9559 s
agent0:                 episode reward: 0.0444,                 loss: nan
agent1:                 episode reward: -0.0444,                 loss: 0.1403
Episode: 2561/30000 (8.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.3036s / 15221.2596 s
agent0:                 episode reward: -0.2667,                 loss: nan
agent1:                 episode reward: 0.2667,                 loss: 0.1379
Episode: 2581/30000 (8.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 143.6082s / 15364.8678 s
agent0:                 episode reward: -0.2354,                 loss: nan
agent1:                 episode reward: 0.2354,                 loss: 0.1373
Episode: 2601/30000 (8.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 142.2557s / 15507.1235 s
agent0:                 episode reward: -0.0505,                 loss: nan
agent1:                 episode reward: 0.0505,                 loss: 0.1380
Episode: 2621/30000 (8.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.4351s / 15648.5586 s
agent0:                 episode reward: 0.0379,                 loss: nan
agent1:                 episode reward: -0.0379,                 loss: 0.1368
Episode: 2641/30000 (8.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 143.5243s / 15792.0830 s
agent0:                 episode reward: 0.2191,                 loss: nan
agent1:                 episode reward: -0.2191,                 loss: 0.1359
Episode: 2661/30000 (8.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.6527s / 15933.7357 s
agent0:                 episode reward: 0.5106,                 loss: nan
agent1:                 episode reward: -0.5106,                 loss: 0.1363
Episode: 2681/30000 (8.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 142.9753s / 16076.7110 s
agent0:                 episode reward: -0.0717,                 loss: nan
agent1:                 episode reward: 0.0717,                 loss: 0.1356
Episode: 2701/30000 (9.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 144.7262s / 16221.4372 s
agent0:                 episode reward: 0.2066,                 loss: nan
agent1:                 episode reward: -0.2066,                 loss: 0.1361
Episode: 2721/30000 (9.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.6531s / 16362.0903 s
agent0:                 episode reward: -0.0309,                 loss: nan
agent1:                 episode reward: 0.0309,                 loss: 0.1352
Episode: 2741/30000 (9.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.9267s / 16502.0170 s
agent0:                 episode reward: -0.3200,                 loss: nan
agent1:                 episode reward: 0.3200,                 loss: 0.1353
Episode: 2761/30000 (9.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.1223s / 16643.1394 s
agent0:                 episode reward: 0.0142,                 loss: nan
agent1:                 episode reward: -0.0142,                 loss: 0.1351
Episode: 2781/30000 (9.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.6160s / 16781.7554 s
agent0:                 episode reward: 0.1039,                 loss: nan
agent1:                 episode reward: -0.1039,                 loss: 0.1365
Episode: 2801/30000 (9.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 143.4657s / 16925.2210 s
agent0:                 episode reward: -0.1902,                 loss: nan
agent1:                 episode reward: 0.1902,                 loss: 0.1369
Episode: 2821/30000 (9.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.6300s / 17063.8511 s
agent0:                 episode reward: -0.0978,                 loss: nan
agent1:                 episode reward: 0.0978,                 loss: 0.1361
Episode: 2841/30000 (9.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.9418s / 17203.7929 s
agent0:                 episode reward: -0.3800,                 loss: nan
agent1:                 episode reward: 0.3800,                 loss: 0.1347
Episode: 2861/30000 (9.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 142.5194s / 17346.3123 s
agent0:                 episode reward: 0.1414,                 loss: nan
agent1:                 episode reward: -0.1414,                 loss: 0.1334
Episode: 2881/30000 (9.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.8178s / 17488.1301 s
agent0:                 episode reward: -0.3219,                 loss: nan
agent1:                 episode reward: 0.3219,                 loss: 0.1366
Episode: 2901/30000 (9.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.1799s / 17627.3101 s
agent0:                 episode reward: 0.3441,                 loss: nan
agent1:                 episode reward: -0.3441,                 loss: 0.1364
Episode: 2921/30000 (9.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.6679s / 17767.9780 s
agent0:                 episode reward: 0.0062,                 loss: nan
agent1:                 episode reward: -0.0062,                 loss: 0.1360
Episode: 2941/30000 (9.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.1525s / 17908.1304 s
agent0:                 episode reward: -0.6627,                 loss: nan
agent1:                 episode reward: 0.6627,                 loss: 0.1362
Episode: 2961/30000 (9.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.1900s / 18049.3204 s
agent0:                 episode reward: -0.3510,                 loss: nan
agent1:                 episode reward: 0.3510,                 loss: 0.1365
Episode: 2981/30000 (9.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.3411s / 18189.6615 s
agent0:                 episode reward: -0.2425,                 loss: nan
agent1:                 episode reward: 0.2425,                 loss: 0.1350
Episode: 3001/30000 (10.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.9492s / 18331.6107 s
agent0:                 episode reward: -0.2348,                 loss: nan
agent1:                 episode reward: 0.2348,                 loss: 0.1361
Episode: 3021/30000 (10.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 143.7892s / 18475.3999 s
agent0:                 episode reward: 0.1662,                 loss: nan
agent1:                 episode reward: -0.1662,                 loss: 0.1362
Episode: 3041/30000 (10.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 137.9589s / 18613.3588 s
agent0:                 episode reward: -0.2362,                 loss: nan
agent1:                 episode reward: 0.2362,                 loss: 0.1359
Episode: 3061/30000 (10.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.3725s / 18753.7312 s
agent0:                 episode reward: -0.4028,                 loss: nan
agent1:                 episode reward: 0.4028,                 loss: 0.1361
Episode: 3081/30000 (10.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 142.5211s / 18896.2524 s
agent0:                 episode reward: -0.2768,                 loss: nan
agent1:                 episode reward: 0.2768,                 loss: 0.1356
Episode: 3101/30000 (10.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.6775s / 19034.9299 s
agent0:                 episode reward: -0.2502,                 loss: nan
agent1:                 episode reward: 0.2502,                 loss: 0.1351
Episode: 3121/30000 (10.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.4914s / 19176.4213 s
agent0:                 episode reward: 0.0505,                 loss: nan