pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
random seed: 91
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7fd8a5789f50>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.249 0.11  0.088 ... 0.    0.    0.   ]
 [0.042 0.025 0.037 ... 0.    0.    0.   ]]
Load checkpoints (policy family):  [['83' '5753' '6419' ... '57305' '57555' '59346']
 ['121' '6342' '6627' ... '57352' '57708' '59367']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220116204408/epi_60000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220116204408_exploit_60000/mdp_arbitrary_mdp_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220116204408_exploit_60000/mdp_arbitrary_mdp_nxdo2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3684s / 1.3684 s
agent0:                 episode reward: -1.1373,                 loss: nan
agent1:                 episode reward: 1.1373,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3324s / 1.7008 s
agent0:                 episode reward: 0.0971,                 loss: nan
agent1:                 episode reward: -0.0971,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1891s / 1.8898 s
agent0:                 episode reward: 0.0704,                 loss: nan
agent1:                 episode reward: -0.0704,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0990s / 1.9888 s
agent0:                 episode reward: -0.3420,                 loss: nan
agent1:                 episode reward: 0.3420,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2163s / 2.2051 s
agent0:                 episode reward: -0.3334,                 loss: nan
agent1:                 episode reward: 0.3334,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.6456s / 2.8507 s
agent0:                 episode reward: -0.1505,                 loss: nan
agent1:                 episode reward: 0.1505,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.6018s / 3.4524 s
agent0:                 episode reward: 0.1557,                 loss: nan
agent1:                 episode reward: -0.1557,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2660s / 3.7185 s
agent0:                 episode reward: -0.4993,                 loss: nan
agent1:                 episode reward: 0.4993,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.5698s / 4.2883 s
agent0:                 episode reward: -0.0269,                 loss: nan
agent1:                 episode reward: 0.0269,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0731s / 5.3614 s
agent0:                 episode reward: 0.5043,                 loss: nan
agent1:                 episode reward: -0.5043,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4646s / 6.8260 s
agent0:                 episode reward: -0.1281,                 loss: nan
agent1:                 episode reward: 0.1281,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 36.8299s / 43.6559 s
agent0:                 episode reward: -0.1006,                 loss: nan
agent1:                 episode reward: 0.1006,                 loss: 0.2391
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 94.7651s / 138.4210 s
agent0:                 episode reward: -0.1866,                 loss: nan
agent1:                 episode reward: 0.1866,                 loss: 0.2104
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 98.7724s / 237.1935 s
agent0:                 episode reward: -0.2444,                 loss: nan
agent1:                 episode reward: 0.2444,                 loss: 0.1838
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 99.6763s / 336.8698 s
agent0:                 episode reward: 0.5944,                 loss: nan
agent1:                 episode reward: -0.5944,                 loss: 0.1688
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 100.5768s / 437.4466 s
agent0:                 episode reward: 0.1129,                 loss: nan
agent1:                 episode reward: -0.1129,                 loss: 0.1655
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 96.8641s / 534.3107 s
agent0:                 episode reward: 0.0642,                 loss: nan
agent1:                 episode reward: -0.0642,                 loss: 0.1641
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 99.6752s / 633.9859 s
agent0:                 episode reward: -0.0052,                 loss: nan
agent1:                 episode reward: 0.0052,                 loss: 0.1637
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 98.5040s / 732.4898 s
agent0:                 episode reward: 0.3750,                 loss: nan
agent1:                 episode reward: -0.3750,                 loss: 0.1629
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 95.6532s / 828.1430 s
agent0:                 episode reward: 0.4842,                 loss: nan
agent1:                 episode reward: -0.4842,                 loss: 0.1610
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 98.6298s / 926.7728 s
agent0:                 episode reward: 0.3516,                 loss: nan
agent1:                 episode reward: -0.3516,                 loss: 0.1602
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 101.2534s / 1028.0262 s
agent0:                 episode reward: 0.1649,                 loss: nan
agent1:                 episode reward: -0.1649,                 loss: 0.1592
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 102.4536s / 1130.4798 s
agent0:                 episode reward: 0.0541,                 loss: nan
agent1:                 episode reward: -0.0541,                 loss: 0.1565
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 103.1436s / 1233.6234 s
agent0:                 episode reward: 0.1290,                 loss: nan
agent1:                 episode reward: -0.1290,                 loss: 0.1576
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 110.8196s / 1344.4430 s
agent0:                 episode reward: -0.6373,                 loss: nan
agent1:                 episode reward: 0.6373,                 loss: 0.1576
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 109.4462s / 1453.8893 s
agent0:                 episode reward: 0.1074,                 loss: nan
agent1:                 episode reward: -0.1074,                 loss: 0.1568
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 105.2036s / 1559.0929 s
agent0:                 episode reward: 0.2977,                 loss: nan
agent1:                 episode reward: -0.2977,                 loss: 0.1570
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 104.8636s / 1663.9565 s
agent0:                 episode reward: -0.1812,                 loss: nan
agent1:                 episode reward: 0.1812,                 loss: 0.1564
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 108.4787s / 1772.4352 s
agent0:                 episode reward: -0.0444,                 loss: nan
agent1:                 episode reward: 0.0444,                 loss: 0.1744
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 106.3418s / 1878.7769 s
agent0:                 episode reward: 0.0398,                 loss: nan
agent1:                 episode reward: -0.0398,                 loss: 0.1664
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 108.6952s / 1987.4722 s
agent0:                 episode reward: 0.1533,                 loss: nan
agent1:                 episode reward: -0.1533,                 loss: 0.1629
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 109.5471s / 2097.0193 s
agent0:                 episode reward: -0.2068,                 loss: nan
agent1:                 episode reward: 0.2068,                 loss: 0.1612
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 108.6502s / 2205.6695 s
agent0:                 episode reward: 0.2120,                 loss: nan
agent1:                 episode reward: -0.2120,                 loss: 0.1598
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 107.6494s / 2313.3189 s
agent0:                 episode reward: 0.0961,                 loss: nan
agent1:                 episode reward: -0.0961,                 loss: 0.1606
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 108.2091s / 2421.5279 s
agent0:                 episode reward: 0.3292,                 loss: nan
agent1:                 episode reward: -0.3292,                 loss: 0.1606
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 109.9387s / 2531.4666 s
agent0:                 episode reward: 0.1833,                 loss: nan
agent1:                 episode reward: -0.1833,                 loss: 0.1599
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 107.8275s / 2639.2941 s
agent0:                 episode reward: 0.0680,                 loss: nan
agent1:                 episode reward: -0.0680,                 loss: 0.1603
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 133.1863s / 2772.4804 s
agent0:                 episode reward: -0.3686,                 loss: nan
agent1:                 episode reward: 0.3686,                 loss: 0.1595
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 236.4125s / 3008.8930 s
agent0:                 episode reward: 0.2993,                 loss: nan
agent1:                 episode reward: -0.2993,                 loss: 0.1580
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 243.4330s / 3252.3260 s
agent0:                 episode reward: 0.0556,                 loss: nan
agent1:                 episode reward: -0.0556,                 loss: 0.1577
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 235.3822s / 3487.7082 s
agent0:                 episode reward: 0.1025,                 loss: nan
agent1:                 episode reward: -0.1025,                 loss: 0.1592
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 241.4822s / 3729.1904 s
agent0:                 episode reward: -0.2027,                 loss: nan
agent1:                 episode reward: 0.2027,                 loss: 0.1586
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.1069s / 3975.2973 s
agent0:                 episode reward: -0.0093,                 loss: nan
agent1:                 episode reward: 0.0093,                 loss: 0.1588
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.6082s / 4215.9055 s
agent0:                 episode reward: 0.2601,                 loss: nan
agent1:                 episode reward: -0.2601,                 loss: 0.1575
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 239.0627s / 4454.9683 s
agent0:                 episode reward: 0.3590,                 loss: nan
agent1:                 episode reward: -0.3590,                 loss: 0.1592
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 238.5692s / 4693.5375 s
agent0:                 episode reward: -0.1778,                 loss: nan
agent1:                 episode reward: 0.1778,                 loss: 0.1616
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.8981s / 4939.4356 s
agent0:                 episode reward: 0.0474,                 loss: nan
agent1:                 episode reward: -0.0474,                 loss: 0.1590
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 235.0386s / 5174.4742 s
agent0:                 episode reward: -0.2986,                 loss: nan
agent1:                 episode reward: 0.2986,                 loss: 0.1598
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.6703s / 5419.1445 s
agent0:                 episode reward: -0.5917,                 loss: nan
agent1:                 episode reward: 0.5917,                 loss: 0.1595
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 233.3643s / 5652.5088 s
agent0:                 episode reward: -0.1548,                 loss: nan
agent1:                 episode reward: 0.1548,                 loss: 0.1598
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 249.3841s / 5901.8929 s
agent0:                 episode reward: 0.3084,                 loss: nan
agent1:                 episode reward: -0.3084,                 loss: 0.1600
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 239.4885s / 6141.3814 s
agent0:                 episode reward: -0.0595,                 loss: nan
agent1:                 episode reward: 0.0595,                 loss: 0.1582
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 236.3274s / 6377.7088 s
agent0:                 episode reward: -0.3168,                 loss: nan
agent1:                 episode reward: 0.3168,                 loss: 0.1593
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 250.9899s / 6628.6987 s
agent0:                 episode reward: -0.2006,                 loss: nan
agent1:                 episode reward: 0.2006,                 loss: 0.1592
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 243.8345s / 6872.5332 s
agent0:                 episode reward: 0.5148,                 loss: nan
agent1:                 episode reward: -0.5148,                 loss: 0.1582
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 237.9767s / 7110.5099 s
agent0:                 episode reward: 0.1668,                 loss: nan
agent1:                 episode reward: -0.1668,                 loss: 0.1594
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 250.2862s / 7360.7961 s
agent0:                 episode reward: -0.0329,                 loss: nan
agent1:                 episode reward: 0.0329,                 loss: 0.1588
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 232.3455s / 7593.1416 s
agent0:                 episode reward: -0.0679,                 loss: nan
agent1:                 episode reward: 0.0679,                 loss: 0.1583
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.1988s / 7835.3404 s
agent0:                 episode reward: 0.1869,                 loss: nan
agent1:                 episode reward: -0.1869,                 loss: 0.1594
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.8037s / 8079.1440 s
agent0:                 episode reward: -0.0463,                 loss: nan
agent1:                 episode reward: 0.0463,                 loss: 0.1588
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.6349s / 8323.7790 s
agent0:                 episode reward: -0.1119,                 loss: nan
agent1:                 episode reward: 0.1119,                 loss: 0.1609
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.8502s / 8564.6292 s
agent0:                 episode reward: 0.3157,                 loss: nan
agent1:                 episode reward: -0.3157,                 loss: 0.1592
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 238.7593s / 8803.3884 s
agent0:                 episode reward: 0.1453,                 loss: nan
agent1:                 episode reward: -0.1453,                 loss: 0.1562
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 250.1274s / 9053.5158 s
agent0:                 episode reward: -0.0670,                 loss: nan
agent1:                 episode reward: 0.0670,                 loss: 0.1572
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.8252s / 9294.3410 s
agent0:                 episode reward: 0.0174,                 loss: nan
agent1:                 episode reward: -0.0174,                 loss: 0.1564
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 238.0388s / 9532.3799 s
agent0:                 episode reward: -0.2504,                 loss: nan
agent1:                 episode reward: 0.2504,                 loss: 0.1560
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 238.1118s / 9770.4917 s
agent0:                 episode reward: -0.4460,                 loss: nan
agent1:                 episode reward: 0.4460,                 loss: 0.1569
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.2645s / 10014.7561 s
agent0:                 episode reward: 0.0141,                 loss: nan
agent1:                 episode reward: -0.0141,                 loss: 0.1564
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 240.4306s / 10255.1867 s
agent0:                 episode reward: 0.1639,                 loss: nan
agent1:                 episode reward: -0.1639,                 loss: 0.1549
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 238.8411s / 10494.0278 s
agent0:                 episode reward: -0.2072,                 loss: nan
agent1:                 episode reward: 0.2072,                 loss: 0.1545
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.3568s / 10740.3846 s
agent0:                 episode reward: -0.0465,                 loss: nan
agent1:                 episode reward: 0.0465,                 loss: 0.1549
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 241.9093s / 10982.2938 s
agent0:                 episode reward: -0.1861,                 loss: nan
agent1:                 episode reward: 0.1861,                 loss: 0.1557
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.5830s / 11223.8769 s
agent0:                 episode reward: -0.1405,                 loss: nan
agent1:                 episode reward: 0.1405,                 loss: 0.1542
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 243.9067s / 11467.7835 s
agent0:                 episode reward: 0.1482,                 loss: nan
agent1:                 episode reward: -0.1482,                 loss: 0.1541
Episode: 1481/30000 (4.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.6055s / 11710.3890 s
agent0:                 episode reward: -0.2303,                 loss: nan
agent1:                 episode reward: 0.2303,                 loss: 0.1539
Episode: 1501/30000 (5.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 250.7455s / 11961.1346 s
agent0:                 episode reward: -0.2061,                 loss: nan
agent1:                 episode reward: 0.2061,                 loss: 0.1552
Episode: 1521/30000 (5.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 238.7532s / 12199.8878 s
agent0:                 episode reward: 0.3883,                 loss: nan
agent1:                 episode reward: -0.3883,                 loss: 0.1541
Episode: 1541/30000 (5.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 248.8238s / 12448.7116 s
agent0:                 episode reward: -0.3304,                 loss: nan
agent1:                 episode reward: 0.3304,                 loss: 0.1554
Episode: 1561/30000 (5.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.2188s / 12692.9304 s
agent0:                 episode reward: -0.1356,                 loss: nan
agent1:                 episode reward: 0.1356,                 loss: 0.1584
Episode: 1581/30000 (5.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.4353s / 12933.3657 s
agent0:                 episode reward: -0.4278,                 loss: nan
agent1:                 episode reward: 0.4278,                 loss: 0.1592
Episode: 1601/30000 (5.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 238.9616s / 13172.3273 s
agent0:                 episode reward: -0.0266,                 loss: nan
agent1:                 episode reward: 0.0266,                 loss: 0.1620
Episode: 1621/30000 (5.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 249.3421s / 13421.6694 s
agent0:                 episode reward: 0.1569,                 loss: nan
agent1:                 episode reward: -0.1569,                 loss: 0.1612
Episode: 1641/30000 (5.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 235.1374s / 13656.8068 s
agent0:                 episode reward: 0.1974,                 loss: nan
agent1:                 episode reward: -0.1974,                 loss: 0.1611
Episode: 1661/30000 (5.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 236.3284s / 13893.1352 s
agent0:                 episode reward: 0.0682,                 loss: nan
agent1:                 episode reward: -0.0682,                 loss: 0.1620
Episode: 1681/30000 (5.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.6391s / 14134.7743 s
agent0:                 episode reward: -0.0796,                 loss: nan
agent1:                 episode reward: 0.0796,                 loss: 0.1606
Episode: 1701/30000 (5.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.1141s / 14381.8884 s
agent0:                 episode reward: 0.1180,                 loss: nan
agent1:                 episode reward: -0.1180,                 loss: 0.1610
Episode: 1721/30000 (5.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 241.9209s / 14623.8092 s
agent0:                 episode reward: -0.4081,                 loss: nan
agent1:                 episode reward: 0.4081,                 loss: 0.1615
Episode: 1741/30000 (5.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 236.5628s / 14860.3720 s
agent0:                 episode reward: 0.2287,                 loss: nan
agent1:                 episode reward: -0.2287,                 loss: 0.1604
Episode: 1761/30000 (5.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 236.2075s / 15096.5795 s
agent0:                 episode reward: -0.3086,                 loss: nan
agent1:                 episode reward: 0.3086,                 loss: 0.1595
Episode: 1781/30000 (5.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 249.6902s / 15346.2697 s
agent0:                 episode reward: 0.1473,                 loss: nan
agent1:                 episode reward: -0.1473,                 loss: 0.1606
Episode: 1801/30000 (6.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.9662s / 15587.2359 s
agent0:                 episode reward: -0.2892,                 loss: nan
agent1:                 episode reward: 0.2892,                 loss: 0.1615
Episode: 1821/30000 (6.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 238.8789s / 15826.1148 s
agent0:                 episode reward: 0.2177,                 loss: nan
agent1:                 episode reward: -0.2177,                 loss: 0.1618
Episode: 1841/30000 (6.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.2014s / 16068.3162 s
agent0:                 episode reward: 0.1278,                 loss: nan
agent1:                 episode reward: -0.1278,                 loss: 0.1611
Episode: 1861/30000 (6.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.6029s / 16312.9191 s
agent0:                 episode reward: -0.1604,                 loss: nan
agent1:                 episode reward: 0.1604,                 loss: 0.1613
Episode: 1881/30000 (6.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.9032s / 16559.8223 s
agent0:                 episode reward: -0.2794,                 loss: nan
agent1:                 episode reward: 0.2794,                 loss: 0.1607
Episode: 1901/30000 (6.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 247.9731s / 16807.7954 s
agent0:                 episode reward: -0.1289,                 loss: nan
agent1:                 episode reward: 0.1289,                 loss: 0.1583
Episode: 1921/30000 (6.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.5372s / 17055.3326 s
agent0:                 episode reward: -0.0981,                 loss: nan
agent1:                 episode reward: 0.0981,                 loss: 0.1590
Episode: 1941/30000 (6.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.0721s / 17297.4047 s
agent0:                 episode reward: -0.3968,                 loss: nan
agent1:                 episode reward: 0.3968,                 loss: 0.1567
Episode: 1961/30000 (6.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 235.5398s / 17532.9445 s
agent0:                 episode reward: 0.0798,                 loss: nan
agent1:                 episode reward: -0.0798,                 loss: 0.1584
Episode: 1981/30000 (6.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 243.5137s / 17776.4582 s
agent0:                 episode reward: 0.1652,                 loss: nan
agent1:                 episode reward: -0.1652,                 loss: 0.1580
Episode: 2001/30000 (6.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.9316s / 18022.3898 s
agent0:                 episode reward: 0.4585,                 loss: nan
agent1:                 episode reward: -0.4585,                 loss: 0.1578
Episode: 2021/30000 (6.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.6416s / 18268.0314 s