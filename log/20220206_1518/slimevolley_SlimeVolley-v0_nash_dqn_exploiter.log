pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 2, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 5, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000, 'exploiter_update_itr': 1}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220206_1518/slimevolley_SlimeVolley-v0_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/20220206_1518/slimevolley_SlimeVolley-v0_nash_dqn_exploiter.
Episode: 1/10000 (0.0100%),                 avg. length: 507.0,                last time consumption/overall running time: 16.6935s / 16.6935 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0277
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0302
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 572.0,                last time consumption/overall running time: 486.7100s / 503.4036 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0213
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0211
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 586.05,                last time consumption/overall running time: 516.3323s / 1019.7358 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0255
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0244
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 523.0,                last time consumption/overall running time: 480.9021s / 1500.6379 s
env0_first_0:                 episode reward: 1.5000,                 loss: 0.0253
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0245
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 577.25,                last time consumption/overall running time: 551.8096s / 2052.4475 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0255
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0251
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 589.6,                last time consumption/overall running time: 589.1797s / 2641.6272 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0248
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0250
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 607.8,                last time consumption/overall running time: 635.9180s / 3277.5452 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0245
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0241
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 521.95,                last time consumption/overall running time: 570.0863s / 3847.6315 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0239
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0238
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 548.35,                last time consumption/overall running time: 627.7265s / 4475.3580 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0234
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0232
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 590.5,                last time consumption/overall running time: 704.4906s / 5179.8486 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0234
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0234
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 553.6,                last time consumption/overall running time: 668.1589s / 5848.0075 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0238
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0235
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 585.6,                last time consumption/overall running time: 709.1374s / 6557.1449 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0239
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0244
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 547.55,                last time consumption/overall running time: 665.7905s / 7222.9354 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0240
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0242
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 591.65,                last time consumption/overall running time: 716.6703s / 7939.6057 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0248
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0246
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 602.75,                last time consumption/overall running time: 730.8330s / 8670.4387 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0248
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0251
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 557.7,                last time consumption/overall running time: 674.1517s / 9344.5904 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0242
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0248
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 603.65,                last time consumption/overall running time: 729.0960s / 10073.6864 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0241
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0247
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 607.6,                last time consumption/overall running time: 734.5237s / 10808.2102 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0239
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0248
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 580.6,                last time consumption/overall running time: 702.2226s / 11510.4327 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0240
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0250
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 631.75,                last time consumption/overall running time: 763.6269s / 12274.0596 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0243
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0255
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 552.25,                last time consumption/overall running time: 668.7127s / 12942.7723 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0242
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0249
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 626.5,                last time consumption/overall running time: 763.6555s / 13706.4278 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0248
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0244
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 614.7,                last time consumption/overall running time: 749.0540s / 14455.4818 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0246
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0253
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 548.75,                last time consumption/overall running time: 675.8280s / 15131.3098 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0249
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0250
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 551.05,                last time consumption/overall running time: 677.2192s / 15808.5289 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0250
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0252
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 589.6,                last time consumption/overall running time: 725.3835s / 16533.9124 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0249
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0243
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 571.15,                last time consumption/overall running time: 695.2004s / 17229.1129 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0249
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0249
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 619.95,                last time consumption/overall running time: 758.7023s / 17987.8152 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0243
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0243
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 603.45,                last time consumption/overall running time: 732.5204s / 18720.3356 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0242
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0244
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 604.4,                last time consumption/overall running time: 738.4128s / 19458.7484 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0239
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0242
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 594.8,                last time consumption/overall running time: 727.3058s / 20186.0542 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0242
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0244
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 636.45,                last time consumption/overall running time: 781.5234s / 20967.5776 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0247
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0242
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 612.05,                last time consumption/overall running time: 748.9458s / 21716.5234 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0240
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0236
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 658.7,                last time consumption/overall running time: 809.1790s / 22525.7024 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0236
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0239
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 608.75,                last time consumption/overall running time: 746.4211s / 23272.1235 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0240
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0237
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 622.35,                last time consumption/overall running time: 764.1385s / 24036.2620 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0244
env0_second_0:                 episode reward: 0.6000,                 loss: 0.0239
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 593.2,                last time consumption/overall running time: 729.1859s / 24765.4479 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0238
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0242
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 593.6,                last time consumption/overall running time: 729.0030s / 25494.4510 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0239
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0240
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 622.05,                last time consumption/overall running time: 765.6543s / 26260.1052 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0239
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0235
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 627.5,                last time consumption/overall running time: 772.8622s / 27032.9674 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0244
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0238
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 610.6,                last time consumption/overall running time: 751.9850s / 27784.9524 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0243
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0235
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 639.55,                last time consumption/overall running time: 788.7983s / 28573.7507 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0238
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0237
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 611.85,                last time consumption/overall running time: 753.2991s / 29327.0498 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0231
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0238
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 633.85,                last time consumption/overall running time: 777.2348s / 30104.2847 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0238
env0_second_0:                 episode reward: 1.6500,                 loss: 0.0231
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 668.75,                last time consumption/overall running time: 831.9281s / 30936.2128 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0235
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0239
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 629.95,                last time consumption/overall running time: 781.3794s / 31717.5922 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0233
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0236
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 653.85,                last time consumption/overall running time: 812.5151s / 32530.1074 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0230
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0239
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 654.6,                last time consumption/overall running time: 810.3153s / 33340.4226 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0232
env0_second_0:                 episode reward: 2.1000,                 loss: 0.0241
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 643.05,                last time consumption/overall running time: 793.9858s / 34134.4084 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0235
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0240
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 684.1,                last time consumption/overall running time: 846.4180s / 34980.8265 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0239
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0239
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 625.45,                last time consumption/overall running time: 776.5368s / 35757.3632 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0237
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0235
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 617.7,                last time consumption/overall running time: 767.8397s / 36525.2030 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0227
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0232
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 646.75,                last time consumption/overall running time: 800.7625s / 37325.9655 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0224
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0230
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 625.35,                last time consumption/overall running time: 774.7345s / 38100.7000 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0231
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0233
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 607.85,                last time consumption/overall running time: 754.5196s / 38855.2195 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0226
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0228
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 668.05,                last time consumption/overall running time: 832.3127s / 39687.5323 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0223
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0231
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 638.8,                last time consumption/overall running time: 789.8479s / 40477.3802 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0223
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0233
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 675.95,                last time consumption/overall running time: 841.2522s / 41318.6324 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0224
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0226
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 643.5,                last time consumption/overall running time: 798.0174s / 42116.6498 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0225
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0229
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 577.75,                last time consumption/overall running time: 717.8020s / 42834.4518 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0223
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0228
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 603.15,                last time consumption/overall running time: 751.3520s / 43585.8038 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0218
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0223
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 640.2,                last time consumption/overall running time: 800.9769s / 44386.7808 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0222
env0_second_0:                 episode reward: 2.9000,                 loss: 0.0226
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 595.3,                last time consumption/overall running time: 740.0015s / 45126.7823 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0214
env0_second_0:                 episode reward: 2.2500,                 loss: 0.0225
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 632.8,                last time consumption/overall running time: 786.1499s / 45912.9322 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0218
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0221
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 634.75,                last time consumption/overall running time: 790.6300s / 46703.5622 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0215
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0219
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 717.7,                last time consumption/overall running time: 897.3274s / 47600.8896 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0213
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0217
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 680.05,                last time consumption/overall running time: 852.2727s / 48453.1623 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0213
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0218
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 618.1,                last time consumption/overall running time: 770.8066s / 49223.9690 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0214
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0216
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 620.95,                last time consumption/overall running time: 771.1240s / 49995.0929 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0212
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0216
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 654.75,                last time consumption/overall running time: 818.0642s / 50813.1571 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0217
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0213
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 565.75,                last time consumption/overall running time: 705.8458s / 51519.0029 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0208
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0206
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 635.35,                last time consumption/overall running time: 789.1006s / 52308.1034 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0214
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0209
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 719.1,                last time consumption/overall running time: 893.8641s / 53201.9676 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0214
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0206
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 543.1,                last time consumption/overall running time: 679.6464s / 53881.6140 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0206
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0204
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 597.95,                last time consumption/overall running time: 748.6690s / 54630.2830 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0205
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0208
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 619.35,                last time consumption/overall running time: 778.8410s / 55409.1240 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0197
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0199
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 572.1,                last time consumption/overall running time: 721.2519s / 56130.3760 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0197
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0198
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 580.1,                last time consumption/overall running time: 731.1628s / 56861.5387 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.0194
env0_second_0:                 episode reward: 2.8500,                 loss: 0.0193
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 567.05,                last time consumption/overall running time: 711.0780s / 57572.6167 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0191
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0197
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 590.65,                last time consumption/overall running time: 738.1414s / 58310.7581 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0194
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0200
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 606.45,                last time consumption/overall running time: 760.3987s / 59071.1569 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0192
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0194
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 628.7,                last time consumption/overall running time: 789.0200s / 59860.1769 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0192
env0_second_0:                 episode reward: 2.5500,                 loss: 0.0196
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 650.85,                last time consumption/overall running time: 821.4361s / 60681.6130 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0192
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0196
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 658.15,                last time consumption/overall running time: 823.2274s / 61504.8404 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0189
env0_second_0:                 episode reward: 2.5000,                 loss: 0.0196
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 596.6,                last time consumption/overall running time: 743.0662s / 62247.9066 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0187
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0197
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 598.15,                last time consumption/overall running time: 752.8925s / 63000.7991 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0187
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0194
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 599.45,                last time consumption/overall running time: 752.5113s / 63753.3103 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0188
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0193
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 564.8,                last time consumption/overall running time: 708.4950s / 64461.8053 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0188
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0192
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 618.3,                last time consumption/overall running time: 776.2688s / 65238.0741 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0189
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0190
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 630.45,                last time consumption/overall running time: 789.1142s / 66027.1884 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0183
env0_second_0:                 episode reward: 3.2000,                 loss: 0.0195
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 597.8,                last time consumption/overall running time: 748.2380s / 66775.4263 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0187
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0199
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 610.65,                last time consumption/overall running time: 765.6824s / 67541.1087 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0187
env0_second_0:                 episode reward: 3.0000,                 loss: 0.0196
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 601.8,                last time consumption/overall running time: 763.1435s / 68304.2522 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0187
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0192
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 618.15,                last time consumption/overall running time: 785.7147s / 69089.9669 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0188
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0191
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 605.2,                last time consumption/overall running time: 766.0332s / 69856.0001 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0186
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0186
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 610.4,                last time consumption/overall running time: 771.4355s / 70627.4356 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0187
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0189
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 614.9,                last time consumption/overall running time: 776.5604s / 71403.9961 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0185
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0187
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 711.35,                last time consumption/overall running time: 892.2587s / 72296.2547 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0178
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0185
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 639.0,                last time consumption/overall running time: 806.3905s / 73102.6452 s