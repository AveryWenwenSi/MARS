pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 2, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 5, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000, 'exploiter_update_itr': 1}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220206_1518/slimevolley_SlimeVolley-v0_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/20220206_1518/slimevolley_SlimeVolley-v0_nash_dqn_exploiter.
Episode: 1/10000 (0.0100%),                 avg. length: 507.0,                last time consumption/overall running time: 16.6935s / 16.6935 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0277
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0302
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 572.0,                last time consumption/overall running time: 486.7100s / 503.4036 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0213
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0211
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 586.05,                last time consumption/overall running time: 516.3323s / 1019.7358 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0255
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0244
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 523.0,                last time consumption/overall running time: 480.9021s / 1500.6379 s
env0_first_0:                 episode reward: 1.5000,                 loss: 0.0253
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0245
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 577.25,                last time consumption/overall running time: 551.8096s / 2052.4475 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0255
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0251
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 589.6,                last time consumption/overall running time: 589.1797s / 2641.6272 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0248
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0250
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 607.8,                last time consumption/overall running time: 635.9180s / 3277.5452 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0245
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0241
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 521.95,                last time consumption/overall running time: 570.0863s / 3847.6315 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0239
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0238
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 548.35,                last time consumption/overall running time: 627.7265s / 4475.3580 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0234
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0232
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 590.5,                last time consumption/overall running time: 704.4906s / 5179.8486 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0234
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0234
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 553.6,                last time consumption/overall running time: 668.1589s / 5848.0075 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0238
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0235
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 585.6,                last time consumption/overall running time: 709.1374s / 6557.1449 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0239
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0244
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 547.55,                last time consumption/overall running time: 665.7905s / 7222.9354 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0240
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0242
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 591.65,                last time consumption/overall running time: 716.6703s / 7939.6057 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0248
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0246
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 602.75,                last time consumption/overall running time: 730.8330s / 8670.4387 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0248
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0251
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 557.7,                last time consumption/overall running time: 674.1517s / 9344.5904 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0242
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0248
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 603.65,                last time consumption/overall running time: 729.0960s / 10073.6864 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0241
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0247
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 607.6,                last time consumption/overall running time: 734.5237s / 10808.2102 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0239
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0248
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 580.6,                last time consumption/overall running time: 702.2226s / 11510.4327 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0240
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0250
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 631.75,                last time consumption/overall running time: 763.6269s / 12274.0596 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0243
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0255
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 552.25,                last time consumption/overall running time: 668.7127s / 12942.7723 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0242
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0249
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 626.5,                last time consumption/overall running time: 763.6555s / 13706.4278 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0248
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0244
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 614.7,                last time consumption/overall running time: 749.0540s / 14455.4818 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0246
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0253
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 548.75,                last time consumption/overall running time: 675.8280s / 15131.3098 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0249
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0250
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 551.05,                last time consumption/overall running time: 677.2192s / 15808.5289 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0250
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0252
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 589.6,                last time consumption/overall running time: 725.3835s / 16533.9124 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0249
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0243
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 571.15,                last time consumption/overall running time: 695.2004s / 17229.1129 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0249
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0249
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 619.95,                last time consumption/overall running time: 758.7023s / 17987.8152 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0243
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0243
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 603.45,                last time consumption/overall running time: 732.5204s / 18720.3356 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0242
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0244
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 604.4,                last time consumption/overall running time: 738.4128s / 19458.7484 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0239
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0242
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 594.8,                last time consumption/overall running time: 727.3058s / 20186.0542 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0242
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0244
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 636.45,                last time consumption/overall running time: 781.5234s / 20967.5776 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0247
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0242
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 612.05,                last time consumption/overall running time: 748.9458s / 21716.5234 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0240
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0236
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 658.7,                last time consumption/overall running time: 809.1790s / 22525.7024 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0236
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0239
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 608.75,                last time consumption/overall running time: 746.4211s / 23272.1235 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0240
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0237
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 622.35,                last time consumption/overall running time: 764.1385s / 24036.2620 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0244
env0_second_0:                 episode reward: 0.6000,                 loss: 0.0239
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 593.2,                last time consumption/overall running time: 729.1859s / 24765.4479 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0238
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0242
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 593.6,                last time consumption/overall running time: 729.0030s / 25494.4510 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0239
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0240
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 622.05,                last time consumption/overall running time: 765.6543s / 26260.1052 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0239
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0235
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 627.5,                last time consumption/overall running time: 772.8622s / 27032.9674 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0244
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0238
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 610.6,                last time consumption/overall running time: 751.9850s / 27784.9524 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0243
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0235
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 639.55,                last time consumption/overall running time: 788.7983s / 28573.7507 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0238
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0237
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 611.85,                last time consumption/overall running time: 753.2991s / 29327.0498 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0231
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0238
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 633.85,                last time consumption/overall running time: 777.2348s / 30104.2847 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0238
env0_second_0:                 episode reward: 1.6500,                 loss: 0.0231
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 668.75,                last time consumption/overall running time: 831.9281s / 30936.2128 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0235
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0239
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 629.95,                last time consumption/overall running time: 781.3794s / 31717.5922 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0233
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0236
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 653.85,                last time consumption/overall running time: 812.5151s / 32530.1074 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0230
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0239
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 654.6,                last time consumption/overall running time: 810.3153s / 33340.4226 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0232
env0_second_0:                 episode reward: 2.1000,                 loss: 0.0241
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 643.05,                last time consumption/overall running time: 793.9858s / 34134.4084 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0235
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0240
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 684.1,                last time consumption/overall running time: 846.4180s / 34980.8265 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0239
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0239
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 625.45,                last time consumption/overall running time: 776.5368s / 35757.3632 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0237
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0235
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 617.7,                last time consumption/overall running time: 767.8397s / 36525.2030 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0227
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0232
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 646.75,                last time consumption/overall running time: 800.7625s / 37325.9655 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0224
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0230
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 625.35,                last time consumption/overall running time: 774.7345s / 38100.7000 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0231
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0233
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 607.85,                last time consumption/overall running time: 754.5196s / 38855.2195 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0226
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0228
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 668.05,                last time consumption/overall running time: 832.3127s / 39687.5323 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0223
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0231
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 638.8,                last time consumption/overall running time: 789.8479s / 40477.3802 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0223
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0233
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 675.95,                last time consumption/overall running time: 841.2522s / 41318.6324 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0224
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0226
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 643.5,                last time consumption/overall running time: 798.0174s / 42116.6498 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0225
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0229
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 577.75,                last time consumption/overall running time: 717.8020s / 42834.4518 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0223
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0228
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 603.15,                last time consumption/overall running time: 751.3520s / 43585.8038 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0218
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0223
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 640.2,                last time consumption/overall running time: 800.9769s / 44386.7808 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0222
env0_second_0:                 episode reward: 2.9000,                 loss: 0.0226
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 595.3,                last time consumption/overall running time: 740.0015s / 45126.7823 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0214
env0_second_0:                 episode reward: 2.2500,                 loss: 0.0225
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 632.8,                last time consumption/overall running time: 786.1499s / 45912.9322 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0218
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0221
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 634.75,                last time consumption/overall running time: 790.6300s / 46703.5622 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0215
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0219
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 717.7,                last time consumption/overall running time: 897.3274s / 47600.8896 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0213
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0217
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 680.05,                last time consumption/overall running time: 852.2727s / 48453.1623 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0213
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0218
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 618.1,                last time consumption/overall running time: 770.8066s / 49223.9690 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0214
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0216
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 620.95,                last time consumption/overall running time: 771.1240s / 49995.0929 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0212
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0216
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 654.75,                last time consumption/overall running time: 818.0642s / 50813.1571 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0217
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0213
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 565.75,                last time consumption/overall running time: 705.8458s / 51519.0029 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0208
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0206
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 635.35,                last time consumption/overall running time: 789.1006s / 52308.1034 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0214
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0209
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 719.1,                last time consumption/overall running time: 893.8641s / 53201.9676 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0214
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0206
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 543.1,                last time consumption/overall running time: 679.6464s / 53881.6140 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0206
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0204
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 597.95,                last time consumption/overall running time: 748.6690s / 54630.2830 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0205
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0208
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 619.35,                last time consumption/overall running time: 778.8410s / 55409.1240 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0197
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0199
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 572.1,                last time consumption/overall running time: 721.2519s / 56130.3760 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0197
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0198
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 580.1,                last time consumption/overall running time: 731.1628s / 56861.5387 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.0194
env0_second_0:                 episode reward: 2.8500,                 loss: 0.0193
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 567.05,                last time consumption/overall running time: 711.0780s / 57572.6167 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0191
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0197
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 590.65,                last time consumption/overall running time: 738.1414s / 58310.7581 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0194
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0200
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 606.45,                last time consumption/overall running time: 760.3987s / 59071.1569 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0192
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0194
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 628.7,                last time consumption/overall running time: 789.0200s / 59860.1769 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0192
env0_second_0:                 episode reward: 2.5500,                 loss: 0.0196
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 650.85,                last time consumption/overall running time: 821.4361s / 60681.6130 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0192
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0196
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 658.15,                last time consumption/overall running time: 823.2274s / 61504.8404 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0189
env0_second_0:                 episode reward: 2.5000,                 loss: 0.0196
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 596.6,                last time consumption/overall running time: 743.0662s / 62247.9066 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0187
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0197
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 598.15,                last time consumption/overall running time: 752.8925s / 63000.7991 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0187
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0194
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 599.45,                last time consumption/overall running time: 752.5113s / 63753.3103 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0188
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0193
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 564.8,                last time consumption/overall running time: 708.4950s / 64461.8053 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0188
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0192
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 618.3,                last time consumption/overall running time: 776.2688s / 65238.0741 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0189
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0190
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 630.45,                last time consumption/overall running time: 789.1142s / 66027.1884 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0183
env0_second_0:                 episode reward: 3.2000,                 loss: 0.0195
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 597.8,                last time consumption/overall running time: 748.2380s / 66775.4263 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0187
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0199
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 610.65,                last time consumption/overall running time: 765.6824s / 67541.1087 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0187
env0_second_0:                 episode reward: 3.0000,                 loss: 0.0196
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 601.8,                last time consumption/overall running time: 763.1435s / 68304.2522 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0187
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0192
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 618.15,                last time consumption/overall running time: 785.7147s / 69089.9669 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0188
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0191
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 605.2,                last time consumption/overall running time: 766.0332s / 69856.0001 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0186
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0186
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 610.4,                last time consumption/overall running time: 771.4355s / 70627.4356 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0187
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0189
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 614.9,                last time consumption/overall running time: 776.5604s / 71403.9961 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0185
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0187
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 711.35,                last time consumption/overall running time: 892.2587s / 72296.2547 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0178
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0185
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 639.0,                last time consumption/overall running time: 806.3905s / 73102.6452 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0177
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0187
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 642.1,                last time consumption/overall running time: 808.4565s / 73911.1017 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0181
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0188
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 657.45,                last time consumption/overall running time: 826.8793s / 74737.9810 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0183
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0186
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 651.5,                last time consumption/overall running time: 814.1705s / 75552.1514 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0187
env0_second_0:                 episode reward: 2.9000,                 loss: 0.0190
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 605.65,                last time consumption/overall running time: 758.3318s / 76310.4832 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0193
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0191
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 667.2,                last time consumption/overall running time: 839.4917s / 77149.9749 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0189
env0_second_0:                 episode reward: 3.0000,                 loss: 0.0194
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 571.75,                last time consumption/overall running time: 725.1480s / 77875.1229 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0189
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0193
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 631.95,                last time consumption/overall running time: 794.2592s / 78669.3821 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0187
env0_second_0:                 episode reward: 3.6000,                 loss: 0.0194
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 634.9,                last time consumption/overall running time: 798.9604s / 79468.3425 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0191
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0197
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 619.65,                last time consumption/overall running time: 783.1153s / 80251.4578 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0185
env0_second_0:                 episode reward: 3.2000,                 loss: 0.0191
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 599.35,                last time consumption/overall running time: 754.8359s / 81006.2937 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0187
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0192
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 642.65,                last time consumption/overall running time: 810.0342s / 81816.3279 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0182
env0_second_0:                 episode reward: 3.0000,                 loss: 0.0187
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 664.9,                last time consumption/overall running time: 832.2239s / 82648.5518 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0185
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0185
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 632.05,                last time consumption/overall running time: 798.5908s / 83447.1425 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0183
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0183
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 626.05,                last time consumption/overall running time: 794.6084s / 84241.7509 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0179
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0185
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 652.5,                last time consumption/overall running time: 826.3893s / 85068.1402 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0181
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0188
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 614.05,                last time consumption/overall running time: 780.4904s / 85848.6306 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.0180
env0_second_0:                 episode reward: 2.8500,                 loss: 0.0185
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 574.85,                last time consumption/overall running time: 730.9000s / 86579.5306 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.0178
env0_second_0:                 episode reward: 3.1500,                 loss: 0.0187
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 622.1,                last time consumption/overall running time: 786.2245s / 87365.7551 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0180
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0189
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 614.8,                last time consumption/overall running time: 781.4778s / 88147.2329 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0180
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0188
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 619.9,                last time consumption/overall running time: 783.2766s / 88930.5095 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0179
env0_second_0:                 episode reward: 2.2500,                 loss: 0.0186
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 655.8,                last time consumption/overall running time: 827.9858s / 89758.4954 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0182
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0186
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 586.1,                last time consumption/overall running time: 739.9977s / 90498.4931 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0179
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0185
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 709.15,                last time consumption/overall running time: 896.5602s / 91395.0533 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0179
env0_second_0:                 episode reward: 2.9000,                 loss: 0.0181
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 647.25,                last time consumption/overall running time: 820.1109s / 92215.1643 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.0180
env0_second_0:                 episode reward: 2.8500,                 loss: 0.0180
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 699.6,                last time consumption/overall running time: 884.0337s / 93099.1980 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0186
env0_second_0:                 episode reward: 3.2000,                 loss: 0.0185
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 651.7,                last time consumption/overall running time: 826.6320s / 93925.8300 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0181
env0_second_0:                 episode reward: 2.5500,                 loss: 0.0190
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 627.65,                last time consumption/overall running time: 792.3519s / 94718.1819 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0185
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0194
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 617.95,                last time consumption/overall running time: 776.5328s / 95494.7147 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0185
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0189
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 606.35,                last time consumption/overall running time: 767.5629s / 96262.2775 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0186
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0188
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 584.05,                last time consumption/overall running time: 737.6723s / 96999.9498 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0187
env0_second_0:                 episode reward: 2.9000,                 loss: 0.0186
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 628.05,                last time consumption/overall running time: 794.6524s / 97794.6022 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0183
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0186
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 635.5,                last time consumption/overall running time: 800.7810s / 98595.3832 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0183
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0185
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 608.95,                last time consumption/overall running time: 770.7194s / 99366.1026 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0180
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0182
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 660.7,                last time consumption/overall running time: 837.4293s / 100203.5319 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0178
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0180
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 646.25,                last time consumption/overall running time: 823.7579s / 101027.2898 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0176
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0178
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 697.4,                last time consumption/overall running time: 886.0651s / 101913.3548 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0170
env0_second_0:                 episode reward: 3.0000,                 loss: 0.0174
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 655.8,                last time consumption/overall running time: 834.9162s / 102748.2710 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0167
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0172
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 673.55,                last time consumption/overall running time: 846.3213s / 103594.5923 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0165
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0171
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 609.35,                last time consumption/overall running time: 768.8846s / 104363.4770 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0165
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0170
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 594.1,                last time consumption/overall running time: 750.2084s / 105113.6854 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0163
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0167
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 619.1,                last time consumption/overall running time: 782.5609s / 105896.2463 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0161
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0167
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 646.35,                last time consumption/overall running time: 816.1777s / 106712.4240 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0166
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0167
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 667.45,                last time consumption/overall running time: 844.9578s / 107557.3818 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.0169
env0_second_0:                 episode reward: 3.1500,                 loss: 0.0168
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 617.1,                last time consumption/overall running time: 780.3813s / 108337.7631 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0168
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0170
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 631.15,                last time consumption/overall running time: 800.1978s / 109137.9609 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0167
env0_second_0:                 episode reward: 3.0000,                 loss: 0.0167
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 716.7,                last time consumption/overall running time: 900.5142s / 110038.4751 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0169
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0170
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 669.05,                last time consumption/overall running time: 843.1736s / 110881.6488 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0171
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0175
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 698.6,                last time consumption/overall running time: 881.8356s / 111763.4844 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0169
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0176
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 718.5,                last time consumption/overall running time: 907.3294s / 112670.8137 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0170
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0174
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 658.45,                last time consumption/overall running time: 836.1615s / 113506.9753 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0168
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0170
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 668.3,                last time consumption/overall running time: 846.6672s / 114353.6425 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0164
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0169
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 642.9,                last time consumption/overall running time: 808.8737s / 115162.5162 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0163
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0167
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 683.45,                last time consumption/overall running time: 863.1944s / 116025.7106 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0167
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0174
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 620.7,                last time consumption/overall running time: 786.3131s / 116812.0237 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0166
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0169
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 688.3,                last time consumption/overall running time: 864.0299s / 117676.0536 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0171
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0170
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 622.3,                last time consumption/overall running time: 786.4948s / 118462.5484 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0172
env0_second_0:                 episode reward: 3.2000,                 loss: 0.0172
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 690.0,                last time consumption/overall running time: 876.7028s / 119339.2512 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0166
env0_second_0:                 episode reward: 3.6000,                 loss: 0.0175
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 694.1,                last time consumption/overall running time: 879.6219s / 120218.8731 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0169
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0171
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 634.4,                last time consumption/overall running time: 800.3498s / 121019.2228 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0170
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0174
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 635.3,                last time consumption/overall running time: 803.7207s / 121822.9435 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0173
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0175
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 641.75,                last time consumption/overall running time: 812.1665s / 122635.1100 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0172
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0173
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 653.35,                last time consumption/overall running time: 829.8894s / 123464.9995 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0170
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0171
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 627.85,                last time consumption/overall running time: 792.3646s / 124257.3641 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0167
env0_second_0:                 episode reward: 3.6000,                 loss: 0.0171
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 625.45,                last time consumption/overall running time: 793.3646s / 125050.7287 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.0168
env0_second_0:                 episode reward: 2.8500,                 loss: 0.0172
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 639.0,                last time consumption/overall running time: 806.2352s / 125856.9639 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0170
env0_second_0:                 episode reward: 3.0000,                 loss: 0.0176
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 710.95,                last time consumption/overall running time: 901.9787s / 126758.9425 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0168
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0174
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 642.45,                last time consumption/overall running time: 809.1577s / 127568.1002 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0171
env0_second_0:                 episode reward: 3.8500,                 loss: 0.0172
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 663.45,                last time consumption/overall running time: 834.0810s / 128402.1812 s