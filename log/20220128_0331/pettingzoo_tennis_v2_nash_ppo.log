pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220128_0331/pettingzoo_tennis_v2_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220128_0331/pettingzoo_tennis_v2_nash_ppo.
Episode: 1/10000 (0.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 56.2888s / 56.2888 s
env0_first_0:                 episode reward: -33.0000,                 loss: -0.0033
env0_second_0:                 episode reward: 33.0000,                 loss: -0.0051
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 6597.65,                last time consumption/overall running time: 734.5071s / 790.7959 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.0484
env0_second_0:                 episode reward: 1.6500,                 loss: -0.0381
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 4977.75,                last time consumption/overall running time: 552.9652s / 1343.7611 s
env0_first_0:                 episode reward: 11.8500,                 loss: -0.0235
env0_second_0:                 episode reward: -11.8500,                 loss: -0.0075
env1_first_0:                 episode reward: 23.5500,                 loss: nan
env1_second_0:                 episode reward: -23.5500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 4425.5,                last time consumption/overall running time: 497.1235s / 1840.8847 s
env0_first_0:                 episode reward: 27.5000,                 loss: 0.0205
env0_second_0:                 episode reward: -27.5000,                 loss: 0.0117
env1_first_0:                 episode reward: 21.8000,                 loss: nan
env1_second_0:                 episode reward: -21.8000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 5117.25,                last time consumption/overall running time: 573.3923s / 2414.2770 s
env0_first_0:                 episode reward: 54.8000,                 loss: -0.0144
env0_second_0:                 episode reward: -54.8000,                 loss: -0.0312
env1_first_0:                 episode reward: 46.1500,                 loss: nan
env1_second_0:                 episode reward: -46.1500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 3693.95,                last time consumption/overall running time: 421.4289s / 2835.7059 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.1183
env0_second_0:                 episode reward: -7.0000,                 loss: -0.1166
env1_first_0:                 episode reward: 7.9000,                 loss: nan
env1_second_0:                 episode reward: -7.9000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 5694.1,                last time consumption/overall running time: 646.7775s / 3482.4834 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.0435
env0_second_0:                 episode reward: 3.3000,                 loss: -0.0544
env1_first_0:                 episode reward: 29.2000,                 loss: nan
env1_second_0:                 episode reward: -29.2000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 6642.0,                last time consumption/overall running time: 752.6484s / 4235.1318 s
env0_first_0:                 episode reward: 14.7000,                 loss: -0.0830
env0_second_0:                 episode reward: -14.7000,                 loss: -0.0807
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 8258.65,                last time consumption/overall running time: 936.2597s / 5171.3915 s
env0_first_0:                 episode reward: -12.7500,                 loss: -0.0485
env0_second_0:                 episode reward: 12.7500,                 loss: -0.0467
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 7024.4,                last time consumption/overall running time: 791.3157s / 5962.7072 s
env0_first_0:                 episode reward: -10.1000,                 loss: -0.0427
env0_second_0:                 episode reward: 10.1000,                 loss: -0.0475
env1_first_0:                 episode reward: -10.0000,                 loss: nan
env1_second_0:                 episode reward: 10.0000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 8532.2,                last time consumption/overall running time: 956.2977s / 6919.0049 s
env0_first_0:                 episode reward: -30.7500,                 loss: 0.0378
env0_second_0:                 episode reward: 30.7500,                 loss: 0.0377
env1_first_0:                 episode reward: -29.3500,                 loss: nan
env1_second_0:                 episode reward: 29.3500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 9369.85,                last time consumption/overall running time: 1054.5572s / 7973.5621 s
env0_first_0:                 episode reward: -15.1000,                 loss: 0.1060
env0_second_0:                 episode reward: 15.1000,                 loss: 0.1143
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 8853.85,                last time consumption/overall running time: 1002.2531s / 8975.8152 s
env0_first_0:                 episode reward: -43.7500,                 loss: 0.0929
env0_second_0:                 episode reward: 43.7500,                 loss: 0.0894
env1_first_0:                 episode reward: -39.7000,                 loss: nan
env1_second_0:                 episode reward: 39.7000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 7288.1,                last time consumption/overall running time: 824.5326s / 9800.3478 s
env0_first_0:                 episode reward: -15.6500,                 loss: -0.0251
env0_second_0:                 episode reward: 15.6500,                 loss: -0.0214
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 9721.4,                last time consumption/overall running time: 1086.1332s / 10886.4811 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1971
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1959
env1_first_0:                 episode reward: -9.9500,                 loss: nan
env1_second_0:                 episode reward: 9.9500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 9975.0,                last time consumption/overall running time: 1098.6750s / 11985.1561 s
env0_first_0:                 episode reward: -8.9000,                 loss: -0.2639
env0_second_0:                 episode reward: 8.9000,                 loss: -0.2558
env1_first_0:                 episode reward: -9.9000,                 loss: nan
env1_second_0:                 episode reward: 9.9000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1119.3580s / 13104.5141 s
env0_first_0:                 episode reward: -15.0500,                 loss: -0.2333
env0_second_0:                 episode reward: 15.0500,                 loss: -0.2247
env1_first_0:                 episode reward: -16.0000,                 loss: nan
env1_second_0:                 episode reward: 16.0000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1107.3232s / 14211.8373 s
env0_first_0:                 episode reward: -16.1500,                 loss: -0.2321
env0_second_0:                 episode reward: 16.1500,                 loss: -0.2244
env1_first_0:                 episode reward: -10.9500,                 loss: nan
env1_second_0:                 episode reward: 10.9500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1089.2162s / 15301.0535 s
env0_first_0:                 episode reward: -20.6000,                 loss: -0.2261
env0_second_0:                 episode reward: 20.6000,                 loss: -0.2230
env1_first_0:                 episode reward: -20.9000,                 loss: nan
env1_second_0:                 episode reward: 20.9000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1104.8998s / 16405.9533 s
env0_first_0:                 episode reward: -10.6000,                 loss: -0.2732
env0_second_0:                 episode reward: 10.6000,                 loss: -0.2620
env1_first_0:                 episode reward: -10.1500,                 loss: nan
env1_second_0:                 episode reward: 10.1500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1089.1146s / 17495.0680 s
env0_first_0:                 episode reward: -15.5000,                 loss: -0.2798
env0_second_0:                 episode reward: 15.5000,                 loss: -0.2736
env1_first_0:                 episode reward: -10.8500,                 loss: nan
env1_second_0:                 episode reward: 10.8500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1092.8257s / 18587.8937 s
env0_first_0:                 episode reward: -16.2500,                 loss: -0.2784
env0_second_0:                 episode reward: 16.2500,                 loss: -0.2721
env1_first_0:                 episode reward: -10.0000,                 loss: nan
env1_second_0:                 episode reward: 10.0000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1083.2362s / 19671.1298 s
env0_first_0:                 episode reward: -9.0500,                 loss: -0.2993
env0_second_0:                 episode reward: 9.0500,                 loss: -0.2926
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1094.3307s / 20765.4606 s
env0_first_0:                 episode reward: -6.7000,                 loss: -0.3003
env0_second_0:                 episode reward: 6.7000,                 loss: -0.2929
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1090.0359s / 21855.4965 s
env0_first_0:                 episode reward: -12.1500,                 loss: -0.2855
env0_second_0:                 episode reward: 12.1500,                 loss: -0.2762
env1_first_0:                 episode reward: -10.3000,                 loss: nan
env1_second_0:                 episode reward: 10.3000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1089.8073s / 22945.3038 s
env0_first_0:                 episode reward: -8.6000,                 loss: -0.2891
env0_second_0:                 episode reward: 8.6000,                 loss: -0.2818
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1079.0739s / 24024.3777 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.2845
env0_second_0:                 episode reward: 4.7000,                 loss: -0.2811
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1077.7039s / 25102.0816 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.3205
env0_second_0:                 episode reward: 2.9000,                 loss: -0.3131
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1093.6172s / 26195.6989 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.2606
env0_second_0:                 episode reward: 9.2000,                 loss: -0.2485
env1_first_0:                 episode reward: -17.7000,                 loss: nan
env1_second_0:                 episode reward: 17.7000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1087.5520s / 27283.2509 s
env0_first_0:                 episode reward: -10.9000,                 loss: -0.2916
env0_second_0:                 episode reward: 10.9000,                 loss: -0.2804
env1_first_0:                 episode reward: -16.8000,                 loss: nan
env1_second_0:                 episode reward: 16.8000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1085.4312s / 28368.6821 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.3051
env0_second_0:                 episode reward: 8.6500,                 loss: -0.2957
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 9912.55,                last time consumption/overall running time: 1075.5162s / 29444.1982 s
env0_first_0:                 episode reward: 6.5500,                 loss: -0.2090
env0_second_0:                 episode reward: -6.5500,                 loss: -0.2029
env1_first_0:                 episode reward: 15.4500,                 loss: nan
env1_second_0:                 episode reward: -15.4500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 9931.8,                last time consumption/overall running time: 1085.9299s / 30530.1281 s
env0_first_0:                 episode reward: 24.0500,                 loss: -0.2442
env0_second_0:                 episode reward: -24.0500,                 loss: -0.2327
env1_first_0:                 episode reward: 7.5500,                 loss: nan
env1_second_0:                 episode reward: -7.5500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1084.5263s / 31614.6544 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.2791
env0_second_0:                 episode reward: 2.3500,                 loss: -0.2719
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1088.9697s / 32703.6241 s
env0_first_0:                 episode reward: -8.3500,                 loss: -0.2477
env0_second_0:                 episode reward: 8.3500,                 loss: -0.2349
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 9793.1,                last time consumption/overall running time: 1074.3454s / 33777.9695 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.2563
env0_second_0:                 episode reward: 2.1500,                 loss: -0.2504
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1086.6102s / 34864.5797 s
env0_first_0:                 episode reward: -7.5000,                 loss: -0.1861
env0_second_0:                 episode reward: 7.5000,                 loss: -0.1792
env1_first_0:                 episode reward: -14.9500,                 loss: nan
env1_second_0:                 episode reward: 14.9500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 9811.7,                last time consumption/overall running time: 1057.8223s / 35922.4020 s
env0_first_0:                 episode reward: 7.3500,                 loss: -0.2733
env0_second_0:                 episode reward: -7.3500,                 loss: -0.2696
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1060.3313s / 36982.7333 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.2280
env0_second_0:                 episode reward: -4.8500,                 loss: -0.2205
env1_first_0:                 episode reward: -16.5000,                 loss: nan
env1_second_0:                 episode reward: 16.5000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 9917.8,                last time consumption/overall running time: 1066.9097s / 38049.6430 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.2795
env0_second_0:                 episode reward: 2.0000,                 loss: -0.2728
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 9930.95,                last time consumption/overall running time: 1080.1325s / 39129.7755 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2746
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2629
env1_first_0:                 episode reward: 8.9000,                 loss: nan
env1_second_0:                 episode reward: -8.9000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1093.5024s / 40223.2779 s
env0_first_0:                 episode reward: -5.5000,                 loss: -0.2372
env0_second_0:                 episode reward: 5.5000,                 loss: -0.2301
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1078.7514s / 41302.0293 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2535
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2430
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1092.2014s / 42394.2308 s
env0_first_0:                 episode reward: -6.5500,                 loss: -0.2492
env0_second_0:                 episode reward: 6.5500,                 loss: -0.2366
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1086.4044s / 43480.6352 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.2678
env0_second_0:                 episode reward: 1.6000,                 loss: -0.2574
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1088.5913s / 44569.2265 s
env0_first_0:                 episode reward: -14.6500,                 loss: -0.2144
env0_second_0:                 episode reward: 14.6500,                 loss: -0.1961
env1_first_0:                 episode reward: -26.4500,                 loss: nan
env1_second_0:                 episode reward: 26.4500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1093.9117s / 45663.1382 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.2299
env0_second_0:                 episode reward: -0.9500,                 loss: -0.2180
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 9790.7,                last time consumption/overall running time: 1056.5400s / 46719.6783 s
env0_first_0:                 episode reward: 11.8500,                 loss: -0.2396
env0_second_0:                 episode reward: -11.8500,                 loss: -0.2243
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 9977.6,                last time consumption/overall running time: 1079.0921s / 47798.7703 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.2232
env0_second_0:                 episode reward: -6.6000,                 loss: -0.2048
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 9930.35,                last time consumption/overall running time: 1068.8453s / 48867.6157 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.2441
env0_second_0:                 episode reward: -3.8000,                 loss: -0.2321
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1082.8230s / 49950.4387 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.2389
env0_second_0:                 episode reward: 1.0000,                 loss: -0.2204
env1_first_0:                 episode reward: -15.7000,                 loss: nan
env1_second_0:                 episode reward: 15.7000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 9839.0,                last time consumption/overall running time: 1062.1355s / 51012.5743 s
env0_first_0:                 episode reward: -27.2000,                 loss: -0.2236
env0_second_0:                 episode reward: 27.2000,                 loss: -0.2127
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1074.6044s / 52087.1787 s
env0_first_0:                 episode reward: -7.5000,                 loss: -0.2373
env0_second_0:                 episode reward: 7.5000,                 loss: -0.2272
env1_first_0:                 episode reward: -12.8000,                 loss: nan
env1_second_0:                 episode reward: 12.8000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1079.9816s / 53167.1602 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.2097
env0_second_0:                 episode reward: 1.8000,                 loss: -0.2015
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1056.4254s / 54223.5856 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.2542
env0_second_0:                 episode reward: 3.9500,                 loss: -0.2459
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1068.9979s / 55292.5835 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.2481
env0_second_0:                 episode reward: 2.7000,                 loss: -0.2318
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 9952.7,                last time consumption/overall running time: 1078.9635s / 56371.5470 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.2206
env0_second_0:                 episode reward: -6.5000,                 loss: -0.2066
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1061.9875s / 57433.5346 s
env0_first_0:                 episode reward: -15.6500,                 loss: -0.2453
env0_second_0:                 episode reward: 15.6500,                 loss: -0.2337
env1_first_0:                 episode reward: -11.9500,                 loss: nan
env1_second_0:                 episode reward: 11.9500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1075.7201s / 58509.2546 s
env0_first_0:                 episode reward: -10.3500,                 loss: -0.2295
env0_second_0:                 episode reward: 10.3500,                 loss: -0.2151
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1090.4196s / 59599.6742 s
env0_first_0:                 episode reward: -4.9500,                 loss: -0.2486
env0_second_0:                 episode reward: 4.9500,                 loss: -0.2334
env1_first_0:                 episode reward: 8.2500,                 loss: nan
env1_second_0:                 episode reward: -8.2500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1078.2668s / 60677.9410 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.2455
env0_second_0:                 episode reward: 7.0000,                 loss: -0.2309
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 9889.9,                last time consumption/overall running time: 1052.3937s / 61730.3347 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.2307
env0_second_0:                 episode reward: -0.6000,                 loss: -0.2122
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 9596.7,                last time consumption/overall running time: 1018.9531s / 62749.2878 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.2340
env0_second_0:                 episode reward: -9.6000,                 loss: -0.2149
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 9568.25,                last time consumption/overall running time: 1006.8087s / 63756.0965 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.2437
env0_second_0:                 episode reward: -5.7000,                 loss: -0.2251
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 9453.8,                last time consumption/overall running time: 1012.5298s / 64768.6264 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2194
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2013
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 9514.55,                last time consumption/overall running time: 1009.9675s / 65778.5939 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.2286
env0_second_0:                 episode reward: 2.4000,                 loss: -0.2130
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 9771.85,                last time consumption/overall running time: 1034.2190s / 66812.8128 s
env0_first_0:                 episode reward: -7.3000,                 loss: -0.2280
env0_second_0:                 episode reward: 7.3000,                 loss: -0.2080
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 9909.1,                last time consumption/overall running time: 1033.6883s / 67846.5012 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2462
env0_second_0:                 episode reward: -2.2500,                 loss: -0.2328
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 9787.9,                last time consumption/overall running time: 1019.7626s / 68866.2638 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.2508
env0_second_0:                 episode reward: 2.9000,                 loss: -0.2358
env1_first_0:                 episode reward: 8.3500,                 loss: nan
env1_second_0:                 episode reward: -8.3500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 9853.85,                last time consumption/overall running time: 1022.2655s / 69888.5293 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.2354
env0_second_0:                 episode reward: -4.4000,                 loss: -0.2215
env1_first_0:                 episode reward: -12.5500,                 loss: nan
env1_second_0:                 episode reward: 12.5500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1035.5866s / 70924.1159 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.2691
env0_second_0:                 episode reward: 1.8000,                 loss: -0.2535
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1029.7243s / 71953.8402 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.2233
env0_second_0:                 episode reward: 2.6500,                 loss: -0.2016
env1_first_0:                 episode reward: -14.7500,                 loss: nan
env1_second_0:                 episode reward: 14.7500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.2134s / 72990.0536 s
env0_first_0:                 episode reward: -15.0000,                 loss: -0.2073
env0_second_0:                 episode reward: 15.0000,                 loss: -0.1807
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1047.5751s / 74037.6287 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.2728
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2526
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1054.6803s / 75092.3090 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2783
env0_second_0:                 episode reward: -2.6500,                 loss: -0.2580
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1045.9745s / 76138.2835 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2811
env0_second_0:                 episode reward: -1.5000,                 loss: -0.2638
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1037.9321s / 77176.2155 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.2623
env0_second_0:                 episode reward: -4.2000,                 loss: -0.2433
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 9825.2,                last time consumption/overall running time: 1010.0469s / 78186.2624 s
env0_first_0:                 episode reward: 6.7500,                 loss: -0.2285
env0_second_0:                 episode reward: -6.7500,                 loss: -0.2099
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 9998.05,                last time consumption/overall running time: 1048.1758s / 79234.4382 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.2717
env0_second_0:                 episode reward: -3.8500,                 loss: -0.2515
env1_first_0:                 episode reward: 8.4500,                 loss: nan
env1_second_0:                 episode reward: -8.4500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 9949.05,                last time consumption/overall running time: 1041.0703s / 80275.5084 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.2768
env0_second_0:                 episode reward: -3.3000,                 loss: -0.2564
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 9909.05,                last time consumption/overall running time: 1036.0053s / 81311.5137 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.2790
env0_second_0:                 episode reward: -5.3000,                 loss: -0.2618
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 9907.8,                last time consumption/overall running time: 1041.1709s / 82352.6846 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2630
env0_second_0:                 episode reward: -2.1500,                 loss: -0.2438
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 9993.1,                last time consumption/overall running time: 1040.0945s / 83392.7792 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2613
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2382
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 9824.9,                last time consumption/overall running time: 1015.7827s / 84408.5619 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.2596
env0_second_0:                 episode reward: -3.1000,                 loss: -0.2379
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 9873.85,                last time consumption/overall running time: 1037.4227s / 85445.9846 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2589
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2361
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1045.1224s / 86491.1070 s
env0_first_0:                 episode reward: 9.1500,                 loss: -0.2485
env0_second_0:                 episode reward: -9.1500,                 loss: -0.2248
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1039.3662s / 87530.4732 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.2537
env0_second_0:                 episode reward: -3.3500,                 loss: -0.2257
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 9642.3,                last time consumption/overall running time: 993.1191s / 88523.5923 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.2510
env0_second_0:                 episode reward: -0.8000,                 loss: -0.2291
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1040.9457s / 89564.5380 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.2748
env0_second_0:                 episode reward: 4.0000,                 loss: -0.2509
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 9978.35,                last time consumption/overall running time: 1051.9715s / 90616.5095 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.2630
env0_second_0:                 episode reward: 2.5000,                 loss: -0.2379
env1_first_0:                 episode reward: -11.9000,                 loss: nan
env1_second_0:                 episode reward: 11.9000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1050.0778s / 91666.5873 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.2785
env0_second_0:                 episode reward: 2.6000,                 loss: -0.2579
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 9879.4,                last time consumption/overall running time: 1032.2149s / 92698.8022 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.2543
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2233
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 9966.25,                last time consumption/overall running time: 1025.8660s / 93724.6682 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.2775
env0_second_0:                 episode reward: 0.8000,                 loss: -0.2510
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1053.2634s / 94777.9317 s
env0_first_0:                 episode reward: -3.5500,                 loss: -0.2866
env0_second_0:                 episode reward: 3.5500,                 loss: -0.2659
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1050.2113s / 95828.1430 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.2807
env0_second_0:                 episode reward: 9.3000,                 loss: -0.2585
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1057.4383s / 96885.5813 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3156
env0_second_0:                 episode reward: -1.2000,                 loss: -0.2965
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1051.0527s / 97936.6340 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.2789
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2570
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1055.7703s / 98992.4044 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.2962
env0_second_0:                 episode reward: 0.6000,                 loss: -0.2786
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1037.4714s / 100029.8757 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.3351
env0_second_0:                 episode reward: 2.2000,                 loss: -0.3220
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1047.9557s / 101077.8314 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3223
env0_second_0:                 episode reward: -0.1000,                 loss: -0.3075
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1052.2089s / 102130.0403 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.3123
env0_second_0:                 episode reward: 1.5500,                 loss: -0.2883
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1039.9048s / 103169.9451 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.2903
env0_second_0:                 episode reward: -3.9000,                 loss: -0.2696
env1_first_0:                 episode reward: -20.8500,                 loss: nan
env1_second_0:                 episode reward: 20.8500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1052.4100s / 104222.3551 s
env0_first_0:                 episode reward: -10.3000,                 loss: -0.2695
env0_second_0:                 episode reward: 10.3000,                 loss: -0.2500
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1059.3682s / 105281.7233 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.2930
env0_second_0:                 episode reward: 1.2500,                 loss: -0.2783
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1063.7859s / 106345.5092 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3036
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2878
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1056.2960s / 107401.8052 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.3239
env0_second_0:                 episode reward: -2.1000,                 loss: -0.3107
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1063.1044s / 108464.9096 s
env0_first_0:                 episode reward: 24.3500,                 loss: -0.1843
env0_second_0:                 episode reward: -24.3500,                 loss: -0.1532
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1023.3391s / 109488.2487 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.2955
env0_second_0:                 episode reward: -2.9000,                 loss: -0.2751
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1040.5897s / 110528.8384 s
env0_first_0:                 episode reward: -6.6000,                 loss: -0.2507
env0_second_0:                 episode reward: 6.6000,                 loss: -0.2293
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1063.1217s / 111591.9600 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.2712
env0_second_0:                 episode reward: 5.1000,                 loss: -0.2455
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1037.8615s / 112629.8215 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.2668
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2429
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1037.3281s / 113667.1497 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.3028
env0_second_0:                 episode reward: 2.1500,                 loss: -0.2815
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1071.4489s / 114738.5986 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3161
env0_second_0:                 episode reward: -1.6000,                 loss: -0.2943
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1041.8601s / 115780.4587 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.3083
env0_second_0:                 episode reward: -4.9500,                 loss: -0.2864
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1066.2659s / 116846.7247 s
env0_first_0:                 episode reward: -8.3000,                 loss: -0.3125
env0_second_0:                 episode reward: 8.3000,                 loss: -0.2905
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1055.0077s / 117901.7324 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.3164
env0_second_0:                 episode reward: 6.0500,                 loss: -0.2916
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1034.5141s / 118936.2465 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.3128
env0_second_0:                 episode reward: -3.9500,                 loss: -0.2855
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1044.5693s / 119980.8157 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.3101
env0_second_0:                 episode reward: -2.3000,                 loss: -0.2870
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1046.1445s / 121026.9602 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.3110
env0_second_0:                 episode reward: -1.7500,                 loss: -0.2855
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1031.9633s / 122058.9235 s
env0_first_0:                 episode reward: -4.1500,                 loss: -0.2859
env0_second_0:                 episode reward: 4.1500,                 loss: -0.2587
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1045.9935s / 123104.9170 s
env0_first_0:                 episode reward: 7.1000,                 loss: -0.2672
env0_second_0:                 episode reward: -7.1000,                 loss: -0.2364
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1043.7206s / 124148.6376 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.2772
env0_second_0:                 episode reward: -4.9000,                 loss: -0.2493
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 9959.7,                last time consumption/overall running time: 1045.3861s / 125194.0237 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.2762
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2466
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1061.0889s / 126255.1126 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3074
env0_second_0:                 episode reward: -2.7500,                 loss: -0.2875
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 9791.2,                last time consumption/overall running time: 1011.9752s / 127267.0879 s
env0_first_0:                 episode reward: 9.3000,                 loss: -0.2628
env0_second_0:                 episode reward: -9.3000,                 loss: -0.2302
env1_first_0:                 episode reward: 9.0000,                 loss: nan
env1_second_0:                 episode reward: -9.0000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 9961.75,                last time consumption/overall running time: 1039.7852s / 128306.8731 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3410
env0_second_0:                 episode reward: -1.2000,                 loss: -0.3120
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 9751.8,                last time consumption/overall running time: 1021.6411s / 129328.5142 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.3109
env0_second_0:                 episode reward: -2.6500,                 loss: -0.2923
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1042.9613s / 130371.4755 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3265
env0_second_0:                 episode reward: -1.2500,                 loss: -0.3038
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1040.5240s / 131411.9995 s
env0_first_0:                 episode reward: -3.5500,                 loss: -0.3255
env0_second_0:                 episode reward: 3.5500,                 loss: -0.3017
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1051.8534s / 132463.8528 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.2995
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2715
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1045.0159s / 133508.8687 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.3489
env0_second_0:                 episode reward: -3.0000,                 loss: -0.3256
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1042.3836s / 134551.2524 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3235
env0_second_0:                 episode reward: -1.5000,                 loss: -0.2888
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 9909.95,                last time consumption/overall running time: 1038.0932s / 135589.3455 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.3025
env0_second_0:                 episode reward: 3.8500,                 loss: -0.2633
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1043.1608s / 136632.5063 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.3221
env0_second_0:                 episode reward: 1.2500,                 loss: -0.2859
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1039.0130s / 137671.5193 s
env0_first_0:                 episode reward: -10.4500,                 loss: -0.2841
env0_second_0:                 episode reward: 10.4500,                 loss: -0.2397
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1050.2136s / 138721.7329 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3212
env0_second_0:                 episode reward: -0.5500,                 loss: -0.2869
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1039.2247s / 139760.9576 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3158
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2828
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1024.9838s / 140785.9413 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3098
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2739
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1030.1070s / 141816.0484 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.3269
env0_second_0:                 episode reward: 0.3000,                 loss: -0.3035
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1028.7321s / 142844.7804 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.3187
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2902
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1040.1980s / 143884.9784 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.3022
env0_second_0:                 episode reward: 1.9000,                 loss: -0.2676
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1030.6878s / 144915.6662 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3125
env0_second_0:                 episode reward: -1.4000,                 loss: -0.2802
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1019.4538s / 145935.1200 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.3289
env0_second_0:                 episode reward: -0.7500,                 loss: -0.3057
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1032.1568s / 146967.2768 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3245
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2910
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1032.4077s / 147999.6845 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3400
env0_second_0:                 episode reward: -1.3500,                 loss: -0.3147
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1037.2777s / 149036.9622 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3532
env0_second_0:                 episode reward: -2.4000,                 loss: -0.3248
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1026.3563s / 150063.3185 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3348
env0_second_0:                 episode reward: -1.2500,                 loss: -0.3035
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.6901s / 151100.0086 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3464
env0_second_0:                 episode reward: -2.4500,                 loss: -0.3203
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1056.0584s / 152156.0670 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.3447
env0_second_0:                 episode reward: -2.3500,                 loss: -0.3207
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1054.2362s / 153210.3032 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3329
env0_second_0:                 episode reward: 0.0000,                 loss: -0.3039
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1045.9066s / 154256.2097 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3337
env0_second_0:                 episode reward: -1.5000,                 loss: -0.3146
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1047.6832s / 155303.8929 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3369
env0_second_0:                 episode reward: -1.5500,                 loss: -0.3125
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1051.9425s / 156355.8354 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3380
env0_second_0:                 episode reward: -2.8500,                 loss: -0.3132
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.3284s / 157392.1639 s
env0_first_0:                 episode reward: -6.3000,                 loss: -0.3023
env0_second_0:                 episode reward: 6.3000,                 loss: -0.2793
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1055.7486s / 158447.9124 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3448
env0_second_0:                 episode reward: -1.2500,                 loss: -0.3129
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1079.0978s / 159527.0102 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3372
env0_second_0:                 episode reward: -1.7000,                 loss: -0.3020
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1042.7834s / 160569.7936 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3391
env0_second_0:                 episode reward: -2.0000,                 loss: -0.3073
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.5943s / 161592.3879 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3398
env0_second_0:                 episode reward: 0.0500,                 loss: -0.3104
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1027.8992s / 162620.2871 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.3348
env0_second_0:                 episode reward: 0.1500,                 loss: -0.3045
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1016.9681s / 163637.2552 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.3195
env0_second_0:                 episode reward: 2.8500,                 loss: -0.2760
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1048.6378s / 164685.8930 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3113
env0_second_0:                 episode reward: -1.0500,                 loss: -0.2739
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1080.8336s / 165766.7266 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3135
env0_second_0:                 episode reward: -1.5000,                 loss: -0.2819
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 9923.35,                last time consumption/overall running time: 1032.5771s / 166799.3037 s
env0_first_0:                 episode reward: -10.9500,                 loss: -0.2962
env0_second_0:                 episode reward: 10.9500,                 loss: -0.2643
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1044.7009s / 167844.0046 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.3333
env0_second_0:                 episode reward: 1.2500,                 loss: -0.3061
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1028.2310s / 168872.2356 s
env0_first_0:                 episode reward: -12.0000,                 loss: -0.3036
env0_second_0:                 episode reward: 12.0000,                 loss: -0.2559
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1050.2834s / 169922.5190 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3079
env0_second_0:                 episode reward: -1.5000,                 loss: -0.2663
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1053.4466s / 170975.9656 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3223
env0_second_0:                 episode reward: -1.2000,                 loss: -0.2619
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1057.6054s / 172033.5710 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.3311
env0_second_0:                 episode reward: 1.2000,                 loss: -0.2902
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1051.3349s / 173084.9059 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.3318
env0_second_0:                 episode reward: -0.5000,                 loss: -0.2970
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1033.1172s / 174118.0231 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.2670
env0_second_0:                 episode reward: -4.7500,                 loss: -0.2105
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 9675.4,                last time consumption/overall running time: 1005.5259s / 175123.5490 s
env0_first_0:                 episode reward: -6.3000,                 loss: -0.2667
env0_second_0:                 episode reward: 6.3000,                 loss: -0.2200
env1_first_0:                 episode reward: -12.1000,                 loss: nan
env1_second_0:                 episode reward: 12.1000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1062.3731s / 176185.9222 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3141
env0_second_0:                 episode reward: -1.5500,                 loss: -0.2458
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1063.1406s / 177249.0627 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3318
env0_second_0:                 episode reward: 1.3500,                 loss: -0.2976
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.5035s / 178285.5663 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3463
env0_second_0:                 episode reward: -0.0500,                 loss: -0.3128
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 9809.0,                last time consumption/overall running time: 1003.8964s / 179289.4627 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.3143
env0_second_0:                 episode reward: -1.6500,                 loss: -0.2811
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1023.2750s / 180312.7377 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.3284
env0_second_0:                 episode reward: -0.6500,                 loss: -0.2982
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 9985.35,                last time consumption/overall running time: 1022.1502s / 181334.8879 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.3144
env0_second_0:                 episode reward: 0.7500,                 loss: -0.2790
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1017.7923s / 182352.6802 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3289
env0_second_0:                 episode reward: -0.9000,                 loss: -0.2950
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1027.3928s / 183380.0731 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.3135
env0_second_0:                 episode reward: 1.4000,                 loss: -0.2681
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.4975s / 184402.5705 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3316
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2961
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1004.7630s / 185407.3335 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.2885
env0_second_0:                 episode reward: 4.4500,                 loss: -0.2390
env1_first_0:                 episode reward: -19.1500,                 loss: nan
env1_second_0:                 episode reward: 19.1500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1009.0243s / 186416.3578 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.3290
env0_second_0:                 episode reward: 0.6500,                 loss: -0.2976
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1013.2389s / 187429.5967 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.3050
env0_second_0:                 episode reward: -1.6500,                 loss: -0.2591
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1024.0434s / 188453.6402 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3125
env0_second_0:                 episode reward: 0.8500,                 loss: -0.2557
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1057.9085s / 189511.5487 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.3291
env0_second_0:                 episode reward: 1.4000,                 loss: -0.2874
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1009.1438s / 190520.6925 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.3128
env0_second_0:                 episode reward: 3.1000,                 loss: -0.2712
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 9797.5,                last time consumption/overall running time: 990.5598s / 191511.2523 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.3092
env0_second_0:                 episode reward: 1.2500,                 loss: -0.2659
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1023.2924s / 192534.5447 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.3066
env0_second_0:                 episode reward: 0.9500,                 loss: -0.2631
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1030.4780s / 193565.0227 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.3151
env0_second_0:                 episode reward: 2.0000,                 loss: -0.2769
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1039.7747s / 194604.7974 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.2977
env0_second_0:                 episode reward: 2.2500,                 loss: -0.2489
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 9995.1,                last time consumption/overall running time: 1084.8433s / 195689.6408 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.3236
env0_second_0:                 episode reward: 2.1000,                 loss: -0.2840
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1057.2154s / 196746.8562 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3265
env0_second_0:                 episode reward: -1.5000,                 loss: -0.2907
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1033.0897s / 197779.9459 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.3130
env0_second_0:                 episode reward: -1.6500,                 loss: -0.2664
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 9606.55,                last time consumption/overall running time: 985.7714s / 198765.7173 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3274
env0_second_0:                 episode reward: -2.7500,                 loss: -0.2810
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 9993.15,                last time consumption/overall running time: 1050.7393s / 199816.4566 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.3187
env0_second_0:                 episode reward: 2.3000,                 loss: -0.2519
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 9633.35,                last time consumption/overall running time: 1005.7539s / 200822.2105 s
env0_first_0:                 episode reward: -4.3500,                 loss: -0.2784
env0_second_0:                 episode reward: 4.3500,                 loss: -0.2299
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 9835.2,                last time consumption/overall running time: 1010.3931s / 201832.6036 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.2836
env0_second_0:                 episode reward: 5.9500,                 loss: -0.1833
env1_first_0:                 episode reward: -17.3000,                 loss: nan
env1_second_0:                 episode reward: 17.3000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1012.8242s / 202845.4278 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.3054
env0_second_0:                 episode reward: 2.5000,                 loss: -0.2306
env1_first_0:                 episode reward: -11.5500,                 loss: nan
env1_second_0:                 episode reward: 11.5500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 9758.65,                last time consumption/overall running time: 1021.4806s / 203866.9084 s
env0_first_0:                 episode reward: -25.3500,                 loss: -0.2731
env0_second_0:                 episode reward: 25.3500,                 loss: -0.2137
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1038.5245s / 204905.4329 s
env0_first_0:                 episode reward: -15.7500,                 loss: -0.2786
env0_second_0:                 episode reward: 15.7500,                 loss: -0.2324
env1_first_0:                 episode reward: -17.7500,                 loss: nan
env1_second_0:                 episode reward: 17.7500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.1909s / 205941.6238 s
env0_first_0:                 episode reward: -21.0000,                 loss: -0.2442
env0_second_0:                 episode reward: 21.0000,                 loss: -0.1882
env1_first_0:                 episode reward: -22.2000,                 loss: nan
env1_second_0:                 episode reward: 22.2000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.8674s / 206978.4912 s
env0_first_0:                 episode reward: -11.8500,                 loss: -0.2692
env0_second_0:                 episode reward: 11.8500,                 loss: -0.2031
env1_first_0:                 episode reward: -20.1500,                 loss: nan
env1_second_0:                 episode reward: 20.1500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 9980.85,                last time consumption/overall running time: 1017.4864s / 207995.9775 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.2874
env0_second_0:                 episode reward: 4.6000,                 loss: -0.2306
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1020.7214s / 209016.6989 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.3065
env0_second_0:                 episode reward: 1.4000,                 loss: -0.2469
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 9144.05,                last time consumption/overall running time: 925.6134s / 209942.3124 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2434
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1550
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.1015s / 210964.4139 s
env0_first_0:                 episode reward: -8.2000,                 loss: -0.3214
env0_second_0:                 episode reward: 8.2000,                 loss: -0.2673
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1021.6309s / 211986.0448 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.3300
env0_second_0:                 episode reward: 0.5000,                 loss: -0.2477
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.6888s / 213008.7336 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3450
env0_second_0:                 episode reward: 0.6000,                 loss: -0.1903
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.0761s / 214030.8097 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3287
env0_second_0:                 episode reward: 1.3500,                 loss: -0.2394
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1029.1340s / 215059.9437 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.2658
env0_second_0:                 episode reward: -4.8500,                 loss: -0.1544
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 9762.3,                last time consumption/overall running time: 1010.9051s / 216070.8488 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.3147
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2476
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1032.0852s / 217102.9340 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.3354
env0_second_0:                 episode reward: 1.6000,                 loss: -0.2809
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1035.7275s / 218138.6615 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.3316
env0_second_0:                 episode reward: 4.7000,                 loss: -0.2681
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1041.1427s / 219179.8042 s
env0_first_0:                 episode reward: -5.2500,                 loss: -0.2964
env0_second_0:                 episode reward: 5.2500,                 loss: -0.2242
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.0531s / 220201.8573 s
env0_first_0:                 episode reward: -4.9000,                 loss: -0.2903
env0_second_0:                 episode reward: 4.9000,                 loss: -0.2219
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 8180.8,                last time consumption/overall running time: 840.9867s / 221042.8441 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.2507
env0_second_0:                 episode reward: -5.7000,                 loss: -0.1492
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 9132.15,                last time consumption/overall running time: 934.0758s / 221976.9199 s
env0_first_0:                 episode reward: 8.4500,                 loss: -0.3067
env0_second_0:                 episode reward: -8.4500,                 loss: -0.2098
env1_first_0:                 episode reward: 10.9500,                 loss: nan
env1_second_0:                 episode reward: -10.9500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1011.6225s / 222988.5424 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.3182
env0_second_0:                 episode reward: 1.8000,                 loss: -0.2309
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 9884.35,                last time consumption/overall running time: 1014.6942s / 224003.2366 s
env0_first_0:                 episode reward: -19.8000,                 loss: -0.2670
env0_second_0:                 episode reward: 19.8000,                 loss: -0.1809
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 9556.7,                last time consumption/overall running time: 978.9872s / 224982.2238 s
env0_first_0:                 episode reward: -17.2000,                 loss: -0.1340
env0_second_0:                 episode reward: 17.2000,                 loss: 0.0219
env1_first_0:                 episode reward: -11.2000,                 loss: nan
env1_second_0:                 episode reward: 11.2000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 8295.75,                last time consumption/overall running time: 853.3164s / 225835.5403 s
env0_first_0:                 episode reward: -119.2500,                 loss: 0.1278
env0_second_0:                 episode reward: 119.2500,                 loss: 0.2871
env1_first_0:                 episode reward: -137.1500,                 loss: nan
env1_second_0:                 episode reward: 137.1500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 8842.35,                last time consumption/overall running time: 916.2237s / 226751.7639 s
env0_first_0:                 episode reward: -15.1500,                 loss: -0.2150
env0_second_0:                 episode reward: 15.1500,                 loss: -0.0961
env1_first_0:                 episode reward: -16.9500,                 loss: nan
env1_second_0:                 episode reward: 16.9500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 9931.55,                last time consumption/overall running time: 1005.8074s / 227757.5713 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3266
env0_second_0:                 episode reward: 0.8500,                 loss: -0.2489
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1032.0240s / 228789.5953 s
env0_first_0:                 episode reward: -5.3000,                 loss: -0.3188
env0_second_0:                 episode reward: 5.3000,                 loss: -0.2248
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 9271.2,                last time consumption/overall running time: 932.0014s / 229721.5967 s
env0_first_0:                 episode reward: -14.7500,                 loss: -0.2137
env0_second_0:                 episode reward: 14.7500,                 loss: -0.0921
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 8836.15,                last time consumption/overall running time: 900.4991s / 230622.0958 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.1426
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0915
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1007.1424s / 231629.2382 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.3090
env0_second_0:                 episode reward: -4.4000,                 loss: -0.2337
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1046.9743s / 232676.2125 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3392
env0_second_0:                 episode reward: -1.8000,                 loss: -0.2751
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1051.5455s / 233727.7580 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.3140
env0_second_0:                 episode reward: 0.8000,                 loss: -0.2534
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1002.1645s / 234729.9225 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3377
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2847
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1047.3514s / 235777.2739 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.3435
env0_second_0:                 episode reward: 0.7000,                 loss: -0.2964
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 9964.5,                last time consumption/overall running time: 1031.9469s / 236809.2208 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3137
env0_second_0:                 episode reward: -1.4000,                 loss: -0.2484
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1079.8055s / 237889.0262 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.3348
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2855
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1014.8312s / 238903.8574 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.3226
env0_second_0:                 episode reward: -3.2500,                 loss: -0.2647
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 9962.3,                last time consumption/overall running time: 1008.6390s / 239912.4964 s