pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7f4e7eb76090>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.007 0.007 0.007 ... 0.007 0.007 0.007]
 [0.007 0.007 0.007 ... 0.007 0.007 0.007]]
Load checkpoints (policy family):  [['50' '5253' '7615' ... '78324' '78707' '79242']
 ['193' '5289' '7712' ... '78482' '78910' '79413']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220117153310/epi_80000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220117153310_exploit_80000/mdp_arbitrary_mdp_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220117153310_exploit_80000/mdp_arbitrary_mdp_fictitious_selfplay2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0058s / 5.0058 s
agent0:                 episode reward: 0.1344,                 loss: nan
agent1:                 episode reward: -0.1344,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5712s / 9.5770 s
agent0:                 episode reward: -0.0221,                 loss: nan
agent1:                 episode reward: 0.0221,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8417s / 14.4188 s
agent0:                 episode reward: 0.0709,                 loss: nan
agent1:                 episode reward: -0.0709,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6134s / 19.0321 s
agent0:                 episode reward: 0.2649,                 loss: nan
agent1:                 episode reward: -0.2649,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6702s / 23.7023 s
agent0:                 episode reward: 0.1258,                 loss: nan
agent1:                 episode reward: -0.1258,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9726s / 28.6750 s
agent0:                 episode reward: 0.0422,                 loss: nan
agent1:                 episode reward: -0.0422,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3628s / 34.0377 s
agent0:                 episode reward: 0.7102,                 loss: nan
agent1:                 episode reward: -0.7102,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2314s / 38.2692 s
agent0:                 episode reward: -0.0818,                 loss: nan
agent1:                 episode reward: 0.0818,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8510s / 43.1202 s
agent0:                 episode reward: 0.1929,                 loss: nan
agent1:                 episode reward: -0.1929,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3025s / 47.4227 s
agent0:                 episode reward: 0.2424,                 loss: nan
agent1:                 episode reward: -0.2424,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2669s / 51.6896 s
agent0:                 episode reward: 0.1987,                 loss: nan
agent1:                 episode reward: -0.1987,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 52.9983s / 104.6879 s
agent0:                 episode reward: -0.4513,                 loss: nan
agent1:                 episode reward: 0.4513,                 loss: 0.1975
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.2231s / 241.9110 s
agent0:                 episode reward: -0.2121,                 loss: nan
agent1:                 episode reward: 0.2121,                 loss: 0.1821
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.1871s / 381.0981 s
agent0:                 episode reward: -0.2297,                 loss: nan
agent1:                 episode reward: 0.2297,                 loss: 0.1720
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 135.2009s / 516.2990 s
agent0:                 episode reward: -0.1031,                 loss: nan
agent1:                 episode reward: 0.1031,                 loss: 0.1681
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 136.4539s / 652.7529 s
agent0:                 episode reward: -0.0448,                 loss: nan
agent1:                 episode reward: 0.0448,                 loss: 0.1652
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 136.3387s / 789.0916 s
agent0:                 episode reward: -0.0449,                 loss: nan
agent1:                 episode reward: 0.0449,                 loss: 0.1641
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.3967s / 927.4883 s
agent0:                 episode reward: 0.2589,                 loss: nan
agent1:                 episode reward: -0.2589,                 loss: 0.1634
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 135.2069s / 1062.6952 s
agent0:                 episode reward: 0.3860,                 loss: nan
agent1:                 episode reward: -0.3860,                 loss: 0.1609
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 136.2339s / 1198.9292 s
agent0:                 episode reward: 0.1906,                 loss: nan
agent1:                 episode reward: -0.1906,                 loss: 0.1601
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 136.8188s / 1335.7479 s
agent0:                 episode reward: 0.1262,                 loss: nan
agent1:                 episode reward: -0.1262,                 loss: 0.1599
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 135.5573s / 1471.3053 s
agent0:                 episode reward: -0.0809,                 loss: nan
agent1:                 episode reward: 0.0809,                 loss: 0.1583
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.5494s / 1606.8547 s
agent0:                 episode reward: 0.1470,                 loss: nan
agent1:                 episode reward: -0.1470,                 loss: 0.1566
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 136.0329s / 1742.8875 s
agent0:                 episode reward: 0.3399,                 loss: nan
agent1:                 episode reward: -0.3399,                 loss: 0.1578
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.9139s / 1881.8014 s
agent0:                 episode reward: 0.3112,                 loss: nan
agent1:                 episode reward: -0.3112,                 loss: 0.1566
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.6273s / 2023.4287 s
agent0:                 episode reward: 0.0517,                 loss: nan
agent1:                 episode reward: -0.0517,                 loss: 0.1551
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.5261s / 2161.9549 s
agent0:                 episode reward: 0.1021,                 loss: nan
agent1:                 episode reward: -0.1021,                 loss: 0.1570
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 142.6867s / 2304.6416 s
agent0:                 episode reward: -0.2154,                 loss: nan
agent1:                 episode reward: 0.2154,                 loss: 0.1551
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.4639s / 2445.1054 s
agent0:                 episode reward: -0.0570,                 loss: nan
agent1:                 episode reward: 0.0570,                 loss: 0.1710
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.5601s / 2584.6655 s
agent0:                 episode reward: 0.0287,                 loss: nan
agent1:                 episode reward: -0.0287,                 loss: 0.1633
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.6368s / 2723.3023 s
agent0:                 episode reward: -0.2391,                 loss: nan
agent1:                 episode reward: 0.2391,                 loss: 0.1606
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.5736s / 2862.8759 s
agent0:                 episode reward: -0.2291,                 loss: nan
agent1:                 episode reward: 0.2291,                 loss: 0.1597
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.4715s / 3001.3474 s
agent0:                 episode reward: 0.0212,                 loss: nan
agent1:                 episode reward: -0.0212,                 loss: 0.1589
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.0822s / 3139.4296 s
agent0:                 episode reward: -0.1457,                 loss: nan
agent1:                 episode reward: 0.1457,                 loss: 0.1604
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.5187s / 3277.9483 s
agent0:                 episode reward: -0.1624,                 loss: nan
agent1:                 episode reward: 0.1624,                 loss: 0.1593
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.1502s / 3417.0985 s
agent0:                 episode reward: -0.2566,                 loss: nan
agent1:                 episode reward: 0.2566,                 loss: 0.1587
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.2326s / 3555.3311 s
agent0:                 episode reward: 0.3005,                 loss: nan
agent1:                 episode reward: -0.3005,                 loss: 0.1584
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.5628s / 3695.8939 s
agent0:                 episode reward: 0.0529,                 loss: nan
agent1:                 episode reward: -0.0529,                 loss: 0.1567
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 136.2779s / 3832.1718 s
agent0:                 episode reward: 0.2950,                 loss: nan
agent1:                 episode reward: -0.2950,                 loss: 0.1570
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.3030s / 3970.4748 s
agent0:                 episode reward: 0.3823,                 loss: nan
agent1:                 episode reward: -0.3823,                 loss: 0.1564
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.7908s / 4109.2656 s
agent0:                 episode reward: 0.1324,                 loss: nan
agent1:                 episode reward: -0.1324,                 loss: 0.1540
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.6267s / 4247.8922 s
agent0:                 episode reward: 0.1758,                 loss: nan
agent1:                 episode reward: -0.1758,                 loss: 0.1556
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.5813s / 4387.4735 s
agent0:                 episode reward: 0.3519,                 loss: nan
agent1:                 episode reward: -0.3519,                 loss: 0.1535
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 136.6928s / 4524.1664 s
agent0:                 episode reward: -0.0758,                 loss: nan
agent1:                 episode reward: 0.0758,                 loss: 0.1529
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 137.4655s / 4661.6318 s
agent0:                 episode reward: -0.2261,                 loss: nan
agent1:                 episode reward: 0.2261,                 loss: 0.1520
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.5991s / 4802.2309 s
agent0:                 episode reward: -0.4574,                 loss: nan
agent1:                 episode reward: 0.4574,                 loss: 0.1482
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.8516s / 4943.0826 s
agent0:                 episode reward: 0.1683,                 loss: nan
agent1:                 episode reward: -0.1683,                 loss: 0.1478
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 137.4975s / 5080.5800 s
agent0:                 episode reward: -0.4739,                 loss: nan
agent1:                 episode reward: 0.4739,                 loss: 0.1473
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.5277s / 5220.1077 s
agent0:                 episode reward: 0.0612,                 loss: nan
agent1:                 episode reward: -0.0612,                 loss: 0.1465
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.3073s / 5359.4151 s
agent0:                 episode reward: -0.2767,                 loss: nan
agent1:                 episode reward: 0.2767,                 loss: 0.1469
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.1096s / 5499.5247 s
agent0:                 episode reward: -0.0341,                 loss: nan
agent1:                 episode reward: 0.0341,                 loss: 0.1443
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.0871s / 5636.6118 s
agent0:                 episode reward: -0.0218,                 loss: nan
agent1:                 episode reward: 0.0218,                 loss: 0.1444
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.8440s / 5775.4558 s
agent0:                 episode reward: 0.1621,                 loss: nan
agent1:                 episode reward: -0.1621,                 loss: 0.1443
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 137.1115s / 5912.5673 s
agent0:                 episode reward: 0.1761,                 loss: nan
agent1:                 episode reward: -0.1761,                 loss: 0.1446
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.8530s / 6050.4203 s
agent0:                 episode reward: -0.0865,                 loss: nan
agent1:                 episode reward: 0.0865,                 loss: 0.1447
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 135.3914s / 6185.8117 s
agent0:                 episode reward: 0.1494,                 loss: nan
agent1:                 episode reward: -0.1494,                 loss: 0.1441
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.9238s / 6324.7355 s
agent0:                 episode reward: -0.3996,                 loss: nan
agent1:                 episode reward: 0.3996,                 loss: 0.1421
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.9869s / 6464.7224 s
agent0:                 episode reward: 0.2406,                 loss: nan
agent1:                 episode reward: -0.2406,                 loss: 0.1441
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.8527s / 6604.5750 s
agent0:                 episode reward: -0.1193,                 loss: nan
agent1:                 episode reward: 0.1193,                 loss: 0.1443
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 142.5171s / 6747.0921 s
agent0:                 episode reward: -0.1235,                 loss: nan
agent1:                 episode reward: 0.1235,                 loss: 0.1421
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 136.4172s / 6883.5093 s
agent0:                 episode reward: -0.0420,                 loss: nan
agent1:                 episode reward: 0.0420,                 loss: 0.1420
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.4168s / 7020.9261 s
agent0:                 episode reward: -0.0705,                 loss: nan
agent1:                 episode reward: 0.0705,                 loss: 0.1442
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.9193s / 7159.8454 s
agent0:                 episode reward: -0.0042,                 loss: nan
agent1:                 episode reward: 0.0042,                 loss: 0.1471
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.4889s / 7297.3343 s
agent0:                 episode reward: -0.3208,                 loss: nan
agent1:                 episode reward: 0.3208,                 loss: 0.1475
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.9722s / 7437.3064 s
agent0:                 episode reward: -0.2893,                 loss: nan
agent1:                 episode reward: 0.2893,                 loss: 0.1460
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.7496s / 7578.0560 s
agent0:                 episode reward: 0.0154,                 loss: nan
agent1:                 episode reward: -0.0154,                 loss: 0.1452
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 135.9735s / 7714.0296 s
agent0:                 episode reward: -0.2588,                 loss: nan
agent1:                 episode reward: 0.2588,                 loss: 0.1457
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.4008s / 7855.4303 s
agent0:                 episode reward: -0.1760,                 loss: nan
agent1:                 episode reward: 0.1760,                 loss: 0.1465
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 137.5782s / 7993.0085 s
agent0:                 episode reward: -0.1871,                 loss: nan
agent1:                 episode reward: 0.1871,                 loss: 0.1465
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 137.0921s / 8130.1006 s
agent0:                 episode reward: -0.0567,                 loss: nan
agent1:                 episode reward: 0.0567,                 loss: 0.1465
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.0681s / 8268.1687 s
agent0:                 episode reward: 0.3925,                 loss: nan
agent1:                 episode reward: -0.3925,                 loss: 0.1447
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 137.7985s / 8405.9671 s
agent0:                 episode reward: -0.1522,                 loss: nan
agent1:                 episode reward: 0.1522,                 loss: 0.1455
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.6152s / 8546.5823 s
agent0:                 episode reward: -0.1892,                 loss: nan
agent1:                 episode reward: 0.1892,                 loss: 0.1443
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 136.7766s / 8683.3590 s
agent0:                 episode reward: -0.3093,                 loss: nan
agent1:                 episode reward: 0.3093,                 loss: 0.1435
Episode: 1481/30000 (4.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.6456s / 8824.0046 s
agent0:                 episode reward: 0.0226,                 loss: nan
agent1:                 episode reward: -0.0226,                 loss: 0.1467
Episode: 1501/30000 (5.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.5969s / 8962.6015 s
agent0:                 episode reward: -0.0222,                 loss: nan
agent1:                 episode reward: 0.0222,                 loss: 0.1428
Episode: 1521/30000 (5.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.2489s / 9100.8505 s
agent0:                 episode reward: 0.0295,                 loss: nan
agent1:                 episode reward: -0.0295,                 loss: 0.1431
Episode: 1541/30000 (5.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 138.8297s / 9239.6802 s
agent0:                 episode reward: -0.2660,                 loss: nan
agent1:                 episode reward: 0.2660,                 loss: 0.1444
Episode: 1561/30000 (5.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 133.9216s / 9373.6018 s
agent0:                 episode reward: -0.1293,                 loss: nan
agent1:                 episode reward: 0.1293,                 loss: 0.1421
Episode: 1581/30000 (5.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 139.2512s / 9512.8530 s
agent0:                 episode reward: -0.0400,                 loss: nan
agent1:                 episode reward: 0.0400,                 loss: 0.1409
Episode: 1601/30000 (5.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.7221s / 9653.5750 s
agent0:                 episode reward: -0.2325,                 loss: nan
agent1:                 episode reward: 0.2325,                 loss: 0.1412
Episode: 1621/30000 (5.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 144.0977s / 9797.6727 s
agent0:                 episode reward: -0.1582,                 loss: nan
agent1:                 episode reward: 0.1582,                 loss: 0.1412
Episode: 1641/30000 (5.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.6482s / 9938.3210 s
agent0:                 episode reward: 0.1266,                 loss: nan
agent1:                 episode reward: -0.1266,                 loss: 0.1407
Episode: 1661/30000 (5.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 143.1915s / 10081.5124 s
agent0:                 episode reward: 0.0925,                 loss: nan
agent1:                 episode reward: -0.0925,                 loss: 0.1397
Episode: 1681/30000 (5.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.9156s / 10223.4280 s
agent0:                 episode reward: -0.0161,                 loss: nan
agent1:                 episode reward: 0.0161,                 loss: 0.1406
Episode: 1701/30000 (5.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.9316s / 10365.3597 s
agent0:                 episode reward: -0.1557,                 loss: nan
agent1:                 episode reward: 0.1557,                 loss: 0.1413
Episode: 1721/30000 (5.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.7278s / 10507.0875 s
agent0:                 episode reward: 0.0736,                 loss: nan
agent1:                 episode reward: -0.0736,                 loss: 0.1394
Episode: 1741/30000 (5.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.9340s / 10647.0215 s
agent0:                 episode reward: -0.2175,                 loss: nan
agent1:                 episode reward: 0.2175,                 loss: 0.1404
Episode: 1761/30000 (5.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.3622s / 10787.3837 s
agent0:                 episode reward: -0.2816,                 loss: nan
agent1:                 episode reward: 0.2816,                 loss: 0.1398
Episode: 1781/30000 (5.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.6040s / 10927.9878 s
agent0:                 episode reward: 0.1766,                 loss: nan
agent1:                 episode reward: -0.1766,                 loss: 0.1401
Episode: 1801/30000 (6.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.4154s / 11069.4032 s
agent0:                 episode reward: 0.0468,                 loss: nan
agent1:                 episode reward: -0.0468,                 loss: 0.1401
Episode: 1821/30000 (6.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.6581s / 11208.0612 s
agent0:                 episode reward: 0.0582,                 loss: nan
agent1:                 episode reward: -0.0582,                 loss: 0.1385
Episode: 1841/30000 (6.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 143.9100s / 11351.9712 s
agent0:                 episode reward: 0.0266,                 loss: nan
agent1:                 episode reward: -0.0266,                 loss: 0.1404
Episode: 1861/30000 (6.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 142.2720s / 11494.2432 s
agent0:                 episode reward: -0.1687,                 loss: nan
agent1:                 episode reward: 0.1687,                 loss: 0.1389
Episode: 1881/30000 (6.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.8715s / 11633.1147 s
agent0:                 episode reward: -0.1996,                 loss: nan
agent1:                 episode reward: 0.1996,                 loss: 0.1397
Episode: 1901/30000 (6.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.7593s / 11772.8740 s
agent0:                 episode reward: -0.3703,                 loss: nan
agent1:                 episode reward: 0.3703,                 loss: 0.1469
Episode: 1921/30000 (6.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.0516s / 11911.9256 s
agent0:                 episode reward: 0.1355,                 loss: nan
agent1:                 episode reward: -0.1355,                 loss: 0.1461
Episode: 1941/30000 (6.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 143.5138s / 12055.4394 s
agent0:                 episode reward: 0.0372,                 loss: nan
agent1:                 episode reward: -0.0372,                 loss: 0.1453
Episode: 1961/30000 (6.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.4490s / 12194.8884 s
agent0:                 episode reward: -0.2626,                 loss: nan
agent1:                 episode reward: 0.2626,                 loss: 0.1462
Episode: 1981/30000 (6.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.8792s / 12336.7676 s
agent0:                 episode reward: -0.0964,                 loss: nan
agent1:                 episode reward: 0.0964,                 loss: 0.1467
Episode: 2001/30000 (6.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.4476s / 12478.2152 s
agent0:                 episode reward: 0.1011,                 loss: nan
agent1:                 episode reward: -0.1011,                 loss: 0.1474
Episode: 2021/30000 (6.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.2602s / 12618.4754 s
agent0:                 episode reward: 0.0151,                 loss: nan
agent1:                 episode reward: -0.0151,                 loss: 0.1457
Episode: 2041/30000 (6.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.2276s / 12759.7029 s
agent0:                 episode reward: -0.4463,                 loss: nan
agent1:                 episode reward: 0.4463,                 loss: 0.1463
Episode: 2061/30000 (6.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 141.6716s / 12901.3746 s
agent0:                 episode reward: -0.5743,                 loss: nan
agent1:                 episode reward: 0.5743,                 loss: 0.1464
Episode: 2081/30000 (6.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 144.7976s / 13046.1722 s
agent0:                 episode reward: -0.2912,                 loss: nan
agent1:                 episode reward: 0.2912,                 loss: 0.1474
Episode: 2101/30000 (7.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.9572s / 13188.1294 s
agent0:                 episode reward: -0.3568,                 loss: nan
agent1:                 episode reward: 0.3568,                 loss: 0.1463
Episode: 2121/30000 (7.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 142.5497s / 13330.6791 s
agent0:                 episode reward: 0.1493,                 loss: nan
agent1:                 episode reward: -0.1493,                 loss: 0.1450
Episode: 2141/30000 (7.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 143.4694s / 13474.1485 s
agent0:                 episode reward: 0.0571,                 loss: nan
agent1:                 episode reward: -0.0571,                 loss: 0.1462
Episode: 2161/30000 (7.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.4355s / 13614.5840 s
agent0:                 episode reward: 0.1574,                 loss: nan
agent1:                 episode reward: -0.1574,                 loss: 0.1472
Episode: 2181/30000 (7.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.2413s / 13752.8253 s
agent0:                 episode reward: 0.0040,                 loss: nan
agent1:                 episode reward: -0.0040,                 loss: 0.1440
Episode: 2201/30000 (7.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.7049s / 13894.5302 s
agent0:                 episode reward: -0.1079,                 loss: nan
agent1:                 episode reward: 0.1079,                 loss: 0.1462
Episode: 2221/30000 (7.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.1388s / 14032.6690 s
agent0:                 episode reward: -0.2496,                 loss: nan
agent1:                 episode reward: 0.2496,                 loss: 0.1464
Episode: 2241/30000 (7.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 142.3457s / 14175.0147 s
agent0:                 episode reward: -0.0475,                 loss: nan
agent1:                 episode reward: 0.0475,                 loss: 0.1459
Episode: 2261/30000 (7.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.0098s / 14314.0245 s
agent0:                 episode reward: -0.0435,                 loss: nan
agent1:                 episode reward: 0.0435,                 loss: 0.1456
Episode: 2281/30000 (7.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 140.4224s / 14454.4470 s
agent0:                 episode reward: -0.1253,                 loss: nan
agent1:                 episode reward: 0.1253,                 loss: 0.1439
Episode: 2301/30000 (7.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 140.2090s / 14594.6560 s
agent0:                 episode reward: 0.1309,                 loss: nan
agent1:                 episode reward: -0.1309,                 loss: 0.1451
Episode: 2321/30000 (7.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.2518s / 14735.9077 s
agent0:                 episode reward: 0.1405,                 loss: nan
agent1:                 episode reward: -0.1405,                 loss: 0.1448
Episode: 2341/30000 (7.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 139.3085s / 14875.2162 s
agent0:                 episode reward: -0.0950,                 loss: nan
agent1:                 episode reward: 0.0950,                 loss: 0.1444
Episode: 2361/30000 (7.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 137.8320s / 15013.0482 s
agent0:                 episode reward: 0.1500,                 loss: nan
agent1:                 episode reward: -0.1500,                 loss: 0.1438
Episode: 2381/30000 (7.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 142.4399s / 15155.4881 s
agent0:                 episode reward: -0.5387,                 loss: nan
agent1:                 episode reward: 0.5387,                 loss: 0.1455
Episode: 2401/30000 (8.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 138.8794s / 15294.3675 s
agent0:                 episode reward: 0.0794,                 loss: nan
agent1:                 episode reward: -0.0794,                 loss: 0.1442
Episode: 2421/30000 (8.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 142.1508s / 15436.5183 s
agent0:                 episode reward: -0.0625,                 loss: nan
agent1:                 episode reward: 0.0625,                 loss: 0.1463
Episode: 2441/30000 (8.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 141.0683s / 15577.5866 s
agent0:                 episode reward: -0.4663,                 loss: nan
agent1:                 episode reward: 0.4663,                 loss: 0.1463
Episode: 2461/30000 (8.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 141.7631s / 15719.3498 s
agent0:                 episode reward: -0.5288,                 loss: nan
agent1:                 episode reward: 0.5288,                 loss: 0.1449
Episode: 2481/30000 (8.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 138.6565s / 15858.0063 s
agent0:                 episode reward: 0.0024,                 loss: nan
agent1:                 episode reward: -0.0024,                 loss: 0.1467
Episode: 2501/30000 (8.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 140.2872s / 15998.2935 s
agent0:                 episode reward: -0.0886,                 loss: nan
agent1:                 episode reward: 0.0886,                 loss: 0.1465
Episode: 2521/30000 (8.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 144.0909s / 16142.3844 s
agent0:                 episode reward: -0.1915,                 loss: nan
agent1:                 episode reward: 0.1915,                 loss: 0.1441
Episode: 2541/30000 (8.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 143.6348s / 16286.0191 s
agent0:                 episode reward: -0.1359,                 loss: nan
agent1:                 episode reward: 0.1359,                 loss: 0.1443
Episode: 2561/30000 (8.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 139.8112s / 16425.8303 s
agent0:                 episode reward: -0.1531,                 loss: nan