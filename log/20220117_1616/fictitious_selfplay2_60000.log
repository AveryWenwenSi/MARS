pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7fbe9c83add0>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.01 0.01 0.01 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.01 0.01 0.01]]
Load checkpoints (policy family):  [['50' '5253' '7615' ... '58686' '59037' '59319']
 ['193' '5289' '7712' ... '58826' '59185' '59425']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220117153310/epi_60000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220117153310_exploit_60000/mdp_arbitrary_mdp_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220117153310_exploit_60000/mdp_arbitrary_mdp_fictitious_selfplay2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4602s / 5.4602 s
agent0:                 episode reward: -1.6958,                 loss: nan
agent1:                 episode reward: 1.6958,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8367s / 12.2969 s
agent0:                 episode reward: 0.0561,                 loss: nan
agent1:                 episode reward: -0.0561,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4299s / 18.7268 s
agent0:                 episode reward: -0.0892,                 loss: nan
agent1:                 episode reward: 0.0892,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2239s / 24.9507 s
agent0:                 episode reward: 0.3302,                 loss: nan
agent1:                 episode reward: -0.3302,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1279s / 31.0786 s
agent0:                 episode reward: -0.2355,                 loss: nan
agent1:                 episode reward: 0.2355,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1913s / 38.2699 s
agent0:                 episode reward: -0.3083,                 loss: nan
agent1:                 episode reward: 0.3083,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0548s / 45.3247 s
agent0:                 episode reward: 0.0128,                 loss: nan
agent1:                 episode reward: -0.0128,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5005s / 52.8252 s
agent0:                 episode reward: -0.1602,                 loss: nan
agent1:                 episode reward: 0.1602,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2800s / 60.1052 s
agent0:                 episode reward: 0.0930,                 loss: nan
agent1:                 episode reward: -0.0930,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6647s / 67.7699 s
agent0:                 episode reward: 0.2745,                 loss: nan
agent1:                 episode reward: -0.2745,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9507s / 74.7206 s
agent0:                 episode reward: 0.3474,                 loss: nan
agent1:                 episode reward: -0.3474,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 96.4758s / 171.1965 s
agent0:                 episode reward: 0.4612,                 loss: nan
agent1:                 episode reward: -0.4612,                 loss: 0.1806
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.9590s / 412.1554 s
agent0:                 episode reward: -0.2442,                 loss: nan
agent1:                 episode reward: 0.2442,                 loss: 0.1748
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.9403s / 655.0958 s
agent0:                 episode reward: -0.3335,                 loss: nan
agent1:                 episode reward: 0.3335,                 loss: 0.1688
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 241.1168s / 896.2125 s
agent0:                 episode reward: 0.2389,                 loss: nan
agent1:                 episode reward: -0.2389,                 loss: 0.1651
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.7384s / 1138.9509 s
agent0:                 episode reward: 0.1247,                 loss: nan
agent1:                 episode reward: -0.1247,                 loss: 0.1622
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 250.6485s / 1389.5994 s
agent0:                 episode reward: 0.0349,                 loss: nan
agent1:                 episode reward: -0.0349,                 loss: 0.1587
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 239.3044s / 1628.9038 s
agent0:                 episode reward: 0.0158,                 loss: nan
agent1:                 episode reward: -0.0158,                 loss: 0.1563
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 243.8103s / 1872.7141 s
agent0:                 episode reward: -0.1236,                 loss: nan
agent1:                 episode reward: 0.1236,                 loss: 0.1540
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.8099s / 2115.5240 s
agent0:                 episode reward: -0.1121,                 loss: nan
agent1:                 episode reward: 0.1121,                 loss: 0.1537
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.0686s / 2358.5926 s
agent0:                 episode reward: -0.1542,                 loss: nan
agent1:                 episode reward: 0.1542,                 loss: 0.1547
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.7808s / 2603.3734 s
agent0:                 episode reward: 0.3092,                 loss: nan
agent1:                 episode reward: -0.3092,                 loss: 0.1527
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.9592s / 2849.3326 s
agent0:                 episode reward: 0.1535,                 loss: nan
agent1:                 episode reward: -0.1535,                 loss: 0.1518
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 251.6982s / 3101.0308 s
agent0:                 episode reward: 0.4208,                 loss: nan
agent1:                 episode reward: -0.4208,                 loss: 0.1515
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 237.2961s / 3338.3269 s
agent0:                 episode reward: 0.4079,                 loss: nan
agent1:                 episode reward: -0.4079,                 loss: 0.1517
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.2550s / 3579.5819 s
agent0:                 episode reward: 0.1429,                 loss: nan
agent1:                 episode reward: -0.1429,                 loss: 0.1496
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.9325s / 3824.5144 s
agent0:                 episode reward: -0.1268,                 loss: nan
agent1:                 episode reward: 0.1268,                 loss: 0.1482
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 250.0942s / 4074.6087 s
agent0:                 episode reward: -0.4935,                 loss: nan
agent1:                 episode reward: 0.4935,                 loss: 0.1470
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 249.7446s / 4324.3532 s
agent0:                 episode reward: 0.1521,                 loss: nan
agent1:                 episode reward: -0.1521,                 loss: 0.1593
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.7968s / 4568.1501 s
agent0:                 episode reward: 0.0073,                 loss: nan
agent1:                 episode reward: -0.0073,                 loss: 0.1591
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.5103s / 4812.6603 s
agent0:                 episode reward: 0.1368,                 loss: nan
agent1:                 episode reward: -0.1368,                 loss: 0.1596
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.8612s / 5060.5215 s
agent0:                 episode reward: -0.0970,                 loss: nan
agent1:                 episode reward: 0.0970,                 loss: 0.1579
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.7013s / 5305.2229 s
agent0:                 episode reward: -0.3485,                 loss: nan
agent1:                 episode reward: 0.3485,                 loss: 0.1563
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.2715s / 5553.4944 s
agent0:                 episode reward: -0.1304,                 loss: nan
agent1:                 episode reward: 0.1304,                 loss: 0.1563
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 251.2064s / 5804.7008 s
agent0:                 episode reward: -0.1165,                 loss: nan
agent1:                 episode reward: 0.1165,                 loss: 0.1555
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 250.3856s / 6055.0864 s
agent0:                 episode reward: 0.0224,                 loss: nan
agent1:                 episode reward: -0.0224,                 loss: 0.1549
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.3584s / 6301.4448 s
agent0:                 episode reward: -0.0407,                 loss: nan
agent1:                 episode reward: 0.0407,                 loss: 0.1544
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.0969s / 6548.5417 s
agent0:                 episode reward: 0.2505,                 loss: nan
agent1:                 episode reward: -0.2505,                 loss: 0.1543
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.2816s / 6793.8233 s
agent0:                 episode reward: -0.0747,                 loss: nan
agent1:                 episode reward: 0.0747,                 loss: 0.1525
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.5295s / 7042.3528 s
agent0:                 episode reward: -0.1216,                 loss: nan
agent1:                 episode reward: 0.1216,                 loss: 0.1530
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 238.5035s / 7280.8562 s
agent0:                 episode reward: -0.3136,                 loss: nan
agent1:                 episode reward: 0.3136,                 loss: 0.1522
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.2236s / 7525.0798 s
agent0:                 episode reward: -0.2833,                 loss: nan
agent1:                 episode reward: 0.2833,                 loss: 0.1495
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 250.5769s / 7775.6567 s
agent0:                 episode reward: -0.1594,                 loss: nan
agent1:                 episode reward: 0.1594,                 loss: 0.1505
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 233.1252s / 8008.7820 s
agent0:                 episode reward: 0.0083,                 loss: nan
agent1:                 episode reward: -0.0083,                 loss: 0.1505
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.9051s / 8252.6871 s
agent0:                 episode reward: -0.0690,                 loss: nan
agent1:                 episode reward: 0.0690,                 loss: 0.1501
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 243.1963s / 8495.8833 s
agent0:                 episode reward: -0.2205,                 loss: nan
agent1:                 episode reward: 0.2205,                 loss: 0.1450
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.6265s / 8740.5098 s
agent0:                 episode reward: 0.0848,                 loss: nan
agent1:                 episode reward: -0.0848,                 loss: 0.1443
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.3030s / 8985.8128 s
agent0:                 episode reward: -0.2579,                 loss: nan
agent1:                 episode reward: 0.2579,                 loss: 0.1451
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.0548s / 9227.8676 s
agent0:                 episode reward: 0.1261,                 loss: nan
agent1:                 episode reward: -0.1261,                 loss: 0.1436
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 248.1527s / 9476.0202 s
agent0:                 episode reward: -0.1057,                 loss: nan
agent1:                 episode reward: 0.1057,                 loss: 0.1421
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.1205s / 9721.1408 s
agent0:                 episode reward: -0.2787,                 loss: nan
agent1:                 episode reward: 0.2787,                 loss: 0.1439
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 245.5596s / 9966.7004 s
agent0:                 episode reward: 0.2799,                 loss: nan
agent1:                 episode reward: -0.2799,                 loss: 0.1418
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.2115s / 10208.9119 s
agent0:                 episode reward: 0.1561,                 loss: nan
agent1:                 episode reward: -0.1561,                 loss: 0.1407
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.2850s / 10451.1969 s
agent0:                 episode reward: -0.4633,                 loss: nan
agent1:                 episode reward: 0.4633,                 loss: 0.1427
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 254.3049s / 10705.5019 s
agent0:                 episode reward: -0.0413,                 loss: nan
agent1:                 episode reward: 0.0413,                 loss: 0.1403
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 236.5095s / 10942.0114 s
agent0:                 episode reward: 0.1807,                 loss: nan
agent1:                 episode reward: -0.1807,                 loss: 0.1403
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 250.4028s / 11192.4142 s
agent0:                 episode reward: 0.3805,                 loss: nan
agent1:                 episode reward: -0.3805,                 loss: 0.1395
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 239.5422s / 11431.9564 s
agent0:                 episode reward: -0.0124,                 loss: nan
agent1:                 episode reward: 0.0124,                 loss: 0.1400
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.6922s / 11679.6486 s
agent0:                 episode reward: 0.3111,                 loss: nan
agent1:                 episode reward: -0.3111,                 loss: 0.1397
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.8397s / 11925.4883 s
agent0:                 episode reward: -0.1955,                 loss: nan
agent1:                 episode reward: 0.1955,                 loss: 0.1395
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 245.3358s / 12170.8241 s
agent0:                 episode reward: 0.1984,                 loss: nan
agent1:                 episode reward: -0.1984,                 loss: 0.1357
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.0160s / 12414.8401 s
agent0:                 episode reward: 0.0257,                 loss: nan
agent1:                 episode reward: -0.0257,                 loss: 0.1381
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 241.2581s / 12656.0982 s
agent0:                 episode reward: -0.0144,                 loss: nan
agent1:                 episode reward: 0.0144,                 loss: 0.1368
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.2542s / 12903.3524 s
agent0:                 episode reward: 0.3920,                 loss: nan
agent1:                 episode reward: -0.3920,                 loss: 0.1363
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 248.6550s / 13152.0074 s
agent0:                 episode reward: 0.2362,                 loss: nan
agent1:                 episode reward: -0.2362,                 loss: 0.1373
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.4520s / 13397.4594 s
agent0:                 episode reward: -0.3826,                 loss: nan
agent1:                 episode reward: 0.3826,                 loss: 0.1374
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 255.3550s / 13652.8144 s
agent0:                 episode reward: 0.0957,                 loss: nan
agent1:                 episode reward: -0.0957,                 loss: 0.1361
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.7694s / 13898.5838 s
agent0:                 episode reward: -0.0441,                 loss: nan
agent1:                 episode reward: 0.0441,                 loss: 0.1359
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 247.2457s / 14145.8295 s
agent0:                 episode reward: 0.1302,                 loss: nan
agent1:                 episode reward: -0.1302,                 loss: 0.1363
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.9956s / 14393.8251 s
agent0:                 episode reward: -0.0154,                 loss: nan
agent1:                 episode reward: 0.0154,                 loss: 0.1353
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 248.9427s / 14642.7678 s
agent0:                 episode reward: -0.1003,                 loss: nan
agent1:                 episode reward: 0.1003,                 loss: 0.1364
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.5741s / 14888.3420 s
agent0:                 episode reward: -0.0430,                 loss: nan
agent1:                 episode reward: 0.0430,                 loss: 0.1335
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 243.3515s / 15131.6934 s
agent0:                 episode reward: -0.4229,                 loss: nan
agent1:                 episode reward: 0.4229,                 loss: 0.1352
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 250.2846s / 15381.9780 s
agent0:                 episode reward: 0.5562,                 loss: nan