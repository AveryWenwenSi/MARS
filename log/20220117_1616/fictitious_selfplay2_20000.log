pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7fb263198050>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.037 0.037 0.037 ... 0.037 0.037 0.037]
 [0.037 0.037 0.037 ... 0.037 0.037 0.037]]
Load checkpoints (policy family):  [['50' '5253' '7615' ... '18640' '19318' '19446']
 ['193' '5289' '7712' ... '18710' '19358' '19474']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220117153310/epi_20000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220117153310_exploit_20000/mdp_arbitrary_mdp_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220117153310_exploit_20000/mdp_arbitrary_mdp_fictitious_selfplay2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8413s / 4.8413 s
agent0:                 episode reward: 1.8927,                 loss: nan
agent1:                 episode reward: -1.8927,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7598s / 11.6011 s
agent0:                 episode reward: -0.0524,                 loss: nan
agent1:                 episode reward: 0.0524,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7471s / 18.3482 s
agent0:                 episode reward: 0.0251,                 loss: nan
agent1:                 episode reward: -0.0251,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2169s / 24.5651 s
agent0:                 episode reward: -0.0938,                 loss: nan
agent1:                 episode reward: 0.0938,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3908s / 30.9559 s
agent0:                 episode reward: 0.0578,                 loss: nan
agent1:                 episode reward: -0.0578,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3678s / 38.3236 s
agent0:                 episode reward: -0.0224,                 loss: nan
agent1:                 episode reward: 0.0224,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8633s / 45.1869 s
agent0:                 episode reward: -0.1636,                 loss: nan
agent1:                 episode reward: 0.1636,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4642s / 52.6511 s
agent0:                 episode reward: 0.4286,                 loss: nan
agent1:                 episode reward: -0.4286,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2619s / 59.9130 s
agent0:                 episode reward: -0.2185,                 loss: nan
agent1:                 episode reward: 0.2185,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7706s / 67.6836 s
agent0:                 episode reward: 0.2451,                 loss: nan
agent1:                 episode reward: -0.2451,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8625s / 74.5461 s
agent0:                 episode reward: 0.1484,                 loss: nan
agent1:                 episode reward: -0.1484,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 97.7865s / 172.3326 s
agent0:                 episode reward: 0.6362,                 loss: nan
agent1:                 episode reward: -0.6362,                 loss: 0.1764
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.6334s / 419.9660 s
agent0:                 episode reward: -0.0667,                 loss: nan
agent1:                 episode reward: 0.0667,                 loss: 0.1720
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.7188s / 662.6848 s
agent0:                 episode reward: -0.0190,                 loss: nan
agent1:                 episode reward: 0.0190,                 loss: 0.1665
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.6615s / 905.3464 s
agent0:                 episode reward: 0.0488,                 loss: nan
agent1:                 episode reward: -0.0488,                 loss: 0.1630
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 237.0220s / 1142.3684 s
agent0:                 episode reward: 0.4486,                 loss: nan
agent1:                 episode reward: -0.4486,                 loss: 0.1602
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.1293s / 1387.4976 s
agent0:                 episode reward: 0.1816,                 loss: nan
agent1:                 episode reward: -0.1816,                 loss: 0.1563
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.7602s / 1630.2578 s
agent0:                 episode reward: 0.0907,                 loss: nan
agent1:                 episode reward: -0.0907,                 loss: 0.1522
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.8087s / 1879.0666 s
agent0:                 episode reward: 0.0608,                 loss: nan
agent1:                 episode reward: -0.0608,                 loss: 0.1502
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.9254s / 2121.9920 s
agent0:                 episode reward: -0.1655,                 loss: nan
agent1:                 episode reward: 0.1655,                 loss: 0.1483
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 239.6382s / 2361.6301 s
agent0:                 episode reward: -0.0641,                 loss: nan
agent1:                 episode reward: 0.0641,                 loss: 0.1456
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 237.9260s / 2599.5561 s
agent0:                 episode reward: -0.1229,                 loss: nan
agent1:                 episode reward: 0.1229,                 loss: 0.1439
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.6660s / 2840.2222 s
agent0:                 episode reward: -0.0131,                 loss: nan
agent1:                 episode reward: 0.0131,                 loss: 0.1439
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 251.5502s / 3091.7723 s
agent0:                 episode reward: 0.3255,                 loss: nan
agent1:                 episode reward: -0.3255,                 loss: 0.1428
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.6966s / 3333.4689 s
agent0:                 episode reward: 0.1565,                 loss: nan
agent1:                 episode reward: -0.1565,                 loss: 0.1434
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.8049s / 3579.2739 s
agent0:                 episode reward: -0.1268,                 loss: nan
agent1:                 episode reward: 0.1268,                 loss: 0.1442
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.9476s / 3824.2215 s
agent0:                 episode reward: 0.0323,                 loss: nan
agent1:                 episode reward: -0.0323,                 loss: 0.1429
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.5326s / 4068.7541 s
agent0:                 episode reward: -0.2005,                 loss: nan
agent1:                 episode reward: 0.2005,                 loss: 0.1426
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.5806s / 4309.3347 s
agent0:                 episode reward: -0.2247,                 loss: nan
agent1:                 episode reward: 0.2247,                 loss: 0.1420
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.6666s / 4553.0013 s
agent0:                 episode reward: -0.1664,                 loss: nan
agent1:                 episode reward: 0.1664,                 loss: 0.1404
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.1821s / 4799.1834 s
agent0:                 episode reward: -0.0581,                 loss: nan
agent1:                 episode reward: 0.0581,                 loss: 0.1404
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 251.9859s / 5051.1693 s
agent0:                 episode reward: -0.0559,                 loss: nan
agent1:                 episode reward: 0.0559,                 loss: 0.1402
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.7266s / 5295.8959 s
agent0:                 episode reward: 0.0430,                 loss: nan
agent1:                 episode reward: -0.0430,                 loss: 0.1398
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.1298s / 5536.0258 s
agent0:                 episode reward: -0.0177,                 loss: nan
agent1:                 episode reward: 0.0177,                 loss: 0.1397
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.3665s / 5781.3922 s
agent0:                 episode reward: 0.1056,                 loss: nan
agent1:                 episode reward: -0.1056,                 loss: 0.1392
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.3737s / 6026.7659 s
agent0:                 episode reward: -0.0669,                 loss: nan
agent1:                 episode reward: 0.0669,                 loss: 0.1395
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.8432s / 6274.6091 s
agent0:                 episode reward: -0.0204,                 loss: nan
agent1:                 episode reward: 0.0204,                 loss: 0.1412
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.2867s / 6515.8959 s
agent0:                 episode reward: 0.0391,                 loss: nan
agent1:                 episode reward: -0.0391,                 loss: 0.1403
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.7101s / 6760.6060 s
agent0:                 episode reward: -0.1499,                 loss: nan
agent1:                 episode reward: 0.1499,                 loss: 0.1393
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 250.0888s / 7010.6948 s
agent0:                 episode reward: -0.1514,                 loss: nan
agent1:                 episode reward: 0.1514,                 loss: 0.1387
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.3625s / 7251.0573 s
agent0:                 episode reward: -0.0245,                 loss: nan
agent1:                 episode reward: 0.0245,                 loss: 0.1373
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.7236s / 7494.7809 s
agent0:                 episode reward: -0.0222,                 loss: nan
agent1:                 episode reward: 0.0222,                 loss: 0.1372
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.7636s / 7739.5445 s
agent0:                 episode reward: -0.1630,                 loss: nan
agent1:                 episode reward: 0.1630,                 loss: 0.1371
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 236.9238s / 7976.4683 s
agent0:                 episode reward: -0.0767,                 loss: nan
agent1:                 episode reward: 0.0767,                 loss: 0.1383
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.0858s / 8219.5541 s
agent0:                 episode reward: 0.1691,                 loss: nan
agent1:                 episode reward: -0.1691,                 loss: 0.1374
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.0461s / 8463.6001 s
agent0:                 episode reward: 0.2023,                 loss: nan
agent1:                 episode reward: -0.2023,                 loss: 0.1376
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.1327s / 8709.7329 s
agent0:                 episode reward: 0.1393,                 loss: nan
agent1:                 episode reward: -0.1393,                 loss: 0.1358
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.8376s / 8955.5705 s
agent0:                 episode reward: 0.4357,                 loss: nan
agent1:                 episode reward: -0.4357,                 loss: 0.1347
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.5593s / 9202.1299 s
agent0:                 episode reward: 0.2508,                 loss: nan
agent1:                 episode reward: -0.2508,                 loss: 0.1359
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.8642s / 9442.9941 s
agent0:                 episode reward: -0.5228,                 loss: nan
agent1:                 episode reward: 0.5228,                 loss: 0.1343
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.0810s / 9687.0751 s
agent0:                 episode reward: -0.0766,                 loss: nan
agent1:                 episode reward: 0.0766,                 loss: 0.1352
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 249.4290s / 9936.5040 s
agent0:                 episode reward: -0.1086,                 loss: nan
agent1:                 episode reward: 0.1086,                 loss: 0.1339
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 249.8327s / 10186.3368 s
agent0:                 episode reward: 0.1392,                 loss: nan
agent1:                 episode reward: -0.1392,                 loss: 0.1343
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.4290s / 10432.7658 s
agent0:                 episode reward: 0.0770,                 loss: nan
agent1:                 episode reward: -0.0770,                 loss: 0.1358
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.3101s / 10675.0759 s
agent0:                 episode reward: -0.0607,                 loss: nan
agent1:                 episode reward: 0.0607,                 loss: 0.1335
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.8499s / 10916.9258 s
agent0:                 episode reward: -0.1379,                 loss: nan
agent1:                 episode reward: 0.1379,                 loss: 0.1339
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 247.5935s / 11164.5193 s
agent0:                 episode reward: 0.0976,                 loss: nan
agent1:                 episode reward: -0.0976,                 loss: 0.1326
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.5462s / 11412.0656 s
agent0:                 episode reward: 0.1992,                 loss: nan
agent1:                 episode reward: -0.1992,                 loss: 0.1351
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 252.6348s / 11664.7004 s
agent0:                 episode reward: -0.0461,                 loss: nan
agent1:                 episode reward: 0.0461,                 loss: 0.1342
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 249.6110s / 11914.3114 s
agent0:                 episode reward: -0.3166,                 loss: nan
agent1:                 episode reward: 0.3166,                 loss: 0.1322
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.5225s / 12154.8339 s
agent0:                 episode reward: 0.3711,                 loss: nan
agent1:                 episode reward: -0.3711,                 loss: 0.1323
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 252.8276s / 12407.6615 s
agent0:                 episode reward: -0.5437,                 loss: nan
agent1:                 episode reward: 0.5437,                 loss: 0.1318
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.5451s / 12654.2065 s
agent0:                 episode reward: -0.1467,                 loss: nan
agent1:                 episode reward: 0.1467,                 loss: 0.1336
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 238.9146s / 12893.1211 s
agent0:                 episode reward: -0.2323,                 loss: nan
agent1:                 episode reward: 0.2323,                 loss: 0.1333
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.8338s / 13135.9549 s
agent0:                 episode reward: -0.3130,                 loss: nan
agent1:                 episode reward: 0.3130,                 loss: 0.1328
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.7373s / 13379.6922 s
agent0:                 episode reward: 0.3046,                 loss: nan
agent1:                 episode reward: -0.3046,                 loss: 0.1313
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 245.7131s / 13625.4053 s
agent0:                 episode reward: 0.0419,                 loss: nan
agent1:                 episode reward: -0.0419,                 loss: 0.1324
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 253.5711s / 13878.9763 s
agent0:                 episode reward: -0.3978,                 loss: nan
agent1:                 episode reward: 0.3978,                 loss: 0.1340
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 253.8127s / 14132.7891 s
agent0:                 episode reward: -0.6230,                 loss: nan
agent1:                 episode reward: 0.6230,                 loss: 0.1333
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.2991s / 14381.0881 s
agent0:                 episode reward: -0.6776,                 loss: nan
agent1:                 episode reward: 0.6776,                 loss: 0.1321
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 251.7888s / 14632.8769 s
agent0:                 episode reward: -0.2157,                 loss: nan
agent1:                 episode reward: 0.2157,                 loss: 0.1325
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 247.0334s / 14879.9103 s
agent0:                 episode reward: -0.1141,                 loss: nan
agent1:                 episode reward: 0.1141,                 loss: 0.1332
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 254.3603s / 15134.2706 s
agent0:                 episode reward: 0.2187,                 loss: nan
agent1:                 episode reward: -0.2187,                 loss: 0.1314
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.8847s / 15375.1553 s
agent0:                 episode reward: -0.1137,                 loss: nan