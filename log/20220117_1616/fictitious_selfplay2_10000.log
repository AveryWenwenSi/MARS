pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7f488f09fb90>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.2 0.2 0.2 0.2 0.2]
 [0.2 0.2 0.2 0.2 0.2]]
Load checkpoints (policy family):  [['50' '5253' '7615' '8835' '9107']
 ['193' '5289' '7712' '9011' '9134']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220117153310/epi_10000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220117153310_exploit_10000/mdp_arbitrary_mdp_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220117153310_exploit_10000/mdp_arbitrary_mdp_fictitious_selfplay2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8183s / 5.8183 s
agent0:                 episode reward: 1.8170,                 loss: nan
agent1:                 episode reward: -1.8170,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7170s / 12.5353 s
agent0:                 episode reward: 0.1497,                 loss: nan
agent1:                 episode reward: -0.1497,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0414s / 19.5766 s
agent0:                 episode reward: -0.3376,                 loss: nan
agent1:                 episode reward: 0.3376,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5117s / 26.0884 s
agent0:                 episode reward: -0.1772,                 loss: nan
agent1:                 episode reward: 0.1772,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0605s / 33.1488 s
agent0:                 episode reward: -0.0070,                 loss: nan
agent1:                 episode reward: 0.0070,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7655s / 39.9144 s
agent0:                 episode reward: -0.3308,                 loss: nan
agent1:                 episode reward: 0.3308,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4658s / 46.3802 s
agent0:                 episode reward: 0.4399,                 loss: nan
agent1:                 episode reward: -0.4399,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0633s / 53.4435 s
agent0:                 episode reward: 0.2257,                 loss: nan
agent1:                 episode reward: -0.2257,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2724s / 60.7159 s
agent0:                 episode reward: 0.0890,                 loss: nan
agent1:                 episode reward: -0.0890,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7631s / 68.4790 s
agent0:                 episode reward: -0.0480,                 loss: nan
agent1:                 episode reward: 0.0480,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0985s / 75.5776 s
agent0:                 episode reward: 0.0510,                 loss: nan
agent1:                 episode reward: -0.0510,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 97.1968s / 172.7744 s
agent0:                 episode reward: -0.2606,                 loss: nan
agent1:                 episode reward: 0.2606,                 loss: 0.1964
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.2654s / 414.0398 s
agent0:                 episode reward: -0.1490,                 loss: nan
agent1:                 episode reward: 0.1490,                 loss: 0.1843
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 243.0752s / 657.1150 s
agent0:                 episode reward: -0.1006,                 loss: nan
agent1:                 episode reward: 0.1006,                 loss: 0.1753
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 239.3007s / 896.4157 s
agent0:                 episode reward: -0.0356,                 loss: nan
agent1:                 episode reward: 0.0356,                 loss: 0.1696
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 237.9460s / 1134.3617 s
agent0:                 episode reward: 0.3441,                 loss: nan
agent1:                 episode reward: -0.3441,                 loss: 0.1658
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.2520s / 1379.6136 s
agent0:                 episode reward: 0.2270,                 loss: nan
agent1:                 episode reward: -0.2270,                 loss: 0.1606
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 240.3450s / 1619.9587 s
agent0:                 episode reward: 0.0892,                 loss: nan
agent1:                 episode reward: -0.0892,                 loss: 0.1577
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.5816s / 1866.5403 s
agent0:                 episode reward: 0.0143,                 loss: nan
agent1:                 episode reward: -0.0143,                 loss: 0.1551
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.9057s / 2114.4460 s
agent0:                 episode reward: 0.0696,                 loss: nan
agent1:                 episode reward: -0.0696,                 loss: 0.1509
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.3577s / 2357.8037 s
agent0:                 episode reward: 0.1323,                 loss: nan
agent1:                 episode reward: -0.1323,                 loss: 0.1478
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.1212s / 2601.9250 s
agent0:                 episode reward: -0.1550,                 loss: nan
agent1:                 episode reward: 0.1550,                 loss: 0.1451
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.7543s / 2848.6792 s
agent0:                 episode reward: -0.2385,                 loss: nan
agent1:                 episode reward: 0.2385,                 loss: 0.1433
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.6870s / 3092.3663 s
agent0:                 episode reward: 0.2453,                 loss: nan
agent1:                 episode reward: -0.2453,                 loss: 0.1418
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 239.8688s / 3332.2351 s
agent0:                 episode reward: -0.1116,                 loss: nan
agent1:                 episode reward: 0.1116,                 loss: 0.1401
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.9441s / 3579.1792 s
agent0:                 episode reward: -0.0660,                 loss: nan
agent1:                 episode reward: 0.0660,                 loss: 0.1386
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 240.8562s / 3820.0354 s
agent0:                 episode reward: -0.0985,                 loss: nan
agent1:                 episode reward: 0.0985,                 loss: 0.1375
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 249.3894s / 4069.4248 s
agent0:                 episode reward: 0.3460,                 loss: nan
agent1:                 episode reward: -0.3460,                 loss: 0.1392
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.3414s / 4316.7662 s
agent0:                 episode reward: 0.0617,                 loss: nan
agent1:                 episode reward: -0.0617,                 loss: 0.1474
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.1859s / 4561.9520 s
agent0:                 episode reward: 0.1309,                 loss: nan
agent1:                 episode reward: -0.1309,                 loss: 0.1441
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.5697s / 4804.5217 s
agent0:                 episode reward: -0.2641,                 loss: nan
agent1:                 episode reward: 0.2641,                 loss: 0.1443
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.2966s / 5051.8183 s
agent0:                 episode reward: 0.1908,                 loss: nan
agent1:                 episode reward: -0.1908,                 loss: 0.1412
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.8971s / 5294.7155 s
agent0:                 episode reward: 0.0647,                 loss: nan
agent1:                 episode reward: -0.0647,                 loss: 0.1423
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.8427s / 5537.5582 s
agent0:                 episode reward: -0.1530,                 loss: nan
agent1:                 episode reward: 0.1530,                 loss: 0.1453
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 248.0969s / 5785.6550 s
agent0:                 episode reward: 0.3378,                 loss: nan
agent1:                 episode reward: -0.3378,                 loss: 0.1449
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 253.3638s / 6039.0188 s
agent0:                 episode reward: -0.0346,                 loss: nan
agent1:                 episode reward: 0.0346,                 loss: 0.1437
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 258.8807s / 6297.8995 s
agent0:                 episode reward: -0.0634,                 loss: nan
agent1:                 episode reward: 0.0634,                 loss: 0.1417
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.8959s / 6545.7954 s
agent0:                 episode reward: -0.4749,                 loss: nan
agent1:                 episode reward: 0.4749,                 loss: 0.1421
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 239.1124s / 6784.9078 s
agent0:                 episode reward: 0.4277,                 loss: nan
agent1:                 episode reward: -0.4277,                 loss: 0.1422
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.9582s / 7031.8660 s
agent0:                 episode reward: 0.0525,                 loss: nan
agent1:                 episode reward: -0.0525,                 loss: 0.1421
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.9050s / 7276.7710 s
agent0:                 episode reward: -0.0802,                 loss: nan
agent1:                 episode reward: 0.0802,                 loss: 0.1411
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 249.5376s / 7526.3086 s
agent0:                 episode reward: -0.0375,                 loss: nan
agent1:                 episode reward: 0.0375,                 loss: 0.1413
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.1438s / 7766.4524 s
agent0:                 episode reward: 0.0272,                 loss: nan
agent1:                 episode reward: -0.0272,                 loss: 0.1382
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 251.9980s / 8018.4504 s
agent0:                 episode reward: -0.2664,                 loss: nan
agent1:                 episode reward: 0.2664,                 loss: 0.1407
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 247.6741s / 8266.1245 s
agent0:                 episode reward: -0.4465,                 loss: nan
agent1:                 episode reward: 0.4465,                 loss: 0.1409
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 254.6669s / 8520.7914 s
agent0:                 episode reward: -0.0612,                 loss: nan
agent1:                 episode reward: 0.0612,                 loss: 0.1458
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.2683s / 8765.0597 s
agent0:                 episode reward: 0.0116,                 loss: nan
agent1:                 episode reward: -0.0116,                 loss: 0.1475
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 252.8930s / 9017.9527 s
agent0:                 episode reward: 0.1091,                 loss: nan
agent1:                 episode reward: -0.1091,                 loss: 0.1460
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 236.8526s / 9254.8054 s
agent0:                 episode reward: -0.1183,                 loss: nan
agent1:                 episode reward: 0.1183,                 loss: 0.1466
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 254.6986s / 9509.5039 s
agent0:                 episode reward: 0.0505,                 loss: nan
agent1:                 episode reward: -0.0505,                 loss: 0.1460
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 250.5628s / 9760.0667 s
agent0:                 episode reward: -0.1795,                 loss: nan
agent1:                 episode reward: 0.1795,                 loss: 0.1488
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 237.9520s / 9998.0187 s
agent0:                 episode reward: 0.2836,                 loss: nan
agent1:                 episode reward: -0.2836,                 loss: 0.1468
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 252.4010s / 10250.4197 s
agent0:                 episode reward: 0.0372,                 loss: nan
agent1:                 episode reward: -0.0372,                 loss: 0.1459
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.0142s / 10494.4339 s
agent0:                 episode reward: 0.1953,                 loss: nan
agent1:                 episode reward: -0.1953,                 loss: 0.1450
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.6557s / 10737.0896 s
agent0:                 episode reward: 0.1986,                 loss: nan
agent1:                 episode reward: -0.1986,                 loss: 0.1454
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 251.1459s / 10988.2355 s
agent0:                 episode reward: 0.1522,                 loss: nan
agent1:                 episode reward: -0.1522,                 loss: 0.1462
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 254.9331s / 11243.1687 s
agent0:                 episode reward: 0.0814,                 loss: nan
agent1:                 episode reward: -0.0814,                 loss: 0.1460
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 245.6603s / 11488.8290 s
agent0:                 episode reward: 0.2114,                 loss: nan
agent1:                 episode reward: -0.2114,                 loss: 0.1467
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 256.1358s / 11744.9648 s
agent0:                 episode reward: -0.0832,                 loss: nan
agent1:                 episode reward: 0.0832,                 loss: 0.1449
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.3677s / 11991.3325 s
agent0:                 episode reward: -0.3725,                 loss: nan
agent1:                 episode reward: 0.3725,                 loss: 0.1454
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.2432s / 12232.5757 s
agent0:                 episode reward: -0.1339,                 loss: nan
agent1:                 episode reward: 0.1339,                 loss: 0.1451
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.3281s / 12477.9037 s
agent0:                 episode reward: -0.0534,                 loss: nan
agent1:                 episode reward: 0.0534,                 loss: 0.1478
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.2516s / 12724.1553 s
agent0:                 episode reward: -0.0453,                 loss: nan
agent1:                 episode reward: 0.0453,                 loss: 0.1482
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.8924s / 12965.0476 s
agent0:                 episode reward: -0.2130,                 loss: nan
agent1:                 episode reward: 0.2130,                 loss: 0.1493
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.8690s / 13211.9167 s
agent0:                 episode reward: -0.2727,                 loss: nan
agent1:                 episode reward: 0.2727,                 loss: 0.1491
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.0234s / 13453.9401 s
agent0:                 episode reward: 0.0629,                 loss: nan
agent1:                 episode reward: -0.0629,                 loss: 0.1510
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.3993s / 13694.3394 s
agent0:                 episode reward: -0.4483,                 loss: nan
agent1:                 episode reward: 0.4483,                 loss: 0.1500
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.8798s / 13937.2191 s
agent0:                 episode reward: -0.1340,                 loss: nan
agent1:                 episode reward: 0.1340,                 loss: 0.1498
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 250.0085s / 14187.2276 s
agent0:                 episode reward: -0.1429,                 loss: nan
agent1:                 episode reward: 0.1429,                 loss: 0.1484
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.1483s / 14433.3759 s
agent0:                 episode reward: -0.1362,                 loss: nan
agent1:                 episode reward: 0.1362,                 loss: 0.1482
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 255.4822s / 14688.8581 s
agent0:                 episode reward: 0.0317,                 loss: nan
agent1:                 episode reward: -0.0317,                 loss: 0.1489
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 250.0746s / 14938.9327 s
agent0:                 episode reward: -0.2314,                 loss: nan
agent1:                 episode reward: 0.2314,                 loss: 0.1476
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 245.8595s / 15184.7921 s
agent0:                 episode reward: -0.2806,                 loss: nan
agent1:                 episode reward: 0.2806,                 loss: 0.1485
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.7384s / 15432.5305 s
agent0:                 episode reward: -0.2531,                 loss: nan