pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7fd4a6f57e50>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.018 0.018 0.018 ... 0.018 0.018 0.018]
 [0.018 0.018 0.018 ... 0.018 0.018 0.018]]
Load checkpoints (policy family):  [['50' '5253' '7615' ... '37270' '38685' '39143']
 ['193' '5289' '7712' ... '37326' '38773' '39204']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220117153310/epi_40000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220117153310_exploit_40000/mdp_arbitrary_mdp_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220117153310_exploit_40000/mdp_arbitrary_mdp_fictitious_selfplay2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0685s / 6.0685 s
agent0:                 episode reward: -0.8113,                 loss: nan
agent1:                 episode reward: 0.8113,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5633s / 12.6318 s
agent0:                 episode reward: 0.2431,                 loss: nan
agent1:                 episode reward: -0.2431,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4618s / 19.0936 s
agent0:                 episode reward: 0.0960,                 loss: nan
agent1:                 episode reward: -0.0960,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4128s / 25.5063 s
agent0:                 episode reward: -0.2529,                 loss: nan
agent1:                 episode reward: 0.2529,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9028s / 32.4092 s
agent0:                 episode reward: -0.0867,                 loss: nan
agent1:                 episode reward: 0.0867,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2578s / 39.6669 s
agent0:                 episode reward: 0.1989,                 loss: nan
agent1:                 episode reward: -0.1989,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1493s / 46.8162 s
agent0:                 episode reward: -0.0528,                 loss: nan
agent1:                 episode reward: 0.0528,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3757s / 54.1920 s
agent0:                 episode reward: 0.1404,                 loss: nan
agent1:                 episode reward: -0.1404,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4681s / 61.6601 s
agent0:                 episode reward: 0.0584,                 loss: nan
agent1:                 episode reward: -0.0584,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6346s / 68.2947 s
agent0:                 episode reward: -0.4329,                 loss: nan
agent1:                 episode reward: 0.4329,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6931s / 74.9878 s
agent0:                 episode reward: 0.2147,                 loss: nan
agent1:                 episode reward: -0.2147,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 95.7513s / 170.7392 s
agent0:                 episode reward: -0.4085,                 loss: nan
agent1:                 episode reward: 0.4085,                 loss: 0.1998
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.2045s / 418.9436 s
agent0:                 episode reward: 0.0879,                 loss: nan
agent1:                 episode reward: -0.0879,                 loss: 0.1865
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.9781s / 664.9217 s
agent0:                 episode reward: 0.1027,                 loss: nan
agent1:                 episode reward: -0.1027,                 loss: 0.1718
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 237.7380s / 902.6597 s
agent0:                 episode reward: 0.2927,                 loss: nan
agent1:                 episode reward: -0.2927,                 loss: 0.1634
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 238.2961s / 1140.9558 s
agent0:                 episode reward: 0.0250,                 loss: nan
agent1:                 episode reward: -0.0250,                 loss: 0.1588
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.2111s / 1387.1668 s
agent0:                 episode reward: 0.2764,                 loss: nan
agent1:                 episode reward: -0.2764,                 loss: 0.1535
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.8142s / 1632.9810 s
agent0:                 episode reward: -0.0430,                 loss: nan
agent1:                 episode reward: 0.0430,                 loss: 0.1511
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.0178s / 1873.9988 s
agent0:                 episode reward: 0.1811,                 loss: nan
agent1:                 episode reward: -0.1811,                 loss: 0.1475
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.6085s / 2118.6072 s
agent0:                 episode reward: 0.7167,                 loss: nan
agent1:                 episode reward: -0.7167,                 loss: 0.1463
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 238.8973s / 2357.5046 s
agent0:                 episode reward: 0.3468,                 loss: nan
agent1:                 episode reward: -0.3468,                 loss: 0.1447
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 234.0211s / 2591.5257 s
agent0:                 episode reward: 0.1509,                 loss: nan
agent1:                 episode reward: -0.1509,                 loss: 0.1434
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.5510s / 2839.0767 s
agent0:                 episode reward: 0.5091,                 loss: nan
agent1:                 episode reward: -0.5091,                 loss: 0.1427
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 253.5573s / 3092.6340 s
agent0:                 episode reward: 0.3138,                 loss: nan
agent1:                 episode reward: -0.3138,                 loss: 0.1404
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.8510s / 3340.4849 s
agent0:                 episode reward: 0.0311,                 loss: nan
agent1:                 episode reward: -0.0311,                 loss: 0.1394
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.5757s / 3581.0606 s
agent0:                 episode reward: 0.4403,                 loss: nan
agent1:                 episode reward: -0.4403,                 loss: 0.1381
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 241.4818s / 3822.5424 s
agent0:                 episode reward: -0.1670,                 loss: nan
agent1:                 episode reward: 0.1670,                 loss: 0.1357
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 252.7923s / 4075.3346 s
agent0:                 episode reward: 0.1858,                 loss: nan
agent1:                 episode reward: -0.1858,                 loss: 0.1342
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.2634s / 4321.5980 s
agent0:                 episode reward: 0.3360,                 loss: nan
agent1:                 episode reward: -0.3360,                 loss: 0.1356
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 247.9297s / 4569.5277 s
agent0:                 episode reward: 0.0701,                 loss: nan
agent1:                 episode reward: -0.0701,                 loss: 0.1317
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.4395s / 4810.9673 s
agent0:                 episode reward: -0.2783,                 loss: nan
agent1:                 episode reward: 0.2783,                 loss: 0.1293
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.5129s / 5056.4802 s
agent0:                 episode reward: -0.1698,                 loss: nan
agent1:                 episode reward: 0.1698,                 loss: 0.1301
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 241.0372s / 5297.5174 s
agent0:                 episode reward: 0.3570,                 loss: nan
agent1:                 episode reward: -0.3570,                 loss: 0.1287
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 236.8481s / 5534.3655 s
agent0:                 episode reward: -0.0291,                 loss: nan
agent1:                 episode reward: 0.0291,                 loss: 0.1297
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.1988s / 5778.5642 s
agent0:                 episode reward: 0.2116,                 loss: nan
agent1:                 episode reward: -0.2116,                 loss: 0.1293
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 247.9429s / 6026.5071 s
agent0:                 episode reward: 0.0226,                 loss: nan
agent1:                 episode reward: -0.0226,                 loss: 0.1282
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 243.9187s / 6270.4258 s
agent0:                 episode reward: 0.3037,                 loss: nan
agent1:                 episode reward: -0.3037,                 loss: 0.1288
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.5741s / 6511.9999 s
agent0:                 episode reward: 0.0268,                 loss: nan
agent1:                 episode reward: -0.0268,                 loss: 0.1263
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 239.1078s / 6751.1077 s
agent0:                 episode reward: -0.2642,                 loss: nan
agent1:                 episode reward: 0.2642,                 loss: 0.1267
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 254.6706s / 7005.7783 s
agent0:                 episode reward: -0.4426,                 loss: nan
agent1:                 episode reward: 0.4426,                 loss: 0.1262
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.1894s / 7246.9677 s
agent0:                 episode reward: -0.3733,                 loss: nan
agent1:                 episode reward: 0.3733,                 loss: 0.1258
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 252.1277s / 7499.0953 s
agent0:                 episode reward: -0.0165,                 loss: nan
agent1:                 episode reward: 0.0165,                 loss: 0.1243
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 249.0771s / 7748.1725 s
agent0:                 episode reward: -0.0605,                 loss: nan
agent1:                 episode reward: 0.0605,                 loss: 0.1251
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 243.2375s / 7991.4100 s
agent0:                 episode reward: -0.0342,                 loss: nan
agent1:                 episode reward: 0.0342,                 loss: 0.1250
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 248.6064s / 8240.0163 s
agent0:                 episode reward: -0.1999,                 loss: nan
agent1:                 episode reward: 0.1999,                 loss: 0.1254
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.6589s / 8482.6753 s
agent0:                 episode reward: -0.0059,                 loss: nan
agent1:                 episode reward: 0.0059,                 loss: 0.1207
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.8820s / 8730.5572 s
agent0:                 episode reward: -0.0051,                 loss: nan
agent1:                 episode reward: 0.0051,                 loss: 0.1210
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.0915s / 8975.6488 s
agent0:                 episode reward: -0.1489,                 loss: nan
agent1:                 episode reward: 0.1489,                 loss: 0.1203
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.3587s / 9216.0075 s
agent0:                 episode reward: 0.0135,                 loss: nan
agent1:                 episode reward: -0.0135,                 loss: 0.1207
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.7517s / 9457.7592 s
agent0:                 episode reward: 0.1805,                 loss: nan
agent1:                 episode reward: -0.1805,                 loss: 0.1192
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 237.9483s / 9695.7075 s
agent0:                 episode reward: -0.0497,                 loss: nan
agent1:                 episode reward: 0.0497,                 loss: 0.1192
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.6339s / 9940.3414 s
agent0:                 episode reward: 0.2468,                 loss: nan
agent1:                 episode reward: -0.2468,                 loss: 0.1178
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.2058s / 10185.5472 s
agent0:                 episode reward: 0.1796,                 loss: nan
agent1:                 episode reward: -0.1796,                 loss: 0.1193
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.9308s / 10430.4780 s
agent0:                 episode reward: -0.2275,                 loss: nan
agent1:                 episode reward: 0.2275,                 loss: 0.1186
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.5331s / 10679.0112 s
agent0:                 episode reward: 0.2251,                 loss: nan
agent1:                 episode reward: -0.2251,                 loss: 0.1176
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.1275s / 10926.1387 s
agent0:                 episode reward: -0.0657,                 loss: nan
agent1:                 episode reward: 0.0657,                 loss: 0.1175
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.7685s / 11169.9072 s
agent0:                 episode reward: -0.4214,                 loss: nan
agent1:                 episode reward: 0.4214,                 loss: 0.1175
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.6952s / 11414.6024 s
agent0:                 episode reward: -0.2750,                 loss: nan
agent1:                 episode reward: 0.2750,                 loss: 0.1168
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 243.3064s / 11657.9088 s
agent0:                 episode reward: 0.1965,                 loss: nan
agent1:                 episode reward: -0.1965,                 loss: 0.1157
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 252.9668s / 11910.8756 s
agent0:                 episode reward: -0.0179,                 loss: nan
agent1:                 episode reward: 0.0179,                 loss: 0.1160
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.9962s / 12158.8718 s
agent0:                 episode reward: 0.0960,                 loss: nan
agent1:                 episode reward: -0.0960,                 loss: 0.1158
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.5021s / 12403.3739 s
agent0:                 episode reward: -0.1109,                 loss: nan
agent1:                 episode reward: 0.1109,                 loss: 0.1164
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 249.7486s / 12653.1225 s
agent0:                 episode reward: -0.1525,                 loss: nan
agent1:                 episode reward: 0.1525,                 loss: 0.1141
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.6299s / 12894.7524 s
agent0:                 episode reward: -0.2290,                 loss: nan
agent1:                 episode reward: 0.2290,                 loss: 0.1149
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.2946s / 13142.0469 s
agent0:                 episode reward: 0.1992,                 loss: nan
agent1:                 episode reward: -0.1992,                 loss: 0.1158
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 240.7634s / 13382.8103 s
agent0:                 episode reward: -0.0574,                 loss: nan
agent1:                 episode reward: 0.0574,                 loss: 0.1157
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 251.1214s / 13633.9316 s
agent0:                 episode reward: 0.0054,                 loss: nan
agent1:                 episode reward: -0.0054,                 loss: 0.1158
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.1433s / 13878.0749 s
agent0:                 episode reward: 0.1980,                 loss: nan
agent1:                 episode reward: -0.1980,                 loss: 0.1166
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 250.2424s / 14128.3173 s
agent0:                 episode reward: 0.2390,                 loss: nan
agent1:                 episode reward: -0.2390,                 loss: 0.1164
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 250.5815s / 14378.8988 s
agent0:                 episode reward: -0.0758,                 loss: nan
agent1:                 episode reward: 0.0758,                 loss: 0.1160
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.0662s / 14624.9650 s
agent0:                 episode reward: 0.5399,                 loss: nan
agent1:                 episode reward: -0.5399,                 loss: 0.1147
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 247.7611s / 14872.7262 s
agent0:                 episode reward: 0.2410,                 loss: nan
agent1:                 episode reward: -0.2410,                 loss: 0.1160
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.1529s / 15119.8790 s
agent0:                 episode reward: -0.1447,                 loss: nan
agent1:                 episode reward: 0.1447,                 loss: 0.1163
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.5920s / 15366.4710 s
agent0:                 episode reward: -0.2725,                 loss: nan