pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7fdd78226fd0>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.024 0.024 0.024 ... 0.024 0.024 0.024]
 [0.024 0.024 0.024 ... 0.024 0.024 0.024]]
Load checkpoints (policy family):  [['50' '5253' '7615' ... '26681' '26940' '28709']
 ['193' '5289' '7712' ... '26747' '26990' '28804']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220117153310/epi_30000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220117153310_exploit_30000/mdp_arbitrary_mdp_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220117153310_exploit_30000/mdp_arbitrary_mdp_fictitious_selfplay2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6640s / 6.6640 s
agent0:                 episode reward: 1.1516,                 loss: nan
agent1:                 episode reward: -1.1516,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7560s / 13.4200 s
agent0:                 episode reward: -0.0878,                 loss: nan
agent1:                 episode reward: 0.0878,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6268s / 20.0468 s
agent0:                 episode reward: 0.1293,                 loss: nan
agent1:                 episode reward: -0.1293,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2080s / 26.2548 s
agent0:                 episode reward: -0.1288,                 loss: nan
agent1:                 episode reward: 0.1288,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5565s / 33.8113 s
agent0:                 episode reward: 0.2587,                 loss: nan
agent1:                 episode reward: -0.2587,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4376s / 41.2489 s
agent0:                 episode reward: -0.0058,                 loss: nan
agent1:                 episode reward: 0.0058,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8436s / 49.0925 s
agent0:                 episode reward: 0.5592,                 loss: nan
agent1:                 episode reward: -0.5592,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6359s / 55.7285 s
agent0:                 episode reward: -0.1682,                 loss: nan
agent1:                 episode reward: 0.1682,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9971s / 62.7256 s
agent0:                 episode reward: 0.0217,                 loss: nan
agent1:                 episode reward: -0.0217,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9675s / 69.6930 s
agent0:                 episode reward: 0.1903,                 loss: nan
agent1:                 episode reward: -0.1903,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9691s / 76.6621 s
agent0:                 episode reward: 0.1722,                 loss: nan
agent1:                 episode reward: -0.1722,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 98.9655s / 175.6276 s
agent0:                 episode reward: 0.1348,                 loss: nan
agent1:                 episode reward: -0.1348,                 loss: 0.2027
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 237.7733s / 413.4009 s
agent0:                 episode reward: 0.1475,                 loss: nan
agent1:                 episode reward: -0.1475,                 loss: 0.1829
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.8171s / 661.2179 s
agent0:                 episode reward: 0.2144,                 loss: nan
agent1:                 episode reward: -0.2144,                 loss: 0.1710
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.9272s / 904.1451 s
agent0:                 episode reward: -0.4600,                 loss: nan
agent1:                 episode reward: 0.4600,                 loss: 0.1620
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.3759s / 1145.5210 s
agent0:                 episode reward: -0.0734,                 loss: nan
agent1:                 episode reward: 0.0734,                 loss: 0.1550
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.6335s / 1392.1545 s
agent0:                 episode reward: -0.0787,                 loss: nan
agent1:                 episode reward: 0.0787,                 loss: 0.1514
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.7141s / 1634.8686 s
agent0:                 episode reward: -0.0232,                 loss: nan
agent1:                 episode reward: 0.0232,                 loss: 0.1480
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.6538s / 1879.5224 s
agent0:                 episode reward: 0.0517,                 loss: nan
agent1:                 episode reward: -0.0517,                 loss: 0.1452
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.0960s / 2119.6184 s
agent0:                 episode reward: 0.1883,                 loss: nan
agent1:                 episode reward: -0.1883,                 loss: 0.1430
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 238.1064s / 2357.7248 s
agent0:                 episode reward: 0.0620,                 loss: nan
agent1:                 episode reward: -0.0620,                 loss: 0.1406
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.4106s / 2600.1354 s
agent0:                 episode reward: 0.2825,                 loss: nan
agent1:                 episode reward: -0.2825,                 loss: 0.1385
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.3063s / 2842.4417 s
agent0:                 episode reward: 0.0469,                 loss: nan
agent1:                 episode reward: -0.0469,                 loss: 0.1381
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 252.6681s / 3095.1098 s
agent0:                 episode reward: -0.1149,                 loss: nan
agent1:                 episode reward: 0.1149,                 loss: 0.1363
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 243.7158s / 3338.8256 s
agent0:                 episode reward: 0.1235,                 loss: nan
agent1:                 episode reward: -0.1235,                 loss: 0.1359
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.1630s / 3584.9886 s
agent0:                 episode reward: 0.3870,                 loss: nan
agent1:                 episode reward: -0.3870,                 loss: 0.1355
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 249.4812s / 3834.4698 s
agent0:                 episode reward: -0.4256,                 loss: nan
agent1:                 episode reward: 0.4256,                 loss: 0.1337
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.9242s / 4076.3940 s
agent0:                 episode reward: -0.2169,                 loss: nan
agent1:                 episode reward: 0.2169,                 loss: 0.1354
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 252.5842s / 4328.9782 s
agent0:                 episode reward: 0.6830,                 loss: nan
agent1:                 episode reward: -0.6830,                 loss: 0.1388
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 247.5227s / 4576.5009 s
agent0:                 episode reward: -0.2079,                 loss: nan
agent1:                 episode reward: 0.2079,                 loss: 0.1321
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.3052s / 4820.8061 s
agent0:                 episode reward: -0.2863,                 loss: nan
agent1:                 episode reward: 0.2863,                 loss: 0.1324
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 238.3466s / 5059.1527 s
agent0:                 episode reward: -0.1370,                 loss: nan
agent1:                 episode reward: 0.1370,                 loss: 0.1327
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.7210s / 5303.8736 s
agent0:                 episode reward: 0.4355,                 loss: nan
agent1:                 episode reward: -0.4355,                 loss: 0.1328
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.8133s / 5550.6869 s
agent0:                 episode reward: 0.0756,                 loss: nan
agent1:                 episode reward: -0.0756,                 loss: 0.1329
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 252.5599s / 5803.2469 s
agent0:                 episode reward: -0.0931,                 loss: nan
agent1:                 episode reward: 0.0931,                 loss: 0.1323
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 248.6355s / 6051.8824 s
agent0:                 episode reward: -0.3184,                 loss: nan
agent1:                 episode reward: 0.3184,                 loss: 0.1329
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 252.8106s / 6304.6929 s
agent0:                 episode reward: -0.0628,                 loss: nan
agent1:                 episode reward: 0.0628,                 loss: 0.1309
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.2690s / 6546.9619 s
agent0:                 episode reward: -0.0068,                 loss: nan
agent1:                 episode reward: 0.0068,                 loss: 0.1310
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 252.7926s / 6799.7545 s
agent0:                 episode reward: 0.2441,                 loss: nan
agent1:                 episode reward: -0.2441,                 loss: 0.1299
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 249.1158s / 7048.8703 s
agent0:                 episode reward: 0.2532,                 loss: nan
agent1:                 episode reward: -0.2532,                 loss: 0.1306
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 249.9076s / 7298.7779 s
agent0:                 episode reward: -0.0983,                 loss: nan
agent1:                 episode reward: 0.0983,                 loss: 0.1305
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 238.8985s / 7537.6764 s
agent0:                 episode reward: 0.1418,                 loss: nan
agent1:                 episode reward: -0.1418,                 loss: 0.1301
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 245.3853s / 7783.0617 s
agent0:                 episode reward: -0.1029,                 loss: nan
agent1:                 episode reward: 0.1029,                 loss: 0.1283
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 254.1648s / 8037.2265 s
agent0:                 episode reward: 0.1065,                 loss: nan
agent1:                 episode reward: -0.1065,                 loss: 0.1286
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 250.0705s / 8287.2969 s
agent0:                 episode reward: 0.1321,                 loss: nan
agent1:                 episode reward: -0.1321,                 loss: 0.1275
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 250.2925s / 8537.5895 s
agent0:                 episode reward: -0.1320,                 loss: nan
agent1:                 episode reward: 0.1320,                 loss: 0.1283
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 250.6925s / 8788.2820 s
agent0:                 episode reward: -0.0303,                 loss: nan
agent1:                 episode reward: 0.0303,                 loss: 0.1283
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 249.0151s / 9037.2971 s
agent0:                 episode reward: 0.2344,                 loss: nan
agent1:                 episode reward: -0.2344,                 loss: 0.1271
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.7059s / 9285.0030 s
agent0:                 episode reward: 0.5768,                 loss: nan
agent1:                 episode reward: -0.5768,                 loss: 0.1272
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 250.4537s / 9535.4567 s
agent0:                 episode reward: 0.5016,                 loss: nan
agent1:                 episode reward: -0.5016,                 loss: 0.1280
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.5748s / 9781.0316 s
agent0:                 episode reward: 0.6355,                 loss: nan
agent1:                 episode reward: -0.6355,                 loss: 0.1281
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.5658s / 10027.5974 s
agent0:                 episode reward: 0.0379,                 loss: nan
agent1:                 episode reward: -0.0379,                 loss: 0.1283
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 237.1188s / 10264.7162 s
agent0:                 episode reward: 0.1995,                 loss: nan
agent1:                 episode reward: -0.1995,                 loss: 0.1262
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 249.0209s / 10513.7370 s
agent0:                 episode reward: -0.0638,                 loss: nan
agent1:                 episode reward: 0.0638,                 loss: 0.1275
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 249.9682s / 10763.7052 s
agent0:                 episode reward: 0.1487,                 loss: nan
agent1:                 episode reward: -0.1487,                 loss: 0.1258
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 237.4920s / 11001.1972 s
agent0:                 episode reward: -0.0061,                 loss: nan
agent1:                 episode reward: 0.0061,                 loss: 0.1269
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 252.8669s / 11254.0642 s
agent0:                 episode reward: -0.2047,                 loss: nan
agent1:                 episode reward: 0.2047,                 loss: 0.1262
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 245.0248s / 11499.0890 s
agent0:                 episode reward: -0.3805,                 loss: nan
agent1:                 episode reward: 0.3805,                 loss: 0.1260
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 249.9789s / 11749.0679 s
agent0:                 episode reward: -0.1740,                 loss: nan
agent1:                 episode reward: 0.1740,                 loss: 0.1273
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 247.6448s / 11996.7126 s
agent0:                 episode reward: 0.0432,                 loss: nan
agent1:                 episode reward: -0.0432,                 loss: 0.1270
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.1223s / 12243.8349 s
agent0:                 episode reward: -0.6857,                 loss: nan
agent1:                 episode reward: 0.6857,                 loss: 0.1266
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 250.8518s / 12494.6867 s
agent0:                 episode reward: -0.1070,                 loss: nan
agent1:                 episode reward: 0.1070,                 loss: 0.1272
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 249.5005s / 12744.1872 s
agent0:                 episode reward: -0.1170,                 loss: nan
agent1:                 episode reward: 0.1170,                 loss: 0.1268
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 236.9774s / 12981.1645 s
agent0:                 episode reward: -0.0119,                 loss: nan
agent1:                 episode reward: 0.0119,                 loss: 0.1258
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.4097s / 13226.5743 s
agent0:                 episode reward: -0.2034,                 loss: nan
agent1:                 episode reward: 0.2034,                 loss: 0.1272
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.4425s / 13472.0168 s
agent0:                 episode reward: 0.0830,                 loss: nan
agent1:                 episode reward: -0.0830,                 loss: 0.1269
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 250.4050s / 13722.4218 s
agent0:                 episode reward: 0.4790,                 loss: nan
agent1:                 episode reward: -0.4790,                 loss: 0.1256
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 249.7815s / 13972.2033 s
agent0:                 episode reward: -0.3892,                 loss: nan
agent1:                 episode reward: 0.3892,                 loss: 0.1273
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 250.6121s / 14222.8154 s
agent0:                 episode reward: 0.0908,                 loss: nan
agent1:                 episode reward: -0.0908,                 loss: 0.1270
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 243.0259s / 14465.8412 s
agent0:                 episode reward: -0.3025,                 loss: nan
agent1:                 episode reward: 0.3025,                 loss: 0.1250
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 251.0650s / 14716.9063 s
agent0:                 episode reward: 0.0380,                 loss: nan
agent1:                 episode reward: -0.0380,                 loss: 0.1257
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.7548s / 14962.6610 s
agent0:                 episode reward: -0.0754,                 loss: nan
agent1:                 episode reward: 0.0754,                 loss: 0.1256
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 251.0276s / 15213.6886 s
agent0:                 episode reward: 0.2840,                 loss: nan
agent1:                 episode reward: -0.2840,                 loss: 0.1235
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.8779s / 15461.5665 s
agent0:                 episode reward: 0.4050,                 loss: nan