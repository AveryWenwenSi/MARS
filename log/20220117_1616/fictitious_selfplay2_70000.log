pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7f848698fbd0>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.008 0.008 0.008 ... 0.008 0.008 0.008]
 [0.008 0.008 0.008 ... 0.008 0.008 0.008]]
Load checkpoints (policy family):  [['50' '5253' '7615' ... '68375' '68763' '69316']
 ['193' '5289' '7712' ... '68517' '68913' '69450']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220117153310/epi_70000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220117153310_exploit_70000/mdp_arbitrary_mdp_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220117153310_exploit_70000/mdp_arbitrary_mdp_fictitious_selfplay2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8494s / 6.8494 s
agent0:                 episode reward: -1.0358,                 loss: nan
agent1:                 episode reward: 1.0358,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7368s / 14.5862 s
agent0:                 episode reward: 0.0813,                 loss: nan
agent1:                 episode reward: -0.0813,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5417s / 21.1280 s
agent0:                 episode reward: -0.2151,                 loss: nan
agent1:                 episode reward: 0.2151,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7939s / 26.9219 s
agent0:                 episode reward: 0.0336,                 loss: nan
agent1:                 episode reward: -0.0336,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0495s / 33.9713 s
agent0:                 episode reward: -0.4656,                 loss: nan
agent1:                 episode reward: 0.4656,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3001s / 41.2714 s
agent0:                 episode reward: 0.1114,                 loss: nan
agent1:                 episode reward: -0.1114,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6232s / 47.8947 s
agent0:                 episode reward: 0.0484,                 loss: nan
agent1:                 episode reward: -0.0484,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6283s / 54.5229 s
agent0:                 episode reward: -0.1023,                 loss: nan
agent1:                 episode reward: 0.1023,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2686s / 61.7916 s
agent0:                 episode reward: 0.2404,                 loss: nan
agent1:                 episode reward: -0.2404,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8058s / 68.5973 s
agent0:                 episode reward: 0.2529,                 loss: nan
agent1:                 episode reward: -0.2529,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0784s / 75.6757 s
agent0:                 episode reward: 0.2391,                 loss: nan
agent1:                 episode reward: -0.2391,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 98.1652s / 173.8409 s
agent0:                 episode reward: 0.0331,                 loss: nan
agent1:                 episode reward: -0.0331,                 loss: 0.1816
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.2812s / 415.1221 s
agent0:                 episode reward: 0.2457,                 loss: nan
agent1:                 episode reward: -0.2457,                 loss: 0.1733
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 249.5916s / 664.7138 s
agent0:                 episode reward: 0.6521,                 loss: nan
agent1:                 episode reward: -0.6521,                 loss: 0.1689
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 233.7834s / 898.4972 s
agent0:                 episode reward: -0.0450,                 loss: nan
agent1:                 episode reward: 0.0450,                 loss: 0.1661
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 237.2478s / 1135.7450 s
agent0:                 episode reward: -0.2593,                 loss: nan
agent1:                 episode reward: 0.2593,                 loss: 0.1630
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 239.1222s / 1374.8672 s
agent0:                 episode reward: -0.1556,                 loss: nan
agent1:                 episode reward: 0.1556,                 loss: 0.1610
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.3728s / 1620.2400 s
agent0:                 episode reward: -0.0988,                 loss: nan
agent1:                 episode reward: 0.0988,                 loss: 0.1587
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 235.0253s / 1855.2653 s
agent0:                 episode reward: 0.2011,                 loss: nan
agent1:                 episode reward: -0.2011,                 loss: 0.1571
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.9973s / 2101.2626 s
agent0:                 episode reward: -0.3265,                 loss: nan
agent1:                 episode reward: 0.3265,                 loss: 0.1569
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 240.6977s / 2341.9603 s
agent0:                 episode reward: -0.0567,                 loss: nan
agent1:                 episode reward: 0.0567,                 loss: 0.1559
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 239.0750s / 2581.0353 s
agent0:                 episode reward: 0.2481,                 loss: nan
agent1:                 episode reward: -0.2481,                 loss: 0.1554
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 238.1619s / 2819.1972 s
agent0:                 episode reward: -0.0391,                 loss: nan
agent1:                 episode reward: 0.0391,                 loss: 0.1547
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 248.7669s / 3067.9641 s
agent0:                 episode reward: -0.2923,                 loss: nan
agent1:                 episode reward: 0.2923,                 loss: 0.1526
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.3605s / 3314.3246 s
agent0:                 episode reward: -0.1185,                 loss: nan
agent1:                 episode reward: 0.1185,                 loss: 0.1506
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.4404s / 3559.7651 s
agent0:                 episode reward: 0.4188,                 loss: nan
agent1:                 episode reward: -0.4188,                 loss: 0.1501
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.5660s / 3806.3311 s
agent0:                 episode reward: 0.2281,                 loss: nan
agent1:                 episode reward: -0.2281,                 loss: 0.1495
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 253.1555s / 4059.4866 s
agent0:                 episode reward: -0.1849,                 loss: nan
agent1:                 episode reward: 0.1849,                 loss: 0.1487
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 252.6263s / 4312.1129 s
agent0:                 episode reward: -0.0767,                 loss: nan
agent1:                 episode reward: 0.0767,                 loss: 0.1592
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.0307s / 4555.1435 s
agent0:                 episode reward: -0.0746,                 loss: nan
agent1:                 episode reward: 0.0746,                 loss: 0.1576
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.9407s / 4797.0843 s
agent0:                 episode reward: -0.0057,                 loss: nan
agent1:                 episode reward: 0.0057,                 loss: 0.1552
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.9821s / 5040.0664 s
agent0:                 episode reward: 0.3350,                 loss: nan
agent1:                 episode reward: -0.3350,                 loss: 0.1535
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 236.8549s / 5276.9213 s
agent0:                 episode reward: 0.0386,                 loss: nan
agent1:                 episode reward: -0.0386,                 loss: 0.1525
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.9639s / 5525.8852 s
agent0:                 episode reward: -0.0560,                 loss: nan
agent1:                 episode reward: 0.0560,                 loss: 0.1520
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.7532s / 5772.6384 s
agent0:                 episode reward: -0.3115,                 loss: nan
agent1:                 episode reward: 0.3115,                 loss: 0.1525
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 251.3577s / 6023.9961 s
agent0:                 episode reward: 0.0389,                 loss: nan
agent1:                 episode reward: -0.0389,                 loss: 0.1515
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.1036s / 6270.0998 s
agent0:                 episode reward: -0.1803,                 loss: nan
agent1:                 episode reward: 0.1803,                 loss: 0.1521
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.2619s / 6512.3616 s
agent0:                 episode reward: -0.2226,                 loss: nan
agent1:                 episode reward: 0.2226,                 loss: 0.1507
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 247.2746s / 6759.6362 s
agent0:                 episode reward: -0.3714,                 loss: nan
agent1:                 episode reward: 0.3714,                 loss: 0.1515
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 250.7805s / 7010.4167 s
agent0:                 episode reward: 0.2948,                 loss: nan
agent1:                 episode reward: -0.2948,                 loss: 0.1502
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 243.7199s / 7254.1366 s
agent0:                 episode reward: 0.1258,                 loss: nan
agent1:                 episode reward: -0.1258,                 loss: 0.1513
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 243.7966s / 7497.9332 s
agent0:                 episode reward: -0.0389,                 loss: nan
agent1:                 episode reward: 0.0389,                 loss: 0.1503
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 255.2067s / 7753.1399 s
agent0:                 episode reward: -0.0080,                 loss: nan
agent1:                 episode reward: 0.0080,                 loss: 0.1498
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.6861s / 7993.8259 s
agent0:                 episode reward: 0.0806,                 loss: nan
agent1:                 episode reward: -0.0806,                 loss: 0.1489
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.6910s / 8238.5169 s
agent0:                 episode reward: 0.1709,                 loss: nan
agent1:                 episode reward: -0.1709,                 loss: 0.1478
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 239.0297s / 8477.5466 s
agent0:                 episode reward: -0.2825,                 loss: nan
agent1:                 episode reward: 0.2825,                 loss: 0.1483
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.3662s / 8721.9128 s
agent0:                 episode reward: 0.3271,                 loss: nan
agent1:                 episode reward: -0.3271,                 loss: 0.1454
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 248.0692s / 8969.9820 s
agent0:                 episode reward: -0.1116,                 loss: nan
agent1:                 episode reward: 0.1116,                 loss: 0.1462
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 249.4421s / 9219.4241 s
agent0:                 episode reward: 0.0361,                 loss: nan
agent1:                 episode reward: -0.0361,                 loss: 0.1438
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.3752s / 9466.7993 s
agent0:                 episode reward: -0.0467,                 loss: nan
agent1:                 episode reward: 0.0467,                 loss: 0.1446
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 234.1219s / 9700.9212 s
agent0:                 episode reward: -0.0220,                 loss: nan
agent1:                 episode reward: 0.0220,                 loss: 0.1435
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.9863s / 9942.9075 s
agent0:                 episode reward: -0.0515,                 loss: nan
agent1:                 episode reward: 0.0515,                 loss: 0.1433
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.9709s / 10185.8784 s
agent0:                 episode reward: 0.0181,                 loss: nan
agent1:                 episode reward: -0.0181,                 loss: 0.1433
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.9700s / 10430.8484 s
agent0:                 episode reward: -0.2285,                 loss: nan
agent1:                 episode reward: 0.2285,                 loss: 0.1428
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 249.4271s / 10680.2755 s
agent0:                 episode reward: -0.3495,                 loss: nan
agent1:                 episode reward: 0.3495,                 loss: 0.1427
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.4781s / 10921.7536 s
agent0:                 episode reward: 0.1327,                 loss: nan
agent1:                 episode reward: -0.1327,                 loss: 0.1432
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.7610s / 11168.5146 s
agent0:                 episode reward: 0.2032,                 loss: nan
agent1:                 episode reward: -0.2032,                 loss: 0.1411
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 245.1154s / 11413.6300 s
agent0:                 episode reward: 0.0601,                 loss: nan
agent1:                 episode reward: -0.0601,                 loss: 0.1406
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 258.0809s / 11671.7109 s
agent0:                 episode reward: -0.3809,                 loss: nan
agent1:                 episode reward: 0.3809,                 loss: 0.1397
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 248.4678s / 11920.1787 s
agent0:                 episode reward: 0.0100,                 loss: nan
agent1:                 episode reward: -0.0100,                 loss: 0.1410
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 247.2857s / 12167.4644 s
agent0:                 episode reward: -0.1525,                 loss: nan
agent1:                 episode reward: 0.1525,                 loss: 0.1393
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 249.6040s / 12417.0684 s
agent0:                 episode reward: -0.1064,                 loss: nan
agent1:                 episode reward: 0.1064,                 loss: 0.1399
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 251.4958s / 12668.5642 s
agent0:                 episode reward: 0.1682,                 loss: nan
agent1:                 episode reward: -0.1682,                 loss: 0.1386
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.5931s / 12913.1572 s
agent0:                 episode reward: -0.2087,                 loss: nan
agent1:                 episode reward: 0.2087,                 loss: 0.1402
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.9698s / 13155.1270 s
agent0:                 episode reward: -0.1998,                 loss: nan
agent1:                 episode reward: 0.1998,                 loss: 0.1386
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 237.6146s / 13392.7417 s
agent0:                 episode reward: 0.3376,                 loss: nan
agent1:                 episode reward: -0.3376,                 loss: 0.1381
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 245.3292s / 13638.0709 s
agent0:                 episode reward: -0.1476,                 loss: nan
agent1:                 episode reward: 0.1476,                 loss: 0.1375
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 248.8173s / 13886.8882 s
agent0:                 episode reward: 0.0885,                 loss: nan
agent1:                 episode reward: -0.0885,                 loss: 0.1375
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 251.8686s / 14138.7568 s
agent0:                 episode reward: 0.0534,                 loss: nan
agent1:                 episode reward: -0.0534,                 loss: 0.1365
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 246.0716s / 14384.8284 s
agent0:                 episode reward: -0.1004,                 loss: nan
agent1:                 episode reward: 0.1004,                 loss: 0.1382
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 239.5175s / 14624.3459 s
agent0:                 episode reward: -0.0454,                 loss: nan
agent1:                 episode reward: 0.0454,                 loss: 0.1359
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 240.3043s / 14864.6502 s
agent0:                 episode reward: -0.1770,                 loss: nan
agent1:                 episode reward: 0.1770,                 loss: 0.1375
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 257.8463s / 15122.4965 s
agent0:                 episode reward: 0.1518,                 loss: nan
agent1:                 episode reward: -0.1518,                 loss: 0.1355
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 241.0881s / 15363.5846 s
agent0:                 episode reward: 0.0306,                 loss: nan