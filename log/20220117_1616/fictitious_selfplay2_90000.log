pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7f43e6585a50>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.006 0.006 0.006 ... 0.006 0.006 0.006]
 [0.006 0.006 0.006 ... 0.006 0.006 0.006]]
Load checkpoints (policy family):  [['50' '5253' '7615' ... '88510' '88868' '89256']
 ['193' '5289' '7712' ... '88692' '89048' '89466']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'update_itr': 1}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220117153310/epi_90000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220117153310_exploit_90000/mdp_arbitrary_mdp_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220117153310_exploit_90000/mdp_arbitrary_mdp_fictitious_selfplay2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4215s / 5.4215 s
agent0:                 episode reward: -1.0566,                 loss: nan
agent1:                 episode reward: 1.0566,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5594s / 11.9809 s
agent0:                 episode reward: 0.1942,                 loss: nan
agent1:                 episode reward: -0.1942,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1665s / 17.1474 s
agent0:                 episode reward: -0.0865,                 loss: nan
agent1:                 episode reward: 0.0865,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4722s / 23.6196 s
agent0:                 episode reward: -0.0049,                 loss: nan
agent1:                 episode reward: 0.0049,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6974s / 30.3170 s
agent0:                 episode reward: -0.1130,                 loss: nan
agent1:                 episode reward: 0.1130,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0704s / 37.3874 s
agent0:                 episode reward: -0.1077,                 loss: nan
agent1:                 episode reward: 0.1077,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0480s / 44.4355 s
agent0:                 episode reward: 0.0400,                 loss: nan
agent1:                 episode reward: -0.0400,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8249s / 51.2604 s
agent0:                 episode reward: 0.1838,                 loss: nan
agent1:                 episode reward: -0.1838,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7430s / 59.0033 s
agent0:                 episode reward: -0.3373,                 loss: nan
agent1:                 episode reward: 0.3373,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1557s / 66.1590 s
agent0:                 episode reward: 0.3859,                 loss: nan
agent1:                 episode reward: -0.3859,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2126s / 73.3716 s
agent0:                 episode reward: -0.0992,                 loss: nan
agent1:                 episode reward: 0.0992,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 98.9031s / 172.2747 s
agent0:                 episode reward: 0.0927,                 loss: nan
agent1:                 episode reward: -0.0927,                 loss: 0.2262
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 237.5221s / 409.7968 s
agent0:                 episode reward: 0.0615,                 loss: nan
agent1:                 episode reward: -0.0615,                 loss: 0.1836
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.2490s / 652.0458 s
agent0:                 episode reward: -0.1045,                 loss: nan
agent1:                 episode reward: 0.1045,                 loss: 0.1695
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.4162s / 894.4620 s
agent0:                 episode reward: -0.0114,                 loss: nan
agent1:                 episode reward: 0.0114,                 loss: 0.1650
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 242.3358s / 1136.7978 s
agent0:                 episode reward: -0.0071,                 loss: nan
agent1:                 episode reward: 0.0071,                 loss: 0.1643
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.6995s / 1381.4973 s
agent0:                 episode reward: 0.3457,                 loss: nan
agent1:                 episode reward: -0.3457,                 loss: 0.1635
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.8729s / 1628.3701 s
agent0:                 episode reward: 0.3214,                 loss: nan
agent1:                 episode reward: -0.3214,                 loss: 0.1611
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 235.9168s / 1864.2870 s
agent0:                 episode reward: 0.0171,                 loss: nan
agent1:                 episode reward: -0.0171,                 loss: 0.1592
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.9386s / 2107.2255 s
agent0:                 episode reward: 0.0190,                 loss: nan
agent1:                 episode reward: -0.0190,                 loss: 0.1589
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 244.9264s / 2352.1519 s
agent0:                 episode reward: 0.1797,                 loss: nan
agent1:                 episode reward: -0.1797,                 loss: 0.1559
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 250.6354s / 2602.7873 s
agent0:                 episode reward: -0.4352,                 loss: nan
agent1:                 episode reward: 0.4352,                 loss: 0.1542
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 243.8723s / 2846.6596 s
agent0:                 episode reward: -0.0663,                 loss: nan
agent1:                 episode reward: 0.0663,                 loss: 0.1525
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.7985s / 3093.4581 s
agent0:                 episode reward: 0.0553,                 loss: nan
agent1:                 episode reward: -0.0553,                 loss: 0.1534
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 243.5256s / 3336.9837 s
agent0:                 episode reward: -0.3926,                 loss: nan
agent1:                 episode reward: 0.3926,                 loss: 0.1515
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 246.7708s / 3583.7545 s
agent0:                 episode reward: 0.1652,                 loss: nan
agent1:                 episode reward: -0.1652,                 loss: 0.1516
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.5678s / 3829.3223 s
agent0:                 episode reward: 0.1497,                 loss: nan
agent1:                 episode reward: -0.1497,                 loss: 0.1524
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 253.0065s / 4082.3288 s
agent0:                 episode reward: -0.1719,                 loss: nan
agent1:                 episode reward: 0.1719,                 loss: 0.1519
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 251.9970s / 4334.3259 s
agent0:                 episode reward: -0.3123,                 loss: nan
agent1:                 episode reward: 0.3123,                 loss: 0.1616
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 233.7539s / 4568.0798 s
agent0:                 episode reward: -0.0809,                 loss: nan
agent1:                 episode reward: 0.0809,                 loss: 0.1592
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 249.9584s / 4818.0381 s
agent0:                 episode reward: -0.3220,                 loss: nan
agent1:                 episode reward: 0.3220,                 loss: 0.1567
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 249.6524s / 5067.6905 s
agent0:                 episode reward: 0.2843,                 loss: nan
agent1:                 episode reward: -0.2843,                 loss: 0.1557
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 253.5297s / 5321.2202 s
agent0:                 episode reward: -0.2118,                 loss: nan
agent1:                 episode reward: 0.2118,                 loss: 0.1549
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.2829s / 5565.5031 s
agent0:                 episode reward: 0.1916,                 loss: nan
agent1:                 episode reward: -0.1916,                 loss: 0.1560
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 248.6288s / 5814.1319 s
agent0:                 episode reward: -0.3440,                 loss: nan
agent1:                 episode reward: 0.3440,                 loss: 0.1562
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.5479s / 6056.6798 s
agent0:                 episode reward: 0.3293,                 loss: nan
agent1:                 episode reward: -0.3293,                 loss: 0.1532
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 252.2947s / 6308.9746 s
agent0:                 episode reward: 0.3372,                 loss: nan
agent1:                 episode reward: -0.3372,                 loss: 0.1543
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 248.1160s / 6557.0906 s
agent0:                 episode reward: 0.0131,                 loss: nan
agent1:                 episode reward: -0.0131,                 loss: 0.1536
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 242.3613s / 6799.4519 s
agent0:                 episode reward: 0.0433,                 loss: nan
agent1:                 episode reward: -0.0433,                 loss: 0.1518
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 240.6722s / 7040.1240 s
agent0:                 episode reward: -0.0139,                 loss: nan
agent1:                 episode reward: 0.0139,                 loss: 0.1499
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.9046s / 7281.0286 s
agent0:                 episode reward: -0.3731,                 loss: nan
agent1:                 episode reward: 0.3731,                 loss: 0.1507
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 240.8897s / 7521.9183 s
agent0:                 episode reward: -0.2952,                 loss: nan
agent1:                 episode reward: 0.2952,                 loss: 0.1492
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.7154s / 7770.6337 s
agent0:                 episode reward: -0.0177,                 loss: nan
agent1:                 episode reward: 0.0177,                 loss: 0.1481
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 240.1223s / 8010.7560 s
agent0:                 episode reward: 0.0371,                 loss: nan
agent1:                 episode reward: -0.0371,                 loss: 0.1484
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 241.4365s / 8252.1925 s
agent0:                 episode reward: -0.1080,                 loss: nan
agent1:                 episode reward: 0.1080,                 loss: 0.1460
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 244.9142s / 8497.1067 s
agent0:                 episode reward: -0.1142,                 loss: nan
agent1:                 episode reward: 0.1142,                 loss: 0.1457
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 250.2668s / 8747.3735 s
agent0:                 episode reward: -0.0079,                 loss: nan
agent1:                 episode reward: 0.0079,                 loss: 0.1467
Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.9232s / 8994.2967 s
agent0:                 episode reward: -0.1507,                 loss: nan
agent1:                 episode reward: 0.1507,                 loss: 0.1451
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 243.6176s / 9237.9143 s
agent0:                 episode reward: 0.3042,                 loss: nan
agent1:                 episode reward: -0.3042,                 loss: 0.1442
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 242.3158s / 9480.2301 s
agent0:                 episode reward: -0.1489,                 loss: nan
agent1:                 episode reward: 0.1489,                 loss: 0.1448
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 241.5387s / 9721.7688 s
agent0:                 episode reward: -0.1772,                 loss: nan
agent1:                 episode reward: 0.1772,                 loss: 0.1435
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 241.4347s / 9963.2035 s
agent0:                 episode reward: 0.3600,                 loss: nan
agent1:                 episode reward: -0.3600,                 loss: 0.1445
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.5464s / 10208.7499 s
agent0:                 episode reward: -0.0662,                 loss: nan
agent1:                 episode reward: 0.0662,                 loss: 0.1440
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.1762s / 10453.9260 s
agent0:                 episode reward: -0.0007,                 loss: nan
agent1:                 episode reward: 0.0007,                 loss: 0.1444
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 238.4142s / 10692.3402 s
agent0:                 episode reward: 0.1154,                 loss: nan
agent1:                 episode reward: -0.1154,                 loss: 0.1437
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 243.7209s / 10936.0611 s
agent0:                 episode reward: -0.2231,                 loss: nan
agent1:                 episode reward: 0.2231,                 loss: 0.1455
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 250.2043s / 11186.2654 s
agent0:                 episode reward: -0.4228,                 loss: nan
agent1:                 episode reward: 0.4228,                 loss: 0.1431
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 251.1373s / 11437.4027 s
agent0:                 episode reward: 0.1361,                 loss: nan
agent1:                 episode reward: -0.1361,                 loss: 0.1439
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 252.8710s / 11690.2737 s
agent0:                 episode reward: -0.0762,                 loss: nan
agent1:                 episode reward: 0.0762,                 loss: 0.1433
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 245.3284s / 11935.6021 s
agent0:                 episode reward: -0.4680,                 loss: nan
agent1:                 episode reward: 0.4680,                 loss: 0.1429
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 249.6249s / 12185.2270 s
agent0:                 episode reward: -0.1059,                 loss: nan
agent1:                 episode reward: 0.1059,                 loss: 0.1424
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 244.0347s / 12429.2617 s
agent0:                 episode reward: -0.1747,                 loss: nan
agent1:                 episode reward: 0.1747,                 loss: 0.1436
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 246.8940s / 12676.1557 s
agent0:                 episode reward: -0.2228,                 loss: nan
agent1:                 episode reward: 0.2228,                 loss: 0.1441
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 234.7850s / 12910.9407 s
agent0:                 episode reward: -0.2785,                 loss: nan
agent1:                 episode reward: 0.2785,                 loss: 0.1431
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 235.2003s / 13146.1410 s
agent0:                 episode reward: -0.2369,                 loss: nan
agent1:                 episode reward: 0.2369,                 loss: 0.1436
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 248.0768s / 13394.2179 s
agent0:                 episode reward: -0.2952,                 loss: nan
agent1:                 episode reward: 0.2952,                 loss: 0.1422
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 238.7946s / 13633.0125 s
agent0:                 episode reward: 0.0216,                 loss: nan
agent1:                 episode reward: -0.0216,                 loss: 0.1419
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 243.4019s / 13876.4144 s
agent0:                 episode reward: 0.0753,                 loss: nan
agent1:                 episode reward: -0.0753,                 loss: 0.1442
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 252.8269s / 14129.2413 s
agent0:                 episode reward: 0.1433,                 loss: nan
agent1:                 episode reward: -0.1433,                 loss: 0.1427
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.7775s / 14378.0188 s
agent0:                 episode reward: 0.2016,                 loss: nan
agent1:                 episode reward: -0.2016,                 loss: 0.1437
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 245.3564s / 14623.3752 s
agent0:                 episode reward: -0.3488,                 loss: nan
agent1:                 episode reward: 0.3488,                 loss: 0.1426
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 237.1235s / 14860.4987 s
agent0:                 episode reward: -0.1379,                 loss: nan
agent1:                 episode reward: 0.1379,                 loss: 0.1415
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 248.7760s / 15109.2747 s
agent0:                 episode reward: -0.0972,                 loss: nan
agent1:                 episode reward: 0.0972,                 loss: 0.1411
Episode: 1461/30000 (4.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 247.0979s / 15356.3725 s
agent0:                 episode reward: -0.2666,                 loss: nan