pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
double_dunk_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'double_dunk_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 3}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220119_1545/pettingzoo_double_dunk_v2_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_1545/pettingzoo_double_dunk_v2_nash_dqn_exploiter.
Episode: 1/10000 (0.0100%),                 avg. length: 4694.0,                last time consumption/overall running time: 199.5304s / 199.5304 s
env0_first_0:                 episode reward: -24.0000,                 loss: 0.0161
env0_second_0:                 episode reward: 24.0000,                 loss: 0.0149
env1_first_0:                 episode reward: -37.0000,                 loss: nan
env1_second_0:                 episode reward: 37.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 3876.1,                last time consumption/overall running time: 4799.6342s / 4999.1646 s
env0_first_0:                 episode reward: -13.5500,                 loss: 0.0153
env0_second_0:                 episode reward: 13.5500,                 loss: 0.0153
env1_first_0:                 episode reward: -12.7000,                 loss: nan
env1_second_0:                 episode reward: 12.7000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 3951.3,                last time consumption/overall running time: 5522.6648s / 10521.8295 s
env0_first_0:                 episode reward: 34.2000,                 loss: 0.0140
env0_second_0:                 episode reward: -34.2000,                 loss: 0.0144
env1_first_0:                 episode reward: 36.6000,                 loss: nan
env1_second_0:                 episode reward: -36.6000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 5052.15,                last time consumption/overall running time: 7105.7672s / 17627.5966 s
env0_first_0:                 episode reward: 50.9500,                 loss: 0.0182
env0_second_0:                 episode reward: -50.9500,                 loss: 0.0176
env1_first_0:                 episode reward: 55.4000,                 loss: nan
env1_second_0:                 episode reward: -55.4000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 3480.8,                last time consumption/overall running time: 4910.0140s / 22537.6106 s
env0_first_0:                 episode reward: 9.0000,                 loss: 0.0107
env0_second_0:                 episode reward: -9.0000,                 loss: 0.0097
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 3641.35,                last time consumption/overall running time: 5142.5003s / 27680.1109 s
env0_first_0:                 episode reward: 10.7500,                 loss: 0.0093
env0_second_0:                 episode reward: -10.7500,                 loss: 0.0071
env1_first_0:                 episode reward: 12.6500,                 loss: nan
env1_second_0:                 episode reward: -12.6500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 3008.5,                last time consumption/overall running time: 4243.7314s / 31923.8424 s
env0_first_0:                 episode reward: 10.0000,                 loss: 0.0107
env0_second_0:                 episode reward: -10.0000,                 loss: 0.0071
env1_first_0:                 episode reward: 13.0500,                 loss: nan
env1_second_0:                 episode reward: -13.0500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 3467.95,                last time consumption/overall running time: 4908.8276s / 36832.6700 s
env0_first_0:                 episode reward: 12.3000,                 loss: 0.0110
env0_second_0:                 episode reward: -12.3000,                 loss: 0.0083
env1_first_0:                 episode reward: 11.1500,                 loss: nan
env1_second_0:                 episode reward: -11.1500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 2854.55,                last time consumption/overall running time: 4058.6198s / 40891.2898 s
env0_first_0:                 episode reward: 10.2000,                 loss: 0.0126
env0_second_0:                 episode reward: -10.2000,                 loss: 0.0114
env1_first_0:                 episode reward: 14.8000,                 loss: nan
env1_second_0:                 episode reward: -14.8000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 3502.7,                last time consumption/overall running time: 4945.3755s / 45836.6653 s
env0_first_0:                 episode reward: 13.4000,                 loss: 0.0103
env0_second_0:                 episode reward: -13.4000,                 loss: 0.0104
env1_first_0:                 episode reward: 10.0500,                 loss: nan
env1_second_0:                 episode reward: -10.0500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 4024.1,                last time consumption/overall running time: 5686.2098s / 51522.8751 s
env0_first_0:                 episode reward: 7.1000,                 loss: 0.0115
env0_second_0:                 episode reward: -7.1000,                 loss: 0.0127
env1_first_0:                 episode reward: 8.9000,                 loss: nan
env1_second_0:                 episode reward: -8.9000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 3843.55,                last time consumption/overall running time: 5423.6450s / 56946.5201 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.0120
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0127
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 3873.35,                last time consumption/overall running time: 5393.7561s / 62340.2762 s
env0_first_0:                 episode reward: -8.1000,                 loss: 0.0142
env0_second_0:                 episode reward: 8.1000,                 loss: 0.0142
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 3897.6,                last time consumption/overall running time: 5401.6422s / 67741.9184 s
env0_first_0:                 episode reward: -9.0500,                 loss: 0.0138
env0_second_0:                 episode reward: 9.0500,                 loss: 0.0139
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 3866.0,                last time consumption/overall running time: 5365.1091s / 73107.0275 s
env0_first_0:                 episode reward: -7.4000,                 loss: 0.0136
env0_second_0:                 episode reward: 7.4000,                 loss: 0.0135
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 3575.1,                last time consumption/overall running time: 4959.9253s / 78066.9529 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0152
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0143
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 4226.5,                last time consumption/overall running time: 5864.9441s / 83931.8969 s
env0_first_0:                 episode reward: -11.2500,                 loss: 0.0137
env0_second_0:                 episode reward: 11.2500,                 loss: 0.0160
env1_first_0:                 episode reward: -10.6500,                 loss: nan
env1_second_0:                 episode reward: 10.6500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 3803.1,                last time consumption/overall running time: 5278.1899s / 89210.0868 s
env0_first_0:                 episode reward: -13.1500,                 loss: 0.0134
env0_second_0:                 episode reward: 13.1500,                 loss: 0.0160
env1_first_0:                 episode reward: -10.7500,                 loss: nan
env1_second_0:                 episode reward: 10.7500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 3824.9,                last time consumption/overall running time: 5299.5300s / 94509.6168 s
env0_first_0:                 episode reward: -12.2000,                 loss: 0.0140
env0_second_0:                 episode reward: 12.2000,                 loss: 0.0148
env1_first_0:                 episode reward: -14.1500,                 loss: nan
env1_second_0:                 episode reward: 14.1500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 4310.05,                last time consumption/overall running time: 5987.8994s / 100497.5162 s
env0_first_0:                 episode reward: -14.3000,                 loss: 0.0140
env0_second_0:                 episode reward: 14.3000,                 loss: 0.0135
env1_first_0:                 episode reward: -18.3500,                 loss: nan
env1_second_0:                 episode reward: 18.3500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 4472.2,                last time consumption/overall running time: 6206.9692s / 106704.4854 s
env0_first_0:                 episode reward: -17.7000,                 loss: 0.0149
env0_second_0:                 episode reward: 17.7000,                 loss: 0.0146
env1_first_0:                 episode reward: -17.9500,                 loss: nan
env1_second_0:                 episode reward: 17.9500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 3590.5,                last time consumption/overall running time: 4895.8001s / 111600.2855 s
env0_first_0:                 episode reward: -16.9500,                 loss: 0.0155
env0_second_0:                 episode reward: 16.9500,                 loss: 0.0153
env1_first_0:                 episode reward: -15.0500,                 loss: nan
env1_second_0:                 episode reward: 15.0500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 3838.05,                last time consumption/overall running time: 5193.9750s / 116794.2605 s
env0_first_0:                 episode reward: -18.0500,                 loss: 0.0142
env0_second_0:                 episode reward: 18.0500,                 loss: 0.0149
env1_first_0:                 episode reward: -19.5000,                 loss: nan
env1_second_0:                 episode reward: 19.5000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 3293.0,                last time consumption/overall running time: 4450.1114s / 121244.3719 s
env0_first_0:                 episode reward: -19.4000,                 loss: 0.0143
env0_second_0:                 episode reward: 19.4000,                 loss: 0.0154
env1_first_0:                 episode reward: -20.7000,                 loss: nan
env1_second_0:                 episode reward: 20.7000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 3025.75,                last time consumption/overall running time: 4092.4882s / 125336.8601 s
env0_first_0:                 episode reward: -15.7000,                 loss: 0.0133
env0_second_0:                 episode reward: 15.7000,                 loss: 0.0142
env1_first_0:                 episode reward: -14.8500,                 loss: nan
env1_second_0:                 episode reward: 14.8500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 3571.6,                last time consumption/overall running time: 4828.0542s / 130164.9143 s
env0_first_0:                 episode reward: -19.8500,                 loss: 0.0134
env0_second_0:                 episode reward: 19.8500,                 loss: 0.0143
env1_first_0:                 episode reward: -18.9500,                 loss: nan
env1_second_0:                 episode reward: 18.9500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 3399.35,                last time consumption/overall running time: 4574.8835s / 134739.7979 s
env0_first_0:                 episode reward: -17.0000,                 loss: 0.0140
env0_second_0:                 episode reward: 17.0000,                 loss: 0.0135
env1_first_0:                 episode reward: -16.2500,                 loss: nan
env1_second_0:                 episode reward: 16.2500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 3224.7,                last time consumption/overall running time: 4343.2557s / 139083.0536 s
env0_first_0:                 episode reward: -19.3000,                 loss: 0.0131
env0_second_0:                 episode reward: 19.3000,                 loss: 0.0135
env1_first_0:                 episode reward: -20.5000,                 loss: nan
env1_second_0:                 episode reward: 20.5000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 3618.05,                last time consumption/overall running time: 4835.1035s / 143918.1571 s
env0_first_0:                 episode reward: -22.7000,                 loss: 0.0133
env0_second_0:                 episode reward: 22.7000,                 loss: 0.0149
env1_first_0:                 episode reward: -23.4000,                 loss: nan
env1_second_0:                 episode reward: 23.4000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 3629.8,                last time consumption/overall running time: 4863.7488s / 148781.9059 s
env0_first_0:                 episode reward: -25.7000,                 loss: 0.0136
env0_second_0:                 episode reward: 25.7000,                 loss: 0.0132
env1_first_0:                 episode reward: -24.0500,                 loss: nan
env1_second_0:                 episode reward: 24.0500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 3286.15,                last time consumption/overall running time: 4373.0785s / 153154.9844 s
env0_first_0:                 episode reward: -22.8500,                 loss: 0.0137
env0_second_0:                 episode reward: 22.8500,                 loss: 0.0122
env1_first_0:                 episode reward: -21.1500,                 loss: nan
env1_second_0:                 episode reward: 21.1500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 3517.3,                last time consumption/overall running time: 4608.3187s / 157763.3030 s
env0_first_0:                 episode reward: -26.6000,                 loss: 0.0130
env0_second_0:                 episode reward: 26.6000,                 loss: 0.0134
env1_first_0:                 episode reward: -24.4000,                 loss: nan
env1_second_0:                 episode reward: 24.4000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 2392.25,                last time consumption/overall running time: 3102.8917s / 160866.1947 s
env0_first_0:                 episode reward: -21.5500,                 loss: 0.0130
env0_second_0:                 episode reward: 21.5500,                 loss: 0.0122
env1_first_0:                 episode reward: -19.8000,                 loss: nan
env1_second_0:                 episode reward: 19.8000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 2770.8,                last time consumption/overall running time: 3554.3227s / 164420.5174 s
env0_first_0:                 episode reward: -19.5500,                 loss: 0.0127
env0_second_0:                 episode reward: 19.5500,                 loss: 0.0114
env1_first_0:                 episode reward: -23.8000,                 loss: nan
env1_second_0:                 episode reward: 23.8000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 3071.0,                last time consumption/overall running time: 3880.2226s / 168300.7400 s
env0_first_0:                 episode reward: -19.4500,                 loss: 0.0147
env0_second_0:                 episode reward: 19.4500,                 loss: 0.0140
env1_first_0:                 episode reward: -22.8000,                 loss: nan
env1_second_0:                 episode reward: 22.8000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 2414.45,                last time consumption/overall running time: 3040.2780s / 171341.0181 s
env0_first_0:                 episode reward: -22.7000,                 loss: 0.0143
env0_second_0:                 episode reward: 22.7000,                 loss: 0.0142
env1_first_0:                 episode reward: -22.4500,                 loss: nan
env1_second_0:                 episode reward: 22.4500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 2391.4,                last time consumption/overall running time: 2991.9398s / 174332.9579 s
env0_first_0:                 episode reward: -21.4500,                 loss: 0.0137
env0_second_0:                 episode reward: 21.4500,                 loss: 0.0151
env1_first_0:                 episode reward: -21.1500,                 loss: nan
env1_second_0:                 episode reward: 21.1500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 2336.25,                last time consumption/overall running time: 2921.0347s / 177253.9926 s
env0_first_0:                 episode reward: -20.1500,                 loss: 0.0152
env0_second_0:                 episode reward: 20.1500,                 loss: 0.0156
env1_first_0:                 episode reward: -20.2000,                 loss: nan
env1_second_0:                 episode reward: 20.2000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2296.15,                last time consumption/overall running time: 2846.5397s / 180100.5323 s
env0_first_0:                 episode reward: -19.1000,                 loss: 0.0157
env0_second_0:                 episode reward: 19.1000,                 loss: 0.0156
env1_first_0:                 episode reward: -21.1500,                 loss: nan
env1_second_0:                 episode reward: 21.1500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 2525.8,                last time consumption/overall running time: 3100.0161s / 183200.5484 s
env0_first_0:                 episode reward: -19.0000,                 loss: 0.0153
env0_second_0:                 episode reward: 19.0000,                 loss: 0.0149
env1_first_0:                 episode reward: -16.2000,                 loss: nan
env1_second_0:                 episode reward: 16.2000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 2722.4,                last time consumption/overall running time: 3360.9189s / 186561.4673 s
env0_first_0:                 episode reward: -17.6000,                 loss: 0.0164
env0_second_0:                 episode reward: 17.6000,                 loss: 0.0152
env1_first_0:                 episode reward: -19.3500,                 loss: nan
env1_second_0:                 episode reward: 19.3500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 2260.9,                last time consumption/overall running time: 2781.7292s / 189343.1966 s
env0_first_0:                 episode reward: -18.7000,                 loss: 0.0153
env0_second_0:                 episode reward: 18.7000,                 loss: 0.0138
env1_first_0:                 episode reward: -20.2500,                 loss: nan
env1_second_0:                 episode reward: 20.2500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 2375.45,                last time consumption/overall running time: 2898.6216s / 192241.8182 s
env0_first_0:                 episode reward: -19.1500,                 loss: 0.0150
env0_second_0:                 episode reward: 19.1500,                 loss: 0.0132
env1_first_0:                 episode reward: -19.0000,                 loss: nan
env1_second_0:                 episode reward: 19.0000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1930.25,                last time consumption/overall running time: 2323.9416s / 194565.7598 s
env0_first_0:                 episode reward: -19.0500,                 loss: 0.0149
env0_second_0:                 episode reward: 19.0500,                 loss: 0.0129
env1_first_0:                 episode reward: -15.2000,                 loss: nan
env1_second_0:                 episode reward: 15.2000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 2339.35,                last time consumption/overall running time: 2828.1179s / 197393.8777 s
env0_first_0:                 episode reward: -18.4000,                 loss: 0.0142
env0_second_0:                 episode reward: 18.4000,                 loss: 0.0123
env1_first_0:                 episode reward: -18.1000,                 loss: nan
env1_second_0:                 episode reward: 18.1000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2102.45,                last time consumption/overall running time: 2546.4399s / 199940.3176 s
env0_first_0:                 episode reward: -17.0000,                 loss: 0.0139
env0_second_0:                 episode reward: 17.0000,                 loss: 0.0120
env1_first_0:                 episode reward: -18.6500,                 loss: nan
env1_second_0:                 episode reward: 18.6500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 2175.6,                last time consumption/overall running time: 2621.5369s / 202561.8545 s
env0_first_0:                 episode reward: -17.4500,                 loss: 0.0131
env0_second_0:                 episode reward: 17.4500,                 loss: 0.0116
env1_first_0:                 episode reward: -16.1000,                 loss: nan
env1_second_0:                 episode reward: 16.1000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 2204.1,                last time consumption/overall running time: 2655.3710s / 205217.2256 s
env0_first_0:                 episode reward: -16.5000,                 loss: 0.0135
env0_second_0:                 episode reward: 16.5000,                 loss: 0.0118
env1_first_0:                 episode reward: -17.6000,                 loss: nan
env1_second_0:                 episode reward: 17.6000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2219.35,                last time consumption/overall running time: 2682.9130s / 207900.1385 s
env0_first_0:                 episode reward: -18.2000,                 loss: 0.0134
env0_second_0:                 episode reward: 18.2000,                 loss: 0.0118
env1_first_0:                 episode reward: -19.1000,                 loss: nan
env1_second_0:                 episode reward: 19.1000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 2165.9,                last time consumption/overall running time: 2610.5930s / 210510.7315 s
env0_first_0:                 episode reward: -17.8500,                 loss: 0.0134
env0_second_0:                 episode reward: 17.8500,                 loss: 0.0119
env1_first_0:                 episode reward: -19.6500,                 loss: nan
env1_second_0:                 episode reward: 19.6500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 2142.55,                last time consumption/overall running time: 2566.0924s / 213076.8239 s
env0_first_0:                 episode reward: -19.0000,                 loss: 0.0125
env0_second_0:                 episode reward: 19.0000,                 loss: 0.0115
env1_first_0:                 episode reward: -19.0500,                 loss: nan
env1_second_0:                 episode reward: 19.0500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 2012.15,                last time consumption/overall running time: 2424.0408s / 215500.8647 s
env0_first_0:                 episode reward: -17.3500,                 loss: 0.0121
env0_second_0:                 episode reward: 17.3500,                 loss: 0.0117
env1_first_0:                 episode reward: -17.6000,                 loss: nan
env1_second_0:                 episode reward: 17.6000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1986.4,                last time consumption/overall running time: 2367.0998s / 217867.9645 s
env0_first_0:                 episode reward: -18.7000,                 loss: 0.0122
env0_second_0:                 episode reward: 18.7000,                 loss: 0.0112
env1_first_0:                 episode reward: -16.4500,                 loss: nan
env1_second_0:                 episode reward: 16.4500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 2135.25,                last time consumption/overall running time: 2552.1452s / 220420.1097 s
env0_first_0:                 episode reward: -18.2000,                 loss: 0.0130
env0_second_0:                 episode reward: 18.2000,                 loss: 0.0121
env1_first_0:                 episode reward: -18.8500,                 loss: nan
env1_second_0:                 episode reward: 18.8500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 2042.95,                last time consumption/overall running time: 2417.8764s / 222837.9861 s
env0_first_0:                 episode reward: -18.1500,                 loss: 0.0122
env0_second_0:                 episode reward: 18.1500,                 loss: 0.0109
env1_first_0:                 episode reward: -15.0500,                 loss: nan
env1_second_0:                 episode reward: 15.0500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 2248.6,                last time consumption/overall running time: 2663.2419s / 225501.2280 s
env0_first_0:                 episode reward: -17.8500,                 loss: 0.0125
env0_second_0:                 episode reward: 17.8500,                 loss: 0.0111
env1_first_0:                 episode reward: -18.6000,                 loss: nan
env1_second_0:                 episode reward: 18.6000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 2340.2,                last time consumption/overall running time: 2753.8265s / 228255.0544 s
env0_first_0:                 episode reward: -19.4000,                 loss: 0.0135
env0_second_0:                 episode reward: 19.4000,                 loss: 0.0131
env1_first_0:                 episode reward: -19.9000,                 loss: nan
env1_second_0:                 episode reward: 19.9000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 2374.2,                last time consumption/overall running time: 2818.1052s / 231073.1596 s
env0_first_0:                 episode reward: -19.8000,                 loss: 0.0132
env0_second_0:                 episode reward: 19.8000,                 loss: 0.0123
env1_first_0:                 episode reward: -19.8000,                 loss: nan
env1_second_0:                 episode reward: 19.8000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 2359.15,                last time consumption/overall running time: 2792.3824s / 233865.5420 s
env0_first_0:                 episode reward: -19.1500,                 loss: 0.0144
env0_second_0:                 episode reward: 19.1500,                 loss: 0.0130
env1_first_0:                 episode reward: -19.6000,                 loss: nan
env1_second_0:                 episode reward: 19.6000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 2106.2,                last time consumption/overall running time: 2486.0826s / 236351.6246 s
env0_first_0:                 episode reward: -14.8500,                 loss: 0.0143
env0_second_0:                 episode reward: 14.8500,                 loss: 0.0133
env1_first_0:                 episode reward: -17.4500,                 loss: nan
env1_second_0:                 episode reward: 17.4500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 2433.25,                last time consumption/overall running time: 2869.9481s / 239221.5727 s
env0_first_0:                 episode reward: -20.1500,                 loss: 0.0135
env0_second_0:                 episode reward: 20.1500,                 loss: 0.0138
env1_first_0:                 episode reward: -20.9000,                 loss: nan
env1_second_0:                 episode reward: 20.9000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 2237.4,                last time consumption/overall running time: 2635.8179s / 241857.3907 s
env0_first_0:                 episode reward: -18.1000,                 loss: 0.0137
env0_second_0:                 episode reward: 18.1000,                 loss: 0.0125
env1_first_0:                 episode reward: -19.9500,                 loss: nan
env1_second_0:                 episode reward: 19.9500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 2188.65,                last time consumption/overall running time: 2569.6202s / 244427.0109 s
env0_first_0:                 episode reward: -17.6000,                 loss: 0.0140
env0_second_0:                 episode reward: 17.6000,                 loss: 0.0122
env1_first_0:                 episode reward: -20.0000,                 loss: nan
env1_second_0:                 episode reward: 20.0000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 2118.9,                last time consumption/overall running time: 2475.5668s / 246902.5776 s
env0_first_0:                 episode reward: -16.2500,                 loss: 0.0131
env0_second_0:                 episode reward: 16.2500,                 loss: 0.0108
env1_first_0:                 episode reward: -16.4500,                 loss: nan
env1_second_0:                 episode reward: 16.4500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 2158.4,                last time consumption/overall running time: 2535.1844s / 249437.7621 s
env0_first_0:                 episode reward: -17.4000,                 loss: 0.0128
env0_second_0:                 episode reward: 17.4000,                 loss: 0.0111
env1_first_0:                 episode reward: -17.5000,                 loss: nan
env1_second_0:                 episode reward: 17.5000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 2129.95,                last time consumption/overall running time: 2491.2855s / 251929.0476 s
env0_first_0:                 episode reward: -17.4500,                 loss: 0.0125
env0_second_0:                 episode reward: 17.4500,                 loss: 0.0109
env1_first_0:                 episode reward: -15.3500,                 loss: nan
env1_second_0:                 episode reward: 15.3500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 2197.65,                last time consumption/overall running time: 2557.6569s / 254486.7045 s
env0_first_0:                 episode reward: -18.6000,                 loss: 0.0113
env0_second_0:                 episode reward: 18.6000,                 loss: 0.0114
env1_first_0:                 episode reward: -17.5000,                 loss: nan
env1_second_0:                 episode reward: 17.5000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1987.05,                last time consumption/overall running time: 2293.9561s / 256780.6606 s
env0_first_0:                 episode reward: -16.1500,                 loss: 0.0106
env0_second_0:                 episode reward: 16.1500,                 loss: 0.0110
env1_first_0:                 episode reward: -18.9000,                 loss: nan
env1_second_0:                 episode reward: 18.9000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 2089.65,                last time consumption/overall running time: 2373.8580s / 259154.5187 s
env0_first_0:                 episode reward: -18.2000,                 loss: 0.0106
env0_second_0:                 episode reward: 18.2000,                 loss: 0.0110
env1_first_0:                 episode reward: -18.5000,                 loss: nan
env1_second_0:                 episode reward: 18.5000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1976.75,                last time consumption/overall running time: 2243.7175s / 261398.2362 s
env0_first_0:                 episode reward: -18.2000,                 loss: 0.0110
env0_second_0:                 episode reward: 18.2000,                 loss: 0.0121
env1_first_0:                 episode reward: -17.6000,                 loss: nan
env1_second_0:                 episode reward: 17.6000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 2191.15,                last time consumption/overall running time: 2479.3038s / 263877.5399 s
env0_first_0:                 episode reward: -17.8500,                 loss: 0.0110
env0_second_0:                 episode reward: 17.8500,                 loss: 0.0121
env1_first_0:                 episode reward: -19.5500,                 loss: nan
env1_second_0:                 episode reward: 19.5500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 2368.7,                last time consumption/overall running time: 2672.7087s / 266550.2486 s
env0_first_0:                 episode reward: -20.9000,                 loss: 0.0114
env0_second_0:                 episode reward: 20.9000,                 loss: 0.0120
env1_first_0:                 episode reward: -19.9500,                 loss: nan
env1_second_0:                 episode reward: 19.9500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 2083.2,                last time consumption/overall running time: 2332.7017s / 268882.9503 s
env0_first_0:                 episode reward: -19.1000,                 loss: 0.0110
env0_second_0:                 episode reward: 19.1000,                 loss: 0.0121
env1_first_0:                 episode reward: -18.3500,                 loss: nan
env1_second_0:                 episode reward: 18.3500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 2099.55,                last time consumption/overall running time: 2341.1376s / 271224.0880 s
env0_first_0:                 episode reward: -19.2000,                 loss: 0.0103
env0_second_0:                 episode reward: 19.2000,                 loss: 0.0116
env1_first_0:                 episode reward: -19.2500,                 loss: nan
env1_second_0:                 episode reward: 19.2500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 2084.4,                last time consumption/overall running time: 2300.3169s / 273524.4049 s
env0_first_0:                 episode reward: -16.9000,                 loss: 0.0106
env0_second_0:                 episode reward: 16.9000,                 loss: 0.0116
env1_first_0:                 episode reward: -18.0000,                 loss: nan
env1_second_0:                 episode reward: 18.0000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 2256.7,                last time consumption/overall running time: 2482.5046s / 276006.9095 s
env0_first_0:                 episode reward: -18.9000,                 loss: 0.0106
env0_second_0:                 episode reward: 18.9000,                 loss: 0.0114
env1_first_0:                 episode reward: -16.4000,                 loss: nan
env1_second_0:                 episode reward: 16.4000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 2067.3,                last time consumption/overall running time: 2270.1235s / 278277.0330 s
env0_first_0:                 episode reward: -16.3500,                 loss: 0.0103
env0_second_0:                 episode reward: 16.3500,                 loss: 0.0113
env1_first_0:                 episode reward: -17.5000,                 loss: nan
env1_second_0:                 episode reward: 17.5000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 2100.0,                last time consumption/overall running time: 2289.3452s / 280566.3782 s
env0_first_0:                 episode reward: -16.8500,                 loss: 0.0104
env0_second_0:                 episode reward: 16.8500,                 loss: 0.0112
env1_first_0:                 episode reward: -18.1500,                 loss: nan
env1_second_0:                 episode reward: 18.1500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 2104.9,                last time consumption/overall running time: 2285.1212s / 282851.4994 s
env0_first_0:                 episode reward: -17.5000,                 loss: 0.0100
env0_second_0:                 episode reward: 17.5000,                 loss: 0.0108
env1_first_0:                 episode reward: -18.3500,                 loss: nan
env1_second_0:                 episode reward: 18.3500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 2125.2,                last time consumption/overall running time: 2303.9074s / 285155.4068 s
env0_first_0:                 episode reward: -15.8500,                 loss: 0.0100
env0_second_0:                 episode reward: 15.8500,                 loss: 0.0113
env1_first_0:                 episode reward: -17.9500,                 loss: nan
env1_second_0:                 episode reward: 17.9500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 2048.05,                last time consumption/overall running time: 2189.1512s / 287344.5580 s
env0_first_0:                 episode reward: -16.5500,                 loss: 0.0097
env0_second_0:                 episode reward: 16.5500,                 loss: 0.0114
env1_first_0:                 episode reward: -17.4500,                 loss: nan
env1_second_0:                 episode reward: 17.4500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 2122.3,                last time consumption/overall running time: 2257.6024s / 289602.1604 s
env0_first_0:                 episode reward: -16.2500,                 loss: 0.0088
env0_second_0:                 episode reward: 16.2500,                 loss: 0.0103
env1_first_0:                 episode reward: -15.9000,                 loss: nan
env1_second_0:                 episode reward: 15.9000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1944.65,                last time consumption/overall running time: 2074.9640s / 291677.1244 s
env0_first_0:                 episode reward: -16.2000,                 loss: 0.0099
env0_second_0:                 episode reward: 16.2000,                 loss: 0.0112
env1_first_0:                 episode reward: -15.8500,                 loss: nan
env1_second_0:                 episode reward: 15.8500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 2103.55,                last time consumption/overall running time: 2242.3809s / 293919.5053 s
env0_first_0:                 episode reward: -18.5000,                 loss: 0.0092
env0_second_0:                 episode reward: 18.5000,                 loss: 0.0106
env1_first_0:                 episode reward: -17.7500,                 loss: nan
env1_second_0:                 episode reward: 17.7500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 2179.85,                last time consumption/overall running time: 2303.2460s / 296222.7513 s
env0_first_0:                 episode reward: -17.7000,                 loss: 0.0094
env0_second_0:                 episode reward: 17.7000,                 loss: 0.0109
env1_first_0:                 episode reward: -20.3000,                 loss: nan
env1_second_0:                 episode reward: 20.3000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 2125.45,                last time consumption/overall running time: 2231.8464s / 298454.5978 s
env0_first_0:                 episode reward: -17.8000,                 loss: 0.0090
env0_second_0:                 episode reward: 17.8000,                 loss: 0.0103
env1_first_0:                 episode reward: -19.4500,                 loss: nan
env1_second_0:                 episode reward: 19.4500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 2250.95,                last time consumption/overall running time: 2377.0900s / 300831.6878 s
env0_first_0:                 episode reward: -18.7000,                 loss: 0.0091
env0_second_0:                 episode reward: 18.7000,                 loss: 0.0109
env1_first_0:                 episode reward: -19.3000,                 loss: nan
env1_second_0:                 episode reward: 19.3000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 2411.7,                last time consumption/overall running time: 2528.4368s / 303360.1246 s
env0_first_0:                 episode reward: -18.6000,                 loss: 0.0089
env0_second_0:                 episode reward: 18.6000,                 loss: 0.0111
env1_first_0:                 episode reward: -20.1000,                 loss: nan
env1_second_0:                 episode reward: 20.1000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 2057.15,                last time consumption/overall running time: 2146.6381s / 305506.7626 s
env0_first_0:                 episode reward: -18.4500,                 loss: 0.0089
env0_second_0:                 episode reward: 18.4500,                 loss: 0.0114
env1_first_0:                 episode reward: -18.2000,                 loss: nan
env1_second_0:                 episode reward: 18.2000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 2111.95,                last time consumption/overall running time: 2201.0796s / 307707.8422 s
env0_first_0:                 episode reward: -19.7500,                 loss: 0.0091
env0_second_0:                 episode reward: 19.7500,                 loss: 0.0113
env1_first_0:                 episode reward: -20.5000,                 loss: nan
env1_second_0:                 episode reward: 20.5000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 2086.65,                last time consumption/overall running time: 2161.2175s / 309869.0597 s
env0_first_0:                 episode reward: -18.8500,                 loss: 0.0096
env0_second_0:                 episode reward: 18.8500,                 loss: 0.0108
env1_first_0:                 episode reward: -18.5000,                 loss: nan
env1_second_0:                 episode reward: 18.5000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 2085.05,                last time consumption/overall running time: 2153.9569s / 312023.0165 s
env0_first_0:                 episode reward: -17.7000,                 loss: 0.0094
env0_second_0:                 episode reward: 17.7000,                 loss: 0.0109
env1_first_0:                 episode reward: -19.1000,                 loss: nan
env1_second_0:                 episode reward: 19.1000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 2093.6,                last time consumption/overall running time: 2173.0077s / 314196.0242 s
env0_first_0:                 episode reward: -20.4500,                 loss: 0.0095
env0_second_0:                 episode reward: 20.4500,                 loss: 0.0107
env1_first_0:                 episode reward: -18.0500,                 loss: nan
env1_second_0:                 episode reward: 18.0500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 2296.05,                last time consumption/overall running time: 2360.5126s / 316556.5368 s
env0_first_0:                 episode reward: -18.4000,                 loss: 0.0096
env0_second_0:                 episode reward: 18.4000,                 loss: 0.0107
env1_first_0:                 episode reward: -20.1000,                 loss: nan
env1_second_0:                 episode reward: 20.1000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 2294.25,                last time consumption/overall running time: 2370.0426s / 318926.5794 s
env0_first_0:                 episode reward: -20.8500,                 loss: 0.0096
env0_second_0:                 episode reward: 20.8500,                 loss: 0.0112
env1_first_0:                 episode reward: -23.0500,                 loss: nan
env1_second_0:                 episode reward: 23.0500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 2145.25,                last time consumption/overall running time: 2172.3043s / 321098.8837 s
env0_first_0:                 episode reward: -18.2000,                 loss: 0.0096
env0_second_0:                 episode reward: 18.2000,                 loss: 0.0106
env1_first_0:                 episode reward: -18.3500,                 loss: nan
env1_second_0:                 episode reward: 18.3500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 2219.15,                last time consumption/overall running time: 2262.8639s / 323361.7476 s
env0_first_0:                 episode reward: -18.2000,                 loss: 0.0093
env0_second_0:                 episode reward: 18.2000,                 loss: 0.0100
env1_first_0:                 episode reward: -17.7500,                 loss: nan
env1_second_0:                 episode reward: 17.7500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 2141.75,                last time consumption/overall running time: 2172.8310s / 325534.5787 s
env0_first_0:                 episode reward: -17.7000,                 loss: 0.0091
env0_second_0:                 episode reward: 17.7000,                 loss: 0.0094
env1_first_0:                 episode reward: -19.4500,                 loss: nan
env1_second_0:                 episode reward: 19.4500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 2006.1,                last time consumption/overall running time: 2025.2126s / 327559.7913 s
env0_first_0:                 episode reward: -17.1000,                 loss: 0.0090
env0_second_0:                 episode reward: 17.1000,                 loss: 0.0101
env1_first_0:                 episode reward: -17.1500,                 loss: nan
env1_second_0:                 episode reward: 17.1500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 2172.15,                last time consumption/overall running time: 2207.5151s / 329767.3064 s
env0_first_0:                 episode reward: -16.1000,                 loss: 0.0086
env0_second_0:                 episode reward: 16.1000,                 loss: 0.0098
env1_first_0:                 episode reward: -16.9000,                 loss: nan
env1_second_0:                 episode reward: 16.9000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 2122.65,                last time consumption/overall running time: 2133.3051s / 331900.6115 s
env0_first_0:                 episode reward: -16.0500,                 loss: 0.0086
env0_second_0:                 episode reward: 16.0500,                 loss: 0.0095
env1_first_0:                 episode reward: -14.7000,                 loss: nan
env1_second_0:                 episode reward: 14.7000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1983.5,                last time consumption/overall running time: 1976.3349s / 333876.9464 s
env0_first_0:                 episode reward: -14.8500,                 loss: 0.0082
env0_second_0:                 episode reward: 14.8500,                 loss: 0.0089
env1_first_0:                 episode reward: -15.6000,                 loss: nan
env1_second_0:                 episode reward: 15.6000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1999.55,                last time consumption/overall running time: 2002.3064s / 335879.2528 s
env0_first_0:                 episode reward: -14.8000,                 loss: 0.0084
env0_second_0:                 episode reward: 14.8000,                 loss: 0.0089
env1_first_0:                 episode reward: -12.7500,                 loss: nan
env1_second_0:                 episode reward: 12.7500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1987.95,                last time consumption/overall running time: 1973.8678s / 337853.1206 s
env0_first_0:                 episode reward: -12.9000,                 loss: 0.0079
env0_second_0:                 episode reward: 12.9000,                 loss: 0.0085
env1_first_0:                 episode reward: -12.6500,                 loss: nan
env1_second_0:                 episode reward: 12.6500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 2087.55,                last time consumption/overall running time: 2058.9048s / 339912.0254 s
env0_first_0:                 episode reward: -12.2500,                 loss: 0.0079
env0_second_0:                 episode reward: 12.2500,                 loss: 0.0081
env1_first_0:                 episode reward: -11.5000,                 loss: nan
env1_second_0:                 episode reward: 11.5000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 2091.25,                last time consumption/overall running time: 2073.0806s / 341985.1060 s
env0_first_0:                 episode reward: -14.1000,                 loss: 0.0081
env0_second_0:                 episode reward: 14.1000,                 loss: 0.0093
env1_first_0:                 episode reward: -14.0000,                 loss: nan
env1_second_0:                 episode reward: 14.0000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 2305.35,                last time consumption/overall running time: 2293.5689s / 344278.6749 s
env0_first_0:                 episode reward: -14.1000,                 loss: 0.0093
env0_second_0:                 episode reward: 14.1000,                 loss: 0.0099
env1_first_0:                 episode reward: -16.4500,                 loss: nan
env1_second_0:                 episode reward: 16.4500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 2356.45,                last time consumption/overall running time: 2337.3663s / 346616.0412 s
env0_first_0:                 episode reward: -14.0000,                 loss: 0.0099
env0_second_0:                 episode reward: 14.0000,                 loss: 0.0106
env1_first_0:                 episode reward: -14.8500,                 loss: nan
env1_second_0:                 episode reward: 14.8500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 2268.55,                last time consumption/overall running time: 2367.8418s / 348983.8830 s
env0_first_0:                 episode reward: -14.4000,                 loss: 0.0092
env0_second_0:                 episode reward: 14.4000,                 loss: 0.0099
env1_first_0:                 episode reward: -12.9000,                 loss: nan
env1_second_0:                 episode reward: 12.9000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 2323.95,                last time consumption/overall running time: 2641.2622s / 351625.1452 s
env0_first_0:                 episode reward: -15.7500,                 loss: 0.0096
env0_second_0:                 episode reward: 15.7500,                 loss: 0.0107
env1_first_0:                 episode reward: -17.3500,                 loss: nan
env1_second_0:                 episode reward: 17.3500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 2011.4,                last time consumption/overall running time: 2293.6817s / 353918.8269 s
env0_first_0:                 episode reward: -15.6000,                 loss: 0.0097
env0_second_0:                 episode reward: 15.6000,                 loss: 0.0108
env1_first_0:                 episode reward: -14.9000,                 loss: nan
env1_second_0:                 episode reward: 14.9000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 2079.2,                last time consumption/overall running time: 2355.8998s / 356274.7267 s
env0_first_0:                 episode reward: -16.3500,                 loss: 0.0104
env0_second_0:                 episode reward: 16.3500,                 loss: 0.0108
env1_first_0:                 episode reward: -15.2000,                 loss: nan
env1_second_0:                 episode reward: 15.2000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 2268.05,                last time consumption/overall running time: 2560.2448s / 358834.9715 s
env0_first_0:                 episode reward: -15.4000,                 loss: 0.0114
env0_second_0:                 episode reward: 15.4000,                 loss: 0.0110
env1_first_0:                 episode reward: -16.2000,                 loss: nan
env1_second_0:                 episode reward: 16.2000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 2081.3,                last time consumption/overall running time: 2332.6952s / 361167.6667 s
env0_first_0:                 episode reward: -13.7500,                 loss: 0.0108
env0_second_0:                 episode reward: 13.7500,                 loss: 0.0111
env1_first_0:                 episode reward: -14.9500,                 loss: nan
env1_second_0:                 episode reward: 14.9500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 2097.5,                last time consumption/overall running time: 2336.5489s / 363504.2156 s
env0_first_0:                 episode reward: -16.6000,                 loss: 0.0108
env0_second_0:                 episode reward: 16.6000,                 loss: 0.0106
env1_first_0:                 episode reward: -14.9000,                 loss: nan
env1_second_0:                 episode reward: 14.9000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 2169.9,                last time consumption/overall running time: 2405.3815s / 365909.5971 s
env0_first_0:                 episode reward: -13.0500,                 loss: 0.0100
env0_second_0:                 episode reward: 13.0500,                 loss: 0.0101
env1_first_0:                 episode reward: -16.2000,                 loss: nan
env1_second_0:                 episode reward: 16.2000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 2091.7,                last time consumption/overall running time: 2314.3660s / 368223.9631 s
env0_first_0:                 episode reward: -16.0500,                 loss: 0.0096
env0_second_0:                 episode reward: 16.0500,                 loss: 0.0105
env1_first_0:                 episode reward: -15.4000,                 loss: nan
env1_second_0:                 episode reward: 15.4000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 2127.7,                last time consumption/overall running time: 2356.7326s / 370580.6957 s
env0_first_0:                 episode reward: -15.2500,                 loss: 0.0096
env0_second_0:                 episode reward: 15.2500,                 loss: 0.0094
env1_first_0:                 episode reward: -12.7000,                 loss: nan
env1_second_0:                 episode reward: 12.7000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 2088.05,                last time consumption/overall running time: 2313.5653s / 372894.2610 s
env0_first_0:                 episode reward: -14.5500,                 loss: 0.0090
env0_second_0:                 episode reward: 14.5500,                 loss: 0.0085
env1_first_0:                 episode reward: -14.6000,                 loss: nan
env1_second_0:                 episode reward: 14.6000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 2251.2,                last time consumption/overall running time: 2461.5764s / 375355.8374 s
env0_first_0:                 episode reward: -13.1000,                 loss: 0.0099
env0_second_0:                 episode reward: 13.1000,                 loss: 0.0096
env1_first_0:                 episode reward: -15.6500,                 loss: nan
env1_second_0:                 episode reward: 15.6500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 2064.25,                last time consumption/overall running time: 2262.4323s / 377618.2697 s
env0_first_0:                 episode reward: -11.8000,                 loss: 0.0098
env0_second_0:                 episode reward: 11.8000,                 loss: 0.0096
env1_first_0:                 episode reward: -14.0000,                 loss: nan
env1_second_0:                 episode reward: 14.0000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 2109.75,                last time consumption/overall running time: 2317.3870s / 379935.6567 s
env0_first_0:                 episode reward: -11.7000,                 loss: 0.0091
env0_second_0:                 episode reward: 11.7000,                 loss: 0.0086
env1_first_0:                 episode reward: -12.0500,                 loss: nan
env1_second_0:                 episode reward: 12.0500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 2131.55,                last time consumption/overall running time: 2364.9938s / 382300.6504 s
env0_first_0:                 episode reward: -16.0000,                 loss: 0.0084
env0_second_0:                 episode reward: 16.0000,                 loss: 0.0081
env1_first_0:                 episode reward: -13.9000,                 loss: nan
env1_second_0:                 episode reward: 13.9000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 2261.45,                last time consumption/overall running time: 2489.5665s / 384790.2169 s
env0_first_0:                 episode reward: -14.6000,                 loss: 0.0093
env0_second_0:                 episode reward: 14.6000,                 loss: 0.0089
env1_first_0:                 episode reward: -15.6500,                 loss: nan
env1_second_0:                 episode reward: 15.6500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 2171.0,                last time consumption/overall running time: 2339.7766s / 387129.9935 s
env0_first_0:                 episode reward: -12.7000,                 loss: 0.0095
env0_second_0:                 episode reward: 12.7000,                 loss: 0.0091
env1_first_0:                 episode reward: -14.8000,                 loss: nan
env1_second_0:                 episode reward: 14.8000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 2124.15,                last time consumption/overall running time: 2297.8982s / 389427.8916 s
env0_first_0:                 episode reward: -15.4000,                 loss: 0.0091
env0_second_0:                 episode reward: 15.4000,                 loss: 0.0087
env1_first_0:                 episode reward: -11.7500,                 loss: nan
env1_second_0:                 episode reward: 11.7500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 2262.3,                last time consumption/overall running time: 2439.3541s / 391867.2458 s
env0_first_0:                 episode reward: -15.5000,                 loss: 0.0085
env0_second_0:                 episode reward: 15.5000,                 loss: 0.0088
env1_first_0:                 episode reward: -14.9500,                 loss: nan
env1_second_0:                 episode reward: 14.9500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 2055.3,                last time consumption/overall running time: 2224.6215s / 394091.8673 s
env0_first_0:                 episode reward: -12.1500,                 loss: 0.0082
env0_second_0:                 episode reward: 12.1500,                 loss: 0.0084
env1_first_0:                 episode reward: -12.1500,                 loss: nan
env1_second_0:                 episode reward: 12.1500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 1918.4,                last time consumption/overall running time: 2079.8991s / 396171.7664 s
env0_first_0:                 episode reward: -10.1500,                 loss: 0.0082
env0_second_0:                 episode reward: 10.1500,                 loss: 0.0085
env1_first_0:                 episode reward: -12.9500,                 loss: nan
env1_second_0:                 episode reward: 12.9500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 2054.2,                last time consumption/overall running time: 2222.6884s / 398394.4548 s
env0_first_0:                 episode reward: -11.2000,                 loss: 0.0087
env0_second_0:                 episode reward: 11.2000,                 loss: 0.0082
env1_first_0:                 episode reward: -12.0500,                 loss: nan
env1_second_0:                 episode reward: 12.0500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 2028.85,                last time consumption/overall running time: 2193.8796s / 400588.3344 s
env0_first_0:                 episode reward: -12.6000,                 loss: 0.0094
env0_second_0:                 episode reward: 12.6000,                 loss: 0.0087
env1_first_0:                 episode reward: -14.2500,                 loss: nan
env1_second_0:                 episode reward: 14.2500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 2008.1,                last time consumption/overall running time: 2152.9298s / 402741.2642 s
env0_first_0:                 episode reward: -11.5000,                 loss: 0.0087
env0_second_0:                 episode reward: 11.5000,                 loss: 0.0087
env1_first_0:                 episode reward: -12.2500,                 loss: nan
env1_second_0:                 episode reward: 12.2500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 2186.0,                last time consumption/overall running time: 2343.7700s / 405085.0342 s
env0_first_0:                 episode reward: -9.0000,                 loss: 0.0080
env0_second_0:                 episode reward: 9.0000,                 loss: 0.0081
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 2094.25,                last time consumption/overall running time: 2233.7797s / 407318.8139 s
env0_first_0:                 episode reward: -12.3000,                 loss: 0.0093
env0_second_0:                 episode reward: 12.3000,                 loss: 0.0093
env1_first_0:                 episode reward: -10.8000,                 loss: nan
env1_second_0:                 episode reward: 10.8000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 2257.35,                last time consumption/overall running time: 2371.4404s / 409690.2543 s
env0_first_0:                 episode reward: -9.7500,                 loss: 0.0093
env0_second_0:                 episode reward: 9.7500,                 loss: 0.0092
env1_first_0:                 episode reward: -10.1000,                 loss: nan
env1_second_0:                 episode reward: 10.1000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 2319.55,                last time consumption/overall running time: 2355.5423s / 412045.7966 s
env0_first_0:                 episode reward: -12.3500,                 loss: 0.0093
env0_second_0:                 episode reward: 12.3500,                 loss: 0.0091
env1_first_0:                 episode reward: -11.3500,                 loss: nan
env1_second_0:                 episode reward: 11.3500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1767.8,                last time consumption/overall running time: 1765.3082s / 413811.1048 s
env0_first_0:                 episode reward: -12.6500,                 loss: 0.0092
env0_second_0:                 episode reward: 12.6500,                 loss: 0.0090
env1_first_0:                 episode reward: -11.1000,                 loss: nan
env1_second_0:                 episode reward: 11.1000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1910.5,                last time consumption/overall running time: 1918.4217s / 415729.5265 s
env0_first_0:                 episode reward: -12.7000,                 loss: 0.0096
env0_second_0:                 episode reward: 12.7000,                 loss: 0.0084
env1_first_0:                 episode reward: -14.5000,                 loss: nan
env1_second_0:                 episode reward: 14.5000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 2044.5,                last time consumption/overall running time: 2044.2659s / 417773.7923 s
env0_first_0:                 episode reward: -11.1000,                 loss: 0.0086
env0_second_0:                 episode reward: 11.1000,                 loss: 0.0088
env1_first_0:                 episode reward: -12.1000,                 loss: nan
env1_second_0:                 episode reward: 12.1000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1889.9,                last time consumption/overall running time: 1837.4116s / 419611.2040 s
env0_first_0:                 episode reward: -10.8500,                 loss: 0.0080
env0_second_0:                 episode reward: 10.8500,                 loss: 0.0087
env1_first_0:                 episode reward: -10.9000,                 loss: nan
env1_second_0:                 episode reward: 10.9000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 2137.35,                last time consumption/overall running time: 2074.2838s / 421685.4878 s
env0_first_0:                 episode reward: -10.2000,                 loss: 0.0077
env0_second_0:                 episode reward: 10.2000,                 loss: 0.0087
env1_first_0:                 episode reward: -12.1500,                 loss: nan
env1_second_0:                 episode reward: 12.1500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 2217.15,                last time consumption/overall running time: 2148.9655s / 423834.4533 s
env0_first_0:                 episode reward: -13.6000,                 loss: 0.0080
env0_second_0:                 episode reward: 13.6000,                 loss: 0.0090
env1_first_0:                 episode reward: -13.0500,                 loss: nan
env1_second_0:                 episode reward: 13.0500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 2123.7,                last time consumption/overall running time: 2059.0532s / 425893.5065 s
env0_first_0:                 episode reward: -14.7500,                 loss: 0.0083
env0_second_0:                 episode reward: 14.7500,                 loss: 0.0089
env1_first_0:                 episode reward: -13.2500,                 loss: nan
env1_second_0:                 episode reward: 13.2500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 1891.5,                last time consumption/overall running time: 1834.5024s / 427728.0088 s
env0_first_0:                 episode reward: -13.2500,                 loss: 0.0087
env0_second_0:                 episode reward: 13.2500,                 loss: 0.0085
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 1837.15,                last time consumption/overall running time: 1824.1071s / 429552.1160 s
env0_first_0:                 episode reward: -13.9500,                 loss: 0.0085
env0_second_0:                 episode reward: 13.9500,                 loss: 0.0083
env1_first_0:                 episode reward: -11.5500,                 loss: nan
env1_second_0:                 episode reward: 11.5500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 2066.6,                last time consumption/overall running time: 2289.2164s / 431841.3324 s
env0_first_0:                 episode reward: -12.3000,                 loss: 0.0084
env0_second_0:                 episode reward: 12.3000,                 loss: 0.0084
env1_first_0:                 episode reward: -13.9500,                 loss: nan
env1_second_0:                 episode reward: 13.9500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 2129.55,                last time consumption/overall running time: 2345.9625s / 434187.2949 s
env0_first_0:                 episode reward: -10.6000,                 loss: 0.0087
env0_second_0:                 episode reward: 10.6000,                 loss: 0.0085
env1_first_0:                 episode reward: -12.3000,                 loss: nan
env1_second_0:                 episode reward: 12.3000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 2016.1,                last time consumption/overall running time: 2223.4040s / 436410.6989 s
env0_first_0:                 episode reward: -13.3000,                 loss: 0.0092
env0_second_0:                 episode reward: 13.3000,                 loss: 0.0086
env1_first_0:                 episode reward: -11.4500,                 loss: nan
env1_second_0:                 episode reward: 11.4500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 2001.3,                last time consumption/overall running time: 2211.6506s / 438622.3494 s
env0_first_0:                 episode reward: -8.5500,                 loss: 0.0086
env0_second_0:                 episode reward: 8.5500,                 loss: 0.0078
env1_first_0:                 episode reward: -10.0500,                 loss: nan
env1_second_0:                 episode reward: 10.0500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 1971.6,                last time consumption/overall running time: 2174.2683s / 440796.6177 s
env0_first_0:                 episode reward: -13.6000,                 loss: 0.0089
env0_second_0:                 episode reward: 13.6000,                 loss: 0.0077
env1_first_0:                 episode reward: -12.8000,                 loss: nan
env1_second_0:                 episode reward: 12.8000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 1891.25,                last time consumption/overall running time: 2068.6831s / 442865.3008 s
env0_first_0:                 episode reward: -12.0500,                 loss: 0.0088
env0_second_0:                 episode reward: 12.0500,                 loss: 0.0080
env1_first_0:                 episode reward: -11.1500,                 loss: nan
env1_second_0:                 episode reward: 11.1500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 1891.75,                last time consumption/overall running time: 2085.1339s / 444950.4348 s
env0_first_0:                 episode reward: -11.3500,                 loss: 0.0081
env0_second_0:                 episode reward: 11.3500,                 loss: 0.0075
env1_first_0:                 episode reward: -10.5000,                 loss: nan
env1_second_0:                 episode reward: 10.5000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 1893.1,                last time consumption/overall running time: 2077.9253s / 447028.3601 s
env0_first_0:                 episode reward: -9.8000,                 loss: 0.0082
env0_second_0:                 episode reward: 9.8000,                 loss: 0.0075
env1_first_0:                 episode reward: -10.7500,                 loss: nan
env1_second_0:                 episode reward: 10.7500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 1930.5,                last time consumption/overall running time: 2123.1317s / 449151.4918 s
env0_first_0:                 episode reward: -9.7500,                 loss: 0.0084
env0_second_0:                 episode reward: 9.7500,                 loss: 0.0079
env1_first_0:                 episode reward: -10.2500,                 loss: nan
env1_second_0:                 episode reward: 10.2500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 1786.2,                last time consumption/overall running time: 1971.9840s / 451123.4759 s
env0_first_0:                 episode reward: -10.9000,                 loss: 0.0087
env0_second_0:                 episode reward: 10.9000,                 loss: 0.0082
env1_first_0:                 episode reward: -9.9000,                 loss: nan
env1_second_0:                 episode reward: 9.9000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 1698.95,                last time consumption/overall running time: 1861.3324s / 452984.8083 s
env0_first_0:                 episode reward: -9.8000,                 loss: 0.0087
env0_second_0:                 episode reward: 9.8000,                 loss: 0.0080
env1_first_0:                 episode reward: -11.9000,                 loss: nan
env1_second_0:                 episode reward: 11.9000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 1737.6,                last time consumption/overall running time: 1905.9115s / 454890.7198 s
env0_first_0:                 episode reward: -12.7500,                 loss: 0.0084
env0_second_0:                 episode reward: 12.7500,                 loss: 0.0080
env1_first_0:                 episode reward: -12.2000,                 loss: nan
env1_second_0:                 episode reward: 12.2000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 1817.45,                last time consumption/overall running time: 2004.7456s / 456895.4653 s
env0_first_0:                 episode reward: -12.3000,                 loss: 0.0088
env0_second_0:                 episode reward: 12.3000,                 loss: 0.0087
env1_first_0:                 episode reward: -15.0500,                 loss: nan
env1_second_0:                 episode reward: 15.0500,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 1876.45,                last time consumption/overall running time: 2064.2414s / 458959.7067 s
env0_first_0:                 episode reward: -10.1500,                 loss: 0.0095
env0_second_0:                 episode reward: 10.1500,                 loss: 0.0092
env1_first_0:                 episode reward: -10.7000,                 loss: nan
env1_second_0:                 episode reward: 10.7000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 1883.7,                last time consumption/overall running time: 2061.8463s / 461021.5530 s
env0_first_0:                 episode reward: -10.6500,                 loss: 0.0084
env0_second_0:                 episode reward: 10.6500,                 loss: 0.0086
env1_first_0:                 episode reward: -10.5000,                 loss: nan
env1_second_0:                 episode reward: 10.5000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 2207.5,                last time consumption/overall running time: 2429.3747s / 463450.9277 s
env0_first_0:                 episode reward: -9.3500,                 loss: 0.0092
env0_second_0:                 episode reward: 9.3500,                 loss: 0.0093
env1_first_0:                 episode reward: -10.2500,                 loss: nan
env1_second_0:                 episode reward: 10.2500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 1900.0,                last time consumption/overall running time: 2093.5830s / 465544.5107 s
env0_first_0:                 episode reward: -11.6000,                 loss: 0.0092
env0_second_0:                 episode reward: 11.6000,                 loss: 0.0092
env1_first_0:                 episode reward: -12.4000,                 loss: nan
env1_second_0:                 episode reward: 12.4000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 1810.05,                last time consumption/overall running time: 2001.9624s / 467546.4731 s
env0_first_0:                 episode reward: -11.1500,                 loss: 0.0084
env0_second_0:                 episode reward: 11.1500,                 loss: 0.0084
env1_first_0:                 episode reward: -11.9000,                 loss: nan
env1_second_0:                 episode reward: 11.9000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 1793.55,                last time consumption/overall running time: 1966.4772s / 469512.9502 s
env0_first_0:                 episode reward: -10.6500,                 loss: 0.0086
env0_second_0:                 episode reward: 10.6500,                 loss: 0.0084
env1_first_0:                 episode reward: -12.6000,                 loss: nan
env1_second_0:                 episode reward: 12.6000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 1843.7,                last time consumption/overall running time: 2005.1158s / 471518.0661 s
env0_first_0:                 episode reward: -9.9500,                 loss: 0.0077
env0_second_0:                 episode reward: 9.9500,                 loss: 0.0080
env1_first_0:                 episode reward: -10.2000,                 loss: nan
env1_second_0:                 episode reward: 10.2000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 2046.15,                last time consumption/overall running time: 3032.8136s / 474550.8797 s
env0_first_0:                 episode reward: -11.1000,                 loss: 0.0086
env0_second_0:                 episode reward: 11.1000,                 loss: 0.0093
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 1861.4,                last time consumption/overall running time: 2785.1182s / 477335.9979 s
env0_first_0:                 episode reward: -12.6500,                 loss: 0.0090
env0_second_0:                 episode reward: 12.6500,                 loss: 0.0102
env1_first_0:                 episode reward: -12.5500,                 loss: nan
env1_second_0:                 episode reward: 12.5500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 1785.6,                last time consumption/overall running time: 2637.3551s / 479973.3530 s
env0_first_0:                 episode reward: -15.8000,                 loss: 0.0082
env0_second_0:                 episode reward: 15.8000,                 loss: 0.0091
env1_first_0:                 episode reward: -14.2500,                 loss: nan
env1_second_0:                 episode reward: 14.2500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1645.15,                last time consumption/overall running time: 2417.7446s / 482391.0976 s
env0_first_0:                 episode reward: -14.4000,                 loss: 0.0077
env0_second_0:                 episode reward: 14.4000,                 loss: 0.0081
env1_first_0:                 episode reward: -12.6500,                 loss: nan
env1_second_0:                 episode reward: 12.6500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1647.3,                last time consumption/overall running time: 2421.6387s / 484812.7362 s
env0_first_0:                 episode reward: -13.7000,                 loss: 0.0072
env0_second_0:                 episode reward: 13.7000,                 loss: 0.0073
env1_first_0:                 episode reward: -12.5500,                 loss: nan
env1_second_0:                 episode reward: 12.5500,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 1702.3,                last time consumption/overall running time: 2461.2491s / 487273.9853 s
env0_first_0:                 episode reward: -12.3500,                 loss: 0.0074
env0_second_0:                 episode reward: 12.3500,                 loss: 0.0077
env1_first_0:                 episode reward: -14.0500,                 loss: nan
env1_second_0:                 episode reward: 14.0500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 1605.55,                last time consumption/overall running time: 2331.7981s / 489605.7835 s
env0_first_0:                 episode reward: -15.3000,                 loss: 0.0075
env0_second_0:                 episode reward: 15.3000,                 loss: 0.0076
env1_first_0:                 episode reward: -12.6500,                 loss: nan
env1_second_0:                 episode reward: 12.6500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 1646.45,                last time consumption/overall running time: 2390.5283s / 491996.3118 s
env0_first_0:                 episode reward: -13.6000,                 loss: 0.0074
env0_second_0:                 episode reward: 13.6000,                 loss: 0.0075
env1_first_0:                 episode reward: -14.3500,                 loss: nan
env1_second_0:                 episode reward: 14.3500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1569.2,                last time consumption/overall running time: 2288.8966s / 494285.2084 s
env0_first_0:                 episode reward: -11.6500,                 loss: 0.0068
env0_second_0:                 episode reward: 11.6500,                 loss: 0.0072
env1_first_0:                 episode reward: -12.4000,                 loss: nan
env1_second_0:                 episode reward: 12.4000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1773.45,                last time consumption/overall running time: 2548.0854s / 496833.2938 s
env0_first_0:                 episode reward: -11.8000,                 loss: 0.0083
env0_second_0:                 episode reward: 11.8000,                 loss: 0.0062
env1_first_0:                 episode reward: -14.2000,                 loss: nan
env1_second_0:                 episode reward: 14.2000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1853.1,                last time consumption/overall running time: 2647.6021s / 499480.8959 s
env0_first_0:                 episode reward: -13.3500,                 loss: 0.0083
env0_second_0:                 episode reward: 13.3500,                 loss: 0.0072
env1_first_0:                 episode reward: -15.0500,                 loss: nan
env1_second_0:                 episode reward: 15.0500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1544.6,                last time consumption/overall running time: 2208.3794s / 501689.2753 s
env0_first_0:                 episode reward: -14.8500,                 loss: 0.0070
env0_second_0:                 episode reward: 14.8500,                 loss: 0.0074
env1_first_0:                 episode reward: -14.5500,                 loss: nan
env1_second_0:                 episode reward: 14.5500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1563.85,                last time consumption/overall running time: 2238.9856s / 503928.2609 s
env0_first_0:                 episode reward: -11.3000,                 loss: 0.0068
env0_second_0:                 episode reward: 11.3000,                 loss: 0.0069
env1_first_0:                 episode reward: -12.2500,                 loss: nan
env1_second_0:                 episode reward: 12.2500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1680.05,                last time consumption/overall running time: 2395.3999s / 506323.6608 s
env0_first_0:                 episode reward: -10.2000,                 loss: 0.0072
env0_second_0:                 episode reward: 10.2000,                 loss: 0.0069
env1_first_0:                 episode reward: -10.1500,                 loss: nan
env1_second_0:                 episode reward: 10.1500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1565.65,                last time consumption/overall running time: 2195.4632s / 508519.1240 s
env0_first_0:                 episode reward: -12.3000,                 loss: 0.0065
env0_second_0:                 episode reward: 12.3000,                 loss: 0.0065
env1_first_0:                 episode reward: -10.6000,                 loss: nan
env1_second_0:                 episode reward: 10.6000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1612.1,                last time consumption/overall running time: 2260.4454s / 510779.5694 s
env0_first_0:                 episode reward: -10.2500,                 loss: 0.0066
env0_second_0:                 episode reward: 10.2500,                 loss: 0.0062
env1_first_0:                 episode reward: -12.0000,                 loss: nan
env1_second_0:                 episode reward: 12.0000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1824.35,                last time consumption/overall running time: 2538.9177s / 513318.4871 s
env0_first_0:                 episode reward: -11.1500,                 loss: 0.0078
env0_second_0:                 episode reward: 11.1500,                 loss: 0.0076
env1_first_0:                 episode reward: -10.0500,                 loss: nan
env1_second_0:                 episode reward: 10.0500,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1843.35,                last time consumption/overall running time: 2586.1847s / 515904.6717 s
env0_first_0:                 episode reward: -11.6500,                 loss: 0.0092
env0_second_0:                 episode reward: 11.6500,                 loss: 0.0083
env1_first_0:                 episode reward: -11.5000,                 loss: nan
env1_second_0:                 episode reward: 11.5000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1609.75,                last time consumption/overall running time: 2275.0008s / 518179.6725 s
env0_first_0:                 episode reward: -11.4000,                 loss: 0.0087
env0_second_0:                 episode reward: 11.4000,                 loss: 0.0079
env1_first_0:                 episode reward: -11.9500,                 loss: nan
env1_second_0:                 episode reward: 11.9500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1666.45,                last time consumption/overall running time: 2326.7791s / 520506.4517 s
env0_first_0:                 episode reward: -11.9500,                 loss: 0.0077
env0_second_0:                 episode reward: 11.9500,                 loss: 0.0071
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1907.75,                last time consumption/overall running time: 2643.1402s / 523149.5919 s
env0_first_0:                 episode reward: -11.2000,                 loss: 0.0086
env0_second_0:                 episode reward: 11.2000,                 loss: 0.0086
env1_first_0:                 episode reward: -12.0500,                 loss: nan
env1_second_0:                 episode reward: 12.0500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1912.85,                last time consumption/overall running time: 2583.7217s / 525733.3135 s
env0_first_0:                 episode reward: -12.0500,                 loss: 0.0100
env0_second_0:                 episode reward: 12.0500,                 loss: 0.0096
env1_first_0:                 episode reward: -10.8000,                 loss: nan
env1_second_0:                 episode reward: 10.8000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1804.15,                last time consumption/overall running time: 2422.5295s / 528155.8431 s
env0_first_0:                 episode reward: -12.9000,                 loss: 0.0093
env0_second_0:                 episode reward: 12.9000,                 loss: 0.0088
env1_first_0:                 episode reward: -13.9500,                 loss: nan
env1_second_0:                 episode reward: 13.9500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 1761.55,                last time consumption/overall running time: 2364.5461s / 530520.3891 s
env0_first_0:                 episode reward: -11.8000,                 loss: 0.0086
env0_second_0:                 episode reward: 11.8000,                 loss: 0.0081
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 2027.9,                last time consumption/overall running time: 2741.6532s / 533262.0423 s
env0_first_0:                 episode reward: -11.6500,                 loss: 0.0089
env0_second_0:                 episode reward: 11.6500,                 loss: 0.0086
env1_first_0:                 episode reward: -11.1000,                 loss: nan
env1_second_0:                 episode reward: 11.1000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 1961.0,                last time consumption/overall running time: 2666.1040s / 535928.1463 s
env0_first_0:                 episode reward: -12.9000,                 loss: 0.0098
env0_second_0:                 episode reward: 12.9000,                 loss: 0.0093
env1_first_0:                 episode reward: -14.5000,                 loss: nan
env1_second_0:                 episode reward: 14.5000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 1644.65,                last time consumption/overall running time: 2205.1127s / 538133.2589 s
env0_first_0:                 episode reward: -10.8000,                 loss: 0.0099
env0_second_0:                 episode reward: 10.8000,                 loss: 0.0091
env1_first_0:                 episode reward: -12.5500,                 loss: nan
env1_second_0:                 episode reward: 12.5500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 1620.9,                last time consumption/overall running time: 2171.7462s / 540305.0052 s
env0_first_0:                 episode reward: -13.0000,                 loss: 0.0089
env0_second_0:                 episode reward: 13.0000,                 loss: 0.0085
env1_first_0:                 episode reward: -11.6000,                 loss: nan
env1_second_0:                 episode reward: 11.6000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 1672.35,                last time consumption/overall running time: 2235.6113s / 542540.6165 s
env0_first_0:                 episode reward: -11.6000,                 loss: 0.0081
env0_second_0:                 episode reward: 11.6000,                 loss: 0.0080
env1_first_0:                 episode reward: -14.2000,                 loss: nan
env1_second_0:                 episode reward: 14.2000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1880.8,                last time consumption/overall running time: 2517.5258s / 545058.1423 s
env0_first_0:                 episode reward: -14.6500,                 loss: 0.0086
env0_second_0:                 episode reward: 14.6500,                 loss: 0.0083
env1_first_0:                 episode reward: -14.2500,                 loss: nan
env1_second_0:                 episode reward: 14.2500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1799.95,                last time consumption/overall running time: 2363.9297s / 547422.0720 s
env0_first_0:                 episode reward: -18.0500,                 loss: 0.0093
env0_second_0:                 episode reward: 18.0500,                 loss: 0.0084
env1_first_0:                 episode reward: -13.6500,                 loss: nan
env1_second_0:                 episode reward: 13.6500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1641.2,                last time consumption/overall running time: 2118.6808s / 549540.7528 s
env0_first_0:                 episode reward: -13.5000,                 loss: 0.0088
env0_second_0:                 episode reward: 13.5000,                 loss: 0.0082
env1_first_0:                 episode reward: -14.4500,                 loss: nan
env1_second_0:                 episode reward: 14.4500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 1665.55,                last time consumption/overall running time: 2163.6847s / 551704.4376 s
env0_first_0:                 episode reward: -12.3000,                 loss: 0.0078
env0_second_0:                 episode reward: 12.3000,                 loss: 0.0074
env1_first_0:                 episode reward: -14.6500,                 loss: nan
env1_second_0:                 episode reward: 14.6500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1735.9,                last time consumption/overall running time: 2235.0350s / 553939.4725 s
env0_first_0:                 episode reward: -11.3500,                 loss: 0.0075
env0_second_0:                 episode reward: 11.3500,                 loss: 0.0071
env1_first_0:                 episode reward: -11.3500,                 loss: nan
env1_second_0:                 episode reward: 11.3500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 1739.7,                last time consumption/overall running time: 2234.9912s / 556174.4637 s
env0_first_0:                 episode reward: -13.8000,                 loss: 0.0077
env0_second_0:                 episode reward: 13.8000,                 loss: 0.0074
env1_first_0:                 episode reward: -13.4000,                 loss: nan
env1_second_0:                 episode reward: 13.4000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 1659.75,                last time consumption/overall running time: 2119.7953s / 558294.2590 s
env0_first_0:                 episode reward: -14.3500,                 loss: 0.0080
env0_second_0:                 episode reward: 14.3500,                 loss: 0.0081
env1_first_0:                 episode reward: -14.9500,                 loss: nan
env1_second_0:                 episode reward: 14.9500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1700.85,                last time consumption/overall running time: 2165.6272s / 560459.8862 s
env0_first_0:                 episode reward: -15.4000,                 loss: 0.0073
env0_second_0:                 episode reward: 15.4000,                 loss: 0.0077
env1_first_0:                 episode reward: -14.5000,                 loss: nan
env1_second_0:                 episode reward: 14.5000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1773.75,                last time consumption/overall running time: 2236.3096s / 562696.1958 s
env0_first_0:                 episode reward: -13.1000,                 loss: 0.0076
env0_second_0:                 episode reward: 13.1000,                 loss: 0.0080
env1_first_0:                 episode reward: -11.1500,                 loss: nan
env1_second_0:                 episode reward: 11.1500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1743.3,                last time consumption/overall running time: 2210.2295s / 564906.4253 s
env0_first_0:                 episode reward: -14.0000,                 loss: 0.0079
env0_second_0:                 episode reward: 14.0000,                 loss: 0.0080
env1_first_0:                 episode reward: -13.3500,                 loss: nan
env1_second_0:                 episode reward: 13.3500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 1688.5,                last time consumption/overall running time: 2111.9985s / 567018.4238 s
env0_first_0:                 episode reward: -14.6500,                 loss: 0.0080
env0_second_0:                 episode reward: 14.6500,                 loss: 0.0083
env1_first_0:                 episode reward: -12.7500,                 loss: nan
env1_second_0:                 episode reward: 12.7500,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1862.1,                last time consumption/overall running time: 2341.8913s / 569360.3151 s
env0_first_0:                 episode reward: -13.5500,                 loss: 0.0080
env0_second_0:                 episode reward: 13.5500,                 loss: 0.0086
env1_first_0:                 episode reward: -12.8000,                 loss: nan
env1_second_0:                 episode reward: 12.8000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 1717.6,                last time consumption/overall running time: 2158.5785s / 571518.8936 s
env0_first_0:                 episode reward: -12.3000,                 loss: 0.0079
env0_second_0:                 episode reward: 12.3000,                 loss: 0.0088
env1_first_0:                 episode reward: -12.3000,                 loss: nan
env1_second_0:                 episode reward: 12.3000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1783.55,                last time consumption/overall running time: 2239.7052s / 573758.5988 s
env0_first_0:                 episode reward: -12.5000,                 loss: 0.0082
env0_second_0:                 episode reward: 12.5000,                 loss: 0.0087
env1_first_0:                 episode reward: -11.3000,                 loss: nan
env1_second_0:                 episode reward: 11.3000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 1654.35,                last time consumption/overall running time: 2059.7825s / 575818.3813 s
env0_first_0:                 episode reward: -14.3000,                 loss: 0.0080
env0_second_0:                 episode reward: 14.3000,                 loss: 0.0086
env1_first_0:                 episode reward: -13.4500,                 loss: nan
env1_second_0:                 episode reward: 13.4500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1756.4,                last time consumption/overall running time: 2186.6272s / 578005.0085 s
env0_first_0:                 episode reward: -11.9000,                 loss: 0.0076
env0_second_0:                 episode reward: 11.9000,                 loss: 0.0088
env1_first_0:                 episode reward: -12.7500,                 loss: nan
env1_second_0:                 episode reward: 12.7500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 1762.0,                last time consumption/overall running time: 2176.8124s / 580181.8209 s
env0_first_0:                 episode reward: -11.6000,                 loss: 0.0083
env0_second_0:                 episode reward: 11.6000,                 loss: 0.0096
env1_first_0:                 episode reward: -11.6500,                 loss: nan
env1_second_0:                 episode reward: 11.6500,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 1909.25,                last time consumption/overall running time: 2304.1255s / 582485.9464 s
env0_first_0:                 episode reward: -10.7500,                 loss: 0.0089
env0_second_0:                 episode reward: 10.7500,                 loss: 0.0102
env1_first_0:                 episode reward: -11.5500,                 loss: nan
env1_second_0:                 episode reward: 11.5500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 1798.7,                last time consumption/overall running time: 2164.3775s / 584650.3239 s
env0_first_0:                 episode reward: -12.3000,                 loss: 0.0093
env0_second_0:                 episode reward: 12.3000,                 loss: 0.0101
env1_first_0:                 episode reward: -12.6500,                 loss: nan
env1_second_0:                 episode reward: 12.6500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 1761.1,                last time consumption/overall running time: 2130.5753s / 586780.8992 s
env0_first_0:                 episode reward: -12.5000,                 loss: 0.0095
env0_second_0:                 episode reward: 12.5000,                 loss: 0.0105
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 1664.15,                last time consumption/overall running time: 1975.1408s / 588756.0401 s
env0_first_0:                 episode reward: -11.9500,                 loss: 0.0089
env0_second_0:                 episode reward: 11.9500,                 loss: 0.0092
env1_first_0:                 episode reward: -14.0000,                 loss: nan
env1_second_0:                 episode reward: 14.0000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1740.55,                last time consumption/overall running time: 2079.7886s / 590835.8287 s
env0_first_0:                 episode reward: -13.1000,                 loss: 0.0086
env0_second_0:                 episode reward: 13.1000,                 loss: 0.0083
env1_first_0:                 episode reward: -14.2000,                 loss: nan
env1_second_0:                 episode reward: 14.2000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 1670.45,                last time consumption/overall running time: 1989.9129s / 592825.7416 s
env0_first_0:                 episode reward: -13.0000,                 loss: 0.0084
env0_second_0:                 episode reward: 13.0000,                 loss: 0.0089
env1_first_0:                 episode reward: -14.7000,                 loss: nan
env1_second_0:                 episode reward: 14.7000,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 1821.55,                last time consumption/overall running time: 2119.3284s / 594945.0700 s
env0_first_0:                 episode reward: -13.6000,                 loss: 0.0086
env0_second_0:                 episode reward: 13.6000,                 loss: 0.0092
env1_first_0:                 episode reward: -14.0000,                 loss: nan
env1_second_0:                 episode reward: 14.0000,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1690.0,                last time consumption/overall running time: 1935.4469s / 596880.5169 s
env0_first_0:                 episode reward: -11.3500,                 loss: 0.0085
env0_second_0:                 episode reward: 11.3500,                 loss: 0.0094
env1_first_0:                 episode reward: -14.8000,                 loss: nan
env1_second_0:                 episode reward: 14.8000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 1670.6,                last time consumption/overall running time: 1890.0219s / 598770.5388 s
env0_first_0:                 episode reward: -12.4500,                 loss: 0.0080
env0_second_0:                 episode reward: 12.4500,                 loss: 0.0086
env1_first_0:                 episode reward: -13.1000,                 loss: nan
env1_second_0:                 episode reward: 13.1000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 1903.6,                last time consumption/overall running time: 2153.6010s / 600924.1398 s
env0_first_0:                 episode reward: -12.9000,                 loss: 0.0082
env0_second_0:                 episode reward: 12.9000,                 loss: 0.0091
env1_first_0:                 episode reward: -12.8500,                 loss: nan
env1_second_0:                 episode reward: 12.8500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 1728.6,                last time consumption/overall running time: 1965.1226s / 602889.2624 s
env0_first_0:                 episode reward: -14.0500,                 loss: 0.0097
env0_second_0:                 episode reward: 14.0500,                 loss: 0.0104
env1_first_0:                 episode reward: -13.0500,                 loss: nan
env1_second_0:                 episode reward: 13.0500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 1743.2,                last time consumption/overall running time: 1958.5540s / 604847.8164 s
env0_first_0:                 episode reward: -10.4000,                 loss: 0.0094
env0_second_0:                 episode reward: 10.4000,                 loss: 0.0099
env1_first_0:                 episode reward: -12.4500,                 loss: nan
env1_second_0:                 episode reward: 12.4500,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 1625.4,                last time consumption/overall running time: 1816.0247s / 606663.8411 s
env0_first_0:                 episode reward: -12.0500,                 loss: 0.0090
env0_second_0:                 episode reward: 12.0500,                 loss: 0.0094
env1_first_0:                 episode reward: -13.4500,                 loss: nan
env1_second_0:                 episode reward: 13.4500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1709.1,                last time consumption/overall running time: 1885.8605s / 608549.7015 s
env0_first_0:                 episode reward: -13.1500,                 loss: 0.0088
env0_second_0:                 episode reward: 13.1500,                 loss: 0.0089
env1_first_0:                 episode reward: -12.5500,                 loss: nan
env1_second_0:                 episode reward: 12.5500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1852.7,                last time consumption/overall running time: 2005.4876s / 610555.1891 s
env0_first_0:                 episode reward: -11.4500,                 loss: 0.0090
env0_second_0:                 episode reward: 11.4500,                 loss: 0.0092
env1_first_0:                 episode reward: -10.6500,                 loss: nan
env1_second_0:                 episode reward: 10.6500,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1768.45,                last time consumption/overall running time: 1908.0480s / 612463.2371 s
env0_first_0:                 episode reward: -13.1000,                 loss: 0.0095
env0_second_0:                 episode reward: 13.1000,                 loss: 0.0100
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 1758.65,                last time consumption/overall running time: 1901.9016s / 614365.1387 s
env0_first_0:                 episode reward: -12.3000,                 loss: 0.0096
env0_second_0:                 episode reward: 12.3000,                 loss: 0.0096
env1_first_0:                 episode reward: -10.8000,                 loss: nan
env1_second_0:                 episode reward: 10.8000,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 1736.3,                last time consumption/overall running time: 1922.2981s / 616287.4368 s
env0_first_0:                 episode reward: -14.2500,                 loss: 0.0095
env0_second_0:                 episode reward: 14.2500,                 loss: 0.0093
env1_first_0:                 episode reward: -13.7000,                 loss: nan
env1_second_0:                 episode reward: 13.7000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 1820.1,                last time consumption/overall running time: 2033.6100s / 618321.0468 s
env0_first_0:                 episode reward: -13.2000,                 loss: 0.0103
env0_second_0:                 episode reward: 13.2000,                 loss: 0.0104
env1_first_0:                 episode reward: -14.8500,                 loss: nan
env1_second_0:                 episode reward: 14.8500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 1780.1,                last time consumption/overall running time: 1969.0764s / 620290.1231 s
env0_first_0:                 episode reward: -13.5500,                 loss: 0.0106
env0_second_0:                 episode reward: 13.5500,                 loss: 0.0116
env1_first_0:                 episode reward: -13.6000,                 loss: nan
env1_second_0:                 episode reward: 13.6000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 1743.55,                last time consumption/overall running time: 1912.3619s / 622202.4851 s
env0_first_0:                 episode reward: -13.1000,                 loss: 0.0103
env0_second_0:                 episode reward: 13.1000,                 loss: 0.0106
env1_first_0:                 episode reward: -12.1500,                 loss: nan
env1_second_0:                 episode reward: 12.1500,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 1681.85,                last time consumption/overall running time: 1782.8293s / 623985.3144 s
env0_first_0:                 episode reward: -12.0500,                 loss: 0.0094
env0_second_0:                 episode reward: 12.0500,                 loss: 0.0103
env1_first_0:                 episode reward: -11.9500,                 loss: nan
env1_second_0:                 episode reward: 11.9500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 1764.25,                last time consumption/overall running time: 1875.8720s / 625861.1864 s
env0_first_0:                 episode reward: -13.0500,                 loss: 0.0085
env0_second_0:                 episode reward: 13.0500,                 loss: 0.0093
env1_first_0:                 episode reward: -12.4500,                 loss: nan
env1_second_0:                 episode reward: 12.4500,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 1640.95,                last time consumption/overall running time: 1751.5476s / 627612.7340 s
env0_first_0:                 episode reward: -10.8500,                 loss: 0.0080
env0_second_0:                 episode reward: 10.8500,                 loss: 0.0089
env1_first_0:                 episode reward: -13.6000,                 loss: nan
env1_second_0:                 episode reward: 13.6000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 1774.0,                last time consumption/overall running time: 1879.6366s / 629492.3706 s
env0_first_0:                 episode reward: -13.4000,                 loss: 0.0080
env0_second_0:                 episode reward: 13.4000,                 loss: 0.0088
env1_first_0:                 episode reward: -12.3500,                 loss: nan
env1_second_0:                 episode reward: 12.3500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 1716.5,                last time consumption/overall running time: 1819.1562s / 631311.5268 s
env0_first_0:                 episode reward: -14.3500,                 loss: 0.0082
env0_second_0:                 episode reward: 14.3500,                 loss: 0.0089
env1_first_0:                 episode reward: -12.2500,                 loss: nan
env1_second_0:                 episode reward: 12.2500,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 1798.0,                last time consumption/overall running time: 1897.2986s / 633208.8254 s
env0_first_0:                 episode reward: -12.6500,                 loss: 0.0084
env0_second_0:                 episode reward: 12.6500,                 loss: 0.0089
env1_first_0:                 episode reward: -13.7500,                 loss: nan
env1_second_0:                 episode reward: 13.7500,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 1836.3,                last time consumption/overall running time: 1922.9349s / 635131.7603 s
env0_first_0:                 episode reward: -16.7000,                 loss: 0.0090
env0_second_0:                 episode reward: 16.7000,                 loss: 0.0091
env1_first_0:                 episode reward: -16.2000,                 loss: nan
env1_second_0:                 episode reward: 16.2000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 1845.35,                last time consumption/overall running time: 1924.1747s / 637055.9350 s
env0_first_0:                 episode reward: -12.1500,                 loss: 0.0088
env0_second_0:                 episode reward: 12.1500,                 loss: 0.0098
env1_first_0:                 episode reward: -15.6000,                 loss: nan
env1_second_0:                 episode reward: 15.6000,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 1962.1,                last time consumption/overall running time: 2010.1635s / 639066.0986 s
env0_first_0:                 episode reward: -12.6500,                 loss: 0.0093
env0_second_0:                 episode reward: 12.6500,                 loss: 0.0099
env1_first_0:                 episode reward: -10.7000,                 loss: nan
env1_second_0:                 episode reward: 10.7000,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 1885.05,                last time consumption/overall running time: 1923.6217s / 640989.7202 s
env0_first_0:                 episode reward: -14.1000,                 loss: 0.0095
env0_second_0:                 episode reward: 14.1000,                 loss: 0.0100
env1_first_0:                 episode reward: -11.1500,                 loss: nan
env1_second_0:                 episode reward: 11.1500,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 1961.2,                last time consumption/overall running time: 1981.4485s / 642971.1687 s
env0_first_0:                 episode reward: -12.6500,                 loss: 0.0094
env0_second_0:                 episode reward: 12.6500,                 loss: 0.0099
env1_first_0:                 episode reward: -11.2000,                 loss: nan
env1_second_0:                 episode reward: 11.2000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 1811.85,                last time consumption/overall running time: 1833.6220s / 644804.7908 s
env0_first_0:                 episode reward: -13.2500,                 loss: 0.0089
env0_second_0:                 episode reward: 13.2500,                 loss: 0.0097
env1_first_0:                 episode reward: -11.6000,                 loss: nan
env1_second_0:                 episode reward: 11.6000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 1734.4,                last time consumption/overall running time: 1757.0426s / 646561.8334 s
env0_first_0:                 episode reward: -13.5000,                 loss: 0.0081
env0_second_0:                 episode reward: 13.5000,                 loss: 0.0087
env1_first_0:                 episode reward: -14.5500,                 loss: nan
env1_second_0:                 episode reward: 14.5500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 1861.95,                last time consumption/overall running time: 1864.1429s / 648425.9763 s
env0_first_0:                 episode reward: -16.8500,                 loss: 0.0078
env0_second_0:                 episode reward: 16.8500,                 loss: 0.0086
env1_first_0:                 episode reward: -14.5000,                 loss: nan
env1_second_0:                 episode reward: 14.5000,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 1727.3,                last time consumption/overall running time: 1737.9880s / 650163.9643 s
env0_first_0:                 episode reward: -14.8000,                 loss: 0.0077
env0_second_0:                 episode reward: 14.8000,                 loss: 0.0085
env1_first_0:                 episode reward: -14.9000,                 loss: nan
env1_second_0:                 episode reward: 14.9000,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 1733.95,                last time consumption/overall running time: 1726.1947s / 651890.1590 s
env0_first_0:                 episode reward: -14.6000,                 loss: 0.0077
env0_second_0:                 episode reward: 14.6000,                 loss: 0.0084
env1_first_0:                 episode reward: -13.7000,                 loss: nan
env1_second_0:                 episode reward: 13.7000,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1759.9,                last time consumption/overall running time: 1751.3800s / 653641.5390 s
env0_first_0:                 episode reward: -14.1000,                 loss: 0.0085
env0_second_0:                 episode reward: 14.1000,                 loss: 0.0087
env1_first_0:                 episode reward: -13.4000,                 loss: nan
env1_second_0:                 episode reward: 13.4000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 1642.75,                last time consumption/overall running time: 1639.3503s / 655280.8893 s
env0_first_0:                 episode reward: -15.9500,                 loss: 0.0085
env0_second_0:                 episode reward: 15.9500,                 loss: 0.0089
env1_first_0:                 episode reward: -14.5500,                 loss: nan
env1_second_0:                 episode reward: 14.5500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 1691.55,                last time consumption/overall running time: 1687.3462s / 656968.2355 s
env0_first_0:                 episode reward: -13.5000,                 loss: 0.0082
env0_second_0:                 episode reward: 13.5000,                 loss: 0.0085
env1_first_0:                 episode reward: -15.4000,                 loss: nan
env1_second_0:                 episode reward: 15.4000,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 1635.8,                last time consumption/overall running time: 1632.2141s / 658600.4496 s
env0_first_0:                 episode reward: -12.1500,                 loss: 0.0075
env0_second_0:                 episode reward: 12.1500,                 loss: 0.0078
env1_first_0:                 episode reward: -13.3500,                 loss: nan
env1_second_0:                 episode reward: 13.3500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 1616.95,                last time consumption/overall running time: 1612.6593s / 660213.1089 s
env0_first_0:                 episode reward: -14.0000,                 loss: 0.0073
env0_second_0:                 episode reward: 14.0000,                 loss: 0.0077
env1_first_0:                 episode reward: -14.0000,                 loss: nan
env1_second_0:                 episode reward: 14.0000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 1528.45,                last time consumption/overall running time: 1507.1373s / 661720.2463 s
env0_first_0:                 episode reward: -14.4000,                 loss: 0.0072
env0_second_0:                 episode reward: 14.4000,                 loss: 0.0078
env1_first_0:                 episode reward: -13.4500,                 loss: nan
env1_second_0:                 episode reward: 13.4500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 1658.45,                last time consumption/overall running time: 1650.3635s / 663370.6098 s
env0_first_0:                 episode reward: -13.7500,                 loss: 0.0072
env0_second_0:                 episode reward: 13.7500,                 loss: 0.0082
env1_first_0:                 episode reward: -13.7500,                 loss: nan
env1_second_0:                 episode reward: 13.7500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 1598.35,                last time consumption/overall running time: 1573.3311s / 664943.9409 s
env0_first_0:                 episode reward: -11.2000,                 loss: 0.0073
env0_second_0:                 episode reward: 11.2000,                 loss: 0.0084
env1_first_0:                 episode reward: -13.7000,                 loss: nan
env1_second_0:                 episode reward: 13.7000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 1632.4,                last time consumption/overall running time: 1596.9624s / 666540.9033 s
env0_first_0:                 episode reward: -14.6500,                 loss: 0.0076
env0_second_0:                 episode reward: 14.6500,                 loss: 0.0084
env1_first_0:                 episode reward: -15.9500,                 loss: nan
env1_second_0:                 episode reward: 15.9500,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 1667.15,                last time consumption/overall running time: 1630.0618s / 668170.9651 s
env0_first_0:                 episode reward: -12.9500,                 loss: 0.0078
env0_second_0:                 episode reward: 12.9500,                 loss: 0.0083
env1_first_0:                 episode reward: -12.9000,                 loss: nan
env1_second_0:                 episode reward: 12.9000,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 1712.95,                last time consumption/overall running time: 1645.6238s / 669816.5889 s
env0_first_0:                 episode reward: -13.5500,                 loss: 0.0075
env0_second_0:                 episode reward: 13.5500,                 loss: 0.0082
env1_first_0:                 episode reward: -13.6000,                 loss: nan
env1_second_0:                 episode reward: 13.6000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 1685.6,                last time consumption/overall running time: 1633.4232s / 671450.0121 s
env0_first_0:                 episode reward: -13.8000,                 loss: 0.0080
env0_second_0:                 episode reward: 13.8000,                 loss: 0.0084
env1_first_0:                 episode reward: -15.9500,                 loss: nan
env1_second_0:                 episode reward: 15.9500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 1561.7,                last time consumption/overall running time: 1513.8014s / 672963.8135 s
env0_first_0:                 episode reward: -13.8000,                 loss: 0.0080
env0_second_0:                 episode reward: 13.8000,                 loss: 0.0088
env1_first_0:                 episode reward: -13.7000,                 loss: nan
env1_second_0:                 episode reward: 13.7000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 1669.4,                last time consumption/overall running time: 1609.0428s / 674572.8563 s
env0_first_0:                 episode reward: -12.4000,                 loss: 0.0079
env0_second_0:                 episode reward: 12.4000,                 loss: 0.0078
env1_first_0:                 episode reward: -11.1500,                 loss: nan
env1_second_0:                 episode reward: 11.1500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 1772.05,                last time consumption/overall running time: 1699.6642s / 676272.5204 s
env0_first_0:                 episode reward: -15.0000,                 loss: 0.0075
env0_second_0:                 episode reward: 15.0000,                 loss: 0.0076
env1_first_0:                 episode reward: -15.3500,                 loss: nan
env1_second_0:                 episode reward: 15.3500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 1599.35,                last time consumption/overall running time: 1533.0780s / 677805.5985 s
env0_first_0:                 episode reward: -12.0500,                 loss: 0.0075
env0_second_0:                 episode reward: 12.0500,                 loss: 0.0073
env1_first_0:                 episode reward: -13.6000,                 loss: nan
env1_second_0:                 episode reward: 13.6000,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 1691.0,                last time consumption/overall running time: 1605.0359s / 679410.6344 s
env0_first_0:                 episode reward: -13.9000,                 loss: 0.0072
env0_second_0:                 episode reward: 13.9000,                 loss: 0.0077
env1_first_0:                 episode reward: -14.0500,                 loss: nan
env1_second_0:                 episode reward: 14.0500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 1781.5,                last time consumption/overall running time: 1684.8435s / 681095.4779 s
env0_first_0:                 episode reward: -14.7500,                 loss: 0.0076
env0_second_0:                 episode reward: 14.7500,                 loss: 0.0082
env1_first_0:                 episode reward: -13.7500,                 loss: nan
env1_second_0:                 episode reward: 13.7500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 1755.45,                last time consumption/overall running time: 1674.9184s / 682770.3963 s
env0_first_0:                 episode reward: -14.5500,                 loss: 0.0076
env0_second_0:                 episode reward: 14.5500,                 loss: 0.0085
env1_first_0:                 episode reward: -14.8500,                 loss: nan
env1_second_0:                 episode reward: 14.8500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 1702.05,                last time consumption/overall running time: 1623.9357s / 684394.3320 s
env0_first_0:                 episode reward: -17.4000,                 loss: 0.0071
env0_second_0:                 episode reward: 17.4000,                 loss: 0.0078
env1_first_0:                 episode reward: -15.0500,                 loss: nan
env1_second_0:                 episode reward: 15.0500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 1664.25,                last time consumption/overall running time: 1565.6501s / 685959.9821 s
env0_first_0:                 episode reward: -13.9000,                 loss: 0.0073
env0_second_0:                 episode reward: 13.9000,                 loss: 0.0080
env1_first_0:                 episode reward: -13.3500,                 loss: nan
env1_second_0:                 episode reward: 13.3500,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 1706.95,                last time consumption/overall running time: 1609.2107s / 687569.1928 s
env0_first_0:                 episode reward: -15.4000,                 loss: 0.0077
env0_second_0:                 episode reward: 15.4000,                 loss: 0.0081
env1_first_0:                 episode reward: -15.1000,                 loss: nan
env1_second_0:                 episode reward: 15.1000,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 1646.4,                last time consumption/overall running time: 1566.6857s / 689135.8785 s
env0_first_0:                 episode reward: -13.3000,                 loss: 0.0074
env0_second_0:                 episode reward: 13.3000,                 loss: 0.0076
env1_first_0:                 episode reward: -11.9000,                 loss: nan
env1_second_0:                 episode reward: 11.9000,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 1659.85,                last time consumption/overall running time: 1575.6052s / 690711.4837 s
env0_first_0:                 episode reward: -14.0000,                 loss: 0.0072
env0_second_0:                 episode reward: 14.0000,                 loss: 0.0077
env1_first_0:                 episode reward: -15.1000,                 loss: nan
env1_second_0:                 episode reward: 15.1000,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 1849.5,                last time consumption/overall running time: 1742.8884s / 692454.3721 s
env0_first_0:                 episode reward: -16.5000,                 loss: 0.0079
env0_second_0:                 episode reward: 16.5000,                 loss: 0.0087
env1_first_0:                 episode reward: -15.8500,                 loss: nan
env1_second_0:                 episode reward: 15.8500,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 1863.05,                last time consumption/overall running time: 1751.3056s / 694205.6777 s
env0_first_0:                 episode reward: -13.7000,                 loss: 0.0083
env0_second_0:                 episode reward: 13.7000,                 loss: 0.0088
env1_first_0:                 episode reward: -14.4500,                 loss: nan
env1_second_0:                 episode reward: 14.4500,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 1793.6,                last time consumption/overall running time: 1688.3467s / 695894.0244 s
env0_first_0:                 episode reward: -13.5000,                 loss: 0.0081
env0_second_0:                 episode reward: 13.5000,                 loss: 0.0090
env1_first_0:                 episode reward: -12.8500,                 loss: nan
env1_second_0:                 episode reward: 12.8500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 1877.25,                last time consumption/overall running time: 1742.4043s / 697636.4287 s
env0_first_0:                 episode reward: -14.7000,                 loss: 0.0081
env0_second_0:                 episode reward: 14.7000,                 loss: 0.0087
env1_first_0:                 episode reward: -15.4500,                 loss: nan
env1_second_0:                 episode reward: 15.4500,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 1822.95,                last time consumption/overall running time: 1688.3023s / 699324.7310 s
env0_first_0:                 episode reward: -14.9000,                 loss: 0.0094
env0_second_0:                 episode reward: 14.9000,                 loss: 0.0101
env1_first_0:                 episode reward: -13.6000,                 loss: nan
env1_second_0:                 episode reward: 13.6000,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 1806.65,                last time consumption/overall running time: 1659.0662s / 700983.7972 s
env0_first_0:                 episode reward: -12.4500,                 loss: 0.0095
env0_second_0:                 episode reward: 12.4500,                 loss: 0.0098
env1_first_0:                 episode reward: -11.7500,                 loss: nan
env1_second_0:                 episode reward: 11.7500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 1766.0,                last time consumption/overall running time: 1608.0866s / 702591.8838 s
env0_first_0:                 episode reward: -14.8500,                 loss: 0.0082
env0_second_0:                 episode reward: 14.8500,                 loss: 0.0083
env1_first_0:                 episode reward: -13.4000,                 loss: nan
env1_second_0:                 episode reward: 13.4000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 1558.55,                last time consumption/overall running time: 1409.2520s / 704001.1358 s
env0_first_0:                 episode reward: -17.8000,                 loss: 0.0082
env0_second_0:                 episode reward: 17.8000,                 loss: 0.0078
env1_first_0:                 episode reward: -16.1500,                 loss: nan
env1_second_0:                 episode reward: 16.1500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 1609.95,                last time consumption/overall running time: 1444.7717s / 705445.9075 s
env0_first_0:                 episode reward: -14.9500,                 loss: 0.0085
env0_second_0:                 episode reward: 14.9500,                 loss: 0.0078
env1_first_0:                 episode reward: -13.4000,                 loss: nan
env1_second_0:                 episode reward: 13.4000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 1578.05,                last time consumption/overall running time: 1425.3141s / 706871.2216 s
env0_first_0:                 episode reward: -12.9000,                 loss: 0.0080
env0_second_0:                 episode reward: 12.9000,                 loss: 0.0079
env1_first_0:                 episode reward: -13.8500,                 loss: nan
env1_second_0:                 episode reward: 13.8500,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1697.55,                last time consumption/overall running time: 1534.8197s / 708406.0413 s
env0_first_0:                 episode reward: -14.5000,                 loss: 0.0078
env0_second_0:                 episode reward: 14.5000,                 loss: 0.0075
env1_first_0:                 episode reward: -15.4500,                 loss: nan
env1_second_0:                 episode reward: 15.4500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 1582.5,                last time consumption/overall running time: 1425.4820s / 709831.5233 s
env0_first_0:                 episode reward: -13.9500,                 loss: 0.0079
env0_second_0:                 episode reward: 13.9500,                 loss: 0.0075
env1_first_0:                 episode reward: -14.7000,                 loss: nan
env1_second_0:                 episode reward: 14.7000,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 1513.5,                last time consumption/overall running time: 1361.0580s / 711192.5814 s