pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
double_dunk_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'double_dunk_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 10000, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nfsp', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'eta': 0.1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_1545/pettingzoo_double_dunk_v2_nfsp. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_1545/pettingzoo_double_dunk_v2_nfsp.
Episode: 1/10000 (0.0100%),                 avg. length: 3353.0,                last time consumption/overall running time: 21.9030s / 21.9030 s
env0_first_0:                 episode reward: -25.0000,                 loss: nan
env0_second_0:                 episode reward: 25.0000,                 loss: nan
env1_first_0:                 episode reward: -13.0000,                 loss: nan
env1_second_0:                 episode reward: 13.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 5375.9,                last time consumption/overall running time: 4474.3693s / 4496.2723 s
env0_first_0:                 episode reward: -17.5000,                 loss: 0.0077
env0_second_0:                 episode reward: 17.5000,                 loss: 0.0091
env1_first_0:                 episode reward: -15.8500,                 loss: nan
env1_second_0:                 episode reward: 15.8500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 3755.0,                last time consumption/overall running time: 3385.5055s / 7881.7778 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0054
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0059
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 2072.35,                last time consumption/overall running time: 1865.7517s / 9747.5295 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0041
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0044
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 2017.35,                last time consumption/overall running time: 1815.7380s / 11563.2675 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0033
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0041
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1735.45,                last time consumption/overall running time: 1565.1295s / 13128.3970 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0030
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0034
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1833.3,                last time consumption/overall running time: 1653.7088s / 14782.1058 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0030
env0_second_0:                 episode reward: 0.6000,                 loss: 0.0032
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1828.9,                last time consumption/overall running time: 1648.7982s / 16430.9040 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0027
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0030
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1772.35,                last time consumption/overall running time: 1598.1120s / 18029.0160 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0025
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0028
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1987.35,                last time consumption/overall running time: 1791.5528s / 19820.5688 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0027
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0031
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1979.3,                last time consumption/overall running time: 1784.5667s / 21605.1355 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0032
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0037
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1818.65,                last time consumption/overall running time: 1639.0369s / 23244.1724 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0035
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0039
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1713.6,                last time consumption/overall running time: 1543.9123s / 24788.0847 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0032
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0035
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 2024.55,                last time consumption/overall running time: 1825.0693s / 26613.1540 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0031
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0031
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 2180.25,                last time consumption/overall running time: 1964.0645s / 28577.2185 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0029
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0032
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 2044.3,                last time consumption/overall running time: 1840.9512s / 30418.1697 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0040
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0046
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1995.25,                last time consumption/overall running time: 1796.8704s / 32215.0401 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0046
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0055
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 2103.75,                last time consumption/overall running time: 1894.2391s / 34109.2793 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0044
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0052
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 2082.4,                last time consumption/overall running time: 1874.3102s / 35983.5894 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0038
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0046
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1887.0,                last time consumption/overall running time: 1701.6465s / 37685.2360 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0038
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0051
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1832.6,                last time consumption/overall running time: 1651.2992s / 39336.5352 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0039
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0046
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1791.2,                last time consumption/overall running time: 1613.8365s / 40950.3717 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0038
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0048
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1850.55,                last time consumption/overall running time: 1665.8428s / 42616.2145 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0037
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0044
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 2006.3,                last time consumption/overall running time: 1807.7235s / 44423.9379 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0041
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0049
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1840.75,                last time consumption/overall running time: 1658.0123s / 46081.9503 s
env0_first_0:                 episode reward: 3.0500,                 loss: 0.0041
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0051
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 2096.75,                last time consumption/overall running time: 1884.8424s / 47966.7927 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0040
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0045
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1896.6,                last time consumption/overall running time: 1712.2181s / 49679.0108 s
env0_first_0:                 episode reward: 2.4000,                 loss: 0.0039
env0_second_0:                 episode reward: -2.4000,                 loss: 0.0048
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1887.0,                last time consumption/overall running time: 1698.6299s / 51377.6407 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0038
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0042
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1819.8,                last time consumption/overall running time: 1638.5718s / 53016.2125 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0034
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0039
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1888.85,                last time consumption/overall running time: 1700.9484s / 54717.1609 s
env0_first_0:                 episode reward: 3.5500,                 loss: 0.0038
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0044
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 2029.9,                last time consumption/overall running time: 1825.9037s / 56543.0646 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0041
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0042
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 2033.2,                last time consumption/overall running time: 1827.5246s / 58370.5891 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0040
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0040
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 2074.6,                last time consumption/overall running time: 1859.9365s / 60230.5257 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0036
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0041
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 2122.25,                last time consumption/overall running time: 1900.6976s / 62131.2233 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0040
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0043
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 2198.35,                last time consumption/overall running time: 1970.7579s / 64101.9812 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0043
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0045
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 2141.95,                last time consumption/overall running time: 1920.8484s / 66022.8295 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0043
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0047
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 2604.05,                last time consumption/overall running time: 2334.0061s / 68356.8356 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0041
env0_second_0:                 episode reward: -4.1500,                 loss: 0.0049
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 2353.9,                last time consumption/overall running time: 2108.1608s / 70464.9965 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0043
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0050
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2173.7,                last time consumption/overall running time: 1946.9404s / 72411.9368 s
env0_first_0:                 episode reward: 3.2000,                 loss: 0.0044
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0050
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 2215.85,                last time consumption/overall running time: 1983.1125s / 74395.0493 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.0044
env0_second_0:                 episode reward: -5.9000,                 loss: 0.0052
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 2171.55,                last time consumption/overall running time: 1944.8955s / 76339.9448 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0036
env0_second_0:                 episode reward: -4.1000,                 loss: 0.0046
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 2130.3,                last time consumption/overall running time: 1909.2965s / 78249.2414 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0037
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0046
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 2349.6,                last time consumption/overall running time: 2105.0830s / 80354.3244 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.0044
env0_second_0:                 episode reward: -5.1500,                 loss: 0.0052
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 2422.75,                last time consumption/overall running time: 2168.8905s / 82523.2149 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0049
env0_second_0:                 episode reward: -5.2000,                 loss: 0.0060
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1925.6,                last time consumption/overall running time: 1725.7926s / 84249.0074 s
env0_first_0:                 episode reward: 7.2500,                 loss: 0.0046
env0_second_0:                 episode reward: -7.2500,                 loss: 0.0057
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2184.4,                last time consumption/overall running time: 1958.8472s / 86207.8546 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0043
env0_second_0:                 episode reward: -4.1000,                 loss: 0.0051
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 2036.9,                last time consumption/overall running time: 1823.4717s / 88031.3263 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0040
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0046
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1901.85,                last time consumption/overall running time: 1703.8777s / 89735.2040 s
env0_first_0:                 episode reward: 4.3500,                 loss: 0.0033
env0_second_0:                 episode reward: -4.3500,                 loss: 0.0043
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2360.7,                last time consumption/overall running time: 2112.2553s / 91847.4593 s
env0_first_0:                 episode reward: 3.5500,                 loss: 0.0038
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0051
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 2149.75,                last time consumption/overall running time: 1923.5479s / 93771.0072 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0046
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0058
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 2230.8,                last time consumption/overall running time: 1991.6753s / 95762.6825 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0047
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0057
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 2269.1,                last time consumption/overall running time: 2029.5512s / 97792.2337 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0041
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0048
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 2260.8,                last time consumption/overall running time: 2021.9848s / 99814.2185 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0043
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0054
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 2209.8,                last time consumption/overall running time: 1980.5353s / 101794.7538 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0044
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0052
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 2221.6,                last time consumption/overall running time: 1987.5967s / 103782.3505 s
env0_first_0:                 episode reward: 2.8500,                 loss: 0.0044
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0052
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 2091.55,                last time consumption/overall running time: 1872.6840s / 105655.0345 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0041
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0052
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 2363.25,                last time consumption/overall running time: 2059.4300s / 107714.4645 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0044
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0051
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 2117.75,                last time consumption/overall running time: 1730.4444s / 109444.9089 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0043
env0_second_0:                 episode reward: -4.4500,                 loss: 0.0054
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 2081.95,                last time consumption/overall running time: 1624.7414s / 111069.6504 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0046
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0053
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1904.65,                last time consumption/overall running time: 1461.9226s / 112531.5730 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0046
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0053
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1931.45,                last time consumption/overall running time: 1482.6114s / 114014.1844 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0044
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0054
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 2395.9,                last time consumption/overall running time: 1840.8292s / 115855.0137 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0041
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0047
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 2310.3,                last time consumption/overall running time: 1774.8161s / 117629.8298 s
env0_first_0:                 episode reward: 4.6000,                 loss: 0.0041
env0_second_0:                 episode reward: -4.6000,                 loss: 0.0047
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 2249.0,                last time consumption/overall running time: 1730.5517s / 119360.3815 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0041
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0048
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 2015.7,                last time consumption/overall running time: 1550.4499s / 120910.8313 s
env0_first_0:                 episode reward: 2.6000,                 loss: 0.0043
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0047
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 2167.7,                last time consumption/overall running time: 1666.8073s / 122577.6386 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0040
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0045
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1948.35,                last time consumption/overall running time: 1495.6960s / 124073.3346 s
env0_first_0:                 episode reward: 3.1500,                 loss: 0.0043
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0048
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1954.35,                last time consumption/overall running time: 1500.3201s / 125573.6548 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0041
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0047
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 2098.65,                last time consumption/overall running time: 1613.3283s / 127186.9830 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0045
env0_second_0:                 episode reward: 2.2500,                 loss: 0.0048
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1785.45,                last time consumption/overall running time: 1373.5103s / 128560.4933 s
env0_first_0:                 episode reward: 3.1500,                 loss: 0.0050
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0054
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 2024.25,                last time consumption/overall running time: 1556.1010s / 130116.5943 s
env0_first_0:                 episode reward: 3.0500,                 loss: 0.0039
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0042
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 2105.05,                last time consumption/overall running time: 1619.0475s / 131735.6418 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0042
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0045
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 2332.15,                last time consumption/overall running time: 1791.6946s / 133527.3364 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0045
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0047
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 2096.85,                last time consumption/overall running time: 1612.9284s / 135140.2648 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0045
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0045
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 2192.8,                last time consumption/overall running time: 1684.7847s / 136825.0495 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0044
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0046
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 2135.3,                last time consumption/overall running time: 1640.8512s / 138465.9007 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0049
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0053
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1897.7,                last time consumption/overall running time: 1457.8665s / 139923.7672 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0052
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0055
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 2162.85,                last time consumption/overall running time: 1660.3567s / 141584.1239 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0049
env0_second_0:                 episode reward: 2.5000,                 loss: 0.0056
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1875.2,                last time consumption/overall running time: 1440.1867s / 143024.3107 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0043
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0049
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 2132.0,                last time consumption/overall running time: 1635.7389s / 144660.0496 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0043
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0048
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 2100.0,                last time consumption/overall running time: 1614.6778s / 146274.7274 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0041
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0046
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 2213.65,                last time consumption/overall running time: 1702.0954s / 147976.8228 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0044
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0048
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1915.45,                last time consumption/overall running time: 1468.2130s / 149445.0358 s
env0_first_0:                 episode reward: 2.0000,                 loss: 0.0043
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0047
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 2091.0,                last time consumption/overall running time: 1605.3312s / 151050.3670 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0043
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0045
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 2159.35,                last time consumption/overall running time: 1657.1582s / 152707.5251 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0046
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0047
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1969.15,                last time consumption/overall running time: 1509.5497s / 154217.0748 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0048
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0046
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1901.15,                last time consumption/overall running time: 1455.5565s / 155672.6313 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0043
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0043
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1795.75,                last time consumption/overall running time: 1378.1182s / 157050.7495 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0039
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0039
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 2008.7,                last time consumption/overall running time: 1535.7722s / 158586.5216 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0039
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0042
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 2180.0,                last time consumption/overall running time: 1665.2739s / 160251.7956 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0043
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0045
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1913.4,                last time consumption/overall running time: 1462.9252s / 161714.7207 s
env0_first_0:                 episode reward: 2.6000,                 loss: 0.0047
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0049
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1962.1,                last time consumption/overall running time: 1495.3126s / 163210.0334 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0043
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0047
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1870.8,                last time consumption/overall running time: 1422.6455s / 164632.6789 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0043
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0048
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 2128.95,                last time consumption/overall running time: 1619.8449s / 166252.5238 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0039
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0045
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 2126.35,                last time consumption/overall running time: 1618.1730s / 167870.6967 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0042
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0047
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 2102.7,                last time consumption/overall running time: 1602.5517s / 169473.2485 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0043
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0046
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 2061.0,                last time consumption/overall running time: 1568.0870s / 171041.3355 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0044
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0048
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 2144.55,                last time consumption/overall running time: 1632.8611s / 172674.1966 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0049
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0052
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 1948.65,                last time consumption/overall running time: 1484.0622s / 174158.2588 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0046
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0051
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 1969.4,                last time consumption/overall running time: 1500.9930s / 175659.2519 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0044
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0044
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1863.8,                last time consumption/overall running time: 1417.5193s / 177076.7712 s
env0_first_0:                 episode reward: 2.1500,                 loss: 0.0045
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0047
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1811.75,                last time consumption/overall running time: 1380.0597s / 178456.8309 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0044
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0045
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1841.2,                last time consumption/overall running time: 1400.6424s / 179857.4733 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0040
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0042
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1920.55,                last time consumption/overall running time: 1462.5917s / 181320.0650 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0039
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0040
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 1957.2,                last time consumption/overall running time: 1490.3087s / 182810.3737 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0038
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0043
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 2017.1,                last time consumption/overall running time: 1534.9850s / 184345.3587 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0040
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0043
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 2033.7,                last time consumption/overall running time: 1545.9046s / 185891.2632 s
env0_first_0:                 episode reward: -5.4500,                 loss: 0.0041
env0_second_0:                 episode reward: 5.4500,                 loss: 0.0044
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 2048.8,                last time consumption/overall running time: 1554.9478s / 187446.2110 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0048
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0049
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1878.1,                last time consumption/overall running time: 1427.9589s / 188874.1698 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0046
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0048
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1946.0,                last time consumption/overall running time: 1481.5647s / 190355.7345 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0044
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0048
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 1751.15,                last time consumption/overall running time: 1330.4910s / 191686.2255 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0044
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0048
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1801.6,                last time consumption/overall running time: 1370.0397s / 193056.2652 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0037
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0044
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 1700.25,                last time consumption/overall running time: 1291.5438s / 194347.8091 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.0035
env0_second_0:                 episode reward: 2.8500,                 loss: 0.0043
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 1974.55,                last time consumption/overall running time: 1497.4932s / 195845.3022 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0038
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0044
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1889.55,                last time consumption/overall running time: 1433.7002s / 197279.0025 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0042
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0049
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 1865.15,                last time consumption/overall running time: 1418.5576s / 198697.5601 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0040
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0044
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 1941.15,                last time consumption/overall running time: 1475.4212s / 200172.9813 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0038
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0043
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 1729.6,                last time consumption/overall running time: 1315.5349s / 201488.5161 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0035
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0040
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1901.5,                last time consumption/overall running time: 1442.0739s / 202930.5900 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0033
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0040
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 1890.25,                last time consumption/overall running time: 1431.5547s / 204362.1447 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0037
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0041
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 2000.95,                last time consumption/overall running time: 1519.4509s / 205881.5956 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0039
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0044
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 2012.95,                last time consumption/overall running time: 1527.6710s / 207409.2666 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0040
env0_second_0:                 episode reward: 1.9500,                 loss: 0.0046
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 2098.45,                last time consumption/overall running time: 1593.9122s / 209003.1788 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0041
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0046
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 2058.9,                last time consumption/overall running time: 1564.5919s / 210567.7708 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0042
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0046
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 1949.2,                last time consumption/overall running time: 1482.8005s / 212050.5713 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0036
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0040
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 2121.8,                last time consumption/overall running time: 1608.4953s / 213659.0666 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0038
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0041
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 2062.25,                last time consumption/overall running time: 1565.5102s / 215224.5768 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0041
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0044
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 1992.75,                last time consumption/overall running time: 1510.0658s / 216734.6425 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0041
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0044
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 2369.25,                last time consumption/overall running time: 1793.4499s / 218528.0924 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0041
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0044
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 2226.5,                last time consumption/overall running time: 1687.8774s / 220215.9698 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0043
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0045
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 2122.3,                last time consumption/overall running time: 1606.0081s / 221821.9780 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0042
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0045
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 2324.85,                last time consumption/overall running time: 1758.4810s / 223580.4590 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0042
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0045
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 2086.85,                last time consumption/overall running time: 1581.7727s / 225162.2316 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0043
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0048
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 1871.55,                last time consumption/overall running time: 1419.1195s / 226581.3511 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0037
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0039
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 1877.15,                last time consumption/overall running time: 1418.8343s / 228000.1854 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0038
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0042
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 1731.0,                last time consumption/overall running time: 1311.4143s / 229311.5998 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0040
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0043
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1980.85,                last time consumption/overall running time: 1499.5417s / 230811.1414 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0035
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0039
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 2049.0,                last time consumption/overall running time: 1554.6116s / 232365.7530 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0037
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0041
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 1864.05,                last time consumption/overall running time: 1413.8767s / 233779.6297 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0037
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0038
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1880.45,                last time consumption/overall running time: 1425.0695s / 235204.6992 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0035
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0037
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 2048.0,                last time consumption/overall running time: 1552.3857s / 236757.0849 s
env0_first_0:                 episode reward: 2.1000,                 loss: 0.0039
env0_second_0:                 episode reward: -2.1000,                 loss: 0.0041
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 1995.4,                last time consumption/overall running time: 1511.7036s / 238268.7885 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0041
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0044
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 2055.75,                last time consumption/overall running time: 1557.7223s / 239826.5108 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0041
env0_second_0:                 episode reward: 2.2000,                 loss: 0.0043
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 1797.4,                last time consumption/overall running time: 1361.1035s / 241187.6143 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0036
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0043
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 1939.65,                last time consumption/overall running time: 1469.9322s / 242657.5465 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0038
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0043
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 1953.1,                last time consumption/overall running time: 1478.2728s / 244135.8192 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0043
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0047
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 1832.05,                last time consumption/overall running time: 1384.8953s / 245520.7146 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0039
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0044
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 2018.75,                last time consumption/overall running time: 1526.2230s / 247046.9376 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0039
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0043
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 1966.6,                last time consumption/overall running time: 1488.5392s / 248535.4768 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0043
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0046
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 1862.45,                last time consumption/overall running time: 1407.1394s / 249942.6162 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0045
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0045
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 1824.9,                last time consumption/overall running time: 1380.3447s / 251322.9609 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0042
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0040
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 2126.35,                last time consumption/overall running time: 1608.8597s / 252931.8206 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0043
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0046
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 2106.0,                last time consumption/overall running time: 1594.4574s / 254526.2780 s
env0_first_0:                 episode reward: 2.4000,                 loss: 0.0044
env0_second_0:                 episode reward: -2.4000,                 loss: 0.0049
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 1961.15,                last time consumption/overall running time: 1483.3135s / 256009.5915 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0043
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0046
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 1733.9,                last time consumption/overall running time: 1310.2829s / 257319.8744 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0045
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0049
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 1841.85,                last time consumption/overall running time: 1388.2413s / 258708.1157 s
env0_first_0:                 episode reward: 1.5000,                 loss: 0.0043
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0048
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 2114.1,                last time consumption/overall running time: 1594.3152s / 260302.4309 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0040
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0045
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 2382.65,                last time consumption/overall running time: 1795.8759s / 262098.3068 s
env0_first_0:                 episode reward: 2.1500,                 loss: 0.0044
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0047
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 2051.7,                last time consumption/overall running time: 1545.1213s / 263643.4281 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0045
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0046
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 2153.95,                last time consumption/overall running time: 1620.6886s / 265264.1167 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0048
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0047
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 2199.7,                last time consumption/overall running time: 1657.1150s / 266921.2317 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0049
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0048
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 2134.35,                last time consumption/overall running time: 1606.9104s / 268528.1420 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0052
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0055
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 1922.95,                last time consumption/overall running time: 1446.3322s / 269974.4742 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0048
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0047
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 1976.0,                last time consumption/overall running time: 1486.8212s / 271461.2955 s
env0_first_0:                 episode reward: 2.1500,                 loss: 0.0047
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0049
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 1901.95,                last time consumption/overall running time: 1429.8936s / 272891.1890 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0041
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0042
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 1902.45,                last time consumption/overall running time: 1432.9083s / 274324.0974 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0042
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0044
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 2036.05,                last time consumption/overall running time: 1529.8201s / 275853.9175 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0041
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0046
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 1867.25,                last time consumption/overall running time: 1401.4073s / 277255.3248 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0040
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0046
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1797.15,                last time consumption/overall running time: 1348.1099s / 278603.4347 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0045
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0048
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1991.4,                last time consumption/overall running time: 1492.9926s / 280096.4274 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0043
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0045
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 2036.05,                last time consumption/overall running time: 1523.6022s / 281620.0295 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0045
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0048
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 2163.25,                last time consumption/overall running time: 1616.1168s / 283236.1464 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0047
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0053
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 2132.75,                last time consumption/overall running time: 1594.3229s / 284830.4693 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0042
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0047
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 2120.0,                last time consumption/overall running time: 1582.4270s / 286412.8962 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0044
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0052
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 2062.15,                last time consumption/overall running time: 1539.2657s / 287952.1619 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0052
env0_second_0:                 episode reward: 3.8500,                 loss: 0.0057
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 2191.2,                last time consumption/overall running time: 1635.3119s / 289587.4738 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0050
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0053
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1931.5,                last time consumption/overall running time: 1442.7629s / 291030.2367 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0046
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0048
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1784.6,                last time consumption/overall running time: 1332.8706s / 292363.1073 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0038
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0040
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1735.65,                last time consumption/overall running time: 1295.1941s / 293658.3014 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0037
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0038
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1610.4,                last time consumption/overall running time: 1202.9718s / 294861.2731 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0036
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0036
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1691.0,                last time consumption/overall running time: 1260.8021s / 296122.0752 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0030
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0034
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1791.2,                last time consumption/overall running time: 1337.2402s / 297459.3154 s
env0_first_0:                 episode reward: 7.6000,                 loss: 0.0038
env0_second_0:                 episode reward: -7.6000,                 loss: 0.0044
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1684.0,                last time consumption/overall running time: 1254.7495s / 298714.0649 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0039
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0043
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1804.05,                last time consumption/overall running time: 1344.4977s / 300058.5626 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0031
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0033
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1691.65,                last time consumption/overall running time: 1259.9084s / 301318.4710 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0032
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0034
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 2210.25,                last time consumption/overall running time: 1645.1686s / 302963.6396 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0034
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0039
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 2146.25,                last time consumption/overall running time: 1597.0339s / 304560.6734 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0050
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0051
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1964.1,                last time consumption/overall running time: 1460.6046s / 306021.2780 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0050
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0053
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 2006.9,                last time consumption/overall running time: 1494.1905s / 307515.4685 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0044
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0051
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 2087.55,                last time consumption/overall running time: 1552.8535s / 309068.3220 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0037
env0_second_0:                 episode reward: 2.1000,                 loss: 0.0042
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 2230.95,                last time consumption/overall running time: 1657.9666s / 310726.2887 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0040
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0041
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 2318.35,                last time consumption/overall running time: 1724.3818s / 312450.6705 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0039
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0043
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 2155.9,                last time consumption/overall running time: 1604.2827s / 314054.9532 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0038
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0042
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 2050.95,                last time consumption/overall running time: 1523.8285s / 315578.7816 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0039
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0040
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1746.7,                last time consumption/overall running time: 1300.6044s / 316879.3861 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0039
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0043
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1759.2,                last time consumption/overall running time: 1308.6752s / 318188.0613 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0037
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0042
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1836.05,                last time consumption/overall running time: 1365.3834s / 319553.4446 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0033
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0038
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 1786.55,                last time consumption/overall running time: 1327.0662s / 320880.5108 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0035
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0041
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1993.1,                last time consumption/overall running time: 1480.0891s / 322360.5999 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0034
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0036
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 1893.1,                last time consumption/overall running time: 1405.6826s / 323766.2826 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0037
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0037
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 1883.05,                last time consumption/overall running time: 1400.3703s / 325166.6529 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0037
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0041
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1790.9,                last time consumption/overall running time: 1331.4875s / 326498.1404 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0042
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0047
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1850.05,                last time consumption/overall running time: 1373.1385s / 327871.2790 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.0043
env0_second_0:                 episode reward: -1.7500,                 loss: 0.0048
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1796.95,                last time consumption/overall running time: 1334.8364s / 329206.1154 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0042
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0049
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 2022.15,                last time consumption/overall running time: 1501.7075s / 330707.8229 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0040
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0044
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1981.5,                last time consumption/overall running time: 1470.8135s / 332178.6364 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0036
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0041
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 2186.25,                last time consumption/overall running time: 1622.7812s / 333801.4176 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0029
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0034
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1933.8,                last time consumption/overall running time: 1437.2136s / 335238.6312 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0032
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0036
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 2221.85,                last time consumption/overall running time: 1649.3214s / 336887.9526 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0037
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0040
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1977.7,                last time consumption/overall running time: 1465.9991s / 338353.9516 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0040
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0043
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 2045.55,                last time consumption/overall running time: 1515.9619s / 339869.9136 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0035
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0040
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 2064.15,                last time consumption/overall running time: 1530.8832s / 341400.7968 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0037
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0040
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 1810.3,                last time consumption/overall running time: 1342.9521s / 342743.7489 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0035
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0036
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 1887.8,                last time consumption/overall running time: 1398.8832s / 344142.6321 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0034
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0035
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 2139.65,                last time consumption/overall running time: 1587.5401s / 345730.1722 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0033
env0_second_0:                 episode reward: 2.5500,                 loss: 0.0038
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1937.2,                last time consumption/overall running time: 1435.9113s / 347166.0835 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0036
env0_second_0:                 episode reward: 2.5000,                 loss: 0.0041
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 1830.05,                last time consumption/overall running time: 1465.9067s / 348631.9903 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0034
env0_second_0:                 episode reward: 1.6500,                 loss: 0.0038
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 1773.15,                last time consumption/overall running time: 1603.7109s / 350235.7011 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0030
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0033
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1886.3,                last time consumption/overall running time: 1705.9035s / 351941.6046 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0029
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0033
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 2119.85,                last time consumption/overall running time: 1915.6518s / 353857.2564 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0034
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0037
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 1962.35,                last time consumption/overall running time: 1773.2910s / 355630.5474 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0042
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0048
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 2022.2,                last time consumption/overall running time: 1830.6580s / 357461.2054 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0039
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0046
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 2067.05,                last time consumption/overall running time: 1869.2282s / 359330.4336 s
env0_first_0:                 episode reward: -4.9000,                 loss: 0.0035
env0_second_0:                 episode reward: 4.9000,                 loss: 0.0039
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 1957.45,                last time consumption/overall running time: 1768.7299s / 361099.1635 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0039
env0_second_0:                 episode reward: -3.8500,                 loss: 0.0043
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1906.25,                last time consumption/overall running time: 1720.6080s / 362819.7715 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0041
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0042
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1685.0,                last time consumption/overall running time: 1521.4551s / 364341.2266 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0035
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0036
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1775.5,                last time consumption/overall running time: 1604.1178s / 365945.3443 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0036
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0039
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 1772.0,                last time consumption/overall running time: 1597.8765s / 367543.2209 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0040
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0041
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 1864.4,                last time consumption/overall running time: 1680.9728s / 369224.1937 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0034
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0036
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 2028.45,                last time consumption/overall running time: 1829.0294s / 371053.2231 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0035
env0_second_0:                 episode reward: 1.6500,                 loss: 0.0040
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 2100.45,                last time consumption/overall running time: 1892.0134s / 372945.2365 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0044
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0051
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 1678.95,                last time consumption/overall running time: 1511.7914s / 374457.0279 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0045
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0047
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 1703.7,                last time consumption/overall running time: 1534.4509s / 375991.4788 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0033
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0034
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 1767.4,                last time consumption/overall running time: 1590.1088s / 377581.5876 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0031
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0032
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 1818.85,                last time consumption/overall running time: 1637.0677s / 379218.6553 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0033
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0036
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 1829.1,                last time consumption/overall running time: 1646.5957s / 380865.2510 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.0031
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0034
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 2117.0,                last time consumption/overall running time: 1901.3660s / 382766.6170 s
env0_first_0:                 episode reward: -8.6500,                 loss: 0.0039
env0_second_0:                 episode reward: 8.6500,                 loss: 0.0040
env1_first_0:                 episode reward: -7.9000,                 loss: nan
env1_second_0:                 episode reward: 7.9000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 2059.1,                last time consumption/overall running time: 1852.5239s / 384619.1409 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0046
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0049
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 1854.45,                last time consumption/overall running time: 1666.5715s / 386285.7124 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0042
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0040
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 1810.7,                last time consumption/overall running time: 1628.6633s / 387914.3757 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0035
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0037
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 1795.0,                last time consumption/overall running time: 1614.1238s / 389528.4995 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0031
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0032
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 1949.65,                last time consumption/overall running time: 1751.9718s / 391280.4712 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0034
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0033
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 1736.95,                last time consumption/overall running time: 1561.2481s / 392841.7193 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.0037
env0_second_0:                 episode reward: 4.2500,                 loss: 0.0040
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 1747.8,                last time consumption/overall running time: 1571.7407s / 394413.4601 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0034
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0035
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 1849.0,                last time consumption/overall running time: 1662.6827s / 396076.1428 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0033
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0036
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 1888.7,                last time consumption/overall running time: 1697.5540s / 397773.6968 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0036
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0040
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 2067.4,                last time consumption/overall running time: 1858.3514s / 399632.0482 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0039
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0042
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 2153.3,                last time consumption/overall running time: 1931.0811s / 401563.1292 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0037
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0042
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1997.2,                last time consumption/overall running time: 1790.9059s / 403354.0351 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0038
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0041
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 2117.3,                last time consumption/overall running time: 1898.5173s / 405252.5524 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0036
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0039
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 1830.75,                last time consumption/overall running time: 1641.2757s / 406893.8281 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0039
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0044
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 1871.0,                last time consumption/overall running time: 1678.2412s / 408572.0693 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.0035
env0_second_0:                 episode reward: 4.2500,                 loss: 0.0038
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 1897.1,                last time consumption/overall running time: 1697.2315s / 410269.3008 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0043
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0046
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 1953.45,                last time consumption/overall running time: 1746.5279s / 412015.8287 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.0042
env0_second_0:                 episode reward: 2.4500,                 loss: 0.0047
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 1871.05,                last time consumption/overall running time: 1671.4005s / 413687.2292 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0041
env0_second_0:                 episode reward: 2.5500,                 loss: 0.0043
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 1988.55,                last time consumption/overall running time: 1777.3991s / 415464.6283 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0035
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0039
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 1901.3,                last time consumption/overall running time: 1699.8408s / 417164.4691 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0038
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0042
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 1863.5,                last time consumption/overall running time: 1664.7239s / 418829.1930 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0040
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0044
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 1940.1,                last time consumption/overall running time: 1730.0489s / 420559.2419 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0040
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0043
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 1842.3,                last time consumption/overall running time: 1643.0056s / 422202.2475 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0042
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0043
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 1734.75,                last time consumption/overall running time: 1550.9090s / 423753.1565 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0033
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0035
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 1812.0,                last time consumption/overall running time: 1619.4339s / 425372.5904 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0031
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0032
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 1922.7,                last time consumption/overall running time: 1714.7265s / 427087.3169 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0034
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0035
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 1825.05,                last time consumption/overall running time: 1629.4451s / 428716.7620 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0039
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0041
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 1768.0,                last time consumption/overall running time: 1733.3049s / 430450.0669 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0037
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0039
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 1745.5,                last time consumption/overall running time: 1788.8368s / 432238.9037 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0031
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0036
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 1677.7,                last time consumption/overall running time: 1719.1852s / 433958.0889 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0036
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0039
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 1671.9,                last time consumption/overall running time: 1712.0724s / 435670.1614 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0039
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0043
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 1761.7,                last time consumption/overall running time: 1802.9592s / 437473.1205 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0039
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0043
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 1659.2,                last time consumption/overall running time: 1698.4856s / 439171.6061 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0037
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0040
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 1755.85,                last time consumption/overall running time: 1796.7997s / 440968.4059 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0061
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0039
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 1784.4,                last time consumption/overall running time: 1830.6709s / 442799.0767 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0052
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0040
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 1680.05,                last time consumption/overall running time: 1724.3813s / 444523.4580 s
env0_first_0:                 episode reward: -6.3500,                 loss: 0.0045
env0_second_0:                 episode reward: 6.3500,                 loss: 0.0043
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 2233.85,                last time consumption/overall running time: 2292.5011s / 446815.9591 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0044
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0046
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 1764.1,                last time consumption/overall running time: 1808.5397s / 448624.4989 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0040
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0044
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 1745.15,                last time consumption/overall running time: 1787.9406s / 450412.4395 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0036
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0039
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 2070.55,                last time consumption/overall running time: 2118.0878s / 452530.5273 s
env0_first_0:                 episode reward: 8.9500,                 loss: 0.0036
env0_second_0:                 episode reward: -8.9500,                 loss: 0.0037
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 2012.3,                last time consumption/overall running time: 2059.1990s / 454589.7263 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0053
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0052
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 1872.25,                last time consumption/overall running time: 1918.3608s / 456508.0871 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0043
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0044
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 1798.55,                last time consumption/overall running time: 1842.8616s / 458350.9487 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0045
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0047
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 1797.3,                last time consumption/overall running time: 1844.4077s / 460195.3564 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0054
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 1827.55,                last time consumption/overall running time: 1871.1984s / 462066.5548 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0047
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0049
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1834.4,                last time consumption/overall running time: 1877.8278s / 463944.3826 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0037
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0038
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 1772.0,                last time consumption/overall running time: 1814.0203s / 465758.4029 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0036
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0036
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 1976.6,                last time consumption/overall running time: 2023.2917s / 467781.6945 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0037
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0040
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 1873.9,                last time consumption/overall running time: 1917.0630s / 469698.7575 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0042
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0046
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 2026.55,                last time consumption/overall running time: 2080.5421s / 471779.2996 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.0046
env0_second_0:                 episode reward: 3.9000,                 loss: 0.0045
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 2182.05,                last time consumption/overall running time: 2297.6421s / 474076.9417 s
env0_first_0:                 episode reward: 2.2000,                 loss: 0.0040
env0_second_0:                 episode reward: -2.2000,                 loss: 0.0041
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 2238.5,                last time consumption/overall running time: 2358.9132s / 476435.8549 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0046
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0048
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 2127.15,                last time consumption/overall running time: 2240.6253s / 478676.4802 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0044
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0046
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 2031.4,                last time consumption/overall running time: 2141.2920s / 480817.7723 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0053
env0_second_0:                 episode reward: 4.0500,                 loss: 0.0048
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 1943.8,                last time consumption/overall running time: 2043.2128s / 482860.9851 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0044
env0_second_0:                 episode reward: 1.9500,                 loss: 0.0046
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 1877.7,                last time consumption/overall running time: 1979.6093s / 484840.5943 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0038
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0040
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 2037.25,                last time consumption/overall running time: 2146.3363s / 486986.9307 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0043
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0045
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 1954.85,                last time consumption/overall running time: 1992.5447s / 488979.4753 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0041
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0044
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 1798.4,                last time consumption/overall running time: 1797.6212s / 490777.0965 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0039
env0_second_0:                 episode reward: 3.2000,                 loss: 0.0044
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 2149.75,                last time consumption/overall running time: 2145.5274s / 492922.6239 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0037
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0038
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 2144.1,                last time consumption/overall running time: 2144.7271s / 495067.3510 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0039
env0_second_0:                 episode reward: 3.8500,                 loss: 0.0041
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 2025.0,                last time consumption/overall running time: 2020.9047s / 497088.2557 s
env0_first_0:                 episode reward: -5.0000,                 loss: 0.0038
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0040
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 1863.75,                last time consumption/overall running time: 1858.8044s / 498947.0601 s
env0_first_0:                 episode reward: -8.6500,                 loss: 0.0044
env0_second_0:                 episode reward: 8.6500,                 loss: 0.0045
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 2189.75,                last time consumption/overall running time: 2185.5274s / 501132.5875 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0045
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0046
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 2147.5,                last time consumption/overall running time: 2137.5267s / 503270.1143 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0045
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0046
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 2160.75,                last time consumption/overall running time: 2149.2607s / 505419.3749 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0051
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0054
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 2258.95,                last time consumption/overall running time: 2245.6935s / 507665.0684 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0051
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0054
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 1995.95,                last time consumption/overall running time: 1984.3002s / 509649.3687 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0049
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0052
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 1911.6,                last time consumption/overall running time: 1900.5300s / 511549.8987 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0045
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0050
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 1761.7,                last time consumption/overall running time: 1751.6871s / 513301.5858 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0042
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0047
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 1760.55,                last time consumption/overall running time: 1750.9150s / 515052.5008 s
env0_first_0:                 episode reward: -4.6500,                 loss: 0.0048
env0_second_0:                 episode reward: 4.6500,                 loss: 0.0053
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 1816.65,                last time consumption/overall running time: 1808.2065s / 516860.7073 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0043
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0048
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 1846.35,                last time consumption/overall running time: 1836.6942s / 518697.4015 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0041
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0044
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 1766.8,                last time consumption/overall running time: 1758.3371s / 520455.7386 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0042
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0039
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 1713.3,                last time consumption/overall running time: 1705.1696s / 522160.9082 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0031
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0033
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 2110.7,                last time consumption/overall running time: 2094.2331s / 524255.1413 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0049
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0035
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 2193.1,                last time consumption/overall running time: 2177.7799s / 526432.9212 s
env0_first_0:                 episode reward: -12.5000,                 loss: 0.0073
env0_second_0:                 episode reward: 12.5000,                 loss: 0.0049
env1_first_0:                 episode reward: -12.7000,                 loss: nan
env1_second_0:                 episode reward: 12.7000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 2004.0,                last time consumption/overall running time: 1998.7431s / 528431.6643 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0054
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0052
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 1980.6,                last time consumption/overall running time: 1966.5012s / 530398.1655 s
env0_first_0:                 episode reward: -5.3000,                 loss: 0.0044
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0045
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 1874.05,                last time consumption/overall running time: 1856.3786s / 532254.5441 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0040
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0046
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 2148.85,                last time consumption/overall running time: 2131.5294s / 534386.0734 s
env0_first_0:                 episode reward: -6.9500,                 loss: 0.0046
env0_second_0:                 episode reward: 6.9500,                 loss: 0.0050
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 1940.5,                last time consumption/overall running time: 1924.0172s / 536310.0906 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0044