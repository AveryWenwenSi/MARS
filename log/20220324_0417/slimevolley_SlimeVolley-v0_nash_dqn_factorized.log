pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [23, 24]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 2, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashDQNFactorized', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'nash_dqn_factorized', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220324_0417/slimevolley_SlimeVolley-v0_nash_dqn_factorized. 
 Save logs to: /home/zihan/research/MARS/data/log/20220324_0417/slimevolley_SlimeVolley-v0_nash_dqn_factorized.
Episode: 1/50000 (0.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 8.6804s / 8.6804 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0114
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0109
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 258.0474s / 266.7279 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0084
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0087
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 263.4983s / 530.2262 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0134
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0139
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 264.8605s / 795.0867 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0169
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0184
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 267.3599s / 1062.4466 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0186
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0189
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 269.8445s / 1332.2911 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0185
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0192
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 271.5732s / 1603.8642 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0189
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0186
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 275.8246s / 1879.6888 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0194
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0196
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 280.4704s / 2160.1592 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0190
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0201
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 281.8718s / 2442.0310 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0200
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0214
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 285.9276s / 2727.9585 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0222
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0218
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 287.2507s / 3015.2092 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0231
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0227
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 293.0978s / 3308.3070 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0228
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0233
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 297.1670s / 3605.4739 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0237
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0232
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 300.0485s / 3905.5225 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0226
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0241
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 303.9384s / 4209.4609 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0235
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0255
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 307.6591s / 4517.1200 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0247
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0268
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 311.9580s / 4829.0780 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0240
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0247
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 313.3897s / 5142.4678 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0259
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0265
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 310.9592s / 5453.4270 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0272
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0273
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 312.0064s / 5765.4334 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0264
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0271
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 312.1958s / 6077.6291 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0267
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0280
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 311.5286s / 6389.1577 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0256
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0267
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 311.6832s / 6700.8409 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0246
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0272
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 311.4538s / 7012.2948 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0246
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0275
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 311.7161s / 7324.0108 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0253
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0272
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 312.5827s / 7636.5935 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0270
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0261
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 312.0677s / 7948.6612 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0273
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0262
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 311.8883s / 8260.5495 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0265
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0273
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 320.9089s / 8581.4584 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0281
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0280
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 315.2140s / 8896.6724 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0274
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0287
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 313.5112s / 9210.1836 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0271
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0297
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 641/50000 (1.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 314.8160s / 9524.9996 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0277
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0292
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 661/50000 (1.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 312.7283s / 9837.7280 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0278
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0293
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 681/50000 (1.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 314.4050s / 10152.1330 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0271
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0290
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 701/50000 (1.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 316.0312s / 10468.1642 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0258
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0293
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 721/50000 (1.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 313.9203s / 10782.0845 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0276
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0293
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 741/50000 (1.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 317.2217s / 11099.3062 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0293
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0294
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 761/50000 (1.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 316.9371s / 11416.2433 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0285
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0282
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 781/50000 (1.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 317.7040s / 11733.9473 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0285
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0313
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 801/50000 (1.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 317.6641s / 12051.6114 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0299
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0308
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 821/50000 (1.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 318.3640s / 12369.9754 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0299
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0305
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 841/50000 (1.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 315.5378s / 12685.5132 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0309
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0300
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 861/50000 (1.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 317.3708s / 13002.8840 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0295
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0302
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 881/50000 (1.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 317.2810s / 13320.1650 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0288
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0311
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 901/50000 (1.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 318.3618s / 13638.5268 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0283
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0298
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 921/50000 (1.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 319.2007s / 13957.7276 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0286
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0306
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 941/50000 (1.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 318.4632s / 14276.1907 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0286
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0316
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 961/50000 (1.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 322.4529s / 14598.6437 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0288
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0304
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 981/50000 (1.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 323.0861s / 14921.7298 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0287
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0312
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1001/50000 (2.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 318.8659s / 15240.5957 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0283
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0315
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1021/50000 (2.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 322.3666s / 15562.9623 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0293
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0322
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1041/50000 (2.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 321.0314s / 15883.9937 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0296
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0325
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1061/50000 (2.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 322.1205s / 16206.1143 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0292
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0316
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 1081/50000 (2.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 322.3110s / 16528.4253 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0298
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0335
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1101/50000 (2.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 320.1603s / 16848.5855 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0291
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0326
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1121/50000 (2.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 322.8522s / 17171.4378 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0308
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0320
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1141/50000 (2.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 321.1461s / 17492.5838 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0295
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0324
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1161/50000 (2.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 319.4295s / 17812.0134 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0296
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0322
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1181/50000 (2.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 320.2316s / 18132.2450 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0302
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0316
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1201/50000 (2.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 321.0861s / 18453.3311 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0308
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0330
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1221/50000 (2.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 323.0628s / 18776.3939 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0312
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0329
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1241/50000 (2.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 321.9390s / 19098.3328 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0314
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0333
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1261/50000 (2.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 322.2449s / 19420.5778 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0325
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0344
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1281/50000 (2.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 322.3524s / 19742.9301 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0331
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0325
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1301/50000 (2.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 323.7727s / 20066.7028 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0300
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0312
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1321/50000 (2.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 321.8295s / 20388.5322 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0288
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0318
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1341/50000 (2.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 322.8573s / 20711.3896 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0298
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0326
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 1361/50000 (2.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 321.3781s / 21032.7677 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0301
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0349
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1381/50000 (2.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 321.4498s / 21354.2175 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0297
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0369
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1401/50000 (2.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 325.5973s / 21679.8148 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0297
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0358
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1421/50000 (2.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 322.0336s / 22001.8484 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0309
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0350
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 1441/50000 (2.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 321.5089s / 22323.3573 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0313
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0358
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1461/50000 (2.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 323.1933s / 22646.5506 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0310
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0356
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 1481/50000 (2.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 326.5725s / 22973.1230 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0314
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0374
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1501/50000 (3.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 322.7220s / 23295.8450 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0329
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0369
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1521/50000 (3.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 322.2836s / 23618.1286 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0320
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0359
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1541/50000 (3.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 323.5403s / 23941.6689 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0306
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0364
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1561/50000 (3.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 322.8066s / 24264.4755 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0318
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0366
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1581/50000 (3.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 323.2605s / 24587.7361 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0310
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0366
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1601/50000 (3.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 336.2367s / 24923.9728 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0309
env0_second_0:                 episode reward: 0.6000,                 loss: 0.0369
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1621/50000 (3.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 324.1470s / 25248.1198 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0305
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0366
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1641/50000 (3.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 324.2981s / 25572.4179 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0305
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0380
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1661/50000 (3.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 323.8468s / 25896.2647 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0314
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0384
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1681/50000 (3.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 321.1760s / 26217.4407 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0318
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0396
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1701/50000 (3.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 320.5214s / 26537.9621 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0318
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0397
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1721/50000 (3.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 321.3586s / 26859.3208 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0328
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0402
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1741/50000 (3.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 328.8389s / 27188.1597 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0322
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0419
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1761/50000 (3.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 327.0254s / 27515.1851 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0333
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0422
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1781/50000 (3.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 323.5712s / 27838.7562 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0332
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0412
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1801/50000 (3.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 321.2022s / 28159.9584 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0332
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0438
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1821/50000 (3.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 324.2532s / 28484.2116 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0337
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0433
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1841/50000 (3.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 322.3699s / 28806.5815 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0330
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0449
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 1861/50000 (3.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 321.0927s / 29127.6742 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0336
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0458
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1881/50000 (3.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 325.2227s / 29452.8969 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0339
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0463
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1901/50000 (3.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 326.0243s / 29778.9212 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0340
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0473
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 1921/50000 (3.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 324.6028s / 30103.5240 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0338
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0482
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 1941/50000 (3.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 324.7465s / 30428.2705 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0353
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0480
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1961/50000 (3.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 325.9763s / 30754.2468 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0337
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0482
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1981/50000 (3.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 324.9167s / 31079.1634 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0347
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0488
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2001/50000 (4.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 326.2963s / 31405.4597 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0357
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0492
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2021/50000 (4.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 325.4003s / 31730.8600 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0355
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0498
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 2041/50000 (4.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 324.6319s / 32055.4919 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0361
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0486
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 2061/50000 (4.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 324.3451s / 32379.8370 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0366
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0491
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2081/50000 (4.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 324.6860s / 32704.5230 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0366
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0494
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2101/50000 (4.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 325.6712s / 33030.1942 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0350
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0517
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2121/50000 (4.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 321.9838s / 33352.1781 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0362
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0530
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2141/50000 (4.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 325.9517s / 33678.1298 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0370
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0508
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2161/50000 (4.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 325.6767s / 34003.8064 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0381
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0508
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2181/50000 (4.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 326.3253s / 34330.1318 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0391
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0523
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 2201/50000 (4.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 326.0702s / 34656.2019 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0400
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0503
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2221/50000 (4.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 325.4720s / 34981.6739 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0409
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0519
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2241/50000 (4.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 327.7398s / 35309.4137 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0393
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0518
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2261/50000 (4.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 325.8314s / 35635.2451 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0401
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0527
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 2281/50000 (4.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 323.2524s / 35958.4975 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0385
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0521
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2301/50000 (4.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 326.2476s / 36284.7451 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0404
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0526
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan