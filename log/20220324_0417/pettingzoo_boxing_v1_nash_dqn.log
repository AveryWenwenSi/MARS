pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [23, 24]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 1000000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 150, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220324_0417/pettingzoo_boxing_v1_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220324_0417/pettingzoo_boxing_v1_nash_dqn.
Episode: 1/50000 (0.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 2.1798s / 2.1798 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0029
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0022
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 171.8608s / 174.0407 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0038
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0037
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 173.8938s / 347.9344 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0039
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0039
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 174.9084s / 522.8429 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0035
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0032
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 175.1216s / 697.9645 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0029
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0028
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 180.1074s / 878.0719 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0032
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0032
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 179.3381s / 1057.4099 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0033
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0036
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 177.2519s / 1234.6619 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0026
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0028
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 185.6497s / 1420.3116 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0030
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0029
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 188.9737s / 1609.2853 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0030
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0031
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 186.2716s / 1795.5568 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0026
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0028
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 185.6469s / 1981.2037 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0028
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0030
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 187.0248s / 2168.2285 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0028
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0029
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 182.5715s / 2350.8000 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0028
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0030
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 190.3329s / 2541.1329 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0031
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0031
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 189.8882s / 2731.0211 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0036
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0034
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 187.6024s / 2918.6235 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0033
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0036
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 184.9165s / 3103.5400 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0032
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0035
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 185.9511s / 3289.4910 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0036
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0037
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 185.3607s / 3474.8517 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0032
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0032
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 185.4726s / 3660.3243 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0032
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0032
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 187.2976s / 3847.6219 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0031
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0035
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 189.9161s / 4037.5381 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0031
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0032
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 188.0914s / 4225.6295 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0032
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0030
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 189.0245s / 4414.6540 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0030
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0030
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 190.4995s / 4605.1534 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0029
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0031
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 191.4119s / 4796.5653 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0030
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0029
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 194.8616s / 4991.4269 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0028
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0031
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 195.8528s / 5187.2797 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0029
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0030
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 196.2455s / 5383.5252 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0030
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0032
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 196.9737s / 5580.4989 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0029
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0031
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 199.3066s / 5779.8055 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0029
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0032
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 641/50000 (1.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 198.1700s / 5977.9755 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0030
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0032
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 661/50000 (1.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 199.3611s / 6177.3366 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0030
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0033
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 681/50000 (1.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 202.1104s / 6379.4470 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0033
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0032
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 701/50000 (1.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 204.5427s / 6583.9897 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0030
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0032
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 721/50000 (1.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 199.6102s / 6783.5999 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0030
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0033
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 741/50000 (1.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 200.5043s / 6984.1043 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0031
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0032
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 761/50000 (1.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 203.2752s / 7187.3795 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0031
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0033
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 781/50000 (1.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 202.7390s / 7390.1185 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0028
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0032
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 801/50000 (1.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 203.4009s / 7593.5194 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0031
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0033
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 821/50000 (1.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 211.9453s / 7805.4647 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0031
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0033
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 841/50000 (1.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 205.1085s / 8010.5732 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0032
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0032
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 861/50000 (1.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 200.8822s / 8211.4555 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0030
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0034
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 881/50000 (1.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 205.9183s / 8417.3737 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0030
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0032
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 901/50000 (1.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 203.9244s / 8621.2982 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0031
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0034
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 921/50000 (1.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 203.2108s / 8824.5090 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0032
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0036
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 941/50000 (1.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 203.8918s / 9028.4008 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0033
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0033
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 961/50000 (1.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 207.2291s / 9235.6298 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0030
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0031
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 981/50000 (1.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 202.3799s / 9438.0097 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0028
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0031
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1001/50000 (2.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 204.9084s / 9642.9181 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0029
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0030
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1021/50000 (2.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 206.5343s / 9849.4524 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0027
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0030
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1041/50000 (2.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 203.3367s / 10052.7891 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0029
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0029
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1061/50000 (2.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 202.0731s / 10254.8622 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0031
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0026
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1081/50000 (2.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 205.5751s / 10460.4373 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0031
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0029
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 1101/50000 (2.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 204.4341s / 10664.8714 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0029
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0028
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1121/50000 (2.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 206.0251s / 10870.8965 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0026
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0025
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1141/50000 (2.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 207.3941s / 11078.2905 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0026
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0026
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1161/50000 (2.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 208.0869s / 11286.3774 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0024
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0026
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1181/50000 (2.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 207.5834s / 11493.9608 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0026
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0028
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1201/50000 (2.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 205.0260s / 11698.9868 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0025
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0027
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1221/50000 (2.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 207.2580s / 11906.2448 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0027
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0027
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1241/50000 (2.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 209.1234s / 12115.3682 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0025
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0026
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1261/50000 (2.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 204.8579s / 12320.2261 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0027
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0026
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1281/50000 (2.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 202.9834s / 12523.2095 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0028
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0023
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1301/50000 (2.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 205.9980s / 12729.2075 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0025
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0027
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1321/50000 (2.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 204.9812s / 12934.1887 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0025
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0025
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1341/50000 (2.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 210.7782s / 13144.9669 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0026
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0023
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1361/50000 (2.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 207.2023s / 13352.1692 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0023
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0022
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1381/50000 (2.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 203.1529s / 13555.3221 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0022
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0022
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1401/50000 (2.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 204.8298s / 13760.1519 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0024
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0024
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1421/50000 (2.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 203.5481s / 13963.7000 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0022
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0021
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1441/50000 (2.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 203.5489s / 14167.2488 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0023
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0023
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1461/50000 (2.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 203.1660s / 14370.4148 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0024
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0024
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1481/50000 (2.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 203.2327s / 14573.6475 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0026
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0026
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1501/50000 (3.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 209.5202s / 14783.1677 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0023
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0029
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1521/50000 (3.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 201.4541s / 14984.6218 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0019
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0025
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1541/50000 (3.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 204.4905s / 15189.1123 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0021
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0026
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1561/50000 (3.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 208.1769s / 15397.2892 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0023
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0030
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1581/50000 (3.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 211.1707s / 15608.4600 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0020
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0024
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1601/50000 (3.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 209.5763s / 15818.0362 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0020
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0027
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1621/50000 (3.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 212.2649s / 16030.3012 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0021
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0025
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1641/50000 (3.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 220.9568s / 16251.2579 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0020
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0025
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1661/50000 (3.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 213.1729s / 16464.4308 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0025
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0026
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1681/50000 (3.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 212.5827s / 16677.0135 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0028
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0025
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1701/50000 (3.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 211.4601s / 16888.4737 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0027
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0028
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1721/50000 (3.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 213.6721s / 17102.1458 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0023
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0026
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1741/50000 (3.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 210.7684s / 17312.9142 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0023
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0026
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1761/50000 (3.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 204.6301s / 17517.5442 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0021
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0028
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1781/50000 (3.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 207.2627s / 17724.8069 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0022
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0027
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1801/50000 (3.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 204.2304s / 17929.0373 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0021
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0027
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1821/50000 (3.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 204.8785s / 18133.9158 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0020
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0026
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1841/50000 (3.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 204.2969s / 18338.2127 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0018
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0025
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1861/50000 (3.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 203.5261s / 18541.7388 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0020
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0025
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1881/50000 (3.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 205.0545s / 18746.7933 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0022
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0025
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1901/50000 (3.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 205.4752s / 18952.2685 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0022
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0025
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1921/50000 (3.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 205.7588s / 19158.0274 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0020
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0025
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1941/50000 (3.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 206.2816s / 19364.3090 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0019
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0023
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1961/50000 (3.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 208.6533s / 19572.9622 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0020
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0026
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1981/50000 (3.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 204.7047s / 19777.6670 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0026
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0027
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2001/50000 (4.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 207.4127s / 19985.0797 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0026
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0025
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2021/50000 (4.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 214.9376s / 20200.0173 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0025
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0023
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2041/50000 (4.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 205.1047s / 20405.1220 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0024
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0024
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2061/50000 (4.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 205.1749s / 20610.2968 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0022
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0024
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2081/50000 (4.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 206.1353s / 20816.4321 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0024
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0023
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2101/50000 (4.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 206.4910s / 21022.9231 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0021
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0023
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2121/50000 (4.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 205.7369s / 21228.6600 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0023
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0020
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2141/50000 (4.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 206.4893s / 21435.1494 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0021
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0023
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 2161/50000 (4.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 206.9804s / 21642.1297 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0023
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0023
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2181/50000 (4.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 212.0772s / 21854.2069 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0024
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0024
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 2201/50000 (4.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 208.2206s / 22062.4275 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0024
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0025
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2221/50000 (4.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 208.2599s / 22270.6874 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0025
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0022
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2241/50000 (4.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 207.0577s / 22477.7451 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0027
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0024
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2261/50000 (4.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 208.4876s / 22686.2326 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0027
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0027
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 2281/50000 (4.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 206.2168s / 22892.4494 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0026
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0025
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2301/50000 (4.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 212.2409s / 23104.6904 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0024
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0024
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2321/50000 (4.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 207.8627s / 23312.5531 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0027
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0026
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2341/50000 (4.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 208.0533s / 23520.6065 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0028
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0024
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2361/50000 (4.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 207.0499s / 23727.6564 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0027
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0024
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 2381/50000 (4.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 206.9336s / 23934.5900 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0026
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0024
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2401/50000 (4.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 207.1026s / 24141.6926 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0025
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0024
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 2421/50000 (4.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 207.8919s / 24349.5846 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0025
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0022
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2441/50000 (4.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 205.5293s / 24555.1139 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0026
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0024
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2461/50000 (4.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 207.0713s / 24762.1852 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0024
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0022
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2481/50000 (4.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 207.3261s / 24969.5114 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0023
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0024
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2501/50000 (5.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 206.0640s / 25175.5754 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0023
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0020
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2521/50000 (5.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 206.6842s / 25382.2596 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0026
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0021
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2541/50000 (5.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 206.7396s / 25588.9992 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0024
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0021
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 2561/50000 (5.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 208.2067s / 25797.2059 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0030
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0021
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2581/50000 (5.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 207.2773s / 26004.4832 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0027
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0021
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2601/50000 (5.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 207.1077s / 26211.5909 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0030
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0024
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 2621/50000 (5.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 208.0747s / 26419.6656 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0031
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0029
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2641/50000 (5.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 211.2526s / 26630.9182 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0031
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0028
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 2661/50000 (5.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 211.5401s / 26842.4583 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0034
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0033
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2681/50000 (5.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 207.3814s / 27049.8398 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0029
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0032
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2701/50000 (5.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 205.7016s / 27255.5414 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0033
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0038
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2721/50000 (5.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 208.3163s / 27463.8577 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0036
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0035
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 2741/50000 (5.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 208.3743s / 27672.2320 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0038
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0032
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2761/50000 (5.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 207.4589s / 27879.6909 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0039
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0035
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2781/50000 (5.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 208.6276s / 28088.3185 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0041
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0033
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2801/50000 (5.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 207.7568s / 28296.0754 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0039
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0037
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 2821/50000 (5.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 209.9879s / 28506.0632 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0035
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0033
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2841/50000 (5.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 206.2693s / 28712.3326 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0037
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0033
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2861/50000 (5.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 206.3875s / 28918.7201 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0039
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0035
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2881/50000 (5.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 204.6335s / 29123.3535 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0040
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0039
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2901/50000 (5.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 210.4797s / 29333.8332 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0042
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0042
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2921/50000 (5.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 207.5539s / 29541.3871 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0041
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0039
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2941/50000 (5.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 210.4406s / 29751.8277 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0048
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0045
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2961/50000 (5.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 205.4629s / 29957.2906 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0046
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0047
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2981/50000 (5.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 209.3436s / 30166.6342 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0043
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0053
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3001/50000 (6.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 212.9157s / 30379.5499 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0044
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0050
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3021/50000 (6.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 207.1462s / 30586.6962 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0043
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0047
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3041/50000 (6.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 209.1233s / 30795.8195 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0043
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0045
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3061/50000 (6.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 206.0510s / 31001.8705 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0046
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0041
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3081/50000 (6.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 209.6099s / 31211.4804 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0042
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0041
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3101/50000 (6.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 208.1867s / 31419.6671 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0040
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0039
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3121/50000 (6.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 208.7088s / 31628.3758 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0039
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0040
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3141/50000 (6.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 207.2905s / 31835.6664 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0040
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0038
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3161/50000 (6.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 207.7760s / 32043.4424 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0043
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0044
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3181/50000 (6.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 207.0135s / 32250.4559 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0042
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0049
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3201/50000 (6.4020%),                 avg. length: 149.0,                last time consumption/overall running time: 208.7980s / 32459.2539 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0044
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0048
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3221/50000 (6.4420%),                 avg. length: 149.0,                last time consumption/overall running time: 209.2188s / 32668.4727 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0049
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0045
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3241/50000 (6.4820%),                 avg. length: 149.0,                last time consumption/overall running time: 208.2211s / 32876.6938 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0052
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0049
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3261/50000 (6.5220%),                 avg. length: 149.0,                last time consumption/overall running time: 208.9478s / 33085.6416 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0052
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0050
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 3281/50000 (6.5620%),                 avg. length: 149.0,                last time consumption/overall running time: 208.7008s / 33294.3424 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0051
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0047
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3301/50000 (6.6020%),                 avg. length: 149.0,                last time consumption/overall running time: 221.9317s / 33516.2742 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0056
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0045
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3321/50000 (6.6420%),                 avg. length: 149.0,                last time consumption/overall running time: 210.5789s / 33726.8530 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0060
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0051
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 3341/50000 (6.6820%),                 avg. length: 149.0,                last time consumption/overall running time: 209.6879s / 33936.5409 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0059
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0053
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 3361/50000 (6.7220%),                 avg. length: 149.0,                last time consumption/overall running time: 209.1323s / 34145.6733 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0056
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0054
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 3381/50000 (6.7620%),                 avg. length: 149.0,                last time consumption/overall running time: 207.0749s / 34352.7482 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0056
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0060
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3401/50000 (6.8020%),                 avg. length: 149.0,                last time consumption/overall running time: 207.3513s / 34560.0994 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0057
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0056
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 3421/50000 (6.8420%),                 avg. length: 149.0,                last time consumption/overall running time: 210.3935s / 34770.4929 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0063
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0057
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3441/50000 (6.8820%),                 avg. length: 149.0,                last time consumption/overall running time: 208.5460s / 34979.0389 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0068
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0053
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3461/50000 (6.9220%),                 avg. length: 149.0,                last time consumption/overall running time: 207.1872s / 35186.2261 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0056
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3481/50000 (6.9620%),                 avg. length: 149.0,                last time consumption/overall running time: 206.3993s / 35392.6254 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0061
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0061
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3501/50000 (7.0020%),                 avg. length: 149.0,                last time consumption/overall running time: 212.5118s / 35605.1372 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0064
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0060
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3521/50000 (7.0420%),                 avg. length: 149.0,                last time consumption/overall running time: 212.8602s / 35817.9974 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0075
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0066
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 3541/50000 (7.0820%),                 avg. length: 149.0,                last time consumption/overall running time: 217.7478s / 36035.7452 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0070
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0062
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3561/50000 (7.1220%),                 avg. length: 149.0,                last time consumption/overall running time: 208.7645s / 36244.5097 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0073
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0057
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3581/50000 (7.1620%),                 avg. length: 149.0,                last time consumption/overall running time: 222.3019s / 36466.8116 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0078
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0062
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3601/50000 (7.2020%),                 avg. length: 149.0,                last time consumption/overall running time: 218.1291s / 36684.9407 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0083
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0064
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 3621/50000 (7.2420%),                 avg. length: 149.0,                last time consumption/overall running time: 208.8967s / 36893.8374 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0091
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0073
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3641/50000 (7.2820%),                 avg. length: 149.0,                last time consumption/overall running time: 210.4612s / 37104.2986 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0094
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0077
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3661/50000 (7.3220%),                 avg. length: 149.0,                last time consumption/overall running time: 209.6811s / 37313.9797 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0094
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0076
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3681/50000 (7.3620%),                 avg. length: 149.0,                last time consumption/overall running time: 209.4875s / 37523.4671 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0100
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0082