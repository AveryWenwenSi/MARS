pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
random seed: [5, 69, 76]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 3, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 1}, 'batch_size': 128, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20211227_1021/pettingzoo_tennis_v2_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/20211227_1021/pettingzoo_tennis_v2_nash_dqn_exploiter.
Episode: 1/30000 (0.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 504.0551s / 504.0551 s
env0_first_0:                 episode reward: 11.0000,                 loss: 0.0201
env0_second_0:                 episode reward: -11.0000,                 loss: 0.0203
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
env2_first_0:                 episode reward: 9.0000,                 loss: nan
env2_second_0:                 episode reward: -9.0000,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 3636.8,                last time consumption/overall running time: 4044.8645s / 4548.9195 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0177
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0176
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
env2_first_0:                 episode reward: 0.8500,                 loss: nan
env2_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 4225.5,                last time consumption/overall running time: 4926.9564s / 9475.8759 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0161
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0163
env1_first_0:                 episode reward: -15.5500,                 loss: nan
env1_second_0:                 episode reward: 15.5500,                 loss: nan
env2_first_0:                 episode reward: -13.3500,                 loss: nan
env2_second_0:                 episode reward: 13.3500,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 3602.55,                last time consumption/overall running time: 4146.5188s / 13622.3947 s
env0_first_0:                 episode reward: 10.8000,                 loss: 0.0123
env0_second_0:                 episode reward: -10.8000,                 loss: 0.0135
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
env2_first_0:                 episode reward: 13.8500,                 loss: nan
env2_second_0:                 episode reward: -13.8500,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2285.75,                last time consumption/overall running time: 2626.7136s / 16249.1083 s
env0_first_0:                 episode reward: 9.2000,                 loss: 0.0066
env0_second_0:                 episode reward: -9.2000,                 loss: 0.0100
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
env2_first_0:                 episode reward: 9.5500,                 loss: nan
env2_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2983.7,                last time consumption/overall running time: 3440.6098s / 19689.7181 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.0092
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0115
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
env2_first_0:                 episode reward: 2.4500,                 loss: nan
env2_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 3150.15,                last time consumption/overall running time: 3641.0587s / 23330.7768 s
env0_first_0:                 episode reward: -4.3000,                 loss: 0.0087
env0_second_0:                 episode reward: 4.3000,                 loss: 0.0100
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
env2_first_0:                 episode reward: -11.7000,                 loss: nan
env2_second_0:                 episode reward: 11.7000,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2520.45,                last time consumption/overall running time: 2883.5372s / 26214.3140 s
env0_first_0:                 episode reward: -10.9000,                 loss: 0.0087
env0_second_0:                 episode reward: 10.9000,                 loss: 0.0071
env1_first_0:                 episode reward: -10.4000,                 loss: nan
env1_second_0:                 episode reward: 10.4000,                 loss: nan
env2_first_0:                 episode reward: -8.0500,                 loss: nan
env2_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 3312.75,                last time consumption/overall running time: 3789.7703s / 30004.0843 s
env0_first_0:                 episode reward: -14.1500,                 loss: 0.0125
env0_second_0:                 episode reward: 14.1500,                 loss: 0.0082
env1_first_0:                 episode reward: -12.2000,                 loss: nan
env1_second_0:                 episode reward: 12.2000,                 loss: nan
env2_first_0:                 episode reward: -4.6500,                 loss: nan
env2_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2823.45,                last time consumption/overall running time: 3274.1909s / 33278.2751 s
env0_first_0:                 episode reward: -11.8000,                 loss: 0.0122
env0_second_0:                 episode reward: 11.8000,                 loss: 0.0066
env1_first_0:                 episode reward: -10.2500,                 loss: nan
env1_second_0:                 episode reward: 10.2500,                 loss: nan
env2_first_0:                 episode reward: -11.4500,                 loss: nan
env2_second_0:                 episode reward: 11.4500,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2956.95,                last time consumption/overall running time: 3445.3487s / 36723.6238 s
env0_first_0:                 episode reward: -8.0500,                 loss: 0.0124
env0_second_0:                 episode reward: 8.0500,                 loss: 0.0068
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
env2_first_0:                 episode reward: -6.5000,                 loss: nan
env2_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2944.55,                last time consumption/overall running time: 3399.5783s / 40123.2021 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0120
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0056
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
env2_first_0:                 episode reward: 3.3000,                 loss: nan
env2_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 241/30000 (0.8033%),                 avg. length: 2819.7,                last time consumption/overall running time: 3255.8563s / 43379.0584 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0108
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0048
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
env2_first_0:                 episode reward: -3.3500,                 loss: nan
env2_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 261/30000 (0.8700%),                 avg. length: 3382.95,                last time consumption/overall running time: 3925.6719s / 47304.7303 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0115
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0053
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
env2_first_0:                 episode reward: -0.4000,                 loss: nan
env2_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 281/30000 (0.9367%),                 avg. length: 3020.6,                last time consumption/overall running time: 3486.4304s / 50791.1608 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0102
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0048
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
env2_first_0:                 episode reward: 2.6500,                 loss: nan
env2_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 301/30000 (1.0033%),                 avg. length: 3588.85,                last time consumption/overall running time: 4138.1554s / 54929.3161 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0103
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0060
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
env2_first_0:                 episode reward: 3.8500,                 loss: nan
env2_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 321/30000 (1.0700%),                 avg. length: 4606.15,                last time consumption/overall running time: 5365.3620s / 60294.6781 s
env0_first_0:                 episode reward: 15.3000,                 loss: 0.0102
env0_second_0:                 episode reward: -15.3000,                 loss: 0.0065
env1_first_0:                 episode reward: 11.0000,                 loss: nan
env1_second_0:                 episode reward: -11.0000,                 loss: nan
env2_first_0:                 episode reward: 6.8000,                 loss: nan
env2_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 341/30000 (1.1367%),                 avg. length: 5106.15,                last time consumption/overall running time: 5980.3662s / 66275.0444 s
env0_first_0:                 episode reward: -39.7000,                 loss: 0.0124
env0_second_0:                 episode reward: 39.7000,                 loss: 0.0101
env1_first_0:                 episode reward: -40.6500,                 loss: nan
env1_second_0:                 episode reward: 40.6500,                 loss: nan
env2_first_0:                 episode reward: -36.4500,                 loss: nan
env2_second_0:                 episode reward: 36.4500,                 loss: nan
Episode: 361/30000 (1.2033%),                 avg. length: 5427.8,                last time consumption/overall running time: 6317.2018s / 72592.2461 s
env0_first_0:                 episode reward: -25.9000,                 loss: 0.0101
env0_second_0:                 episode reward: 25.9000,                 loss: 0.0074
env1_first_0:                 episode reward: -27.5000,                 loss: nan
env1_second_0:                 episode reward: 27.5000,                 loss: nan
env2_first_0:                 episode reward: -26.5000,                 loss: nan
env2_second_0:                 episode reward: 26.5000,                 loss: nan
Episode: 381/30000 (1.2700%),                 avg. length: 3715.4,                last time consumption/overall running time: 4349.9340s / 76942.1801 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0095
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0034
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
env2_first_0:                 episode reward: -3.6000,                 loss: nan
env2_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 401/30000 (1.3367%),                 avg. length: 3899.1,                last time consumption/overall running time: 4572.6812s / 81514.8613 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0107
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0052
env1_first_0:                 episode reward: 8.7500,                 loss: nan
env1_second_0:                 episode reward: -8.7500,                 loss: nan
env2_first_0:                 episode reward: 1.8000,                 loss: nan
env2_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 421/30000 (1.4033%),                 avg. length: 3949.1,                last time consumption/overall running time: 4630.1173s / 86144.9787 s
env0_first_0:                 episode reward: -8.9500,                 loss: 0.0100
env0_second_0:                 episode reward: 8.9500,                 loss: 0.0042
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
env2_first_0:                 episode reward: -7.7500,                 loss: nan
env2_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 441/30000 (1.4700%),                 avg. length: 4777.15,                last time consumption/overall running time: 5638.1610s / 91783.1397 s
env0_first_0:                 episode reward: -14.0500,                 loss: 0.0122
env0_second_0:                 episode reward: 14.0500,                 loss: 0.0078
env1_first_0:                 episode reward: -14.9000,                 loss: nan
env1_second_0:                 episode reward: 14.9000,                 loss: nan
env2_first_0:                 episode reward: -21.0000,                 loss: nan
env2_second_0:                 episode reward: 21.0000,                 loss: nan
Episode: 461/30000 (1.5367%),                 avg. length: 7948.05,                last time consumption/overall running time: 9316.5648s / 101099.7044 s
env0_first_0:                 episode reward: 52.7000,                 loss: 0.0136
env0_second_0:                 episode reward: -52.7000,                 loss: 0.0120
env1_first_0:                 episode reward: 56.9000,                 loss: nan
env1_second_0:                 episode reward: -56.9000,                 loss: nan
env2_first_0:                 episode reward: 56.5000,                 loss: nan
env2_second_0:                 episode reward: -56.5000,                 loss: nan
Episode: 481/30000 (1.6033%),                 avg. length: 6147.2,                last time consumption/overall running time: 7146.7780s / 108246.4824 s
env0_first_0:                 episode reward: -21.9500,                 loss: 0.0107
env0_second_0:                 episode reward: 21.9500,                 loss: 0.0084
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
env2_first_0:                 episode reward: 9.7000,                 loss: nan
env2_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 501/30000 (1.6700%),                 avg. length: 5579.1,                last time consumption/overall running time: 6488.6264s / 114735.1089 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0105
env0_second_0:                 episode reward: -5.8500,                 loss: 0.0066
env1_first_0:                 episode reward: 10.3000,                 loss: nan
env1_second_0:                 episode reward: -10.3000,                 loss: nan
env2_first_0:                 episode reward: 18.3500,                 loss: nan
env2_second_0:                 episode reward: -18.3500,                 loss: nan
Episode: 521/30000 (1.7367%),                 avg. length: 6537.85,                last time consumption/overall running time: 7609.9451s / 122345.0540 s
env0_first_0:                 episode reward: 3.4000,                 loss: 0.0082
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0056
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
env2_first_0:                 episode reward: 16.6500,                 loss: nan
env2_second_0:                 episode reward: -16.6500,                 loss: nan
Episode: 541/30000 (1.8033%),                 avg. length: 5954.35,                last time consumption/overall running time: 6894.2329s / 129239.2869 s
env0_first_0:                 episode reward: 4.0500,                 loss: 0.0085
env0_second_0:                 episode reward: -4.0500,                 loss: 0.0056
env1_first_0:                 episode reward: 17.1500,                 loss: nan
env1_second_0:                 episode reward: -17.1500,                 loss: nan
env2_first_0:                 episode reward: 4.8000,                 loss: nan
env2_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 561/30000 (1.8700%),                 avg. length: 8012.55,                last time consumption/overall running time: 9304.0351s / 138543.3220 s
env0_first_0:                 episode reward: -29.3000,                 loss: 0.0117
env0_second_0:                 episode reward: 29.3000,                 loss: 0.0099
env1_first_0:                 episode reward: -38.7000,                 loss: nan
env1_second_0:                 episode reward: 38.7000,                 loss: nan
env2_first_0:                 episode reward: -49.8500,                 loss: nan
env2_second_0:                 episode reward: 49.8500,                 loss: nan
Episode: 581/30000 (1.9367%),                 avg. length: 8502.85,                last time consumption/overall running time: 9881.9724s / 148425.2944 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0073
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0052
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
env2_first_0:                 episode reward: 4.3000,                 loss: nan
env2_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 601/30000 (2.0033%),                 avg. length: 9306.65,                last time consumption/overall running time: 10872.7844s / 159298.0788 s
env0_first_0:                 episode reward: 12.5000,                 loss: 0.0060
env0_second_0:                 episode reward: -12.5000,                 loss: 0.0046
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
env2_first_0:                 episode reward: 10.4500,                 loss: nan
env2_second_0:                 episode reward: -10.4500,                 loss: nan
Episode: 621/30000 (2.0700%),                 avg. length: 7621.55,                last time consumption/overall running time: 8943.9079s / 168241.9867 s
env0_first_0:                 episode reward: 13.5000,                 loss: 0.0068
env0_second_0:                 episode reward: -13.5000,                 loss: 0.0048
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
env2_first_0:                 episode reward: 10.4500,                 loss: nan
env2_second_0:                 episode reward: -10.4500,                 loss: nan
Episode: 641/30000 (2.1367%),                 avg. length: 8422.25,                last time consumption/overall running time: 9878.5497s / 178120.5364 s
env0_first_0:                 episode reward: 4.2500,                 loss: 0.0072
env0_second_0:                 episode reward: -4.2500,                 loss: 0.0042
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
env2_first_0:                 episode reward: 7.0500,                 loss: nan
env2_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 661/30000 (2.2033%),                 avg. length: 6940.5,                last time consumption/overall running time: 8119.3463s / 186239.8827 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0040
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0020
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
env2_first_0:                 episode reward: -1.2000,                 loss: nan
env2_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 681/30000 (2.2700%),                 avg. length: 8819.2,                last time consumption/overall running time: 10345.0146s / 196584.8973 s
env0_first_0:                 episode reward: -27.5500,                 loss: 0.0071
env0_second_0:                 episode reward: 27.5500,                 loss: 0.0044
env1_first_0:                 episode reward: -34.3500,                 loss: nan
env1_second_0:                 episode reward: 34.3500,                 loss: nan
env2_first_0:                 episode reward: -26.9500,                 loss: nan
env2_second_0:                 episode reward: 26.9500,                 loss: nan
Episode: 701/30000 (2.3367%),                 avg. length: 8197.85,                last time consumption/overall running time: 9697.6081s / 206282.5054 s
env0_first_0:                 episode reward: -28.9000,                 loss: 0.0116
env0_second_0:                 episode reward: 28.9000,                 loss: 0.0103
env1_first_0:                 episode reward: -33.4000,                 loss: nan
env1_second_0:                 episode reward: 33.4000,                 loss: nan
env2_first_0:                 episode reward: -24.9000,                 loss: nan
env2_second_0:                 episode reward: 24.9000,                 loss: nan
Episode: 721/30000 (2.4033%),                 avg. length: 9126.25,                last time consumption/overall running time: 10728.9380s / 217011.4434 s