pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
ice_hockey_v1 pettingzoo
random seed: [89, 79, 44]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'ice_hockey_v1', 'env_type': 'pettingzoo', 'num_envs': 3, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20211227_1021/pettingzoo_ice_hockey_v1_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20211227_1021/pettingzoo_ice_hockey_v1_nash_dqn.
Episode: 1/30000 (0.0033%),                 avg. length: 2760.0,                last time consumption/overall running time: 139.1744s / 139.1744 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0014
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0017
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
env2_first_0:                 episode reward: 0.0000,                 loss: nan
env2_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2811.9,                last time consumption/overall running time: 3012.5370s / 3151.7115 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0012
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0012
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
env2_first_0:                 episode reward: -1.0500,                 loss: nan
env2_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2839.4,                last time consumption/overall running time: 3196.7785s / 6348.4899 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0015
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0014
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
env2_first_0:                 episode reward: -0.0500,                 loss: nan
env2_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2846.15,                last time consumption/overall running time: 3278.5807s / 9627.0706 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0015
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0015
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
env2_first_0:                 episode reward: -0.2000,                 loss: nan
env2_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2847.7,                last time consumption/overall running time: 3274.1643s / 12901.2349 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0016
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0016
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
env2_first_0:                 episode reward: -0.4000,                 loss: nan
env2_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2832.5,                last time consumption/overall running time: 3270.6798s / 16171.9147 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0014
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0015
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
env2_first_0:                 episode reward: -0.7000,                 loss: nan
env2_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2856.9,                last time consumption/overall running time: 3234.9265s / 19406.8412 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0015
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0015
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
env2_first_0:                 episode reward: -1.4500,                 loss: nan
env2_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2841.65,                last time consumption/overall running time: 3230.5547s / 22637.3959 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0013
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0014
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
env2_first_0:                 episode reward: 0.4500,                 loss: nan
env2_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2827.55,                last time consumption/overall running time: 3219.2012s / 25856.5971 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0013
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0013
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
env2_first_0:                 episode reward: -0.0500,                 loss: nan
env2_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2832.55,                last time consumption/overall running time: 3216.2037s / 29072.8008 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0015
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0014
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
env2_first_0:                 episode reward: 0.3500,                 loss: nan
env2_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2851.05,                last time consumption/overall running time: 3222.6335s / 32295.4344 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0013
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0014
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
env2_first_0:                 episode reward: -0.9500,                 loss: nan
env2_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2839.6,                last time consumption/overall running time: 3218.3249s / 35513.7593 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0015
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0014
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
env2_first_0:                 episode reward: -0.3500,                 loss: nan
env2_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 241/30000 (0.8033%),                 avg. length: 2833.35,                last time consumption/overall running time: 3213.2827s / 38727.0419 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0014
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0014
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
env2_first_0:                 episode reward: 0.4000,                 loss: nan
env2_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 261/30000 (0.8700%),                 avg. length: 2840.05,                last time consumption/overall running time: 3251.4305s / 41978.4724 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0013
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0013
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
env2_first_0:                 episode reward: 0.1000,                 loss: nan
env2_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 281/30000 (0.9367%),                 avg. length: 2828.5,                last time consumption/overall running time: 3225.1710s / 45203.6434 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0012
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0012
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
env2_first_0:                 episode reward: 0.4000,                 loss: nan
env2_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 301/30000 (1.0033%),                 avg. length: 2831.65,                last time consumption/overall running time: 3203.4041s / 48407.0475 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0013
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0013
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
env2_first_0:                 episode reward: -0.6500,                 loss: nan
env2_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 321/30000 (1.0700%),                 avg. length: 2833.5,                last time consumption/overall running time: 3279.6116s / 51686.6591 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0012
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0012
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
env2_first_0:                 episode reward: 0.0500,                 loss: nan
env2_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 341/30000 (1.1367%),                 avg. length: 2829.1,                last time consumption/overall running time: 3195.0688s / 54881.7279 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0013
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0012
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
env2_first_0:                 episode reward: 0.5000,                 loss: nan
env2_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 361/30000 (1.2033%),                 avg. length: 2837.7,                last time consumption/overall running time: 3209.4737s / 58091.2016 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0013
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0012
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
env2_first_0:                 episode reward: 0.8500,                 loss: nan
env2_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 381/30000 (1.2700%),                 avg. length: 2832.55,                last time consumption/overall running time: 3206.8087s / 61298.0103 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0012
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0012
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
env2_first_0:                 episode reward: 0.9000,                 loss: nan
env2_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 401/30000 (1.3367%),                 avg. length: 2843.95,                last time consumption/overall running time: 3280.6348s / 64578.6451 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0012
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0012
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
env2_first_0:                 episode reward: 0.3500,                 loss: nan
env2_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 421/30000 (1.4033%),                 avg. length: 2836.55,                last time consumption/overall running time: 3270.7903s / 67849.4354 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0013
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0013
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
env2_first_0:                 episode reward: 0.6000,                 loss: nan
env2_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 441/30000 (1.4700%),                 avg. length: 2839.75,                last time consumption/overall running time: 3293.0588s / 71142.4943 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0013
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0013
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
env2_first_0:                 episode reward: -0.4500,                 loss: nan
env2_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 461/30000 (1.5367%),                 avg. length: 2826.85,                last time consumption/overall running time: 3252.9762s / 74395.4705 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0011
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0012
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
env2_first_0:                 episode reward: -0.3500,                 loss: nan
env2_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 481/30000 (1.6033%),                 avg. length: 2859.85,                last time consumption/overall running time: 3234.2866s / 77629.7571 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0014
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0013
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
env2_first_0:                 episode reward: -0.5500,                 loss: nan
env2_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 501/30000 (1.6700%),                 avg. length: 2838.3,                last time consumption/overall running time: 3200.8110s / 80830.5681 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0013
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0012
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
env2_first_0:                 episode reward: 0.0500,                 loss: nan
env2_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 521/30000 (1.7367%),                 avg. length: 2846.0,                last time consumption/overall running time: 3221.7657s / 84052.3338 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0012
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0012
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
env2_first_0:                 episode reward: 0.7500,                 loss: nan
env2_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 541/30000 (1.8033%),                 avg. length: 2849.9,                last time consumption/overall running time: 3284.5162s / 87336.8500 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0012
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0012
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
env2_first_0:                 episode reward: -0.9500,                 loss: nan
env2_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 561/30000 (1.8700%),                 avg. length: 2830.9,                last time consumption/overall running time: 3270.9178s / 90607.7678 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0012
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0012
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
env2_first_0:                 episode reward: 0.3500,                 loss: nan
env2_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 581/30000 (1.9367%),                 avg. length: 2823.75,                last time consumption/overall running time: 3262.1816s / 93869.9494 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0011
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0012
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
env2_first_0:                 episode reward: 0.6500,                 loss: nan
env2_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 601/30000 (2.0033%),                 avg. length: 2848.1,                last time consumption/overall running time: 3262.7703s / 97132.7197 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0012
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0011
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
env2_first_0:                 episode reward: 0.1500,                 loss: nan
env2_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 621/30000 (2.0700%),                 avg. length: 2852.7,                last time consumption/overall running time: 3228.9441s / 100361.6638 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0012
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0011
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
env2_first_0:                 episode reward: 0.9500,                 loss: nan
env2_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 641/30000 (2.1367%),                 avg. length: 2852.15,                last time consumption/overall running time: 3229.3421s / 103591.0060 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0013
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0011
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
env2_first_0:                 episode reward: -0.2000,                 loss: nan
env2_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 661/30000 (2.2033%),                 avg. length: 2829.25,                last time consumption/overall running time: 3212.6817s / 106803.6877 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0012
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0011
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
env2_first_0:                 episode reward: 1.1500,                 loss: nan
env2_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 681/30000 (2.2700%),                 avg. length: 2827.7,                last time consumption/overall running time: 3207.1315s / 110010.8192 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0010
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0010
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
env2_first_0:                 episode reward: 0.0000,                 loss: nan
env2_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 701/30000 (2.3367%),                 avg. length: 2852.3,                last time consumption/overall running time: 3242.0266s / 113252.8458 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0012
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0011
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
env2_first_0:                 episode reward: 0.8500,                 loss: nan
env2_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 721/30000 (2.4033%),                 avg. length: 2855.7,                last time consumption/overall running time: 3224.3578s / 116477.2036 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0012
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0010
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
env2_first_0:                 episode reward: 1.0500,                 loss: nan
env2_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 741/30000 (2.4700%),                 avg. length: 2859.15,                last time consumption/overall running time: 3257.0035s / 119734.2071 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0011
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0010
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
env2_first_0:                 episode reward: 0.2000,                 loss: nan
env2_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 761/30000 (2.5367%),                 avg. length: 2823.2,                last time consumption/overall running time: 3182.0185s / 122916.2256 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0012
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0010
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
env2_first_0:                 episode reward: 1.3500,                 loss: nan
env2_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 781/30000 (2.6033%),                 avg. length: 2835.85,                last time consumption/overall running time: 3234.7035s / 126150.9291 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0011
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0009
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
env2_first_0:                 episode reward: -0.3000,                 loss: nan
env2_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 801/30000 (2.6700%),                 avg. length: 2824.45,                last time consumption/overall running time: 3204.8009s / 129355.7301 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0011
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0009
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
env2_first_0:                 episode reward: 0.4000,                 loss: nan
env2_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 821/30000 (2.7367%),                 avg. length: 2854.35,                last time consumption/overall running time: 3235.8651s / 132591.5952 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0010
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0009
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
env2_first_0:                 episode reward: 0.9000,                 loss: nan
env2_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 841/30000 (2.8033%),                 avg. length: 2854.25,                last time consumption/overall running time: 3273.5890s / 135865.1842 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0010
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0010
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
env2_first_0:                 episode reward: 0.2000,                 loss: nan
env2_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 861/30000 (2.8700%),                 avg. length: 2838.25,                last time consumption/overall running time: 3276.1398s / 139141.3240 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0009
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0009
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
env2_first_0:                 episode reward: -0.5000,                 loss: nan
env2_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 881/30000 (2.9367%),                 avg. length: 2834.2,                last time consumption/overall running time: 3204.6355s / 142345.9595 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0009
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0009
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
env2_first_0:                 episode reward: 0.9000,                 loss: nan
env2_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 901/30000 (3.0033%),                 avg. length: 2850.4,                last time consumption/overall running time: 3249.4571s / 145595.4166 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0009
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0009
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
env2_first_0:                 episode reward: 1.4500,                 loss: nan
env2_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 921/30000 (3.0700%),                 avg. length: 2827.2,                last time consumption/overall running time: 3227.9455s / 148823.3621 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0009
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0008
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
env2_first_0:                 episode reward: 0.0000,                 loss: nan
env2_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 941/30000 (3.1367%),                 avg. length: 2853.3,                last time consumption/overall running time: 3229.4165s / 152052.7786 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0009
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0008
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
env2_first_0:                 episode reward: -0.6500,                 loss: nan
env2_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 961/30000 (3.2033%),                 avg. length: 2841.7,                last time consumption/overall running time: 3219.2290s / 155272.0076 s
env0_first_0:                 episode reward: 2.1000,                 loss: 0.0008
env0_second_0:                 episode reward: -2.1000,                 loss: 0.0007
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
env2_first_0:                 episode reward: 1.1000,                 loss: nan
env2_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 981/30000 (3.2700%),                 avg. length: 2825.75,                last time consumption/overall running time: 3209.5589s / 158481.5665 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0009
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0009
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
env2_first_0:                 episode reward: 0.9000,                 loss: nan
env2_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 1001/30000 (3.3367%),                 avg. length: 2814.7,                last time consumption/overall running time: 3196.8339s / 161678.4004 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0008
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0008
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
env2_first_0:                 episode reward: -0.4500,                 loss: nan
env2_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 1021/30000 (3.4033%),                 avg. length: 2844.55,                last time consumption/overall running time: 3210.2497s / 164888.6501 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0008
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0008
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
env2_first_0:                 episode reward: 0.9500,                 loss: nan
env2_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 1041/30000 (3.4700%),                 avg. length: 2822.15,                last time consumption/overall running time: 3212.5938s / 168101.2439 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0009
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0009
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
env2_first_0:                 episode reward: 0.2000,                 loss: nan
env2_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1061/30000 (3.5367%),                 avg. length: 2831.95,                last time consumption/overall running time: 3234.4175s / 171335.6614 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0009
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0008
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
env2_first_0:                 episode reward: -0.2500,                 loss: nan
env2_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1081/30000 (3.6033%),                 avg. length: 2812.8,                last time consumption/overall running time: 3254.4071s / 174590.0684 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0008
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0007
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
env2_first_0:                 episode reward: 0.1500,                 loss: nan
env2_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1101/30000 (3.6700%),                 avg. length: 2831.55,                last time consumption/overall running time: 3256.6189s / 177846.6873 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0008
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0007
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
env2_first_0:                 episode reward: 0.1000,                 loss: nan
env2_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1121/30000 (3.7367%),                 avg. length: 2837.7,                last time consumption/overall running time: 3255.0415s / 181101.7288 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0008
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0007
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
env2_first_0:                 episode reward: 0.2500,                 loss: nan
env2_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1141/30000 (3.8033%),                 avg. length: 2837.9,                last time consumption/overall running time: 3272.5663s / 184374.2951 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0009
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0007
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
env2_first_0:                 episode reward: 0.5000,                 loss: nan
env2_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 1161/30000 (3.8700%),                 avg. length: 2810.7,                last time consumption/overall running time: 3228.8448s / 187603.1399 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0009
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0008
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
env2_first_0:                 episode reward: 0.9000,                 loss: nan
env2_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 1181/30000 (3.9367%),                 avg. length: 2811.15,                last time consumption/overall running time: 3245.2610s / 190848.4009 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0008
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0008
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
env2_first_0:                 episode reward: 0.9500,                 loss: nan
env2_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 1201/30000 (4.0033%),                 avg. length: 2865.2,                last time consumption/overall running time: 3316.4968s / 194164.8977 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0009
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0008
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
env2_first_0:                 episode reward: 0.3500,                 loss: nan
env2_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1221/30000 (4.0700%),                 avg. length: 2851.4,                last time consumption/overall running time: 3282.6426s / 197447.5403 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0009
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0009
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
env2_first_0:                 episode reward: 0.3000,                 loss: nan
env2_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1241/30000 (4.1367%),                 avg. length: 2826.1,                last time consumption/overall running time: 3247.9466s / 200695.4869 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0007
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0007
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
env2_first_0:                 episode reward: 0.3500,                 loss: nan
env2_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1261/30000 (4.2033%),                 avg. length: 2830.85,                last time consumption/overall running time: 3264.9224s / 203960.4093 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0008
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0007
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
env2_first_0:                 episode reward: 0.9000,                 loss: nan
env2_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 1281/30000 (4.2700%),                 avg. length: 2849.35,                last time consumption/overall running time: 3286.4237s / 207246.8330 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0008
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0007
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
env2_first_0:                 episode reward: 0.9000,                 loss: nan
env2_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 1301/30000 (4.3367%),                 avg. length: 2847.2,                last time consumption/overall running time: 3294.6561s / 210541.4891 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0009
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0008
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
env2_first_0:                 episode reward: -0.3500,                 loss: nan
env2_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1321/30000 (4.4033%),                 avg. length: 2840.0,                last time consumption/overall running time: 3278.9016s / 213820.3907 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0008
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0007
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
env2_first_0:                 episode reward: 1.4000,                 loss: nan
env2_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 1341/30000 (4.4700%),                 avg. length: 2837.65,                last time consumption/overall running time: 3278.8270s / 217099.2178 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0008
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0007
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
env2_first_0:                 episode reward: 0.7000,                 loss: nan
env2_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 1361/30000 (4.5367%),                 avg. length: 2855.45,                last time consumption/overall running time: 3298.3647s / 220397.5824 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0008
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0007
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
env2_first_0:                 episode reward: 1.6500,                 loss: nan
env2_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 1381/30000 (4.6033%),                 avg. length: 2846.4,                last time consumption/overall running time: 3278.1589s / 223675.7414 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0008
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0008
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
env2_first_0:                 episode reward: 0.8500,                 loss: nan
env2_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 1401/30000 (4.6700%),                 avg. length: 2861.9,                last time consumption/overall running time: 3290.3099s / 226966.0512 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0010
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0009
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
env2_first_0:                 episode reward: 1.9500,                 loss: nan
env2_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 1421/30000 (4.7367%),                 avg. length: 2853.85,                last time consumption/overall running time: 3270.4723s / 230236.5236 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0009
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0009
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
env2_first_0:                 episode reward: 2.3000,                 loss: nan
env2_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 1441/30000 (4.8033%),                 avg. length: 2890.75,                last time consumption/overall running time: 3333.0166s / 233569.5402 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0011
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0011
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
env2_first_0:                 episode reward: 1.9500,                 loss: nan
env2_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 1461/30000 (4.8700%),                 avg. length: 2912.75,                last time consumption/overall running time: 3361.0165s / 236930.5566 s
env0_first_0:                 episode reward: 2.9500,                 loss: 0.0011
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0011
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
env2_first_0:                 episode reward: 1.9000,                 loss: nan
env2_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 1481/30000 (4.9367%),                 avg. length: 2887.65,                last time consumption/overall running time: 3340.9368s / 240271.4934 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0011
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0012
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan