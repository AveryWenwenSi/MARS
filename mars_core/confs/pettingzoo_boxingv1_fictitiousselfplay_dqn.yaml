env_args:
    env_name: boxing_v1
    env_type: pettingzoo
    num_envs: 1
    ram: True
    seed: 1122

agent_args:
    algorithm: DQN
    algorithm_spec:
        dueling: False
        replay_buffer_size: 1e5
        gamma: 0.99
        multi_step: 1
        target_update_interval: 1000 # updates skipped to update the target
        eps_start: 1.
        eps_final: 0.01
        eps_decay: 30000  # tune according to env

train_args:
    batch_size: 32
    max_episodes: 30000
    max_steps_per_episode: 10000
    train_start_frame: 0
    optimizer: adam
    learning_rate: 1e-4  # 1e-4 for small net
    device: gpu
    update_itr:  1 # iterations of updates per frame, 0~inf; <1 means several steps are skipped per update
    log_avg_window: 20 # average window length in logging
    log_interval: 20  # log print interval 
    save_interval: 2000 # episode interval to save models
    # net_architecture: 
        # hidden_dim_list: [1024, 1024, 1024, 1024]
        # hidden_dim_list: [64, 64, 64]  
        # hidden_activation: Tanh
        # output_activation: False
    # render: True
    # test: True
    # load_model_idx: 0/1  # in model/zoo, 0/0 is less trained model, 0/1 is well-trained model

    marl_method: fictitious_selfplay
    marl_spec:  # configurations for specific MARL method
        selfplay_score_delta: 60 # the score that current learning agent must beat its opponent to update opponent's policy
        trainable_agent_idx: 0   # the index of trainable agent, with its opponent delayed updated
        opponent_idx: 1          # the one to be updated from trainable agent
