env_args:
    env_name: SlimeVolley-v0
    env_type: slimevolley
    num_envs: 2
    against_baseline: False
    ram: True
    seed: 1122

agent_args:
    algorithm: PPO
    algorithm_spec:
        episodic_update: True  # as PPO is on-policy, it uses episodic update instead of update per timestep
        gamma: 0.99
        lambda: 0.95
        eps_clip: 0.2
        K_epoch: 4
        GAE: True  # generalized advantage estimation

train_args:
    max_episodes: 100000
    max_steps_per_episode: 10000
    train_start_frame: 0
    optimizer: adam
    learning_rate: 1e-4
    device: gpu
    update_itr: 1  # iterations of updates per frame, 0~inf; <1 means several steps are skipped per update
    log_avg_window: 40 # average window length in logging
    log_interval: 20  # log print interval 
    net_architecture:   
        policy:
            hidden_dim_list: [64, 64, 64]  
            hidden_activation: Tanh
            output_activation: Softmax
        value:
            hidden_dim_list: [64, 64, 64]  
            hidden_activation: Tanh
            output_activation: False

    # render: True
    # test: True
    # load_model_idx: 0/1  # in model/zoo, 0/0 is less trained model, 0/1 is well-trained model

    marl_method: selfplay
    marl_spec:  # configurations for specific MARL method
        selfplay_score_delta: 10. # the score that current learning agent must beat its opponent to update opponent's policy
        trainable_agent_idx: 1   # the index of trainable agent, with its opponent delayed updated
        opponent_idx: 0          # the one to be updated from trainable agent
