env_args:
  env_name: combat_plane_v1
  env_type: pettingzoo
  num_envs: 1
  ram: true
  seed: random
agent_args:
  algorithm: NashPPO
  algorithm_spec:
    episodic_update: True  # as PPO is on-policy, it uses episodic update instead of update per timestep
    gamma: 0.99
    lambda: 0.95
    eps_clip: 0.2
    K_epoch: 4
    GAE: True  # generalized advantage estimation
train_args:
  batch_size: 128
  max_episodes: 30000
  max_steps_per_episode: 10000
  train_start_frame: 0
  optimizer: adam
  learning_rate: 1e-4
  device: gpu
  update_itr: 0.1
  log_avg_window: 20
  log_interval: 20
  net_architecture:
    policy:
      hidden_dim_list:
      - 64
      - 64
      - 64
      - 64
      hidden_activation: ReLU
      output_activation: Softmax
    value:
      hidden_dim_list:
      - 64
      - 64
      - 64
      - 64
      hidden_activation: ReLU
      output_activation: false
  marl_method: nash_ppo
  marl_spec:
    global_state: true
