{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Space Response Oracle (PSRO)\n",
    "This tutorial demonstrates Policy Space Response Oracle (PSRO) on a zero-sum Markov game, which is more general than extensive-from game (more general than normal-form game). \n",
    "\n",
    "For the payoff matrix, row player is maximizer, coloumn player is minimizer.\n",
    "\n",
    "Reference: A unified game-theoretic approach to multiagent reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 12.0, (1,), float32) Discrete(3)\n",
      "[[0], [0]]\n",
      "[[5], [5]] [0.5610583525729109, -0.5610583525729109] [False, False]\n",
      "[[6], [6]] [-0.7261994566288021, 0.7261994566288021] [False, False]\n",
      "[[9], [9]] [-0.8460871060267345, 0.8460871060267345] [True, True]\n",
      "Average initial state value of oracle Nash equilibrium for the first player:  -0.29607107415447115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.\n",
      "  warn(\"Converting G to a CSC matrix; may take a while.\")\n",
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.\n",
      "  warn(\"Converting A to a CSC matrix; may take a while.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from mars.env.mdp import ArbitraryMDP, MDPWrapper\n",
    "import numpy as np\n",
    "\n",
    "num_states = 3\n",
    "num_actions_per_player = 3\n",
    "num_trans = 3\n",
    "\n",
    "env = MDPWrapper(ArbitraryMDP(num_states=num_states, num_actions_per_player=num_actions_per_player, num_trans=num_trans))\n",
    "trans_matrices = env.env.trans_prob_matrices # shape: [dim_transition, dim_state, dim_action (p1*p2), dim_state]\n",
    "reward_matrices = env.env.reward_matrices # shape: [dim_transition, dim_state, dim_action (p1*p2), dim_state]\n",
    "\n",
    "oracle_nash_v, oracle_nash_q, oracle_nash_strategies = env.NEsolver(verbose=False)\n",
    "oracle_v_star = oracle_nash_v[0]\n",
    "\n",
    "oracle_v_star = np.mean(oracle_v_star, axis=0)\n",
    "print(env.observation_space, env.action_space)\n",
    "# env.render()\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "while not np.any(done):\n",
    "    obs, r, done, _ = env.step([1,0])\n",
    "    print(obs, r, done)\n",
    "print('Average initial state value of oracle Nash equilibrium for the first player: ', oracle_v_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 2, 3)\n",
      "(3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(oracle_nash_strategies).shape)\n",
    "oracle_max_policy = np.array(oracle_nash_strategies)[:, :, 0, :]\n",
    "print(oracle_max_policy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy shape: \n",
      "(3, 3)\n",
      "(3, 3, 3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3, 3, 3)\n",
      "Q table shape: \n",
      "(3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3, 3, 3, 3)\n",
      "itr: 0, exploitability: 0.7608750685387422\n",
      "itr: 10, exploitability: 0.6376484629002362\n",
      "itr: 20, exploitability: 0.5712701680558167\n",
      "itr: 30, exploitability: 0.4885990951171036\n",
      "itr: 40, exploitability: 0.49823624270646377\n",
      "itr: 50, exploitability: 0.4414286175737034\n",
      "itr: 60, exploitability: 0.4497553799052441\n",
      "itr: 70, exploitability: 0.41266073665931896\n",
      "itr: 80, exploitability: 0.3776896500833102\n",
      "itr: 90, exploitability: 0.4090806504519\n",
      "itr: 100, exploitability: 0.36277023361898625\n",
      "itr: 110, exploitability: 0.3655436213761152\n",
      "itr: 120, exploitability: 0.36700103218322516\n",
      "itr: 130, exploitability: 0.3525810454654257\n",
      "itr: 140, exploitability: 0.3496366521265412\n",
      "itr: 150, exploitability: 0.3439277478444022\n",
      "itr: 160, exploitability: 0.3440173415810263\n",
      "itr: 170, exploitability: 0.3304426331108204\n",
      "itr: 180, exploitability: 0.33348757454717654\n",
      "itr: 190, exploitability: 0.329003741604775\n",
      "itr: 200, exploitability: 0.3327442730661414\n",
      "itr: 210, exploitability: 0.3332860946173302\n",
      "itr: 220, exploitability: 0.3290368576714508\n",
      "itr: 230, exploitability: 0.3324808551469078\n",
      "itr: 240, exploitability: 0.3256248224453515\n",
      "itr: 250, exploitability: 0.3231889958457552\n",
      "itr: 260, exploitability: 0.3234820125271402\n",
      "itr: 270, exploitability: 0.3198934491382734\n",
      "itr: 280, exploitability: 0.32049772034766694\n",
      "itr: 290, exploitability: 0.31677249601188456\n",
      "itr: 300, exploitability: 0.3197274592573817\n",
      "itr: 310, exploitability: 0.31796958312870655\n",
      "itr: 320, exploitability: 0.3186855295796431\n",
      "itr: 330, exploitability: 0.3180516545698335\n",
      "itr: 340, exploitability: 0.31531092598676186\n",
      "itr: 350, exploitability: 0.31685418594666326\n",
      "itr: 360, exploitability: 0.31700027771565126\n",
      "itr: 370, exploitability: 0.31356382155657386\n",
      "itr: 380, exploitability: 0.31048051698254947\n",
      "itr: 390, exploitability: 0.31453096344327236\n",
      "itr: 400, exploitability: 0.30960575097941695\n",
      "itr: 410, exploitability: 0.3092597505807445\n",
      "itr: 420, exploitability: 0.31219457521405686\n",
      "itr: 430, exploitability: 0.3105749201635285\n",
      "itr: 440, exploitability: 0.3095701844094966\n",
      "itr: 450, exploitability: 0.3076744809571157\n",
      "itr: 460, exploitability: 0.3067945896575814\n",
      "itr: 470, exploitability: 0.3083578301798755\n",
      "itr: 480, exploitability: 0.3068285723907146\n",
      "itr: 490, exploitability: 0.3060053633082102\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from mars.equilibrium_solver import NashEquilibriumECOSSolver, NashEquilibriumCVXPYSolver\n",
    "import itertools\n",
    "\n",
    "\n",
    "def sample_from_categorical(dist):\n",
    "    \"\"\"\n",
    "    sample once from a categorical distribution, return the entry index.\n",
    "    dist: should be a list or array of probabilities for a categorical distribution\n",
    "    \"\"\"\n",
    "    sample_id = np.argmax(np.random.multinomial(1, dist))\n",
    "    sample_prob = dist[sample_id]\n",
    "    return sample_id, sample_prob\n",
    "\n",
    "def unified_state(s):\n",
    "    unified_s = s[0]%num_states\n",
    "    return unified_s\n",
    "\n",
    "def create_expand_policy(zero_ini=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim, action_dim],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    policies = []\n",
    "    intial_dim = [num_states]\n",
    "    for i in range(num_trans):\n",
    "        if zero_ini:\n",
    "            policy =  (1./num_actions_per_player) * np.zeros((*intial_dim, num_actions_per_player))\n",
    "        else:\n",
    "            policy =  (1./num_actions_per_player) * np.ones((*intial_dim, num_actions_per_player))\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_actions_per_player, num_actions_per_player, num_states])\n",
    "        policies.append(policy)\n",
    "\n",
    "    return policies\n",
    "\n",
    "def create_expand_value():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim],\n",
    "        ...\n",
    "    ]    \n",
    "    \"\"\"\n",
    "    values = []\n",
    "    intial_dim = [num_states]\n",
    "    for i in range(num_trans):\n",
    "        value =  (1./num_actions_per_player) * np.ones(intial_dim)\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_actions_per_player, num_actions_per_player, num_states])\n",
    "        values.append(value)\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "def create_expand_Q(zero_ini=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim, action_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim, action_dim, action_dim],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    intial_dim = [num_states, num_actions_per_player, num_actions_per_player]\n",
    "    for i in range(num_trans):\n",
    "        if zero_ini:\n",
    "            value =  (1./num_actions_per_player) * np.zeros(intial_dim)\n",
    "        else:\n",
    "            value =  (1./num_actions_per_player) * np.ones(intial_dim)\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_states, num_actions_per_player, num_actions_per_player])\n",
    "        values.append(value)\n",
    "\n",
    "    return values\n",
    "\n",
    "def get_posterior_policy(policy_set, meta_prior, side):\n",
    "    posterior_policy = create_expand_policy(zero_ini=True)  # zero initiate to sum later\n",
    "    num_policies = len(policy_set)\n",
    "\n",
    "    def get_all_traj_for_transition(num_transition, single_side=False):  # TESTED\n",
    "        \"\"\"\n",
    "        step = 1:\n",
    "        [\n",
    "            [s0,a0,b0], [s0,a0,b1],...\n",
    "        ]\n",
    "        step = 2:\n",
    "        [\n",
    "            [s0,a0,b0,s1,a1,b1], [s1,a0,b0,s1,a1,b1],\n",
    "        ]\n",
    "        ...\n",
    "        \"\"\"\n",
    "        if single_side: # lack of one action\n",
    "            ranges = (num_transition-1)*[range(num_states), range(num_actions_per_player), range(num_actions_per_player)] + [range(num_states), range(num_actions_per_player)]\n",
    "        else:\n",
    "            ranges = num_transition*[range(num_states), range(num_actions_per_player), range(num_actions_per_player)]\n",
    "        all_possible_trajs = list(itertools.product(*ranges))\n",
    "\n",
    "        return all_possible_trajs\n",
    "\n",
    "    \n",
    "    def split_traj(traj, side):  # TESTED\n",
    "        \"\"\"\n",
    "        s0, a0, b0, s1, a1, b1, ... sn-1, an-1, bn-1, sn ->\n",
    "        side = max:\n",
    "        [], [s0,a0], [s0,a0,b0,s1,a1], [s0,a0,b0,s1,a1,b1,s2,a2], ...\n",
    "        side = min:\n",
    "        [s0,b0], [s0,a0,b0,s1,b1], [s0,a0,b0,s1,a1,b1,s2,b2], ...\n",
    "        \"\"\"\n",
    "        split_trajs = []\n",
    "        i=2\n",
    "        while i<=len(traj):  # max length same as original traj but lack of one action (due to the side choice)\n",
    "            if side == 'max':\n",
    "                split_trajs.append(traj[:i])\n",
    "            else:\n",
    "                split_trajs.append(np.concatenate([traj[:i-1], [traj[i]]]))\n",
    "            i = i+3 # 3 due to (s,a,b)\n",
    "\n",
    "        return split_trajs\n",
    "\n",
    "\n",
    "    def get_likelihood(policy, side):\n",
    "        likelihoods = create_expand_Q() # (s,a,b) rather than (s,a) or (s,b)\n",
    "        for i in range(num_trans):\n",
    "            # number of transitions is i+1\n",
    "            # each transition is (s,a,b)\n",
    "            # each trajectory of transitions is [(s1,a1,b1), (s2,a2,b2), ...] of length num_transition\n",
    "            trajs = get_all_traj_for_transition(num_transition=i+1)   \n",
    "            for traj in trajs:\n",
    "                # split trajectories to be a powerset, \n",
    "                # and return only one-side action for the last transition,\n",
    "                # b.c. in likelihood there is only one-side probability\n",
    "                trajs_list = split_traj(traj, side) \n",
    "                likelihood = 1.\n",
    "                for j, t in enumerate(trajs_list):  # this product corresponding to likelihood for h step (not h-1)\n",
    "                    likelihood = likelihood*policy[j][tuple(t)]  # scalar\n",
    "                likelihoods[i][tuple(traj)] = likelihood  # needs to know this likelihood stored for transition i should be used for posterior transition i+1\n",
    "        return likelihoods\n",
    "\n",
    "    likelihoods = []\n",
    "    for pi_i in policy_set:\n",
    "        likelihoods.append(get_likelihood(pi_i, side))\n",
    "\n",
    "    def get_denoms(likelihoods, rho):\n",
    "        denoms = create_expand_Q(zero_ini=True)\n",
    "        for j, rho_j in enumerate(rho):  # over policy in policy set\n",
    "            for i in range(len(likelihoods[j])):  # over transition\n",
    "                trajs = get_all_traj_for_transition(i+1) \n",
    "                for traj in trajs:\n",
    "                    denoms[i][tuple(traj)] += rho_j*likelihoods[j][i][tuple(traj)]\n",
    "\n",
    "        return denoms\n",
    "\n",
    "    denoms = get_denoms(likelihoods, meta_prior)\n",
    "\n",
    "    for pi_i, rho_i, likelihood_i, in zip(policy_set, meta_prior, likelihoods):  # loop over policy set\n",
    "\n",
    "        for i, (p,) in enumerate(zip(pi_i)): # loop over transition\n",
    "            taus = get_all_traj_for_transition(i+1, single_side=True)\n",
    "            for tau in taus:\n",
    "                if i==0:\n",
    "                    posterior_policy[i][tuple(tau)] += p[tuple(tau)]*rho_i # posterior = prior for the first transition\n",
    "                else:\n",
    "                    tau_h_1 = tau[:-2]  # remove the last (s,a/b) for the likelihood and denominator since they take trajectory until (h-1)\n",
    "                    # import pdb; pdb.set_trace()\n",
    "                    posterior_policy[i][tuple(tau)] += p[tuple(tau)]*rho_i*likelihood_i[i-1][tuple(tau_h_1)]/denoms[i-1][tuple(tau_h_1)]\n",
    "\n",
    "    return posterior_policy\n",
    "\n",
    "def broadcast_shape(input, output_shape):\n",
    "    \"\"\" broadcast input to have the same shape as output_to_be \"\"\"\n",
    "    len_input_shape = len(np.array(input).shape)\n",
    "    incrs_shape = output_shape[:-len_input_shape]+len_input_shape*(1,)\n",
    "    output = np.tile(input, incrs_shape)\n",
    "    return output\n",
    "\n",
    "def get_best_response_policy(player):\n",
    "    given_policy = get_posterior_policy(player['policy_set'], player['meta_strategy'], player['side'])\n",
    "    # print(given_policy)\n",
    "    br_policy = create_expand_policy()  # best response policy\n",
    "    br_v = create_expand_value()\n",
    "    br_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "\n",
    "        expand_rm = broadcast_shape(rm_, br_q[i].shape+(num_states,))  # broadcast the shape of reward matrix to be expand Q shape plus additional state dimension\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            br_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = br_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            br_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        if player['side'] == 'max':\n",
    "            mu_dot_q = np.einsum('...i, ...ij->...j', given_policy[i], br_q[i])\n",
    "            arg_id = np.argmin(mu_dot_q, axis=-1)  # min player takes minimum as best response against max player's policy\n",
    "            br_v[i] = np.min(mu_dot_q, axis=-1)\n",
    "        else:\n",
    "            q_dot_nu = np.einsum('...ij, ...j->...i', br_q[i], given_policy[i])\n",
    "            arg_id = np.argmax(q_dot_nu, axis=-1)   # vice versa    \n",
    "            br_v[i] = np.max(q_dot_nu, axis=-1)     \n",
    "        \n",
    "        br_policy[i] = (np.arange(num_actions_per_player) == arg_id[...,None]).astype(int)  # from extreme (min/max) idx to one-hot simplex\n",
    "        # print(br_policy[i].shape, br_q[i].shape)\n",
    "\n",
    "    return br_policy\n",
    "\n",
    "def best_response_value(trans_prob_matrices, reward_matrices, policy, num_actions, side='max'):\n",
    "    \"\"\"\n",
    "    Formulas for calculating best response values:\n",
    "    1. Nash strategies: (\\pi_a^*, \\pi_b^*) = \\min \\max Q(s,a,b), \n",
    "        where Q(s,a,b) = r(s,a,b) + \\gamma \\min \\max Q(s',a',b') (this is the definition of Nash Q-value);\n",
    "    2. Best response (of max player) value: Br V(s) = \\min_b \\pi(s,a) Br Q(s,a,b)  (Br Q is the oracle best response Q value)\n",
    "    \"\"\"\n",
    "    br_v = create_expand_value()\n",
    "    br_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        expand_rm = broadcast_shape(rm_, br_q[i].shape+(num_states,))\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            br_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = br_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            br_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        if side == 'max':\n",
    "            mu_dot_q = np.einsum('...i, ...ij->...j', policy[i], br_q[i])\n",
    "            br_v[i] = np.min(mu_dot_q, axis=-1)\n",
    "        else:\n",
    "            q_dot_nu = np.einsum('...ij, ...j->...i', br_q[i], policy[i])\n",
    "            br_v[i] = np.max(q_dot_nu, axis=-1)         \n",
    "\n",
    "    avg_init_br_v = -np.mean(br_v[0])  # average best response value of initial states; minus for making it positive\n",
    "    return avg_init_br_v\n",
    "\n",
    "def get_best_response_value(player):\n",
    "    # wrong! this is the weighted average (by meta strategy) of exploitability for each policy in policy set\n",
    "    # per_policy_exploits = []\n",
    "    # for p in policy_set:\n",
    "    #     per_policy_exploits.append(best_response_value(trans_matrices, reward_matrices, p, num_actions_per_player, player['side']))\n",
    "    # per_policy_exploits = np.array(per_policy_exploits)\n",
    "    # print(per_policy_exploits)\n",
    "    # exploitability = per_policy_exploits @ meta_strategy  # average over policy set weighted by meta strategy\n",
    "\n",
    "    # should still get the posterior policy as the mixture policy to exploit!\n",
    "    posterior_policy = get_posterior_policy(player['policy_set'], player['meta_strategy'], player['side'])\n",
    "    exploitability = best_response_value(trans_matrices, reward_matrices, posterior_policy, num_actions_per_player, player['side'])\n",
    "\n",
    "    return exploitability\n",
    "\n",
    "def policy_against_policy_value(trans_prob_matrices, reward_matrices, max_policy, min_policy, num_actions):\n",
    "    \"\"\"\n",
    "    Formulas for calculating best response values:\n",
    "    1. Nash strategies: (\\pi_a^*, \\pi_b^*) = \\min \\max Q(s,a,b), \n",
    "        where Q(s,a,b) = r(s,a,b) + \\gamma \\min \\max Q(s',a',b') (this is the definition of Nash Q-value);\n",
    "    2. Best response (of max player) value: Br V(s) = \\min_b \\pi(s,a) Br Q(s,a,b)  (Br Q is the oracle best response Q value)\n",
    "    \"\"\"\n",
    "    eval_v = create_expand_value()\n",
    "    eval_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        expand_rm = broadcast_shape(rm_, eval_q[i].shape+(num_states,))\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            eval_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = eval_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            eval_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        mu_dot_q = np.einsum('...i, ...ij->...j', max_policy[i], eval_q[i])\n",
    "        mu_dot_q_dot_nu = np.einsum('...kj, ...kj->...k', mu_dot_q, min_policy[i])\n",
    "        eval_v[i] = mu_dot_q_dot_nu    \n",
    "\n",
    "    avg_init_eval_v = np.mean(eval_v[0])  # average best response value of initial states; minus for making it positive\n",
    "    return avg_init_eval_v\n",
    "\n",
    "def update_matrix(matrix, side, row):\n",
    "    \"\"\"\n",
    "    Adding a new policy to the league will add a row or column to the present evaluation matrix, after it has\n",
    "    been evaluated against all policies in the opponent's league. Whether adding a row or column depends on whether\n",
    "    it is the row player (idx=0) or column player (idx=1). \n",
    "    For example, if current evaluation matrix is:\n",
    "    2  1\n",
    "    -1 3\n",
    "    After adding a new policy for row player with evaluated average episode reward (a,b) against current two policies in \n",
    "    the league of the column player, it gives: \n",
    "    it gives:\n",
    "    2   1\n",
    "    -1  3\n",
    "    a   b    \n",
    "    \"\"\" \n",
    "    if side == 'max': # for row player\n",
    "        if matrix.shape[0] == 0: \n",
    "            matrix = np.array([row])\n",
    "        else:  # add row\n",
    "            matrix = np.vstack([matrix, row])\n",
    "    else: # for column player\n",
    "        if matrix.shape[0] == 0:  # the first update\n",
    "            matrix = np.array([row]) \n",
    "        else:  # add column\n",
    "            matrix=np.hstack([matrix, np.array([row]).T])\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def sample_policy(policy_set, dist):\n",
    "    policy_id, _ = sample_from_categorical(dist)\n",
    "    policy = policy_set[policy_id]   \n",
    "    return policy \n",
    "\n",
    "# test_policy = create_expand_policy() \n",
    "# get_best_response(test_policy)\n",
    "\n",
    "def psro(env, save_path, solve_episodes = 1000):\n",
    "    evaluation_matrix = np.array([[0]])\n",
    "    ini_max_policy = create_expand_policy()\n",
    "    ini_min_policy = create_expand_policy()\n",
    "    max_player = {\n",
    "        'side': 'max',\n",
    "        'policy_set': [ini_max_policy],\n",
    "        'meta_strategy': np.array([1.])\n",
    "    }\n",
    "    min_player = {\n",
    "        'side': 'min',\n",
    "        'policy_set': [ini_min_policy],\n",
    "        'meta_strategy': np.array([1.])\n",
    "    }\n",
    "\n",
    "    max_side_q_table = create_expand_Q()\n",
    "    min_side_q_table = create_expand_Q()\n",
    " \n",
    "    print('policy shape: ')\n",
    "    for p in ini_max_policy:\n",
    "        print(p.shape)\n",
    "    print('Q table shape: ')\n",
    "    for q in max_side_q_table:\n",
    "        print(q.shape)\n",
    "\n",
    "    exploitability_records = []\n",
    "\n",
    "    for i in range(solve_episodes):\n",
    "        \n",
    "        if i % 2 == 0:  # update min player side\n",
    "            new_policy = get_best_response_policy(max_player)  # get best response against the max player\n",
    "            min_player['policy_set'].append(new_policy) # add new policy to policy set to form mixture\n",
    "\n",
    "            # update evaluation matrix\n",
    "            eval_scores = []\n",
    "            for p in max_player['policy_set']:\n",
    "                score = policy_against_policy_value(trans_matrices, reward_matrices, max_policy=p, min_policy=new_policy, num_actions=num_actions_per_player)\n",
    "                eval_scores.append(score)  # score always from the max player's view\n",
    "            evaluation_matrix = update_matrix(evaluation_matrix, 'min', eval_scores)\n",
    "\n",
    "        else: \n",
    "            new_policy = get_best_response_policy(min_player) # get best response against the min player\n",
    "            max_player['policy_set'].append(new_policy) # add new policy to policy set to form mixture\n",
    "\n",
    "            # update evaluation matrix\n",
    "            eval_scores = []\n",
    "            for p in min_player['policy_set']:\n",
    "                score = policy_against_policy_value(trans_matrices, reward_matrices, max_policy=new_policy, min_policy=p, num_actions=num_actions_per_player)\n",
    "                eval_scores.append(score)  # score always from the max player's view\n",
    "            evaluation_matrix = update_matrix(evaluation_matrix, 'max', eval_scores)\n",
    "\n",
    "        if len(max_player['policy_set']) * len(min_player['policy_set']) >= 2:  # enough policies to get Nash\n",
    "            (max_player['meta_strategy'], min_player['meta_strategy']), _ = NashEquilibriumCVXPYSolver(evaluation_matrix)\n",
    "        else: # uniform for initialization only\n",
    "            max_policies = len(max_player['policy_set'])\n",
    "            max_player['meta_strategy'] = 1./max_policies*np.ones(max_policies)\n",
    "            min_policies = len(min_player['policy_set'])\n",
    "            min_player['meta_strategy'] = 1./min_policies*np.ones(min_policies)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            exploitability = get_best_response_value(max_player)  # best response of the max player\n",
    "            print(f'itr: {i}, exploitability: {exploitability}', )\n",
    "            exploitability_records.append(exploitability)\n",
    "            np.save(save_path, exploitability_records)\n",
    "\n",
    "save_path = 'psro_exp.npy'\n",
    "psro(env, save_path, solve_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f362d3c3320>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "exp_data = np.load(save_path)\n",
    "plt.title('PSRO')\n",
    "plt.xlabel('iter(x10)')\n",
    "plt.ylabel('exploitability')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.plot(exp_data+oracle_v_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4, 5, 6, 7]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "num_states = 3\n",
    "\n",
    "\n",
    "def get_all_traj_for_transition(transition_step):\n",
    "    \"\"\"\n",
    "    step = 1:\n",
    "    [\n",
    "        [s0,a0,b0], [s0,a0,b1],...\n",
    "    ]\n",
    "    step = 2:\n",
    "    [\n",
    "        [s0,a0,b0,s1,a1,b1], [s1,a0,b0,s1,a1,b1],\n",
    "    ]\n",
    "    ...\n",
    "    \"\"\"\n",
    "    ranges = transition_step*[range(num_states), range(num_actions_per_player), range(num_actions_per_player)]\n",
    "    all_possible_trajs = list(itertools.product(*ranges))\n",
    "\n",
    "    return all_possible_trajs\n",
    "\n",
    "def split_traj(traj, side):\n",
    "    \"\"\"\n",
    "    s0, a0, b0, s1, a1, b1, ... sn-1, an-1, bn-1, sn ->\n",
    "    side = max:\n",
    "    [], [s0,a0], [s0,a0,b0,s1,a1], [s0,a0,b0,s1,a1,b1,s2,a2], ...\n",
    "    side = min:\n",
    "    [s0,b0], [s0,a0,b0,s1,b1], [s0,a0,b0,s1,a1,b1,s2,b2], ...\n",
    "    \"\"\"\n",
    "    split_trajs = []\n",
    "    i=2\n",
    "    while i<=len(traj):  # shorter than original traj\n",
    "        if side == 'max':\n",
    "            split_trajs.append(traj[:i])\n",
    "        else:\n",
    "            split_trajs.append(np.concatenate([traj[:i-1], [traj[i]]]))\n",
    "        i = i+3 # 3 due to (s,a,b)\n",
    "\n",
    "    return split_trajs\n",
    "\n",
    "\n",
    "# get_all_traj_for_transition(2)\n",
    "split_traj(traj=[0,1,2,3,4,5,6,7,8], side='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28c6861e59928cb790236f7047915368f37afc12f670e78fd0101a6f825a02b1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('x': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
