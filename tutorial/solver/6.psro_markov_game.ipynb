{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Space Response Oracle (PSRO)\n",
    "This tutorial demonstrates Policy Space Response Oracle (PSRO) on a zero-sum Markov game, which is more general than extensive-from game (more general than normal-form game). \n",
    "\n",
    "For the payoff matrix, row player is maximizer, coloumn player is minimizer.\n",
    "\n",
    "Reference: A unified game-theoretic approach to multiagent reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 12.0, (1,), float32) Discrete(3)\n",
      "[[0], [0]]\n",
      "[[5], [5]] [0.5610583525729109, -0.5610583525729109] [False, False]\n",
      "[[6], [6]] [-0.7261994566288021, 0.7261994566288021] [False, False]\n",
      "[[9], [9]] [-0.8460871060267345, 0.8460871060267345] [True, True]\n",
      "Average initial state value of oracle Nash equilibrium for the first player:  -0.29607107415447115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.\n",
      "  warn(\"Converting G to a CSC matrix; may take a while.\")\n",
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.\n",
      "  warn(\"Converting A to a CSC matrix; may take a while.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from mars.env.mdp import ArbitraryMDP, MDPWrapper\n",
    "import numpy as np\n",
    "\n",
    "num_states = 3\n",
    "num_actions_per_player = 3\n",
    "num_trans = 3\n",
    "\n",
    "env = MDPWrapper(ArbitraryMDP(num_states=num_states, num_actions_per_player=num_actions_per_player, num_trans=num_trans))\n",
    "trans_matrices = env.env.trans_prob_matrices # shape: [dim_transition, dim_state, dim_action (p1*p2), dim_state]\n",
    "reward_matrices = env.env.reward_matrices # shape: [dim_transition, dim_state, dim_action (p1*p2), dim_state]\n",
    "\n",
    "oracle_nash_v, oracle_nash_q, oracle_nash_strategies = env.NEsolver(verbose=False)\n",
    "oracle_v_star = oracle_nash_v[0]\n",
    "\n",
    "oracle_v_star = np.mean(oracle_v_star, axis=0)\n",
    "print(env.observation_space, env.action_space)\n",
    "# env.render()\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "while not np.any(done):\n",
    "    obs, r, done, _ = env.step([1,0])\n",
    "    print(obs, r, done)\n",
    "print('Average initial state value of oracle Nash equilibrium for the first player: ', oracle_v_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 2, 3)\n",
      "(3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(oracle_nash_strategies).shape)\n",
    "oracle_max_policy = np.array(oracle_nash_strategies)[:, :, 0, :]\n",
    "print(oracle_max_policy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy shape: \n",
      "(3, 3)\n",
      "(3, 3, 3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3, 3, 3)\n",
      "Q table shape: \n",
      "(3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3, 3, 3, 3)\n",
      "itr: 0, exploitability: 0.7608750685387422\n",
      "itr: 10, exploitability: 0.6376484629002362\n",
      "itr: 20, exploitability: 0.5712701680558167\n",
      "itr: 30, exploitability: 0.4885990951171036\n",
      "itr: 40, exploitability: 0.49823624270646377\n",
      "itr: 50, exploitability: 0.4414286175737034\n",
      "itr: 60, exploitability: 0.4497553799052441\n",
      "itr: 70, exploitability: 0.41266073665931896\n",
      "itr: 80, exploitability: 0.3776896500833102\n",
      "itr: 90, exploitability: 0.4090806504519\n",
      "itr: 100, exploitability: 0.36277023361898625\n",
      "itr: 110, exploitability: 0.3655436213761152\n",
      "itr: 120, exploitability: 0.36700103218322516\n",
      "itr: 130, exploitability: 0.3525810454654257\n",
      "itr: 140, exploitability: 0.3496366521265412\n",
      "itr: 150, exploitability: 0.3439277478444022\n",
      "itr: 160, exploitability: 0.3440173415810263\n",
      "itr: 170, exploitability: 0.3304426331108204\n",
      "itr: 180, exploitability: 0.33348757454717654\n",
      "itr: 190, exploitability: 0.329003741604775\n",
      "itr: 200, exploitability: 0.3327442730661414\n",
      "itr: 210, exploitability: 0.3332860946173302\n",
      "itr: 220, exploitability: 0.3290368576714508\n",
      "itr: 230, exploitability: 0.3324808551469078\n",
      "itr: 240, exploitability: 0.3256248224453515\n",
      "itr: 250, exploitability: 0.3231889958457552\n",
      "itr: 260, exploitability: 0.3234820125271402\n",
      "itr: 270, exploitability: 0.3198934491382734\n",
      "itr: 280, exploitability: 0.32049772034766694\n",
      "itr: 290, exploitability: 0.31677249601188456\n",
      "itr: 300, exploitability: 0.3197274592573817\n",
      "itr: 310, exploitability: 0.31796958312870655\n",
      "itr: 320, exploitability: 0.3186855295796431\n",
      "itr: 330, exploitability: 0.3180516545698335\n",
      "itr: 340, exploitability: 0.31531092598676186\n",
      "itr: 350, exploitability: 0.31685418594666326\n",
      "itr: 360, exploitability: 0.31700027771565126\n",
      "itr: 370, exploitability: 0.31356382155657386\n",
      "itr: 380, exploitability: 0.31048051698254947\n",
      "itr: 390, exploitability: 0.31453096344327236\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from mars.equilibrium_solver import NashEquilibriumECOSSolver, NashEquilibriumCVXPYSolver\n",
    "import itertools\n",
    "\n",
    "\n",
    "def sample_from_categorical(dist):\n",
    "    \"\"\"\n",
    "    sample once from a categorical distribution, return the entry index.\n",
    "    dist: should be a list or array of probabilities for a categorical distribution\n",
    "    \"\"\"\n",
    "    sample_id = np.argmax(np.random.multinomial(1, dist))\n",
    "    sample_prob = dist[sample_id]\n",
    "    return sample_id, sample_prob\n",
    "\n",
    "def unified_state(s):\n",
    "    unified_s = s[0]%num_states\n",
    "    return unified_s\n",
    "\n",
    "def create_expand_policy(zero_ini=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim, action_dim],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    policies = []\n",
    "    intial_dim = [num_states]\n",
    "    for i in range(num_trans):\n",
    "        if zero_ini:\n",
    "            policy =  (1./num_actions_per_player) * np.zeros((*intial_dim, num_actions_per_player))\n",
    "        else:\n",
    "            policy =  (1./num_actions_per_player) * np.ones((*intial_dim, num_actions_per_player))\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_actions_per_player, num_actions_per_player, num_states])\n",
    "        policies.append(policy)\n",
    "\n",
    "    return policies\n",
    "\n",
    "def create_expand_value():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim],\n",
    "        ...\n",
    "    ]    \n",
    "    \"\"\"\n",
    "    values = []\n",
    "    intial_dim = [num_states]\n",
    "    for i in range(num_trans):\n",
    "        value =  (1./num_actions_per_player) * np.ones(intial_dim)\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_actions_per_player, num_actions_per_player, num_states])\n",
    "        values.append(value)\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "def create_expand_Q(zero_ini=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim, action_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim, action_dim, action_dim],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    intial_dim = [num_states, num_actions_per_player, num_actions_per_player]\n",
    "    for i in range(num_trans):\n",
    "        if zero_ini:\n",
    "            value =  (1./num_actions_per_player) * np.zeros(intial_dim)\n",
    "        else:\n",
    "            value =  (1./num_actions_per_player) * np.ones(intial_dim)\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_states, num_actions_per_player, num_actions_per_player])\n",
    "        values.append(value)\n",
    "\n",
    "    return values\n",
    "\n",
    "def get_posterior_policy(policy_set, meta_prior, side):\n",
    "    posterior_policy = create_expand_policy(zero_ini=True)  # zero initiate to sum later\n",
    "    num_policies = len(policy_set)\n",
    "\n",
    "    def get_all_traj_for_transition(num_transition, single_side=False):  # TESTED\n",
    "        \"\"\"\n",
    "        step = 1:\n",
    "        [\n",
    "            [s0,a0,b0], [s0,a0,b1],...\n",
    "        ]\n",
    "        step = 2:\n",
    "        [\n",
    "            [s0,a0,b0,s1,a1,b1], [s1,a0,b0,s1,a1,b1],\n",
    "        ]\n",
    "        ...\n",
    "        \"\"\"\n",
    "        if single_side: # lack of one action\n",
    "            ranges = (num_transition-1)*[range(num_states), range(num_actions_per_player), range(num_actions_per_player)] + [range(num_states), range(num_actions_per_player)]\n",
    "        else:\n",
    "            ranges = num_transition*[range(num_states), range(num_actions_per_player), range(num_actions_per_player)]\n",
    "        all_possible_trajs = list(itertools.product(*ranges))\n",
    "\n",
    "        return all_possible_trajs\n",
    "\n",
    "    \n",
    "    def split_traj(traj, side):  # TESTED\n",
    "        \"\"\"\n",
    "        s0, a0, b0, s1, a1, b1, ... sn-1, an-1, bn-1, sn ->\n",
    "        side = max:\n",
    "        [], [s0,a0], [s0,a0,b0,s1,a1], [s0,a0,b0,s1,a1,b1,s2,a2], ...\n",
    "        side = min:\n",
    "        [s0,b0], [s0,a0,b0,s1,b1], [s0,a0,b0,s1,a1,b1,s2,b2], ...\n",
    "        \"\"\"\n",
    "        split_trajs = []\n",
    "        i=2\n",
    "        while i<=len(traj):  # max length same as original traj but lack of one action (due to the side choice)\n",
    "            if side == 'max':\n",
    "                split_trajs.append(traj[:i])\n",
    "            else:\n",
    "                split_trajs.append(np.concatenate([traj[:i-1], [traj[i]]]))\n",
    "            i = i+3 # 3 due to (s,a,b)\n",
    "\n",
    "        return split_trajs\n",
    "\n",
    "\n",
    "    def get_likelihood(policy, side):\n",
    "        likelihoods = create_expand_Q() # (s,a,b) rather than (s,a) or (s,b)\n",
    "        for i in range(num_trans):\n",
    "            # number of transitions is i+1\n",
    "            # each transition is (s,a,b)\n",
    "            # each trajectory of transitions is [(s1,a1,b1), (s2,a2,b2), ...] of length num_transition\n",
    "            trajs = get_all_traj_for_transition(num_transition=i+1)   \n",
    "            for traj in trajs:\n",
    "                # split trajectories to be a powerset, \n",
    "                # and return only one-side action for the last transition,\n",
    "                # b.c. in likelihood there is only one-side probability\n",
    "                trajs_list = split_traj(traj, side) \n",
    "                likelihood = 1.\n",
    "                for j, t in enumerate(trajs_list):  # this product corresponding to likelihood for h step (not h-1)\n",
    "                    likelihood = likelihood*policy[j][tuple(t)]  # scalar\n",
    "                likelihoods[i][tuple(traj)] = likelihood  # needs to know this likelihood stored for transition i should be used for posterior transition i+1\n",
    "        return likelihoods\n",
    "\n",
    "    likelihoods = []\n",
    "    for pi_i in policy_set:\n",
    "        likelihoods.append(get_likelihood(pi_i, side))\n",
    "\n",
    "    def get_denoms(likelihoods, rho):\n",
    "        denoms = create_expand_Q(zero_ini=True)\n",
    "        for j, rho_j in enumerate(rho):  # over policy in policy set\n",
    "            for i in range(len(likelihoods[j])):  # over transition\n",
    "                trajs = get_all_traj_for_transition(i+1) \n",
    "                for traj in trajs:\n",
    "                    denoms[i][tuple(traj)] += rho_j*likelihoods[j][i][tuple(traj)]\n",
    "\n",
    "        return denoms\n",
    "\n",
    "    denoms = get_denoms(likelihoods, meta_prior)\n",
    "\n",
    "    for pi_i, rho_i, likelihood_i, in zip(policy_set, meta_prior, likelihoods):  # loop over policy set\n",
    "\n",
    "        for i, (p,) in enumerate(zip(pi_i)): # loop over transition\n",
    "            taus = get_all_traj_for_transition(i+1, single_side=True)\n",
    "            for tau in taus:\n",
    "                if i==0:\n",
    "                    posterior_policy[i][tuple(tau)] += p[tuple(tau)]*rho_i # posterior = prior for the first transition\n",
    "                else:\n",
    "                    tau_h_1 = tau[:-2]  # remove the last (s,a/b) for the likelihood and denominator since they take trajectory until (h-1)\n",
    "                    # import pdb; pdb.set_trace()\n",
    "                    posterior_policy[i][tuple(tau)] += p[tuple(tau)]*rho_i*likelihood_i[i-1][tuple(tau_h_1)]/denoms[i-1][tuple(tau_h_1)]\n",
    "\n",
    "    return posterior_policy\n",
    "\n",
    "def broadcast_shape(input, output_shape):\n",
    "    \"\"\" broadcast input to have the same shape as output_to_be \"\"\"\n",
    "    len_input_shape = len(np.array(input).shape)\n",
    "    incrs_shape = output_shape[:-len_input_shape]+len_input_shape*(1,)\n",
    "    output = np.tile(input, incrs_shape)\n",
    "    return output\n",
    "\n",
    "def get_best_response_policy(player):\n",
    "    given_policy = get_posterior_policy(player['policy_set'], player['meta_strategy'], player['side'])\n",
    "    # print(given_policy)\n",
    "    br_policy = create_expand_policy()  # best response policy\n",
    "    br_v = create_expand_value()\n",
    "    br_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "\n",
    "        expand_rm = broadcast_shape(rm_, br_q[i].shape+(num_states,))  # broadcast the shape of reward matrix to be expand Q shape plus additional state dimension\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            br_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = br_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            br_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        if player['side'] == 'max':\n",
    "            mu_dot_q = np.einsum('...i, ...ij->...j', given_policy[i], br_q[i])\n",
    "            arg_id = np.argmin(mu_dot_q, axis=-1)  # min player takes minimum as best response against max player's policy\n",
    "            br_v[i] = np.min(mu_dot_q, axis=-1)\n",
    "        else:\n",
    "            q_dot_nu = np.einsum('...ij, ...j->...i', br_q[i], given_policy[i])\n",
    "            arg_id = np.argmax(q_dot_nu, axis=-1)   # vice versa    \n",
    "            br_v[i] = np.max(q_dot_nu, axis=-1)     \n",
    "        \n",
    "        br_policy[i] = (np.arange(num_actions_per_player) == arg_id[...,None]).astype(int)  # from extreme (min/max) idx to one-hot simplex\n",
    "        # print(br_policy[i].shape, br_q[i].shape)\n",
    "\n",
    "    return br_policy\n",
    "\n",
    "def best_response_value(trans_prob_matrices, reward_matrices, policy, num_actions, side='max'):\n",
    "    \"\"\"\n",
    "    Formulas for calculating best response values:\n",
    "    1. Nash strategies: (\\pi_a^*, \\pi_b^*) = \\min \\max Q(s,a,b), \n",
    "        where Q(s,a,b) = r(s,a,b) + \\gamma \\min \\max Q(s',a',b') (this is the definition of Nash Q-value);\n",
    "    2. Best response (of max player) value: Br V(s) = \\min_b \\pi(s,a) Br Q(s,a,b)  (Br Q is the oracle best response Q value)\n",
    "    \"\"\"\n",
    "    br_v = create_expand_value()\n",
    "    br_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        expand_rm = broadcast_shape(rm_, br_q[i].shape+(num_states,))\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            br_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = br_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            br_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        if side == 'max':\n",
    "            mu_dot_q = np.einsum('...i, ...ij->...j', policy[i], br_q[i])\n",
    "            br_v[i] = np.min(mu_dot_q, axis=-1)\n",
    "        else:\n",
    "            q_dot_nu = np.einsum('...ij, ...j->...i', br_q[i], policy[i])\n",
    "            br_v[i] = np.max(q_dot_nu, axis=-1)         \n",
    "\n",
    "    avg_init_br_v = -np.mean(br_v[0])  # average best response value of initial states; minus for making it positive\n",
    "    return avg_init_br_v\n",
    "\n",
    "def get_best_response_value(player):\n",
    "    # wrong! this is the weighted average (by meta strategy) of exploitability for each policy in policy set\n",
    "    # per_policy_exploits = []\n",
    "    # for p in policy_set:\n",
    "    #     per_policy_exploits.append(best_response_value(trans_matrices, reward_matrices, p, num_actions_per_player, player['side']))\n",
    "    # per_policy_exploits = np.array(per_policy_exploits)\n",
    "    # print(per_policy_exploits)\n",
    "    # exploitability = per_policy_exploits @ meta_strategy  # average over policy set weighted by meta strategy\n",
    "\n",
    "    # should still get the posterior policy as the mixture policy to exploit!\n",
    "    posterior_policy = get_posterior_policy(player['policy_set'], player['meta_strategy'], player['side'])\n",
    "    exploitability = best_response_value(trans_matrices, reward_matrices, posterior_policy, num_actions_per_player, player['side'])\n",
    "\n",
    "    return exploitability\n",
    "\n",
    "def policy_against_policy_value(trans_prob_matrices, reward_matrices, max_policy, min_policy, num_actions):\n",
    "    \"\"\"\n",
    "    Formulas for calculating best response values:\n",
    "    1. Nash strategies: (\\pi_a^*, \\pi_b^*) = \\min \\max Q(s,a,b), \n",
    "        where Q(s,a,b) = r(s,a,b) + \\gamma \\min \\max Q(s',a',b') (this is the definition of Nash Q-value);\n",
    "    2. Best response (of max player) value: Br V(s) = \\min_b \\pi(s,a) Br Q(s,a,b)  (Br Q is the oracle best response Q value)\n",
    "    \"\"\"\n",
    "    eval_v = create_expand_value()\n",
    "    eval_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        expand_rm = broadcast_shape(rm_, eval_q[i].shape+(num_states,))\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            eval_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = eval_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            eval_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        mu_dot_q = np.einsum('...i, ...ij->...j', max_policy[i], eval_q[i])\n",
    "        mu_dot_q_dot_nu = np.einsum('...kj, ...kj->...k', mu_dot_q, min_policy[i])\n",
    "        eval_v[i] = mu_dot_q_dot_nu    \n",
    "\n",
    "    avg_init_eval_v = np.mean(eval_v[0])  # average best response value of initial states; minus for making it positive\n",
    "    return avg_init_eval_v\n",
    "\n",
    "def update_matrix(matrix, side, row):\n",
    "    \"\"\"\n",
    "    Adding a new policy to the league will add a row or column to the present evaluation matrix, after it has\n",
    "    been evaluated against all policies in the opponent's league. Whether adding a row or column depends on whether\n",
    "    it is the row player (idx=0) or column player (idx=1). \n",
    "    For example, if current evaluation matrix is:\n",
    "    2  1\n",
    "    -1 3\n",
    "    After adding a new policy for row player with evaluated average episode reward (a,b) against current two policies in \n",
    "    the league of the column player, it gives: \n",
    "    it gives:\n",
    "    2   1\n",
    "    -1  3\n",
    "    a   b    \n",
    "    \"\"\" \n",
    "    if side == 'max': # for row player\n",
    "        if matrix.shape[0] == 0: \n",
    "            matrix = np.array([row])\n",
    "        else:  # add row\n",
    "            matrix = np.vstack([matrix, row])\n",
    "    else: # for column player\n",
    "        if matrix.shape[0] == 0:  # the first update\n",
    "            matrix = np.array([row]) \n",
    "        else:  # add column\n",
    "            matrix=np.hstack([matrix, np.array([row]).T])\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def sample_policy(policy_set, dist):\n",
    "    policy_id, _ = sample_from_categorical(dist)\n",
    "    policy = policy_set[policy_id]   \n",
    "    return policy \n",
    "\n",
    "# test_policy = create_expand_policy() \n",
    "# get_best_response(test_policy)\n",
    "\n",
    "def psro(env, save_path, solve_episodes = 1000):\n",
    "    evaluation_matrix = np.array([[0]])\n",
    "    ini_max_policy = create_expand_policy()\n",
    "    ini_min_policy = create_expand_policy()\n",
    "    max_player = {\n",
    "        'side': 'max',\n",
    "        'policy_set': [ini_max_policy],\n",
    "        'meta_strategy': np.array([1.])\n",
    "    }\n",
    "    min_player = {\n",
    "        'side': 'min',\n",
    "        'policy_set': [ini_min_policy],\n",
    "        'meta_strategy': np.array([1.])\n",
    "    }\n",
    "\n",
    "    max_side_q_table = create_expand_Q()\n",
    "    min_side_q_table = create_expand_Q()\n",
    " \n",
    "    print('policy shape: ')\n",
    "    for p in ini_max_policy:\n",
    "        print(p.shape)\n",
    "    print('Q table shape: ')\n",
    "    for q in max_side_q_table:\n",
    "        print(q.shape)\n",
    "\n",
    "    exploitability_records = []\n",
    "\n",
    "    for i in range(solve_episodes):\n",
    "        \n",
    "        if i % 2 == 0:  # update min player side\n",
    "            new_policy = get_best_response_policy(max_player)  # get best response against the max player\n",
    "            min_player['policy_set'].append(new_policy) # add new policy to policy set to form mixture\n",
    "\n",
    "            # update evaluation matrix\n",
    "            eval_scores = []\n",
    "            for p in max_player['policy_set']:\n",
    "                score = policy_against_policy_value(trans_matrices, reward_matrices, max_policy=p, min_policy=new_policy, num_actions=num_actions_per_player)\n",
    "                eval_scores.append(score)  # score always from the max player's view\n",
    "            evaluation_matrix = update_matrix(evaluation_matrix, 'min', eval_scores)\n",
    "\n",
    "        else: \n",
    "            new_policy = get_best_response_policy(min_player) # get best response against the min player\n",
    "            max_player['policy_set'].append(new_policy) # add new policy to policy set to form mixture\n",
    "\n",
    "            # update evaluation matrix\n",
    "            eval_scores = []\n",
    "            for p in min_player['policy_set']:\n",
    "                score = policy_against_policy_value(trans_matrices, reward_matrices, max_policy=new_policy, min_policy=p, num_actions=num_actions_per_player)\n",
    "                eval_scores.append(score)  # score always from the max player's view\n",
    "            evaluation_matrix = update_matrix(evaluation_matrix, 'max', eval_scores)\n",
    "\n",
    "        if len(max_player['policy_set']) * len(min_player['policy_set']) >= 2:  # enough policies to get Nash\n",
    "            (max_player['meta_strategy'], min_player['meta_strategy']), _ = NashEquilibriumCVXPYSolver(evaluation_matrix)\n",
    "        else: # uniform for initialization only\n",
    "            max_policies = len(max_player['policy_set'])\n",
    "            max_player['meta_strategy'] = 1./max_policies*np.ones(max_policies)\n",
    "            min_policies = len(min_player['policy_set'])\n",
    "            min_player['meta_strategy'] = 1./min_policies*np.ones(min_policies)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            exploitability = get_best_response_value(max_player)  # best response of the max player\n",
    "            print(f'itr: {i}, exploitability: {exploitability}', )\n",
    "            exploitability_records.append(exploitability)\n",
    "            np.save(save_path, exploitability_records)\n",
    "\n",
    "save_path = 'psro_exp.npy'\n",
    "psro(env, save_path, solve_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe787014f98>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "exp_data = np.load(save_path)\n",
    "plt.title('PSRO')\n",
    "plt.xlabel('iter(x10)')\n",
    "plt.ylabel('exploitability')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.plot(exp_data+oracle_v_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4, 5, 6, 7]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "num_states = 3\n",
    "\n",
    "\n",
    "def get_all_traj_for_transition(transition_step):\n",
    "    \"\"\"\n",
    "    step = 1:\n",
    "    [\n",
    "        [s0,a0,b0], [s0,a0,b1],...\n",
    "    ]\n",
    "    step = 2:\n",
    "    [\n",
    "        [s0,a0,b0,s1,a1,b1], [s1,a0,b0,s1,a1,b1],\n",
    "    ]\n",
    "    ...\n",
    "    \"\"\"\n",
    "    ranges = transition_step*[range(num_states), range(num_actions_per_player), range(num_actions_per_player)]\n",
    "    all_possible_trajs = list(itertools.product(*ranges))\n",
    "\n",
    "    return all_possible_trajs\n",
    "\n",
    "def split_traj(traj, side):\n",
    "    \"\"\"\n",
    "    s0, a0, b0, s1, a1, b1, ... sn-1, an-1, bn-1, sn ->\n",
    "    side = max:\n",
    "    [], [s0,a0], [s0,a0,b0,s1,a1], [s0,a0,b0,s1,a1,b1,s2,a2], ...\n",
    "    side = min:\n",
    "    [s0,b0], [s0,a0,b0,s1,b1], [s0,a0,b0,s1,a1,b1,s2,b2], ...\n",
    "    \"\"\"\n",
    "    split_trajs = []\n",
    "    i=2\n",
    "    while i<=len(traj):  # shorter than original traj\n",
    "        if side == 'max':\n",
    "            split_trajs.append(traj[:i])\n",
    "        else:\n",
    "            split_trajs.append(np.concatenate([traj[:i-1], [traj[i]]]))\n",
    "        i = i+3 # 3 due to (s,a,b)\n",
    "\n",
    "    return split_trajs\n",
    "\n",
    "\n",
    "# get_all_traj_for_transition(2)\n",
    "split_traj(traj=[0,1,2,3,4,5,6,7,8], side='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28c6861e59928cb790236f7047915368f37afc12f670e78fd0101a6f825a02b1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('x': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
