{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Space Response Oracle (PSRO)\n",
    "This tutorial demonstrates Policy Space Response Oracle (PSRO) on a zero-sum Markov game, which is more general than extensive-from game (more general than normal-form game). \n",
    "\n",
    "For the payoff matrix, row player is maximizer, coloumn player is minimizer.\n",
    "\n",
    "Reference: A unified game-theoretic approach to multiagent reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 12.0, (1,), float32) Discrete(3)\n",
      "[[0], [0]]\n",
      "[[5], [5]] [0.5610583525729109, -0.5610583525729109] [False, False]\n",
      "[[6], [6]] [-0.7261994566288021, 0.7261994566288021] [False, False]\n",
      "[[9], [9]] [-0.8460871060267345, 0.8460871060267345] [True, True]\n",
      "Average initial state value of oracle Nash equilibrium for the first player:  -0.29607107415447115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.\n",
      "  warn(\"Converting G to a CSC matrix; may take a while.\")\n",
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.\n",
      "  warn(\"Converting A to a CSC matrix; may take a while.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from mars.env.mdp import ArbitraryMDP, MDPWrapper\n",
    "import numpy as np\n",
    "\n",
    "num_states = 3\n",
    "num_actions_per_player = 3\n",
    "num_trans = 3\n",
    "\n",
    "env = MDPWrapper(ArbitraryMDP(num_states=num_states, num_actions_per_player=num_actions_per_player, num_trans=num_trans))\n",
    "trans_matrices = env.env.trans_prob_matrices # shape: [dim_transition, dim_state, dim_action (p1*p2), dim_state]\n",
    "reward_matrices = env.env.reward_matrices # shape: [dim_transition, dim_state, dim_action (p1*p2), dim_state]\n",
    "\n",
    "oracle_nash_v, oracle_nash_q, oracle_nash_strategies = env.NEsolver(verbose=False)\n",
    "oracle_v_star = oracle_nash_v[0]\n",
    "\n",
    "oracle_v_star = np.mean(oracle_v_star, axis=0)\n",
    "print(env.observation_space, env.action_space)\n",
    "# env.render()\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "while not np.any(done):\n",
    "    obs, r, done, _ = env.step([1,0])\n",
    "    print(obs, r, done)\n",
    "print('Average initial state value of oracle Nash equilibrium for the first player: ', oracle_v_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 2, 3)\n",
      "(3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(oracle_nash_strategies).shape)\n",
    "oracle_max_policy = np.array(oracle_nash_strategies)[:, :, 0, :]\n",
    "print(oracle_max_policy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy shape: \n",
      "(3, 3)\n",
      "(3, 3, 3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3, 3, 3)\n",
      "Q table shape: \n",
      "(3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3, 3, 3, 3)\n",
      "itr: 0, exploitability: 0.7608750685387422\n",
      "itr: 10, exploitability: 0.9514198961458852\n",
      "itr: 20, exploitability: 0.9546113831408998\n",
      "itr: 30, exploitability: 0.9172358103212459\n",
      "itr: 40, exploitability: 0.9201121295933313\n",
      "itr: 50, exploitability: 0.920112129590521\n",
      "itr: 60, exploitability: 0.9305401780275679\n",
      "itr: 70, exploitability: 0.9305401780301246\n",
      "itr: 80, exploitability: 0.9305401780287403\n",
      "itr: 90, exploitability: 0.9305401780852534\n",
      "itr: 100, exploitability: 0.9297250868977636\n",
      "itr: 110, exploitability: 0.9297250868181208\n",
      "itr: 120, exploitability: 0.929725086909929\n",
      "itr: 130, exploitability: 0.9297250868945836\n",
      "itr: 140, exploitability: 0.9297250867800395\n",
      "itr: 150, exploitability: 0.9225304551552806\n",
      "itr: 160, exploitability: 0.9225304554768592\n",
      "itr: 170, exploitability: 0.9225304551250317\n",
      "itr: 180, exploitability: 0.9225304553253058\n",
      "itr: 190, exploitability: 0.9225304553941402\n",
      "itr: 200, exploitability: 0.9225304552976994\n",
      "itr: 210, exploitability: 0.9225304555251703\n",
      "itr: 220, exploitability: 0.9225304555243621\n",
      "itr: 230, exploitability: 0.922530454815444\n",
      "itr: 240, exploitability: 0.9225304555402922\n",
      "itr: 250, exploitability: 0.9225304555105074\n",
      "itr: 260, exploitability: 0.9229561397406482\n",
      "itr: 270, exploitability: 0.9229561397883881\n",
      "itr: 280, exploitability: 0.9229561397561372\n",
      "itr: 290, exploitability: 0.9229561398274155\n",
      "itr: 300, exploitability: 0.9229561397713135\n",
      "itr: 310, exploitability: 0.9229561397139798\n",
      "itr: 320, exploitability: 0.9229561398105036\n",
      "itr: 330, exploitability: 0.9229561397382743\n",
      "itr: 340, exploitability: 0.9229561397404349\n",
      "itr: 350, exploitability: 0.9229561397410408\n",
      "itr: 360, exploitability: 0.9229561397419951\n",
      "itr: 370, exploitability: 0.922956139746316\n",
      "itr: 380, exploitability: 0.9229561397486025\n",
      "itr: 390, exploitability: 0.9229561397512176\n",
      "itr: 400, exploitability: 0.922956139742052\n",
      "itr: 410, exploitability: 0.9229561397402927\n",
      "itr: 420, exploitability: 0.9229561397385457\n",
      "itr: 430, exploitability: 0.9229561397391325\n",
      "itr: 440, exploitability: 0.9229561397389213\n",
      "itr: 450, exploitability: 0.9229561397749286\n",
      "itr: 460, exploitability: 0.922956139909445\n",
      "itr: 470, exploitability: 0.9229561397390535\n",
      "itr: 480, exploitability: 0.9229561398549451\n",
      "itr: 490, exploitability: 0.9229561397477838\n",
      "itr: 500, exploitability: 0.9229561397443239\n",
      "itr: 510, exploitability: 0.9229561397519865\n",
      "itr: 520, exploitability: 0.9229561398077558\n",
      "itr: 530, exploitability: 0.9229561397383558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/cvxpy/problems/problem.py:1279: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 540, exploitability: 0.9329222465268796\n",
      "itr: 550, exploitability: 0.9321638643491656\n",
      "itr: 560, exploitability: 0.9321638643597465\n",
      "itr: 570, exploitability: 0.9321638643031045\n",
      "itr: 580, exploitability: 0.9321638643037979\n",
      "itr: 590, exploitability: 0.9321638643765245\n",
      "itr: 600, exploitability: 0.9321638642945992\n",
      "itr: 610, exploitability: 0.932163864325493\n",
      "itr: 620, exploitability: 0.9321638643486116\n",
      "itr: 630, exploitability: 0.9321638642432057\n",
      "itr: 640, exploitability: 0.9321638643566911\n",
      "itr: 650, exploitability: 0.9321638643093421\n",
      "itr: 660, exploitability: 0.9212943772193066\n",
      "itr: 670, exploitability: 0.9237150038499079\n",
      "itr: 680, exploitability: 0.9113434785696819\n",
      "itr: 690, exploitability: 0.9113434785797189\n",
      "itr: 700, exploitability: 0.9101881642885488\n",
      "itr: 710, exploitability: 0.9085683867744931\n",
      "itr: 720, exploitability: 0.9085683868014987\n",
      "itr: 730, exploitability: 0.9085683867925403\n",
      "itr: 740, exploitability: 0.902523854108635\n",
      "itr: 750, exploitability: 0.9013308197922507\n",
      "itr: 760, exploitability: 0.893479361589083\n",
      "itr: 770, exploitability: 0.8934793615158896\n",
      "itr: 780, exploitability: 0.8934793615800614\n",
      "itr: 790, exploitability: 0.893479361518601\n",
      "itr: 800, exploitability: 0.8924516443988938\n",
      "itr: 810, exploitability: 0.8777096567964024\n",
      "itr: 820, exploitability: 0.9101468744190176\n",
      "itr: 830, exploitability: 0.9125269797077741\n",
      "itr: 840, exploitability: 0.9121629538459655\n",
      "itr: 850, exploitability: 0.9121629537895464\n",
      "itr: 860, exploitability: 0.9172623158243455\n",
      "itr: 870, exploitability: 0.917262315824003\n",
      "itr: 880, exploitability: 0.9172930580713621\n",
      "itr: 890, exploitability: 0.9131967227615365\n",
      "itr: 900, exploitability: 0.9131967226119385\n",
      "itr: 910, exploitability: 0.9083683818802935\n",
      "itr: 920, exploitability: 0.9143310470429267\n",
      "itr: 930, exploitability: 0.9147082669025157\n",
      "itr: 940, exploitability: 0.9131274774305166\n",
      "itr: 950, exploitability: 0.9137937074648139\n",
      "itr: 960, exploitability: 0.9137937078158211\n",
      "itr: 970, exploitability: 0.9137937073993526\n",
      "itr: 980, exploitability: 0.9137937073573771\n",
      "itr: 990, exploitability: 0.9137937073801312\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from mars.equilibrium_solver import NashEquilibriumECOSSolver, NashEquilibriumCVXPYSolver\n",
    "\n",
    "def sample_from_categorical(dist):\n",
    "    \"\"\"\n",
    "    sample once from a categorical distribution, return the entry index.\n",
    "    dist: should be a list or array of probabilities for a categorical distribution\n",
    "    \"\"\"\n",
    "    sample_id = np.argmax(np.random.multinomial(1, dist))\n",
    "    sample_prob = dist[sample_id]\n",
    "    return sample_id, sample_prob\n",
    "\n",
    "def unified_state(s):\n",
    "    unified_s = s[0]%num_states\n",
    "    return unified_s\n",
    "\n",
    "def create_expand_policy(zero_ini=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim, action_dim],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    policies = []\n",
    "    intial_dim = [num_states]\n",
    "    for i in range(num_trans):\n",
    "        if zero_ini:\n",
    "            policy =  (1./num_actions_per_player) * np.zeros((*intial_dim, num_actions_per_player))\n",
    "        else:\n",
    "            policy =  (1./num_actions_per_player) * np.ones((*intial_dim, num_actions_per_player))\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_actions_per_player, num_actions_per_player, num_states])\n",
    "        policies.append(policy)\n",
    "\n",
    "    return policies\n",
    "\n",
    "def create_expand_value():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim],\n",
    "        ...\n",
    "    ]    \n",
    "    \"\"\"\n",
    "    values = []\n",
    "    intial_dim = [num_states]\n",
    "    for i in range(num_trans):\n",
    "        value =  (1./num_actions_per_player) * np.ones(intial_dim)\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_actions_per_player, num_actions_per_player, num_states])\n",
    "        values.append(value)\n",
    "\n",
    "    return values\n",
    "\n",
    "def create_expand_Q():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim, action_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim, action_dim, action_dim],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    intial_dim = [num_states, num_actions_per_player, num_actions_per_player]\n",
    "    for i in range(num_trans):\n",
    "        value =  (1./num_actions_per_player) * np.ones(intial_dim)\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_states, num_actions_per_player, num_actions_per_player])\n",
    "        values.append(value)\n",
    "\n",
    "    return values\n",
    "\n",
    "def get_posterior_policy(policy_set, meta_prior, likelihood):\n",
    "    posterior_policy = create_expand_policy(zero_ini=True)  # zero initiate to sum later\n",
    "    num_policies = len(policy_set)\n",
    "    denom = meta_prior @ likelihood  # [#policy] * [#policy, H] -> [H]\n",
    "    for pi_i, rho_i, likelihood_per_policy in zip(policy_set, meta_prior, likelihood):  # loop over policy\n",
    "        for i, (p, d, l) in enumerate(zip(pi_i, denom, likelihood_per_policy)): # loop over transition\n",
    "            posterior_policy[i] += np.array(p*rho_i*l/d)  # sum over policies in mixture\n",
    "\n",
    "    return posterior_policy\n",
    "\n",
    "def broadcast_shape(input, output_shape):\n",
    "    \"\"\" broadcast input to have the same shape as output_to_be \"\"\"\n",
    "    len_input_shape = len(np.array(input).shape)\n",
    "    incrs_shape = output_shape[:-len_input_shape]+len_input_shape*(1,)\n",
    "    output = np.tile(input, incrs_shape)\n",
    "    return output\n",
    "\n",
    "def get_best_response_policy(player, prob_seq):\n",
    "    given_policy = get_posterior_policy(player['policy_set'], player['meta_strategy'], prob_seq)\n",
    "    br_policy = create_expand_policy()  # best response policy\n",
    "    br_v = create_expand_value()\n",
    "    br_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "\n",
    "        expand_rm = broadcast_shape(rm_, br_q[i].shape+(num_states,))  # broadcast the shape of reward matrix to be expand Q shape plus additional state dimension\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            br_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = br_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            br_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        if player['side'] == 'max':\n",
    "            mu_dot_q = np.einsum('...i, ...ij->...j', given_policy[i], br_q[i])\n",
    "            arg_id = np.argmin(mu_dot_q, axis=-1)  # min player takes minimum as best response against max player's policy\n",
    "            br_v[i] = np.min(mu_dot_q, axis=-1)\n",
    "        else:\n",
    "            q_dot_nu = np.einsum('...ij, ...j->...i', br_q[i], given_policy[i])\n",
    "            arg_id = np.argmax(q_dot_nu, axis=-1)   # vice versa    \n",
    "            br_v[i] = np.max(q_dot_nu, axis=-1)     \n",
    "\n",
    "        br_policy[i] = (np.arange(num_actions_per_player) == arg_id[...,None]).astype(int)  # from extreme (min/max) idx to one-hot simplex\n",
    "\n",
    "        # print(br_policy[i].shape, br_q[i].shape)\n",
    "\n",
    "    return br_policy\n",
    "\n",
    "def best_response_value(trans_prob_matrices, reward_matrices, policy, num_actions, side='max'):\n",
    "    \"\"\"\n",
    "    Formulas for calculating best response values:\n",
    "    1. Nash strategies: (\\pi_a^*, \\pi_b^*) = \\min \\max Q(s,a,b), \n",
    "        where Q(s,a,b) = r(s,a,b) + \\gamma \\min \\max Q(s',a',b') (this is the definition of Nash Q-value);\n",
    "    2. Best response (of max player) value: Br V(s) = \\min_b \\pi(s,a) Br Q(s,a,b)  (Br Q is the oracle best response Q value)\n",
    "    \"\"\"\n",
    "    br_v = create_expand_value()\n",
    "    br_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        expand_rm = broadcast_shape(rm_, br_q[i].shape+(num_states,))\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            br_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = br_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            br_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        if side == 'max':\n",
    "            mu_dot_q = np.einsum('...i, ...ij->...j', policy[i], br_q[i])\n",
    "            br_v[i] = np.min(mu_dot_q, axis=-1)\n",
    "        else:\n",
    "            q_dot_nu = np.einsum('...ij, ...j->...i', br_q[i], policy[i])\n",
    "            br_v[i] = np.max(q_dot_nu, axis=-1)         \n",
    "\n",
    "    avg_init_br_v = -np.mean(br_v[0])  # average best response value of initial states; minus for making it positive\n",
    "    return avg_init_br_v\n",
    "\n",
    "def get_best_response_value(player):\n",
    "    policy_set = player['policy_set']\n",
    "    meta_strategy = player['meta_strategy']\n",
    "    per_policy_exploits = []\n",
    "    for p in policy_set:\n",
    "        per_policy_exploits.append(best_response_value(trans_matrices, reward_matrices, p, num_actions_per_player, player['side']))\n",
    "    per_policy_exploits = np.array(per_policy_exploits)\n",
    "    exploitability = per_policy_exploits @ meta_strategy  # average over policy set weighted by meta strategy\n",
    "    return exploitability\n",
    "\n",
    "def policy_against_policy_value(trans_prob_matrices, reward_matrices, max_policy, min_policy, num_actions):\n",
    "    \"\"\"\n",
    "    Formulas for calculating best response values:\n",
    "    1. Nash strategies: (\\pi_a^*, \\pi_b^*) = \\min \\max Q(s,a,b), \n",
    "        where Q(s,a,b) = r(s,a,b) + \\gamma \\min \\max Q(s',a',b') (this is the definition of Nash Q-value);\n",
    "    2. Best response (of max player) value: Br V(s) = \\min_b \\pi(s,a) Br Q(s,a,b)  (Br Q is the oracle best response Q value)\n",
    "    \"\"\"\n",
    "    eval_v = create_expand_value()\n",
    "    eval_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        expand_rm = broadcast_shape(rm_, eval_q[i].shape+(num_states,))\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            eval_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = eval_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            eval_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        mu_dot_q = np.einsum('...i, ...ij->...j', max_policy[i], eval_q[i])\n",
    "        mu_dot_q_dot_nu = np.einsum('...kj, ...kj->...k', mu_dot_q, min_policy[i])\n",
    "        eval_v[i] = mu_dot_q_dot_nu    \n",
    "\n",
    "    avg_init_eval_v = np.mean(eval_v[0])  # average best response value of initial states; minus for making it positive\n",
    "    return avg_init_eval_v\n",
    "\n",
    "def update_matrix(matrix, side, row):\n",
    "    \"\"\"\n",
    "    Adding a new policy to the league will add a row or column to the present evaluation matrix, after it has\n",
    "    been evaluated against all policies in the opponent's league. Whether adding a row or column depends on whether\n",
    "    it is the row player (idx=0) or column player (idx=1). \n",
    "    For example, if current evaluation matrix is:\n",
    "    2  1\n",
    "    -1 3\n",
    "    After adding a new policy for row player with evaluated average episode reward (a,b) against current two policies in \n",
    "    the league of the column player, it gives: \n",
    "    it gives:\n",
    "    2   1\n",
    "    -1  3\n",
    "    a   b    \n",
    "    \"\"\" \n",
    "    if side == 'max': # for row player\n",
    "        if matrix.shape[0] == 0: \n",
    "            matrix = np.array([row])\n",
    "        else:  # add row\n",
    "            matrix = np.vstack([matrix, row])\n",
    "    else: # for column player\n",
    "        if matrix.shape[0] == 0:  # the first update\n",
    "            matrix = np.array([row]) \n",
    "        else:  # add column\n",
    "            matrix=np.hstack([matrix, np.array([row]).T])\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def sample_policy(policy_set, dist):\n",
    "    policy_id, _ = sample_from_categorical(dist)\n",
    "    policy = policy_set[policy_id]   \n",
    "    return policy \n",
    "\n",
    "# test_policy = create_expand_policy() \n",
    "# get_best_response(test_policy)\n",
    "\n",
    "def psro(env, save_path, solve_episodes = 1000):\n",
    "    evaluation_matrix = np.array([[0]])\n",
    "    ini_max_policy = create_expand_policy()\n",
    "    ini_min_policy = create_expand_policy()\n",
    "    max_player = {\n",
    "        'side': 'max',\n",
    "        'policy_set': [ini_max_policy],\n",
    "        'meta_strategy': np.array([1.])\n",
    "    }\n",
    "    min_player = {\n",
    "        'side': 'min',\n",
    "        'policy_set': [ini_min_policy],\n",
    "        'meta_strategy': np.array([1.])\n",
    "    }\n",
    "\n",
    "    max_side_q_table = create_expand_Q()\n",
    "    min_side_q_table = create_expand_Q()\n",
    " \n",
    "    print('policy shape: ')\n",
    "    for p in ini_max_policy:\n",
    "        print(p.shape)\n",
    "    print('Q table shape: ')\n",
    "    for q in max_side_q_table:\n",
    "        print(q.shape)\n",
    "\n",
    "    exploitability_records = []\n",
    "\n",
    "    for i in range(solve_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        max_player_observed_sequence = []\n",
    "        min_player_observed_sequence = []\n",
    "        max_player_action_prob_sequence = [[] for _ in max_player['policy_set']]\n",
    "        min_player_action_prob_sequence = [[] for _ in min_player['policy_set']]\n",
    "        shared_sequence_before_actions = []\n",
    "        step = 0\n",
    "\n",
    "        # sample policy from meta strategy for the current episode\n",
    "        max_policy = sample_policy(max_player['policy_set'], max_player['meta_strategy'])\n",
    "        min_policy = sample_policy(min_player['policy_set'], min_player['meta_strategy'])\n",
    "\n",
    "        while not np.any(done):    \n",
    "            # get observed sequence\n",
    "            max_player_observed_sequence.extend([unified_state(s[0])])\n",
    "            min_player_observed_sequence.extend([unified_state(s[1])])\n",
    "            shared_sequence_before_actions = copy.deepcopy(max_player_observed_sequence)\n",
    "\n",
    "            # get action distribution given specific observed history\n",
    "            max_policy_to_choose = max_policy[step][tuple(max_player_observed_sequence)]  # a=np.ones([1,2]), b=(0,0), a[b] -> 1.\n",
    "            min_policy_to_choose = min_policy[step][tuple(min_player_observed_sequence)]\n",
    "\n",
    "            # choose action\n",
    "            max_action, _ = sample_from_categorical(max_policy_to_choose)\n",
    "            min_action, _ = sample_from_categorical(min_policy_to_choose)\n",
    "\n",
    "            # roullout info for mixture policy\n",
    "            if i % 2 == 0:  # update min player side\n",
    "                for p_id, p in enumerate(max_player['policy_set']):  # get trajectory probabilities for each policy in policy set\n",
    "                    max_p = p[step][tuple(max_player_observed_sequence)]\n",
    "                    # _, max_a_prob = sample_from_categorical(max_p) # this is wrong\n",
    "                    max_a_prob = max_p[max_action]  # get the probability of real action (rollout) under each policy in policy set\n",
    "                    if step == 0:\n",
    "                        max_player_action_prob_sequence[p_id].append(max_a_prob)\n",
    "                    else:\n",
    "                        max_player_action_prob_sequence[p_id].append(max_player_action_prob_sequence[p_id][-1]*max_a_prob)  # [p1, p1p2, p1p2p3, ...]\n",
    "            else:\n",
    "                for p_id, p in enumerate(min_player['policy_set']):\n",
    "                    min_p = p[step][tuple(min_player_observed_sequence)]\n",
    "                    # _, min_a_prob = sample_from_categorical(min_p)  # this is wrong\n",
    "                    min_a_prob = min_p[min_action]  # get the probability of real action (rollout) under each policy in policy set\n",
    "                    if step == 0:\n",
    "                        min_player_action_prob_sequence[p_id].append(min_a_prob)\n",
    "                    else:\n",
    "                        min_player_action_prob_sequence[p_id].append(min_player_action_prob_sequence[p_id][-1]*min_a_prob)  # [p1, p1p2, p1p2p3, ...]\n",
    "\n",
    "            action = [max_action, min_action]\n",
    "            max_player_observed_sequence.extend(action)\n",
    "            min_player_observed_sequence.extend(action)\n",
    "\n",
    "            s_, r, done, _  = env.step(action)  \n",
    "            s = s_\n",
    "\n",
    "            step += 1\n",
    "        \n",
    "        if i % 2 == 0:  # update min player side\n",
    "            new_policy = get_best_response_policy(max_player, np.array(max_player_action_prob_sequence))  # get best response against the max player, max_player_action_prob_sequence: [#policies, H]\n",
    "            min_player['policy_set'].append(new_policy) # add new policy to policy set to form mixture\n",
    "\n",
    "            # update evaluation matrix\n",
    "            eval_scores = []\n",
    "            for p in max_player['policy_set']:\n",
    "                score = policy_against_policy_value(trans_matrices, reward_matrices, max_policy=p, min_policy=new_policy, num_actions=num_actions_per_player)\n",
    "                eval_scores.append(score)  # score always from the max player's view\n",
    "            evaluation_matrix = update_matrix(evaluation_matrix, 'min', eval_scores)\n",
    "\n",
    "        else: \n",
    "            new_policy = get_best_response_policy(min_player, np.array(min_player_action_prob_sequence)) # get best response against the min player\n",
    "            max_player['policy_set'].append(new_policy) # add new policy to policy set to form mixture\n",
    "\n",
    "            # update evaluation matrix\n",
    "            eval_scores = []\n",
    "            for p in min_player['policy_set']:\n",
    "                score = policy_against_policy_value(trans_matrices, reward_matrices, max_policy=new_policy, min_policy=p, num_actions=num_actions_per_player)\n",
    "                eval_scores.append(score)  # score always from the max player's view\n",
    "            evaluation_matrix = update_matrix(evaluation_matrix, 'max', eval_scores)\n",
    "\n",
    "        if len(max_player['policy_set']) * len(min_player['policy_set']) >= 2:  # enough policies to get Nash\n",
    "            (max_player['meta_strategy'], min_player['meta_strategy']), _ = NashEquilibriumCVXPYSolver(evaluation_matrix)\n",
    "        else: # uniform for initialization only\n",
    "            max_policies = len(max_player['policy_set'])\n",
    "            max_player['meta_strategy'] = 1./max_policies*np.ones(max_policies)\n",
    "            min_policies = len(min_player['policy_set'])\n",
    "            min_player['meta_strategy'] = 1./min_policies*np.ones(min_policies)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            exploitability = get_best_response_value(max_player)  # best response of the max player\n",
    "            print(f'itr: {i}, exploitability: {exploitability}', )\n",
    "            exploitability_records.append(exploitability)\n",
    "            np.save(save_path, exploitability_records)\n",
    "\n",
    "save_path = 'psro_exp.npy'\n",
    "psro(env, save_path, solve_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f46e49fd8d0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "exp_data = np.load(save_path)\n",
    "plt.title('PSRO')\n",
    "plt.xlabel('iter(x10)')\n",
    "plt.ylabel('exploitability')\n",
    "plt.plot(exp_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28c6861e59928cb790236f7047915368f37afc12f670e78fd0101a6f825a02b1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('x': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
