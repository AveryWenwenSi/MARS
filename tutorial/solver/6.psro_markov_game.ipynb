{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Space Response Oracle (PSRO)\n",
    "This tutorial demonstrates Policy Space Response Oracle (PSRO) on a zero-sum Markov game, which is more general than extensive-from game (more general than normal-form game). \n",
    "\n",
    "For the payoff matrix, row player is maximizer, coloumn player is minimizer.\n",
    "\n",
    "Reference: A unified game-theoretic approach to multiagent reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 12.0, (1,), float32) Discrete(3)\n",
      "[[0], [0]]\n",
      "[[5], [5]] [0.5610583525729109, -0.5610583525729109] [False, False]\n",
      "[[6], [6]] [-0.7261994566288021, 0.7261994566288021] [False, False]\n",
      "[[9], [9]] [-0.8460871060267345, 0.8460871060267345] [True, True]\n",
      "Average initial state value of oracle Nash equilibrium for the first player:  -0.29607107415447115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.\n",
      "  warn(\"Converting G to a CSC matrix; may take a while.\")\n",
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.\n",
      "  warn(\"Converting A to a CSC matrix; may take a while.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from mars.env.mdp import ArbitraryMDP, MDPWrapper\n",
    "import numpy as np\n",
    "\n",
    "num_states = 3\n",
    "num_actions_per_player = 3\n",
    "num_trans = 3\n",
    "\n",
    "env = MDPWrapper(ArbitraryMDP(num_states=num_states, num_actions_per_player=num_actions_per_player, num_trans=num_trans))\n",
    "trans_matrices = env.env.trans_prob_matrices # shape: [dim_transition, dim_state, dim_action (p1*p2), dim_state]\n",
    "reward_matrices = env.env.reward_matrices # shape: [dim_transition, dim_state, dim_action (p1*p2), dim_state]\n",
    "\n",
    "oracle_nash_v, oracle_nash_q, oracle_nash_strategies = env.NEsolver(verbose=False)\n",
    "oracle_v_star = oracle_nash_v[0]\n",
    "\n",
    "oracle_v_star = np.mean(oracle_v_star, axis=0)\n",
    "print(env.observation_space, env.action_space)\n",
    "# env.render()\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "while not np.any(done):\n",
    "    obs, r, done, _ = env.step([1,0])\n",
    "    print(obs, r, done)\n",
    "print('Average initial state value of oracle Nash equilibrium for the first player: ', oracle_v_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 2, 3)\n",
      "(3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(oracle_nash_strategies).shape)\n",
    "oracle_max_policy = np.array(oracle_nash_strategies)[:, :, 0, :]\n",
    "print(oracle_max_policy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy shape: \n",
      "(3, 3)\n",
      "(3, 3, 3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3, 3, 3)\n",
      "Q table shape: \n",
      "(3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3, 3, 3, 3)\n",
      "itr: 0, exploitability: 0.7608750685387422\n",
      "itr: 10, exploitability: 0.9546113831371213\n",
      "itr: 20, exploitability: 0.959894889652096\n",
      "itr: 30, exploitability: 0.9153170421795291\n",
      "itr: 40, exploitability: 0.9458273236265649\n",
      "itr: 50, exploitability: 0.9193412162464841\n",
      "itr: 60, exploitability: 0.9178964035395948\n",
      "itr: 70, exploitability: 0.9178964028254258\n",
      "itr: 80, exploitability: 0.9178964029047625\n",
      "itr: 90, exploitability: 0.933068986389539\n",
      "itr: 100, exploitability: 0.9330689864247289\n",
      "itr: 110, exploitability: 0.9330689864176167\n",
      "itr: 120, exploitability: 0.9330689864487233\n",
      "itr: 130, exploitability: 0.9330689863075199\n",
      "itr: 140, exploitability: 0.9330689865646112\n",
      "itr: 150, exploitability: 0.9330689864697332\n",
      "itr: 160, exploitability: 0.9330689863085537\n",
      "itr: 170, exploitability: 0.9330689864462659\n",
      "itr: 180, exploitability: 0.9330689863483949\n",
      "itr: 190, exploitability: 0.9330689864138925\n",
      "itr: 200, exploitability: 0.933068986659686\n",
      "itr: 210, exploitability: 0.9330689862923649\n",
      "itr: 220, exploitability: 0.9330689865832367\n",
      "itr: 230, exploitability: 0.9330689869707693\n",
      "itr: 240, exploitability: 0.9330689868835624\n",
      "itr: 250, exploitability: 0.9330689873196597\n",
      "itr: 260, exploitability: 0.9330689874019384\n",
      "itr: 270, exploitability: 0.9330689870609397\n",
      "itr: 280, exploitability: 0.9330689871694693\n",
      "itr: 290, exploitability: 0.9330689863038205\n",
      "itr: 300, exploitability: 0.933068986305503\n",
      "itr: 310, exploitability: 0.933068986980219\n",
      "itr: 320, exploitability: 0.9330689865408412\n",
      "itr: 330, exploitability: 0.9330689869180325\n",
      "itr: 340, exploitability: 0.9330689872010794\n",
      "itr: 350, exploitability: 0.9330689868297992\n",
      "itr: 360, exploitability: 0.9330689868633263\n",
      "itr: 370, exploitability: 0.9330689869563984\n",
      "itr: 380, exploitability: 0.9330689863034184\n",
      "itr: 390, exploitability: 0.9330689863205147\n",
      "itr: 400, exploitability: 0.9330689863339343\n",
      "itr: 410, exploitability: 0.9330689864432781\n",
      "itr: 420, exploitability: 0.9330689867861921\n",
      "itr: 430, exploitability: 0.9330689870058754\n",
      "itr: 440, exploitability: 0.9330689869190345\n",
      "itr: 450, exploitability: 0.9330689866140249\n",
      "itr: 460, exploitability: 0.9330689872732079\n",
      "itr: 470, exploitability: 0.9330689867827611\n",
      "itr: 480, exploitability: 0.9330689870784412\n",
      "itr: 490, exploitability: 0.9330689874545139\n",
      "itr: 500, exploitability: 0.9330689874019904\n",
      "itr: 510, exploitability: 0.9330689862965771\n",
      "itr: 520, exploitability: 0.9330689868265286\n",
      "itr: 530, exploitability: 0.9330689862952577\n",
      "itr: 540, exploitability: 0.9330689869388766\n",
      "itr: 550, exploitability: 0.9330689863022028\n",
      "itr: 560, exploitability: 0.9330689870282969\n",
      "itr: 570, exploitability: 0.9330689870857123\n",
      "itr: 580, exploitability: 0.9330689872662565\n",
      "itr: 590, exploitability: 0.9330689868784963\n",
      "itr: 600, exploitability: 0.9330689863027624\n",
      "itr: 610, exploitability: 0.93306898715139\n",
      "itr: 620, exploitability: 0.9330689869760954\n",
      "itr: 630, exploitability: 0.933068987019624\n",
      "itr: 640, exploitability: 0.9330689866471646\n",
      "itr: 650, exploitability: 0.9330689870522251\n",
      "itr: 660, exploitability: 0.9330689870376644\n",
      "itr: 670, exploitability: 0.9330689872296427\n",
      "itr: 680, exploitability: 0.9330689870823023\n",
      "itr: 690, exploitability: 0.933068986727601\n",
      "itr: 700, exploitability: 0.9330689867904418\n",
      "itr: 710, exploitability: 0.9330689873684134\n",
      "itr: 720, exploitability: 0.9330689872979732\n",
      "itr: 730, exploitability: 0.9330689863186794\n",
      "itr: 740, exploitability: 0.9330689872608987\n",
      "itr: 750, exploitability: 0.933068986418432\n",
      "itr: 760, exploitability: 0.933068986979809\n",
      "itr: 770, exploitability: 0.9330689869762454\n",
      "itr: 780, exploitability: 0.93306898742441\n",
      "itr: 790, exploitability: 0.9330689869756469\n",
      "itr: 800, exploitability: 0.9330689873900015\n",
      "itr: 810, exploitability: 0.9330689869023725\n",
      "itr: 820, exploitability: 0.9330689863349034\n",
      "itr: 830, exploitability: 0.9330689863019493\n",
      "itr: 840, exploitability: 0.9330689872063183\n",
      "itr: 850, exploitability: 0.9330689865216794\n",
      "itr: 860, exploitability: 0.9330689868894753\n",
      "itr: 870, exploitability: 0.9330689868747474\n",
      "itr: 880, exploitability: 0.9330689869615953\n",
      "itr: 890, exploitability: 0.9330689868117552\n",
      "itr: 900, exploitability: 0.9330689864983326\n",
      "itr: 910, exploitability: 0.9330689869439789\n",
      "itr: 920, exploitability: 0.9330689869968367\n",
      "itr: 930, exploitability: 0.9330689872285545\n",
      "itr: 940, exploitability: 0.9330689869327073\n",
      "itr: 950, exploitability: 0.9330689863016813\n",
      "itr: 960, exploitability: 0.9330689863684317\n",
      "itr: 970, exploitability: 0.933068986797214\n",
      "itr: 980, exploitability: 0.9330689874810896\n",
      "itr: 990, exploitability: 0.9330689863039874\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from mars.equilibrium_solver import NashEquilibriumECOSSolver, NashEquilibriumCVXPYSolver\n",
    "\n",
    "def sample_from_categorical(dist):\n",
    "    \"\"\"\n",
    "    sample once from a categorical distribution, return the entry index.\n",
    "    dist: should be a list or array of probabilities for a categorical distribution\n",
    "    \"\"\"\n",
    "    sample_id = np.argmax(np.random.multinomial(1, dist))\n",
    "    sample_prob = dist[sample_id]\n",
    "    return sample_id, sample_prob\n",
    "\n",
    "def unified_state(s):\n",
    "    unified_s = s[0]%num_states\n",
    "    return unified_s\n",
    "\n",
    "def create_expand_policy(zero_ini=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim, action_dim],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    policies = []\n",
    "    intial_dim = [num_states]\n",
    "    for i in range(num_trans):\n",
    "        if zero_ini:\n",
    "            policy =  (1./num_actions_per_player) * np.zeros((*intial_dim, num_actions_per_player))\n",
    "        else:\n",
    "            policy =  (1./num_actions_per_player) * np.ones((*intial_dim, num_actions_per_player))\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_actions_per_player, num_actions_per_player, num_states])\n",
    "        policies.append(policy)\n",
    "\n",
    "    return policies\n",
    "\n",
    "def create_expand_value():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim],\n",
    "        ...\n",
    "    ]    \n",
    "    \"\"\"\n",
    "    values = []\n",
    "    intial_dim = [num_states]\n",
    "    for i in range(num_trans):\n",
    "        value =  (1./num_actions_per_player) * np.ones(intial_dim)\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_actions_per_player, num_actions_per_player, num_states])\n",
    "        values.append(value)\n",
    "\n",
    "    return values\n",
    "\n",
    "def create_expand_Q():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim, action_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim, action_dim, action_dim],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    intial_dim = [num_states, num_actions_per_player, num_actions_per_player]\n",
    "    for i in range(num_trans):\n",
    "        value =  (1./num_actions_per_player) * np.ones(intial_dim)\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_states, num_actions_per_player, num_actions_per_player])\n",
    "        values.append(value)\n",
    "\n",
    "    return values\n",
    "\n",
    "def get_posterior_policy(policy_set, meta_prior, likelihood):\n",
    "    posterior_policy = create_expand_policy(zero_ini=True)  # zero initiate to sum later\n",
    "    num_policies = len(policy_set)\n",
    "    denom = meta_prior @ likelihood  # [#policy] * [#policy, H] -> [H]\n",
    "    for pi_i, rho_i, likelihood_per_policy in zip(policy_set, meta_prior, likelihood):  # loop over policy\n",
    "        for i, (p, d, l) in enumerate(zip(pi_i, denom, likelihood_per_policy)): # loop over transition\n",
    "            posterior_policy[i] += np.array(p*rho_i*l/d)  # sum over policies in mixture\n",
    "\n",
    "    return posterior_policy\n",
    "\n",
    "def broadcast_shape(input, output_shape):\n",
    "    \"\"\" broadcast input to have the same shape as output_to_be \"\"\"\n",
    "    len_input_shape = len(np.array(input).shape)\n",
    "    incrs_shape = output_shape[:-len_input_shape]+len_input_shape*(1,)\n",
    "    output = np.tile(input, incrs_shape)\n",
    "    return output\n",
    "\n",
    "def get_best_response_policy(player, prob_seq):\n",
    "    given_policy = get_posterior_policy(player['policy_set'], player['meta_strategy'], prob_seq)\n",
    "    br_policy = create_expand_policy()  # best response policy\n",
    "    br_v = create_expand_value()\n",
    "    br_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "\n",
    "        expand_rm = broadcast_shape(rm_, br_q[i].shape+(num_states,))  # broadcast the shape of reward matrix to be expand Q shape plus additional state dimension\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            br_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = br_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            br_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        if player['side'] == 'max':\n",
    "            mu_dot_q = np.einsum('...i, ...ij->...j', given_policy[i], br_q[i])\n",
    "            arg_id = np.argmin(mu_dot_q, axis=-1)  # min player takes minimum as best response against max player's policy\n",
    "            br_v[i] = np.min(mu_dot_q, axis=-1)\n",
    "        else:\n",
    "            q_dot_nu = np.einsum('...ij, ...j->...i', br_q[i], given_policy[i])\n",
    "            arg_id = np.argmax(q_dot_nu, axis=-1)   # vice versa    \n",
    "            br_v[i] = np.max(q_dot_nu, axis=-1)     \n",
    "\n",
    "        br_policy[i] = (np.arange(num_actions_per_player) == arg_id[...,None]).astype(int)  # from extreme (min/max) idx to one-hot simplex\n",
    "\n",
    "        # print(br_policy[i].shape, br_q[i].shape)\n",
    "\n",
    "    return br_policy\n",
    "\n",
    "def best_response_value(trans_prob_matrices, reward_matrices, policy, num_actions, side='max'):\n",
    "    \"\"\"\n",
    "    Formulas for calculating best response values:\n",
    "    1. Nash strategies: (\\pi_a^*, \\pi_b^*) = \\min \\max Q(s,a,b), \n",
    "        where Q(s,a,b) = r(s,a,b) + \\gamma \\min \\max Q(s',a',b') (this is the definition of Nash Q-value);\n",
    "    2. Best response (of max player) value: Br V(s) = \\min_b \\pi(s,a) Br Q(s,a,b)  (Br Q is the oracle best response Q value)\n",
    "    \"\"\"\n",
    "    br_v = create_expand_value()\n",
    "    br_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        expand_rm = broadcast_shape(rm_, br_q[i].shape+(num_states,))\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            br_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = br_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            br_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        if side == 'max':\n",
    "            mu_dot_q = np.einsum('...i, ...ij->...j', policy[i], br_q[i])\n",
    "            br_v[i] = np.min(mu_dot_q, axis=-1)\n",
    "        else:\n",
    "            q_dot_nu = np.einsum('...ij, ...j->...i', br_q[i], policy[i])\n",
    "            br_v[i] = np.max(q_dot_nu, axis=-1)         \n",
    "\n",
    "    avg_init_br_v = -np.mean(br_v[0])  # average best response value of initial states; minus for making it positive\n",
    "    return avg_init_br_v\n",
    "\n",
    "def get_best_response_value(player):\n",
    "    policy_set = player['policy_set']\n",
    "    meta_strategy = player['meta_strategy']\n",
    "    per_policy_exploits = []\n",
    "    for p in policy_set:\n",
    "        per_policy_exploits.append(best_response_value(trans_matrices, reward_matrices, p, num_actions_per_player, player['side']))\n",
    "    per_policy_exploits = np.array(per_policy_exploits)\n",
    "    exploitability = per_policy_exploits @ meta_strategy  # average over policy set weighted by meta strategy\n",
    "    return exploitability\n",
    "\n",
    "def policy_against_policy_value(trans_prob_matrices, reward_matrices, max_policy, min_policy, num_actions):\n",
    "    \"\"\"\n",
    "    Formulas for calculating best response values:\n",
    "    1. Nash strategies: (\\pi_a^*, \\pi_b^*) = \\min \\max Q(s,a,b), \n",
    "        where Q(s,a,b) = r(s,a,b) + \\gamma \\min \\max Q(s',a',b') (this is the definition of Nash Q-value);\n",
    "    2. Best response (of max player) value: Br V(s) = \\min_b \\pi(s,a) Br Q(s,a,b)  (Br Q is the oracle best response Q value)\n",
    "    \"\"\"\n",
    "    eval_v = create_expand_value()\n",
    "    eval_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        expand_rm = broadcast_shape(rm_, eval_q[i].shape+(num_states,))\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            eval_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = eval_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            eval_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        mu_dot_q = np.einsum('...i, ...ij->...j', max_policy[i], eval_q[i])\n",
    "        mu_dot_q_dot_nu = np.einsum('...kj, ...kj->...k', mu_dot_q, min_policy[i])\n",
    "        eval_v[i] = mu_dot_q_dot_nu    \n",
    "\n",
    "    avg_init_eval_v = np.mean(eval_v[0])  # average best response value of initial states; minus for making it positive\n",
    "    return avg_init_eval_v\n",
    "\n",
    "def update_matrix(matrix, side, row):\n",
    "    \"\"\"\n",
    "    Adding a new policy to the league will add a row or column to the present evaluation matrix, after it has\n",
    "    been evaluated against all policies in the opponent's league. Whether adding a row or column depends on whether\n",
    "    it is the row player (idx=0) or column player (idx=1). \n",
    "    For example, if current evaluation matrix is:\n",
    "    2  1\n",
    "    -1 3\n",
    "    After adding a new policy for row player with evaluated average episode reward (a,b) against current two policies in \n",
    "    the league of the column player, it gives: \n",
    "    it gives:\n",
    "    2   1\n",
    "    -1  3\n",
    "    a   b    \n",
    "    \"\"\" \n",
    "    if side == 'max': # for row player\n",
    "        if matrix.shape[0] == 0: \n",
    "            matrix = np.array([row])\n",
    "        else:  # add row\n",
    "            matrix = np.vstack([matrix, row])\n",
    "    else: # for column player\n",
    "        if matrix.shape[0] == 0:  # the first update\n",
    "            matrix = np.array([row]) \n",
    "        else:  # add column\n",
    "            matrix=np.hstack([matrix, np.array([row]).T])\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def sample_policy(policy_set, dist):\n",
    "    policy_id, _ = sample_from_categorical(dist)\n",
    "    policy = policy_set[policy_id]   \n",
    "    return policy \n",
    "\n",
    "# test_policy = create_expand_policy() \n",
    "# get_best_response(test_policy)\n",
    "\n",
    "def psro(env, save_path, solve_episodes = 1000):\n",
    "    evaluation_matrix = np.array([[0]])\n",
    "    ini_max_policy = create_expand_policy()\n",
    "    ini_min_policy = create_expand_policy()\n",
    "    max_player = {\n",
    "        'side': 'max',\n",
    "        'policy_set': [ini_max_policy],\n",
    "        'meta_strategy': np.array([1.])\n",
    "    }\n",
    "    min_player = {\n",
    "        'side': 'min',\n",
    "        'policy_set': [ini_min_policy],\n",
    "        'meta_strategy': np.array([1.])\n",
    "    }\n",
    "\n",
    "    max_side_q_table = create_expand_Q()\n",
    "    min_side_q_table = create_expand_Q()\n",
    " \n",
    "    print('policy shape: ')\n",
    "    for p in ini_max_policy:\n",
    "        print(p.shape)\n",
    "    print('Q table shape: ')\n",
    "    for q in max_side_q_table:\n",
    "        print(q.shape)\n",
    "\n",
    "    exploitability_records = []\n",
    "\n",
    "    for i in range(solve_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        max_player_observed_sequence = []\n",
    "        min_player_observed_sequence = []\n",
    "        max_player_action_prob_sequence = [[] for _ in max_player['policy_set']]\n",
    "        min_player_action_prob_sequence = [[] for _ in min_player['policy_set']]\n",
    "        shared_sequence_before_actions = []\n",
    "        step = 0\n",
    "\n",
    "        # sample policy from meta strategy for the current episode\n",
    "        max_policy = sample_policy(max_player['policy_set'], max_player['meta_strategy'])\n",
    "        min_policy = sample_policy(min_player['policy_set'], min_player['meta_strategy'])\n",
    "\n",
    "        while not np.any(done):    \n",
    "            # get observed sequence\n",
    "            max_player_observed_sequence.extend([unified_state(s[0])])\n",
    "            min_player_observed_sequence.extend([unified_state(s[1])])\n",
    "            shared_sequence_before_actions = copy.deepcopy(max_player_observed_sequence)\n",
    "\n",
    "            # get action distribution given specific observed history\n",
    "            max_policy_to_choose = max_policy[step][tuple(max_player_observed_sequence)]  # a=np.ones([1,2]), b=(0,0), a[b] -> 1.\n",
    "            min_policy_to_choose = min_policy[step][tuple(min_player_observed_sequence)]\n",
    "\n",
    "            # choose action\n",
    "            max_action, _ = sample_from_categorical(max_policy_to_choose)\n",
    "            min_action, _ = sample_from_categorical(min_policy_to_choose)\n",
    "\n",
    "            # roullout info for mixture policy\n",
    "            if i % 2 == 0:  # update min player side\n",
    "                for p_id, p in enumerate(max_player['policy_set']):  # get trajectory probabilities for each policy in policy set\n",
    "                    max_p = p[step][tuple(max_player_observed_sequence)]\n",
    "                    # _, max_a_prob = sample_from_categorical(max_p) # this is wrong\n",
    "                    max_a_prob = max_p[max_action]  # get the probability of real action (rollout) under each policy in policy set\n",
    "                    if step == 0:\n",
    "                        max_player_action_prob_sequence[p_id].append(max_a_prob)\n",
    "                    else:\n",
    "                        max_player_action_prob_sequence[p_id].append(max_player_action_prob_sequence[p_id][-1]*max_a_prob)  # [p1, p1p2, p1p2p3, ...]\n",
    "            else:\n",
    "                for p_id, p in enumerate(min_player['policy_set']):\n",
    "                    min_p = p[step][tuple(min_player_observed_sequence)]\n",
    "                    # _, min_a_prob = sample_from_categorical(min_p)  # this is wrong\n",
    "                    min_a_prob = min_p[min_action]  # get the probability of real action (rollout) under each policy in policy set\n",
    "                    if step == 0:\n",
    "                        min_player_action_prob_sequence[p_id].append(min_a_prob)\n",
    "                    else:\n",
    "                        min_player_action_prob_sequence[p_id].append(min_player_action_prob_sequence[p_id][-1]*min_a_prob)  # [p1, p1p2, p1p2p3, ...]\n",
    "\n",
    "            action = [max_action, min_action]\n",
    "            max_player_observed_sequence.extend(action)\n",
    "            min_player_observed_sequence.extend(action)\n",
    "\n",
    "            s_, r, done, _  = env.step(action)  \n",
    "            s = s_\n",
    "\n",
    "            step += 1\n",
    "        \n",
    "        if i % 2 == 0:  # update min player side\n",
    "            new_policy = get_best_response_policy(max_player, np.array(max_player_action_prob_sequence))  # get best response against the max player, max_player_action_prob_sequence: [#policies, H]\n",
    "            min_player['policy_set'].append(new_policy) # add new policy to policy set to form mixture\n",
    "\n",
    "            # update evaluation matrix\n",
    "            eval_scores = []\n",
    "            for p in max_player['policy_set']:\n",
    "                score = policy_against_policy_value(trans_matrices, reward_matrices, max_policy=p, min_policy=new_policy, num_actions=num_actions_per_player)\n",
    "                eval_scores.append(score)  # score always from the max player's view\n",
    "            evaluation_matrix = update_matrix(evaluation_matrix, 'min', eval_scores)\n",
    "\n",
    "        else: \n",
    "            new_policy = get_best_response_policy(min_player, np.array(min_player_action_prob_sequence)) # get best response against the min player\n",
    "            max_player['policy_set'].append(new_policy) # add new policy to policy set to form mixture\n",
    "\n",
    "            # update evaluation matrix\n",
    "            eval_scores = []\n",
    "            for p in min_player['policy_set']:\n",
    "                score = policy_against_policy_value(trans_matrices, reward_matrices, max_policy=new_policy, min_policy=p, num_actions=num_actions_per_player)\n",
    "                eval_scores.append(score)  # score always from the max player's view\n",
    "            evaluation_matrix = update_matrix(evaluation_matrix, 'max', eval_scores)\n",
    "\n",
    "        if len(max_player['policy_set']) * len(min_player['policy_set']) >= 2:  # enough policies to get Nash\n",
    "            (max_player['meta_strategy'], min_player['meta_strategy']), _ = NashEquilibriumCVXPYSolver(evaluation_matrix)\n",
    "        else: # uniform for initialization only\n",
    "            max_policies = len(max_player['policy_set'])\n",
    "            max_player['meta_strategy'] = 1./max_policies*np.ones(max_policies)\n",
    "            min_policies = len(min_player['policy_set'])\n",
    "            min_player['meta_strategy'] = 1./min_policies*np.ones(min_policies)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            exploitability = get_best_response_value(max_player)  # best response of the max player\n",
    "            print(f'itr: {i}, exploitability: {exploitability}', )\n",
    "            exploitability_records.append(exploitability)\n",
    "            np.save(save_path, exploitability_records)\n",
    "\n",
    "save_path = 'psro_exp.npy'\n",
    "psro(env, save_path, solve_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1210323/2262408106.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exploitability'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "exp_data = np.load(save_path)\n",
    "plt.xlabel('iter')\n",
    "plt.ylabel('exploitability')\n",
    "plt.plot(exp_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28c6861e59928cb790236f7047915368f37afc12f670e78fd0101a6f825a02b1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('x': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
