{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Space Response Oracle (PSRO)\n",
    "This tutorial demonstrates Policy Space Response Oracle (PSRO) on a zero-sum Markov game, which is more general than extensive-from game (more general than normal-form game). \n",
    "\n",
    "For the payoff matrix, row player is maximizer, coloumn player is minimizer.\n",
    "\n",
    "Reference: A unified game-theoretic approach to multiagent reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 12.0, (1,), float32) Discrete(3)\n",
      "[[0], [0]]\n",
      "[[5], [5]] [0.5610583525729109, -0.5610583525729109] [False, False]\n",
      "[[6], [6]] [-0.7261994566288021, 0.7261994566288021] [False, False]\n",
      "[[9], [9]] [-0.8460871060267345, 0.8460871060267345] [True, True]\n",
      "Average initial state value of oracle Nash equilibrium for the first player:  -0.29607107415447115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.\n",
      "  warn(\"Converting G to a CSC matrix; may take a while.\")\n",
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.\n",
      "  warn(\"Converting A to a CSC matrix; may take a while.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from mars.env.mdp import ArbitraryMDP, MDPWrapper\n",
    "import numpy as np\n",
    "\n",
    "num_states = 3\n",
    "num_actions_per_player = 3\n",
    "num_trans = 3\n",
    "\n",
    "env = MDPWrapper(ArbitraryMDP(num_states=num_states, num_actions_per_player=num_actions_per_player, num_trans=num_trans))\n",
    "trans_matrices = env.env.trans_prob_matrices # shape: [dim_transition, dim_state, dim_action (p1*p2), dim_state]\n",
    "reward_matrices = env.env.reward_matrices # shape: [dim_transition, dim_state, dim_action (p1*p2), dim_state]\n",
    "\n",
    "oracle_nash_v, oracle_nash_q, oracle_nash_strategies = env.NEsolver(verbose=False)\n",
    "oracle_v_star = oracle_nash_v[0]\n",
    "\n",
    "oracle_v_star = np.mean(oracle_v_star, axis=0)\n",
    "print(env.observation_space, env.action_space)\n",
    "# env.render()\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "while not np.any(done):\n",
    "    obs, r, done, _ = env.step([1,0])\n",
    "    print(obs, r, done)\n",
    "print('Average initial state value of oracle Nash equilibrium for the first player: ', oracle_v_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 2, 3)\n",
      "(3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(oracle_nash_strategies).shape)\n",
    "oracle_max_policy = np.array(oracle_nash_strategies)[:, :, 0, :]\n",
    "print(oracle_max_policy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy shape: \n",
      "(3, 3)\n",
      "(3, 3, 3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3, 3, 3)\n",
      "Q table shape: \n",
      "(3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3, 3, 3, 3)\n",
      "itr: 0, exploitability: 0.7608750685387422\n",
      "itr: 10, exploitability: 0.9620188916888365\n",
      "itr: 20, exploitability: 0.8741372012292192\n",
      "itr: 30, exploitability: 0.8844367549274047\n",
      "itr: 40, exploitability: 0.887707735964259\n",
      "itr: 50, exploitability: 0.9012389829419045\n",
      "itr: 60, exploitability: 0.891333088680538\n",
      "itr: 70, exploitability: 0.8833786995437573\n",
      "itr: 80, exploitability: 0.906305477925333\n",
      "itr: 90, exploitability: 0.8994565816788184\n",
      "itr: 100, exploitability: 0.9007264337996839\n",
      "itr: 110, exploitability: 0.8989420974886003\n",
      "itr: 120, exploitability: 0.8967598924291824\n",
      "itr: 130, exploitability: 0.8967598936911625\n",
      "itr: 140, exploitability: 0.8972628376709233\n",
      "itr: 150, exploitability: 0.8973197716352229\n",
      "itr: 160, exploitability: 0.8973197715787659\n",
      "itr: 170, exploitability: 0.8978830853218919\n",
      "itr: 180, exploitability: 0.8976392298390121\n",
      "itr: 190, exploitability: 0.8953047480812938\n",
      "itr: 200, exploitability: 0.8977359872126831\n",
      "itr: 210, exploitability: 0.8977622701881512\n",
      "itr: 220, exploitability: 0.8970437039470933\n",
      "itr: 230, exploitability: 0.8976630326644094\n",
      "itr: 240, exploitability: 0.8997859737152085\n",
      "itr: 250, exploitability: 0.8997859736529732\n",
      "itr: 260, exploitability: 0.8991577513689317\n",
      "itr: 270, exploitability: 0.8990298989952297\n",
      "itr: 280, exploitability: 0.8990452175322803\n",
      "itr: 290, exploitability: 0.8999240963970239\n",
      "itr: 300, exploitability: 0.900164782444028\n",
      "itr: 310, exploitability: 0.8999432474855993\n",
      "itr: 320, exploitability: 0.8994521705054043\n",
      "itr: 330, exploitability: 0.8992925305309338\n",
      "itr: 340, exploitability: 0.8998235616071899\n",
      "itr: 350, exploitability: 0.89980279694025\n",
      "itr: 360, exploitability: 0.8992959585402449\n",
      "itr: 370, exploitability: 0.8995672301701776\n",
      "itr: 380, exploitability: 0.8967790259888944\n",
      "itr: 390, exploitability: 0.897975108485749\n",
      "itr: 400, exploitability: 0.8979447370800366\n",
      "itr: 410, exploitability: 0.8984896473543665\n",
      "itr: 420, exploitability: 0.8970491315984969\n",
      "itr: 430, exploitability: 0.8988670148168048\n",
      "itr: 440, exploitability: 0.8999486761128179\n",
      "itr: 450, exploitability: 0.8972652072143974\n",
      "itr: 460, exploitability: 0.8974718435111234\n",
      "itr: 470, exploitability: 0.8974718447880308\n",
      "itr: 480, exploitability: 0.8970158584072179\n",
      "itr: 490, exploitability: 0.8970158583831775\n",
      "itr: 500, exploitability: 0.8970158583814273\n",
      "itr: 510, exploitability: 0.8966750569938093\n",
      "itr: 520, exploitability: 0.8971773145989068\n",
      "itr: 530, exploitability: 0.8970364458159906\n",
      "itr: 540, exploitability: 0.8972856304598951\n",
      "itr: 550, exploitability: 0.8972856300701197\n",
      "itr: 560, exploitability: 0.8972856297778665\n",
      "itr: 570, exploitability: 0.8972856305430341\n",
      "itr: 580, exploitability: 0.8965500608830441\n",
      "itr: 590, exploitability: 0.8965500608841779\n",
      "itr: 600, exploitability: 0.8965500609167659\n",
      "itr: 610, exploitability: 0.8965500608959711\n",
      "itr: 620, exploitability: 0.8965500608858644\n",
      "itr: 630, exploitability: 0.8965500609921545\n",
      "itr: 640, exploitability: 0.8965500613878459\n",
      "itr: 650, exploitability: 0.8965500609159369\n",
      "itr: 660, exploitability: 0.8965500608918524\n",
      "itr: 670, exploitability: 0.8965500608922938\n",
      "itr: 680, exploitability: 0.8965500609298256\n",
      "itr: 690, exploitability: 0.8965500608865178\n",
      "itr: 700, exploitability: 0.8965500608868991\n",
      "itr: 710, exploitability: 0.8965500608864726\n",
      "itr: 720, exploitability: 0.8965500608748829\n",
      "itr: 730, exploitability: 0.8965500619943242\n",
      "itr: 740, exploitability: 0.8965500614252134\n",
      "itr: 750, exploitability: 0.8965500608813723\n",
      "itr: 760, exploitability: 0.8965500616905422\n",
      "itr: 770, exploitability: 0.8965500608686949\n",
      "itr: 780, exploitability: 0.8965500609060162\n",
      "itr: 790, exploitability: 0.8965500608805892\n",
      "itr: 800, exploitability: 0.8965500614519408\n",
      "itr: 810, exploitability: 0.8965500610773076\n",
      "itr: 820, exploitability: 0.8965500608928434\n",
      "itr: 830, exploitability: 0.8965500609563831\n",
      "itr: 840, exploitability: 0.8965500608951328\n",
      "itr: 850, exploitability: 0.8965500611192587\n",
      "itr: 860, exploitability: 0.8965500608806809\n",
      "itr: 870, exploitability: 0.8965500619973725\n",
      "itr: 880, exploitability: 0.8965500617533578\n",
      "itr: 890, exploitability: 0.8965500609796415\n",
      "itr: 900, exploitability: 0.8965500608818031\n",
      "itr: 910, exploitability: 0.8965500608732797\n",
      "itr: 920, exploitability: 0.8965500611015469\n",
      "itr: 930, exploitability: 0.8965500609140455\n",
      "itr: 940, exploitability: 0.8965500614256392\n",
      "itr: 950, exploitability: 0.8965500608852783\n",
      "itr: 960, exploitability: 0.8965500611203885\n",
      "itr: 970, exploitability: 0.896550060877308\n",
      "itr: 980, exploitability: 0.8965500608659323\n",
      "itr: 990, exploitability: 0.8965500608708286\n",
      "itr: 1000, exploitability: 0.8965500608716572\n",
      "itr: 1010, exploitability: 0.8965500609741609\n",
      "itr: 1020, exploitability: 0.8965500611930284\n",
      "itr: 1030, exploitability: 0.8965500610089039\n",
      "itr: 1040, exploitability: 0.8965500612518368\n",
      "itr: 1050, exploitability: 0.8965500609784467\n",
      "itr: 1060, exploitability: 0.8965500609033465\n",
      "itr: 1070, exploitability: 0.8965500609210773\n",
      "itr: 1080, exploitability: 0.8965500611692527\n",
      "itr: 1090, exploitability: 0.8965500608850158\n",
      "itr: 1100, exploitability: 0.8965500608739514\n",
      "itr: 1110, exploitability: 0.8965500609558459\n",
      "itr: 1120, exploitability: 0.8965500608799236\n",
      "itr: 1130, exploitability: 0.896550061247185\n",
      "itr: 1140, exploitability: 0.8965500609159345\n",
      "itr: 1150, exploitability: 0.8973479827696456\n",
      "itr: 1160, exploitability: 0.8958054960275974\n",
      "itr: 1170, exploitability: 0.8961670140046397\n",
      "itr: 1180, exploitability: 0.8961670141296685\n",
      "itr: 1190, exploitability: 0.8961365375889585\n",
      "itr: 1200, exploitability: 0.8970422969471058\n",
      "itr: 1210, exploitability: 0.8970422969421553\n",
      "itr: 1220, exploitability: 0.8968270433205368\n",
      "itr: 1230, exploitability: 0.8968270432784886\n",
      "itr: 1240, exploitability: 0.8968270432798591\n",
      "itr: 1250, exploitability: 0.8962775903651716\n",
      "itr: 1260, exploitability: 0.8959508477699265\n",
      "itr: 1270, exploitability: 0.8961196258978392\n",
      "itr: 1280, exploitability: 0.8946673156605627\n",
      "itr: 1290, exploitability: 0.8946673154270621\n",
      "itr: 1300, exploitability: 0.8970731711915574\n",
      "itr: 1310, exploitability: 0.8970731717773761\n",
      "itr: 1320, exploitability: 0.8970731710801957\n",
      "itr: 1330, exploitability: 0.8970731711967799\n",
      "itr: 1340, exploitability: 0.8970731718225525\n",
      "itr: 1350, exploitability: 0.897073169624357\n",
      "itr: 1360, exploitability: 0.8970731717803436\n",
      "itr: 1370, exploitability: 0.8970731708268743\n",
      "itr: 1380, exploitability: 0.8970731714944125\n",
      "itr: 1390, exploitability: 0.8970731716365696\n",
      "itr: 1400, exploitability: 0.8970731716464897\n",
      "itr: 1410, exploitability: 0.8970731718004589\n",
      "itr: 1420, exploitability: 0.8970731717642537\n",
      "itr: 1430, exploitability: 0.8969887024051537\n",
      "itr: 1440, exploitability: 0.8969887024749543\n",
      "itr: 1450, exploitability: 0.8969887028501775\n",
      "itr: 1460, exploitability: 0.8969887025995902\n",
      "itr: 1470, exploitability: 0.8969887023968308\n",
      "itr: 1480, exploitability: 0.89698870239418\n",
      "itr: 1490, exploitability: 0.8969887024164187\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from mars.equilibrium_solver import NashEquilibriumECOSSolver, NashEquilibriumCVXPYSolver\n",
    "\n",
    "def sample_from_categorical(dist):\n",
    "    \"\"\"\n",
    "    sample once from a categorical distribution, return the entry index.\n",
    "    dist: should be a list or array of probabilities for a categorical distribution\n",
    "    \"\"\"\n",
    "    sample_id = np.argmax(np.random.multinomial(1, dist))\n",
    "    sample_prob = dist[sample_id]\n",
    "    return sample_id, sample_prob\n",
    "\n",
    "def unified_state(s):\n",
    "    unified_s = s[0]%num_states\n",
    "    return unified_s\n",
    "\n",
    "def create_expand_policy(zero_ini=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim, action_dim],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    policies = []\n",
    "    intial_dim = [num_states]\n",
    "    for i in range(num_trans):\n",
    "        if zero_ini:\n",
    "            policy =  (1./num_actions_per_player) * np.zeros((*intial_dim, num_actions_per_player))\n",
    "        else:\n",
    "            policy =  (1./num_actions_per_player) * np.ones((*intial_dim, num_actions_per_player))\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_actions_per_player, num_actions_per_player, num_states])\n",
    "        policies.append(policy)\n",
    "\n",
    "    return policies\n",
    "\n",
    "def create_expand_value():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim],\n",
    "        ...\n",
    "    ]    \n",
    "    \"\"\"\n",
    "    values = []\n",
    "    intial_dim = [num_states]\n",
    "    for i in range(num_trans):\n",
    "        value =  (1./num_actions_per_player) * np.ones(intial_dim)\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_actions_per_player, num_actions_per_player, num_states])\n",
    "        values.append(value)\n",
    "\n",
    "    return values\n",
    "\n",
    "def create_expand_Q():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim, action_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim, action_dim, action_dim],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    intial_dim = [num_states, num_actions_per_player, num_actions_per_player]\n",
    "    for i in range(num_trans):\n",
    "        value =  (1./num_actions_per_player) * np.ones(intial_dim)\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_states, num_actions_per_player, num_actions_per_player])\n",
    "        values.append(value)\n",
    "\n",
    "    return values\n",
    "\n",
    "def get_posterior_policy(policy_set, meta_prior, likelihood, con_seq, only_update_observed=True):\n",
    "    posterior_policy = create_expand_policy(zero_ini=True)  # zero initiate to sum later\n",
    "    num_policies = len(policy_set)\n",
    "    denom = meta_prior @ likelihood  # [#policy] * [#policy, H] -> [H]\n",
    "    if only_update_observed:  # only update observed sequences, others use prior\n",
    "        for pi_i, rho_i, likelihood_per_policy in zip(policy_set, meta_prior, likelihood):  # loop over policy\n",
    "            for i, (p, d, l, s) in enumerate(zip(pi_i, denom, likelihood_per_policy, con_seq)): # loop over transition \n",
    "                posterior_policy[i] += np.array(p*rho_i)  # all entries using prior\n",
    "\n",
    "        for pi_i in policy_set:  # loop over policy\n",
    "            for i, s in enumerate(con_seq): # loop over transition \n",
    "                posterior_policy[i][tuple(s)] = 0  # clear the observed entries\n",
    "\n",
    "        for pi_i, rho_i, likelihood_per_policy in zip(policy_set, meta_prior, likelihood):  # loop over policy\n",
    "            for i, (p, d, l, s) in enumerate(zip(pi_i, denom, likelihood_per_policy, con_seq)): # loop over transition \n",
    "                posterior_policy[i][tuple(s)] += np.array(p[tuple(s)]*rho_i*l/d)  # only update observed\n",
    "\n",
    "    else:  # update all entries\n",
    "        for pi_i, rho_i, likelihood_per_policy in zip(policy_set, meta_prior, likelihood):  # loop over policy\n",
    "            for i, (p, d, l) in enumerate(zip(pi_i, denom, likelihood_per_policy)): # loop over transition\n",
    "                posterior_policy[i] += np.array(p*rho_i*l/d)  # sum over policies in mixture\n",
    "\n",
    "    return posterior_policy\n",
    "\n",
    "def broadcast_shape(input, output_shape):\n",
    "    \"\"\" broadcast input to have the same shape as output_to_be \"\"\"\n",
    "    len_input_shape = len(np.array(input).shape)\n",
    "    incrs_shape = output_shape[:-len_input_shape]+len_input_shape*(1,)\n",
    "    output = np.tile(input, incrs_shape)\n",
    "    return output\n",
    "\n",
    "def get_best_response_policy(player, prob_seq, con_seq):\n",
    "    given_policy = get_posterior_policy(player['policy_set'], player['meta_strategy'], prob_seq, con_seq)\n",
    "    br_policy = create_expand_policy()  # best response policy\n",
    "    br_v = create_expand_value()\n",
    "    br_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "\n",
    "        expand_rm = broadcast_shape(rm_, br_q[i].shape+(num_states,))  # broadcast the shape of reward matrix to be expand Q shape plus additional state dimension\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            br_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = br_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            br_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        if player['side'] == 'max':\n",
    "            mu_dot_q = np.einsum('...i, ...ij->...j', given_policy[i], br_q[i])\n",
    "            arg_id = np.argmin(mu_dot_q, axis=-1)  # min player takes minimum as best response against max player's policy\n",
    "            br_v[i] = np.min(mu_dot_q, axis=-1)\n",
    "        else:\n",
    "            q_dot_nu = np.einsum('...ij, ...j->...i', br_q[i], given_policy[i])\n",
    "            arg_id = np.argmax(q_dot_nu, axis=-1)   # vice versa    \n",
    "            br_v[i] = np.max(q_dot_nu, axis=-1)     \n",
    "\n",
    "        br_policy[i] = (np.arange(num_actions_per_player) == arg_id[...,None]).astype(int)  # from extreme (min/max) idx to one-hot simplex\n",
    "\n",
    "        # print(br_policy[i].shape, br_q[i].shape)\n",
    "\n",
    "    return br_policy\n",
    "\n",
    "def best_response_value(trans_prob_matrices, reward_matrices, policy, num_actions, side='max'):\n",
    "    \"\"\"\n",
    "    Formulas for calculating best response values:\n",
    "    1. Nash strategies: (\\pi_a^*, \\pi_b^*) = \\min \\max Q(s,a,b), \n",
    "        where Q(s,a,b) = r(s,a,b) + \\gamma \\min \\max Q(s',a',b') (this is the definition of Nash Q-value);\n",
    "    2. Best response (of max player) value: Br V(s) = \\min_b \\pi(s,a) Br Q(s,a,b)  (Br Q is the oracle best response Q value)\n",
    "    \"\"\"\n",
    "    br_v = create_expand_value()\n",
    "    br_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        expand_rm = broadcast_shape(rm_, br_q[i].shape+(num_states,))\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            br_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = br_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            br_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        if side == 'max':\n",
    "            mu_dot_q = np.einsum('...i, ...ij->...j', policy[i], br_q[i])\n",
    "            br_v[i] = np.min(mu_dot_q, axis=-1)\n",
    "        else:\n",
    "            q_dot_nu = np.einsum('...ij, ...j->...i', br_q[i], policy[i])\n",
    "            br_v[i] = np.max(q_dot_nu, axis=-1)         \n",
    "\n",
    "    avg_init_br_v = -np.mean(br_v[0])  # average best response value of initial states; minus for making it positive\n",
    "    return avg_init_br_v\n",
    "\n",
    "def get_best_response_value(player):\n",
    "    policy_set = player['policy_set']\n",
    "    meta_strategy = player['meta_strategy']\n",
    "    per_policy_exploits = []\n",
    "    for p in policy_set:\n",
    "        per_policy_exploits.append(best_response_value(trans_matrices, reward_matrices, p, num_actions_per_player, player['side']))\n",
    "    per_policy_exploits = np.array(per_policy_exploits)\n",
    "    exploitability = per_policy_exploits @ meta_strategy  # average over policy set weighted by meta strategy\n",
    "    return exploitability\n",
    "\n",
    "def policy_against_policy_value(trans_prob_matrices, reward_matrices, max_policy, min_policy, num_actions):\n",
    "    \"\"\"\n",
    "    Formulas for calculating best response values:\n",
    "    1. Nash strategies: (\\pi_a^*, \\pi_b^*) = \\min \\max Q(s,a,b), \n",
    "        where Q(s,a,b) = r(s,a,b) + \\gamma \\min \\max Q(s',a',b') (this is the definition of Nash Q-value);\n",
    "    2. Best response (of max player) value: Br V(s) = \\min_b \\pi(s,a) Br Q(s,a,b)  (Br Q is the oracle best response Q value)\n",
    "    \"\"\"\n",
    "    eval_v = create_expand_value()\n",
    "    eval_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        expand_rm = broadcast_shape(rm_, eval_q[i].shape+(num_states,))\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            eval_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = eval_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            eval_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        mu_dot_q = np.einsum('...i, ...ij->...j', max_policy[i], eval_q[i])\n",
    "        mu_dot_q_dot_nu = np.einsum('...kj, ...kj->...k', mu_dot_q, min_policy[i])\n",
    "        eval_v[i] = mu_dot_q_dot_nu    \n",
    "\n",
    "    avg_init_eval_v = np.mean(eval_v[0])  # average best response value of initial states; minus for making it positive\n",
    "    return avg_init_eval_v\n",
    "\n",
    "def update_matrix(matrix, side, row):\n",
    "    \"\"\"\n",
    "    Adding a new policy to the league will add a row or column to the present evaluation matrix, after it has\n",
    "    been evaluated against all policies in the opponent's league. Whether adding a row or column depends on whether\n",
    "    it is the row player (idx=0) or column player (idx=1). \n",
    "    For example, if current evaluation matrix is:\n",
    "    2  1\n",
    "    -1 3\n",
    "    After adding a new policy for row player with evaluated average episode reward (a,b) against current two policies in \n",
    "    the league of the column player, it gives: \n",
    "    it gives:\n",
    "    2   1\n",
    "    -1  3\n",
    "    a   b    \n",
    "    \"\"\" \n",
    "    if side == 'max': # for row player\n",
    "        if matrix.shape[0] == 0: \n",
    "            matrix = np.array([row])\n",
    "        else:  # add row\n",
    "            matrix = np.vstack([matrix, row])\n",
    "    else: # for column player\n",
    "        if matrix.shape[0] == 0:  # the first update\n",
    "            matrix = np.array([row]) \n",
    "        else:  # add column\n",
    "            matrix=np.hstack([matrix, np.array([row]).T])\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def sample_policy(policy_set, dist):\n",
    "    policy_id, _ = sample_from_categorical(dist)\n",
    "    policy = policy_set[policy_id]   \n",
    "    return policy \n",
    "\n",
    "# test_policy = create_expand_policy() \n",
    "# get_best_response(test_policy)\n",
    "\n",
    "def psro(env, save_path, solve_episodes = 1000):\n",
    "    evaluation_matrix = np.array([[0]])\n",
    "    ini_max_policy = create_expand_policy()\n",
    "    ini_min_policy = create_expand_policy()\n",
    "    max_player = {\n",
    "        'side': 'max',\n",
    "        'policy_set': [ini_max_policy],\n",
    "        'meta_strategy': np.array([1.])\n",
    "    }\n",
    "    min_player = {\n",
    "        'side': 'min',\n",
    "        'policy_set': [ini_min_policy],\n",
    "        'meta_strategy': np.array([1.])\n",
    "    }\n",
    "\n",
    "    max_side_q_table = create_expand_Q()\n",
    "    min_side_q_table = create_expand_Q()\n",
    " \n",
    "    print('policy shape: ')\n",
    "    for p in ini_max_policy:\n",
    "        print(p.shape)\n",
    "    print('Q table shape: ')\n",
    "    for q in max_side_q_table:\n",
    "        print(q.shape)\n",
    "\n",
    "    exploitability_records = []\n",
    "\n",
    "    for i in range(solve_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        max_player_observed_sequence = []\n",
    "        min_player_observed_sequence = []\n",
    "        max_player_conditional_sequence = []\n",
    "        min_player_conditional_sequence = []\n",
    "        max_player_action_prob_sequence = [[] for _ in max_player['policy_set']]\n",
    "        min_player_action_prob_sequence = [[] for _ in min_player['policy_set']]\n",
    "        shared_sequence_before_actions = []\n",
    "        step = 0\n",
    "\n",
    "        # sample policy from meta strategy for the current episode\n",
    "        max_policy = sample_policy(max_player['policy_set'], max_player['meta_strategy'])\n",
    "        min_policy = sample_policy(min_player['policy_set'], min_player['meta_strategy'])\n",
    "\n",
    "        while not np.any(done):    \n",
    "            # get observed sequence\n",
    "            max_player_observed_sequence.extend([unified_state(s[0])])\n",
    "            min_player_observed_sequence.extend([unified_state(s[1])])\n",
    "            shared_sequence_before_actions = copy.deepcopy(max_player_observed_sequence)\n",
    "\n",
    "            # get action distribution given specific observed history\n",
    "            max_policy_to_choose = max_policy[step][tuple(max_player_observed_sequence)]  # a=np.ones([1,2]), b=(0,0), a[b] -> 1.\n",
    "            min_policy_to_choose = min_policy[step][tuple(min_player_observed_sequence)]\n",
    "\n",
    "            # choose action\n",
    "            max_action, _ = sample_from_categorical(max_policy_to_choose)\n",
    "            min_action, _ = sample_from_categorical(min_policy_to_choose)\n",
    "\n",
    "            # roullout info for mixture policy\n",
    "            if i % 2 == 0:  # update min player side\n",
    "                for p_id, p in enumerate(max_player['policy_set']):  # get trajectory probabilities for each policy in policy set\n",
    "                    max_p = p[step][tuple(max_player_observed_sequence)]\n",
    "                    # _, max_a_prob = sample_from_categorical(max_p) # this is wrong\n",
    "                    max_a_prob = max_p[max_action]  # get the probability of real action (rollout) under each policy in policy set\n",
    "                    if step == 0:\n",
    "                        max_player_action_prob_sequence[p_id].append(max_a_prob)\n",
    "                    else:\n",
    "                        max_player_action_prob_sequence[p_id].append(max_player_action_prob_sequence[p_id][-1]*max_a_prob)  # [p1, p1p2, p1p2p3, ...]\n",
    "                max_player_conditional_sequence.append(copy.deepcopy(max_player_observed_sequence))\n",
    "\n",
    "            else:\n",
    "                for p_id, p in enumerate(min_player['policy_set']):\n",
    "                    min_p = p[step][tuple(min_player_observed_sequence)]\n",
    "                    # _, min_a_prob = sample_from_categorical(min_p)  # this is wrong\n",
    "                    min_a_prob = min_p[min_action]  # get the probability of real action (rollout) under each policy in policy set\n",
    "                    if step == 0:\n",
    "                        min_player_action_prob_sequence[p_id].append(min_a_prob)\n",
    "                    else:\n",
    "                        min_player_action_prob_sequence[p_id].append(min_player_action_prob_sequence[p_id][-1]*min_a_prob)  # [p1, p1p2, p1p2p3, ...]\n",
    "                min_player_conditional_sequence.append(copy.deepcopy(min_player_observed_sequence))\n",
    "\n",
    "            action = [max_action, min_action]\n",
    "            max_player_observed_sequence.extend(action)\n",
    "            min_player_observed_sequence.extend(action)\n",
    "\n",
    "            s_, r, done, _  = env.step(action)  \n",
    "            s = s_\n",
    "\n",
    "            step += 1\n",
    "        \n",
    "        if i % 2 == 0:  # update min player side\n",
    "            new_policy = get_best_response_policy(max_player, np.array(max_player_action_prob_sequence), max_player_conditional_sequence)  # get best response against the max player, max_player_action_prob_sequence: [#policies, H]\n",
    "            min_player['policy_set'].append(new_policy) # add new policy to policy set to form mixture\n",
    "\n",
    "            # update evaluation matrix\n",
    "            eval_scores = []\n",
    "            for p in max_player['policy_set']:\n",
    "                score = policy_against_policy_value(trans_matrices, reward_matrices, max_policy=p, min_policy=new_policy, num_actions=num_actions_per_player)\n",
    "                eval_scores.append(score)  # score always from the max player's view\n",
    "            evaluation_matrix = update_matrix(evaluation_matrix, 'min', eval_scores)\n",
    "\n",
    "        else: \n",
    "            new_policy = get_best_response_policy(min_player, np.array(min_player_action_prob_sequence), min_player_conditional_sequence) # get best response against the min player\n",
    "            max_player['policy_set'].append(new_policy) # add new policy to policy set to form mixture\n",
    "\n",
    "            # update evaluation matrix\n",
    "            eval_scores = []\n",
    "            for p in min_player['policy_set']:\n",
    "                score = policy_against_policy_value(trans_matrices, reward_matrices, max_policy=new_policy, min_policy=p, num_actions=num_actions_per_player)\n",
    "                eval_scores.append(score)  # score always from the max player's view\n",
    "            evaluation_matrix = update_matrix(evaluation_matrix, 'max', eval_scores)\n",
    "\n",
    "        if len(max_player['policy_set']) * len(min_player['policy_set']) >= 2:  # enough policies to get Nash\n",
    "            (max_player['meta_strategy'], min_player['meta_strategy']), _ = NashEquilibriumCVXPYSolver(evaluation_matrix)\n",
    "        else: # uniform for initialization only\n",
    "            max_policies = len(max_player['policy_set'])\n",
    "            max_player['meta_strategy'] = 1./max_policies*np.ones(max_policies)\n",
    "            min_policies = len(min_player['policy_set'])\n",
    "            min_player['meta_strategy'] = 1./min_policies*np.ones(min_policies)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            exploitability = get_best_response_value(max_player)  # best response of the max player\n",
    "            print(f'itr: {i}, exploitability: {exploitability}', )\n",
    "            exploitability_records.append(exploitability)\n",
    "            np.save(save_path, exploitability_records)\n",
    "\n",
    "save_path = 'psro_exp_stochastic.npy'\n",
    "psro(env, save_path, solve_episodes=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe787014f98>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "exp_data = np.load(save_path)\n",
    "plt.title('PSRO')\n",
    "plt.xlabel('iter(x10)')\n",
    "plt.ylabel('exploitability')\n",
    "plt.plot(exp_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28c6861e59928cb790236f7047915368f37afc12f670e78fd0101a6f825a02b1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('x': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
