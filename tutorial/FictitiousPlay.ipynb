{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fictitious Play\n",
    "This tutorial  demonstrates fictitious play (FP) on a zero-sum strategic (non-extensive) game. \n",
    "\n",
    "For the payoff matrix, row player is maximizer, coloumn player is minimizer.\n",
    "\n",
    "Reference: https://towardsdatascience.com/introduction-to-fictitious-play-12a8bc4ed1bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/quantumiracle/.conda/envs/robo/lib/python3.6/site-packages (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/quantumiracle/.conda/envs/robo/lib/python3.6/site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/quantumiracle/.conda/envs/robo/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /home/quantumiracle/.conda/envs/robo/lib/python3.6/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/quantumiracle/.local/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Game matrix: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  0  2 -1\n",
       "1 -1  0  1\n",
       "2  1 -1  0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "! pip install pandas\n",
    "import pandas as pd\n",
    "# GameMatrix = np.array([[-2,3], [3,-4]])\n",
    "GameMatrix = np.array([[0,2,-1], [-1,0,1], [1,-1,0]])\n",
    "\n",
    "Itr = 10000\n",
    "print('Game matrix: ')\n",
    "pd.DataFrame(GameMatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For row player, strategy is [0.25   0.3332 0.4168], game value [lower bound, upper bound]: [0.0836 0.0832 0.0832]\n",
      "For column player, strategy is [0.3333 0.2499 0.4168], game value [lower bound, upper bound]: [0.083  0.0835 0.0834]\n"
     ]
    }
   ],
   "source": [
    "# a random initial step on row\n",
    "row_value = np.zeros(GameMatrix.shape[0])\n",
    "col_value = np.zeros(GameMatrix.shape[1])\n",
    "min_id_list, max_id_list = [], []\n",
    "\n",
    "for i in range(Itr):\n",
    "    # current l is row\n",
    "    min_id = np.argmin(row_value)\n",
    "    min_id_list.append(min_id)\n",
    "    l = GameMatrix[:, min_id] # l is column now\n",
    "    col_value += np.array(l)\n",
    "    \n",
    "    # current l is column\n",
    "    max_id = np.argmax(col_value)\n",
    "    max_id_list.append(max_id)\n",
    "    l = GameMatrix[max_id]  #  l is row now\n",
    "    row_value += np.array(l)\n",
    "    \n",
    "# The statistical frequencies of occurrence of different entries are just their probability masses in final policy,\n",
    "# which is the average best response over history.\n",
    "hist, _ = np.histogram(max_id_list, bins=GameMatrix.shape[0])\n",
    "max_policy = hist/np.sum(hist)\n",
    "hist, _ = np.histogram(min_id_list, bins=GameMatrix.shape[1])\n",
    "min_policy = hist/np.sum(hist)\n",
    "\n",
    "row_value = row_value / (i+1)  # historical average\n",
    "col_value = col_value / (i+1)\n",
    "print(f'For row player, strategy is {max_policy}, game value [lower bound, upper bound]: {row_value}')\n",
    "print(f'For column player, strategy is {min_policy}, game value [lower bound, upper bound]: {col_value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Fictious Self-Play\n",
    "Here we try to demonstrate the fictious self-play algorithm (or say a non-neural verison of Neural Fictious Self-Play, NFSP, [2]) on a zero-sum strategic (non-extensive) game.\n",
    "\n",
    "Note that in original paper of FSP [1], the algorithm of FSP actually uses the supervised learning (SL) and reinforcement learning (RL) language to describe it, quite close to NFSP. SL is for achieving the average policy and RL is for reaching the best response policy. But here we have simple normal-form game with utility written in matrix, so it is a non-neural version. For the average policy, from above FP example we can see, it can be actually directly estimated with the historical frequency of actions for each player. Best response policy is easily calculated since it should be deterministic.\n",
    "\n",
    "Also note that it is not the XFP algorithm for extensive-form games.\n",
    "\n",
    "Refereneces:\n",
    "\n",
    "[1] FSP http://proceedings.mlr.press/v37/heinrich15.html\n",
    "\n",
    "[2] NFSP https://arxiv.org/abs/1603.01121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/quantumiracle/.conda/envs/robo/lib/python3.6/site-packages (0.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/quantumiracle/.conda/envs/robo/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /home/quantumiracle/.conda/envs/robo/lib/python3.6/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/quantumiracle/.conda/envs/robo/lib/python3.6/site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/quantumiracle/.local/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Game matrix: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  0  2 -1\n",
       "1 -1  0  1\n",
       "2  1 -1  0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "! pip install pandas\n",
    "import pandas as pd\n",
    "# GameMatrix = np.array([[-2,3], [3,-4]])\n",
    "GameMatrix = np.array([[0,2,-1], [-1,0,1], [1,-1,0]])\n",
    "\n",
    "print('Game matrix: ')\n",
    "pd.DataFrame(GameMatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For row player, strategy is [0.49797776 0.42163802 0.08038423], game value [lower bound, upper bound]: [ 0.0155  0.948  -0.3812]\n",
      "For column player, strategy is [0.04058116 0.13777555 0.82164329], game value [lower bound, upper bound]: [-0.0518  0.4711 -0.1458]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters for FSP\n",
    "Eta = 0.2 # eta=1, only best response policy as FP; eta=0, only average policy; see either [1] or [2]\n",
    "Avg_warmup = 0.2 # start to get average policy after certain ratio of overall iterations\n",
    "Itr = 10000\n",
    "\n",
    "def sample_from_categorical(dist):\n",
    "    \"\"\"\n",
    "    sample once from a categorical distribution, return the entry index.\n",
    "    dist: should be a list or array of probabilities for a categorical distribution\n",
    "    \"\"\"\n",
    "    return np.argmax(np.random.multinomial(1, dist))\n",
    "\n",
    "# a random initial step on row\n",
    "row_value = np.zeros(GameMatrix.shape[0])\n",
    "col_value = np.zeros(GameMatrix.shape[1])\n",
    "min_id_list, max_id_list = [], []\n",
    "\n",
    "# initialize the average policy as uniform\n",
    "max_policy = 1./GameMatrix.shape[0]*np.ones(GameMatrix.shape[0])\n",
    "min_policy = 1./GameMatrix.shape[1]*np.ones(GameMatrix.shape[1])\n",
    "\n",
    "for i in range(Itr):\n",
    "    # current l is row\n",
    "    if i< Avg_warmup or np.isnan(max_policy).any() or sample_from_categorical([1-Eta, Eta]): # if True, the case under Eta, best response (greedy) policy\n",
    "        min_id = np.argmin(row_value)\n",
    "        min_id_list.append(min_id)  # the calculation of average policy only uses actions from the best response policy\n",
    "    else:  # sample action from the average policy\n",
    "        min_id = sample_from_categorical(min_policy)\n",
    "    l = GameMatrix[:, min_id] # l is column now\n",
    "    col_value += np.array(l)\n",
    "    \n",
    "    # current l is column\n",
    "    if i< Avg_warmup or np.isnan(max_policy).any() or sample_from_categorical([1-Eta, Eta]):\n",
    "        max_id = np.argmax(col_value)\n",
    "        max_id_list.append(max_id)\n",
    "    else:\n",
    "        max_id = sample_from_categorical(max_policy)\n",
    "    l = GameMatrix[max_id]  #  l is row now\n",
    "    row_value += np.array(l)\n",
    "    \n",
    "    # get the average policy for each player at each iteration\n",
    "    hist, _ = np.histogram(max_id_list, bins=GameMatrix.shape[0])\n",
    "    max_policy = hist/np.sum(hist)\n",
    "    hist, _ = np.histogram(min_id_list, bins=GameMatrix.shape[1])\n",
    "    min_policy = hist/np.sum(hist)\n",
    "    \n",
    "\n",
    "# The statistical frequencies of occurrence of different entries are just their probability masses in final policy,\n",
    "# which is the average best response over history.\n",
    "hist, _ = np.histogram(max_id_list, bins=GameMatrix.shape[0])\n",
    "max_policy = hist/np.sum(hist)\n",
    "hist, _ = np.histogram(min_id_list, bins=GameMatrix.shape[1])\n",
    "min_policy = hist/np.sum(hist)\n",
    "\n",
    "row_value = row_value / (i+1)  # historical average\n",
    "col_value = col_value / (i+1)\n",
    "print(f'For row player, strategy is {max_policy}, game value [lower bound, upper bound]: {row_value}')\n",
    "print(f'For column player, strategy is {min_policy}, game value [lower bound, upper bound]: {col_value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it seems FSP does not work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modified version, never get best response against the opponent's best response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [0.33333333 0.33333333 0.33333333]\n",
      "[1] [0. 1. 0.]\n",
      "[1, 0] [0. 1. 0.]\n",
      "[1, 0] [0.5 0.  0.5]\n",
      "[1, 0] [0.5 0.  0.5]\n",
      "[1, 0, 0] [0.5 0.  0.5]\n",
      "[1, 0, 0] [0.66666667 0.         0.33333333]\n",
      "[1, 0, 0] [0.66666667 0.         0.33333333]\n",
      "[1, 0, 0] [0.66666667 0.         0.33333333]\n",
      "[1, 0, 0] [0.66666667 0.         0.33333333]\n",
      "[1, 0, 0] [0.66666667 0.         0.33333333]\n",
      "[1, 0, 0, 1] [0.66666667 0.         0.33333333]\n",
      "[1, 0, 0, 1] [0.5 0.  0.5]\n",
      "[1, 0, 0, 1] [0.5 0.  0.5]\n",
      "[1, 0, 0, 1] [0.5 0.  0.5]\n",
      "[1, 0, 0, 1] [0.5 0.  0.5]\n",
      "[1, 0, 0, 1] [0.5 0.  0.5]\n",
      "[1, 0, 0, 1] [0.5 0.  0.5]\n",
      "[1, 0, 0, 1] [0.5 0.  0.5]\n",
      "[1, 0, 0, 1] [0.5 0.  0.5]\n",
      "For row player, strategy is [0.5 0.  0.5], game value [lower bound, upper bound]: [ 0.05 -0.05  0.  ]\n",
      "For column player, strategy is [0.5 0.  0.5], game value [lower bound, upper bound]: [-0.05  0.05  0.  ]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters for FSP\n",
    "Eta = 0.2 # eta=1, only best response policy as FP; eta=0, only average policy; see either [1] or [2]\n",
    "Avg_warmup = 0.2 # start to get average policy after certain ratio of overall iterations\n",
    "Itr = 20\n",
    "\n",
    "def sample_from_categorical(dist):\n",
    "    \"\"\"\n",
    "    sample once from a categorical distribution, return the entry index.\n",
    "    dist: should be a list or array of probabilities for a categorical distribution\n",
    "    \"\"\"\n",
    "    return np.argmax(np.random.multinomial(1, dist))\n",
    "\n",
    "# a random initial step on row\n",
    "row_value = np.zeros(GameMatrix.shape[0])\n",
    "col_value = np.zeros(GameMatrix.shape[1])\n",
    "min_id_list, max_id_list = [], []\n",
    "\n",
    "# initialize the average policy as uniform\n",
    "max_policy = 1./GameMatrix.shape[0]*np.ones(GameMatrix.shape[0])\n",
    "min_policy = 1./GameMatrix.shape[1]*np.ones(GameMatrix.shape[1])\n",
    "\n",
    "for i in range(Itr):\n",
    "    # current l is row\n",
    "    if i< Avg_warmup or np.isnan(max_policy).any() or sample_from_categorical([1-Eta, Eta]): # if True, the case under Eta, best response (greedy) policy\n",
    "        min_id = np.argmin(row_value)\n",
    "        min_id_list.append(min_id)  # the calculation of average policy only uses actions from the best response policy\n",
    "    # sample action from the average policy\n",
    "#     print(min_id_list, min_policy)\n",
    "    min_id = sample_from_categorical(min_policy)\n",
    "    l = GameMatrix[:, min_id] # l is column now\n",
    "    col_value = np.array(l)\n",
    "    \n",
    "    # current l is column\n",
    "    if i< Avg_warmup or np.isnan(max_policy).any() or sample_from_categorical([1-Eta, Eta]):\n",
    "        max_id = np.argmax(col_value)\n",
    "        max_id_list.append(max_id)\n",
    "    print(max_id_list, max_policy)\n",
    "    max_id = sample_from_categorical(max_policy)\n",
    "    l = GameMatrix[max_id]  #  l is row now\n",
    "    row_value = np.array(l)\n",
    "    \n",
    "    # get the average policy for each player at each iteration\n",
    "    hist, _ = np.histogram(max_id_list, bins=GameMatrix.shape[0])\n",
    "    max_policy = hist/np.sum(hist)\n",
    "    hist, _ = np.histogram(min_id_list, bins=GameMatrix.shape[1])\n",
    "    min_policy = hist/np.sum(hist)\n",
    "    \n",
    "\n",
    "# The statistical frequencies of occurrence of different entries are just their probability masses in final policy,\n",
    "# which is the average best response over history.\n",
    "hist, _ = np.histogram(max_id_list, bins=GameMatrix.shape[0])\n",
    "max_policy = hist/np.sum(hist)\n",
    "hist, _ = np.histogram(min_id_list, bins=GameMatrix.shape[1])\n",
    "min_policy = hist/np.sum(hist)\n",
    "\n",
    "row_value = row_value / (i+1)  # historical average\n",
    "col_value = col_value / (i+1)\n",
    "print(f'For row player, strategy is {max_policy}, game value [lower bound, upper bound]: {row_value}')\n",
    "print(f'For column player, strategy is {min_policy}, game value [lower bound, upper bound]: {col_value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test increasing eta version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
