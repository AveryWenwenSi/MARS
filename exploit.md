## Exploitability Test
We exploit the models trained with self-play using the exploiter model, which has the same basic settings (network architecture, optimizer, learning hyper-parameters, etc) as the trained ones. To demonstrate the improved exploitability via self-play, we compare two models: a 'good' model (trained for 5000 episodes via self-play, converge to some almost unexploitable policy) and a 'bad' model (trained for 500 episodes via self-play). For exploiting the two models, the exploiter is trained via RL setting for 2000 episodes. In experiments, we set the trained model as 'first_0' player and the exploiter as 'second_0' in *SlimeVolley-v0* environment.


**Bad model**:

<p float="left">
<img src="https://github.com/quantumiracle/MARS/blob/master/img/slimevolley_exploit/slimevolley_badmodel_reward.png" alt="drawing" height=220 width=310/>
<img src="https://github.com/quantumiracle/MARS/blob/master/img/slimevolley_exploit/slimevolley_badmodel_length.png" alt="drawing" height=220 width=310/>
<img src="https://github.com/quantumiracle/MARS/blob/master/img/slimevolley_exploit/slimevolley_badmodel_loss.png" alt="drawing" height=220 width=310/>
</p>

**Good model**:

<p float="left">
<img src="https://github.com/quantumiracle/MARS/blob/master/img/slimevolley_exploit/slimevolley_goodmodel_reward.png" alt="drawing" height=220 width=310/>
<img src="https://github.com/quantumiracle/MARS/blob/master/img/slimevolley_exploit/slimevolley_goodmodel_length.png" alt="drawing" height=220 width=310/>
<img src="https://github.com/quantumiracle/MARS/blob/master/img/slimevolley_exploit/slimevolley_goodmodel_loss.png" alt="drawing" height=220 width=310/>
</p>

Observations:
* The good trained model is significantly harder to be exploited by the exploiter, compared with the bad model. It can be seen that for the good model, 'second_0' almost never reach >0 reward value during the whole training process.
* At the initial training stage of the exploiter against the good model, the relatively high reward of the exploiter is probably caused by its atypical behavior due to the learning from scratch setting, while the good model in its lastest self-play stage is playing against an opponent also with good performance.
* The large episode length in the learning of exploiter against the good model shows that the two players both reach some high-performance behaviors, but the exploiter can sill not exploit the good model with positive reward. For the bad model, the episode length is not that high because the bad model is so easier to exploit without the need of playing a full episode of the game. 
