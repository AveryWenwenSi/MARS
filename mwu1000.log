pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7f9135eabed0>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 640, 'max_episodes': 101000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2.
Episode: 1/101000 (0.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3834s / 4.3834 s
agent0:                 episode reward: 2.0701,                 loss: nan
agent1:                 episode reward: -2.0701,                 loss: nan
Episode: 21/101000 (0.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1123s / 4.4957 s
agent0:                 episode reward: -0.0346,                 loss: nan
agent1:                 episode reward: 0.0346,                 loss: nan
Episode: 41/101000 (0.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1276s / 4.6233 s
agent0:                 episode reward: 0.0111,                 loss: nan
agent1:                 episode reward: -0.0111,                 loss: nan
Episode: 61/101000 (0.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1619s / 4.7852 s
agent0:                 episode reward: -0.1790,                 loss: nan
agent1:                 episode reward: 0.1790,                 loss: nan
Episode: 81/101000 (0.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1267s / 4.9119 s
agent0:                 episode reward: 0.0938,                 loss: nan
agent1:                 episode reward: -0.0938,                 loss: nan
Episode: 101/101000 (0.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1547s / 5.0665 s
agent0:                 episode reward: 0.1329,                 loss: nan
agent1:                 episode reward: -0.1329,                 loss: nan
Episode: 121/101000 (0.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1688s / 5.2354 s
agent0:                 episode reward: -0.0624,                 loss: nan
agent1:                 episode reward: 0.0624,                 loss: nan
Episode: 141/101000 (0.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1504s / 5.3857 s
agent0:                 episode reward: 0.1182,                 loss: nan
agent1:                 episode reward: -0.1182,                 loss: nan
Episode: 161/101000 (0.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1869s / 5.5726 s
agent0:                 episode reward: 0.3013,                 loss: nan
agent1:                 episode reward: -0.3013,                 loss: nan
Episode: 181/101000 (0.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2193s / 5.7919 s
agent0:                 episode reward: 0.0312,                 loss: nan
agent1:                 episode reward: -0.0312,                 loss: nan
Episode: 201/101000 (0.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1393s / 5.9312 s
agent0:                 episode reward: -0.1073,                 loss: nan
agent1:                 episode reward: 0.1073,                 loss: nan
Episode: 221/101000 (0.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 0.6162s / 6.5473 s
agent0:                 episode reward: 0.2818,                 loss: 0.1639
agent1:                 episode reward: -0.2818,                 loss: nan
Episode: 241/101000 (0.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1233s / 7.6706 s
agent0:                 episode reward: 0.0615,                 loss: 0.1544
agent1:                 episode reward: -0.0615,                 loss: nan
Episode: 261/101000 (0.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 0.9960s / 8.6666 s
agent0:                 episode reward: 0.0732,                 loss: 0.1519
agent1:                 episode reward: -0.0732,                 loss: nan
Episode: 281/101000 (0.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0928s / 9.7595 s
agent0:                 episode reward: 0.7408,                 loss: 0.1503
agent1:                 episode reward: -0.7408,                 loss: nan
Score delta: 1.6786324871453036, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/62_0.
Episode: 301/101000 (0.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2864s / 10.0459 s
agent0:                 episode reward: 0.1918,                 loss: nan
agent1:                 episode reward: -0.1918,                 loss: nan
Episode: 321/101000 (0.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2365s / 10.2823 s
agent0:                 episode reward: -0.1806,                 loss: nan
agent1:                 episode reward: 0.1806,                 loss: nan
Episode: 341/101000 (0.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2197s / 10.5020 s
agent0:                 episode reward: -0.2635,                 loss: nan
agent1:                 episode reward: 0.2635,                 loss: nan
Episode: 361/101000 (0.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1602s / 10.6622 s
agent0:                 episode reward: -0.2801,                 loss: nan
agent1:                 episode reward: 0.2801,                 loss: nan
Episode: 381/101000 (0.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1532s / 10.8154 s
agent0:                 episode reward: 0.1224,                 loss: nan
agent1:                 episode reward: -0.1224,                 loss: nan
Episode: 401/101000 (0.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1741s / 10.9895 s
agent0:                 episode reward: 0.2531,                 loss: nan
agent1:                 episode reward: -0.2531,                 loss: nan
Episode: 421/101000 (0.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1919s / 11.1814 s
agent0:                 episode reward: -0.2297,                 loss: nan
agent1:                 episode reward: 0.2297,                 loss: nan
Episode: 441/101000 (0.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3278s / 11.5092 s
agent0:                 episode reward: -0.1753,                 loss: nan
agent1:                 episode reward: 0.1753,                 loss: nan
Episode: 461/101000 (0.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1561s / 11.6653 s
agent0:                 episode reward: -0.1302,                 loss: nan
agent1:                 episode reward: 0.1302,                 loss: nan
Episode: 481/101000 (0.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1991s / 11.8644 s
agent0:                 episode reward: 0.1985,                 loss: nan
agent1:                 episode reward: -0.1985,                 loss: nan
Episode: 501/101000 (0.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1266s / 12.9910 s
agent0:                 episode reward: 0.2181,                 loss: nan
agent1:                 episode reward: -0.2181,                 loss: 0.1788
Episode: 521/101000 (0.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1312s / 14.1223 s
agent0:                 episode reward: -0.0506,                 loss: nan
agent1:                 episode reward: 0.0506,                 loss: 0.1693
Episode: 541/101000 (0.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4399s / 15.5621 s
agent0:                 episode reward: 0.0706,                 loss: nan
agent1:                 episode reward: -0.0706,                 loss: 0.1627
Episode: 561/101000 (0.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5875s / 17.1496 s
agent0:                 episode reward: 0.2102,                 loss: nan
agent1:                 episode reward: -0.2102,                 loss: 0.1574
Episode: 581/101000 (0.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1801s / 18.3297 s
agent0:                 episode reward: -0.1092,                 loss: nan
agent1:                 episode reward: 0.1092,                 loss: 0.1531
Episode: 601/101000 (0.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1910s / 19.5208 s
agent0:                 episode reward: -0.3231,                 loss: nan
agent1:                 episode reward: 0.3231,                 loss: 0.1486
Episode: 621/101000 (0.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3821s / 20.9028 s
agent0:                 episode reward: 0.3701,                 loss: nan
agent1:                 episode reward: -0.3701,                 loss: 0.1451
Episode: 641/101000 (0.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2926s / 22.1954 s
agent0:                 episode reward: -0.3109,                 loss: nan
agent1:                 episode reward: 0.3109,                 loss: 0.1420
Episode: 661/101000 (0.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6175s / 23.8129 s
agent0:                 episode reward: -0.0695,                 loss: nan
agent1:                 episode reward: 0.0695,                 loss: 0.1396
Episode: 681/101000 (0.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5611s / 25.3740 s
agent0:                 episode reward: 0.4793,                 loss: nan
agent1:                 episode reward: -0.4793,                 loss: 0.1362
Episode: 701/101000 (0.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5618s / 26.9358 s
agent0:                 episode reward: 0.2727,                 loss: nan
agent1:                 episode reward: -0.2727,                 loss: 0.1317
Episode: 721/101000 (0.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4205s / 28.3563 s
agent0:                 episode reward: -0.5096,                 loss: nan
agent1:                 episode reward: 0.5096,                 loss: 0.1288
Episode: 741/101000 (0.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5549s / 29.9112 s
agent0:                 episode reward: -0.6154,                 loss: 0.1496
agent1:                 episode reward: 0.6154,                 loss: 0.1268
Score delta: 1.565236289296665, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/296_1.
Episode: 761/101000 (0.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4399s / 31.3511 s
agent0:                 episode reward: -0.9950,                 loss: 0.1541
agent1:                 episode reward: 0.9950,                 loss: nan
Episode: 781/101000 (0.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6167s / 32.9679 s
agent0:                 episode reward: -0.5374,                 loss: 0.1538
agent1:                 episode reward: 0.5374,                 loss: nan
Episode: 801/101000 (0.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7550s / 34.7229 s
agent0:                 episode reward: -1.0576,                 loss: 0.1554
agent1:                 episode reward: 1.0576,                 loss: nan
Episode: 821/101000 (0.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5323s / 36.2552 s
agent0:                 episode reward: -1.0642,                 loss: 0.1559
agent1:                 episode reward: 1.0642,                 loss: nan
Episode: 841/101000 (0.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1871s / 37.4423 s
agent0:                 episode reward: -0.6135,                 loss: 0.1585
agent1:                 episode reward: 0.6135,                 loss: nan
Episode: 861/101000 (0.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7903s / 39.2326 s
agent0:                 episode reward: -0.8949,                 loss: 0.1578
agent1:                 episode reward: 0.8949,                 loss: nan
Episode: 881/101000 (0.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3061s / 40.5388 s
agent0:                 episode reward: -0.6260,                 loss: 0.1555
agent1:                 episode reward: 0.6260,                 loss: nan
Episode: 901/101000 (0.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1871s / 41.7259 s
agent0:                 episode reward: -0.9393,                 loss: 0.1538
agent1:                 episode reward: 0.9393,                 loss: nan
Episode: 921/101000 (0.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6741s / 43.4000 s
agent0:                 episode reward: 0.0421,                 loss: 0.1531
agent1:                 episode reward: -0.0421,                 loss: nan
Episode: 941/101000 (0.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4186s / 44.8185 s
agent0:                 episode reward: -0.3700,                 loss: 0.1534
agent1:                 episode reward: 0.3700,                 loss: nan
Episode: 961/101000 (0.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5306s / 46.3491 s
agent0:                 episode reward: -0.5452,                 loss: 0.1505
agent1:                 episode reward: 0.5452,                 loss: nan
Episode: 981/101000 (0.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5809s / 47.9300 s
agent0:                 episode reward: -0.4855,                 loss: 0.1468
agent1:                 episode reward: 0.4855,                 loss: nan
Episode: 1001/101000 (0.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7539s / 49.6839 s
agent0:                 episode reward: -0.4624,                 loss: 0.1487
agent1:                 episode reward: 0.4624,                 loss: nan
Episode: 1021/101000 (1.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0724s / 50.7563 s
agent0:                 episode reward: -0.6189,                 loss: 0.1596
agent1:                 episode reward: 0.6189,                 loss: nan
Episode: 1041/101000 (1.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5398s / 52.2961 s
agent0:                 episode reward: -0.7076,                 loss: 0.1572
agent1:                 episode reward: 0.7076,                 loss: nan
Episode: 1061/101000 (1.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5218s / 53.8179 s
agent0:                 episode reward: -0.4738,                 loss: 0.1519
agent1:                 episode reward: 0.4738,                 loss: nan
Episode: 1081/101000 (1.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7693s / 55.5872 s
agent0:                 episode reward: -0.8105,                 loss: 0.1489
agent1:                 episode reward: 0.8105,                 loss: nan
Episode: 1101/101000 (1.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2838s / 56.8710 s
agent0:                 episode reward: -0.6849,                 loss: 0.1446
agent1:                 episode reward: 0.6849,                 loss: nan
Episode: 1121/101000 (1.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6096s / 58.4806 s
agent0:                 episode reward: 0.0025,                 loss: 0.1430
agent1:                 episode reward: -0.0025,                 loss: nan
Episode: 1141/101000 (1.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2603s / 59.7409 s
agent0:                 episode reward: -0.7726,                 loss: 0.1416
agent1:                 episode reward: 0.7726,                 loss: nan
Episode: 1161/101000 (1.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6498s / 61.3907 s
agent0:                 episode reward: -0.4641,                 loss: 0.1393
agent1:                 episode reward: 0.4641,                 loss: nan
Episode: 1181/101000 (1.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2731s / 62.6638 s
agent0:                 episode reward: -0.8005,                 loss: 0.1394
agent1:                 episode reward: 0.8005,                 loss: nan
Episode: 1201/101000 (1.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5147s / 64.1785 s
agent0:                 episode reward: -0.4926,                 loss: 0.1395
agent1:                 episode reward: 0.4926,                 loss: nan
Episode: 1221/101000 (1.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1951s / 65.3735 s
agent0:                 episode reward: -0.7209,                 loss: 0.1365
agent1:                 episode reward: 0.7209,                 loss: nan
Episode: 1241/101000 (1.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2686s / 66.6422 s
agent0:                 episode reward: -0.8852,                 loss: 0.1354
agent1:                 episode reward: 0.8852,                 loss: nan
Episode: 1261/101000 (1.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6092s / 68.2513 s
agent0:                 episode reward: -0.5465,                 loss: 0.1350
agent1:                 episode reward: 0.5465,                 loss: nan
Episode: 1281/101000 (1.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8053s / 70.0566 s
agent0:                 episode reward: -0.3715,                 loss: 0.1315
agent1:                 episode reward: 0.3715,                 loss: nan
Episode: 1301/101000 (1.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 0.9592s / 71.0157 s
agent0:                 episode reward: -0.7837,                 loss: 0.1322
agent1:                 episode reward: 0.7837,                 loss: nan
Episode: 1321/101000 (1.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4288s / 72.4445 s
agent0:                 episode reward: -0.5905,                 loss: 0.1329
agent1:                 episode reward: 0.5905,                 loss: nan
Episode: 1341/101000 (1.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3367s / 73.7812 s
agent0:                 episode reward: -0.5517,                 loss: 0.1577
agent1:                 episode reward: 0.5517,                 loss: nan
Episode: 1361/101000 (1.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1028s / 75.8840 s
agent0:                 episode reward: -0.1065,                 loss: 0.1653
agent1:                 episode reward: 0.1065,                 loss: nan
Episode: 1381/101000 (1.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4040s / 77.2880 s
agent0:                 episode reward: -0.4514,                 loss: 0.1644
agent1:                 episode reward: 0.4514,                 loss: nan
Episode: 1401/101000 (1.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6253s / 78.9133 s
agent0:                 episode reward: -0.7177,                 loss: 0.1644
agent1:                 episode reward: 0.7177,                 loss: nan
Episode: 1421/101000 (1.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8096s / 80.7229 s
agent0:                 episode reward: -0.0626,                 loss: 0.1628
agent1:                 episode reward: 0.0626,                 loss: nan
Episode: 1441/101000 (1.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6882s / 82.4111 s
agent0:                 episode reward: -0.4560,                 loss: 0.1623
agent1:                 episode reward: 0.4560,                 loss: nan
Episode: 1461/101000 (1.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2885s / 83.6997 s
agent0:                 episode reward: -0.5217,                 loss: 0.1624
agent1:                 episode reward: 0.5217,                 loss: nan
Episode: 1481/101000 (1.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6088s / 85.3084 s
agent0:                 episode reward: -0.4919,                 loss: 0.1619
agent1:                 episode reward: 0.4919,                 loss: nan
Episode: 1501/101000 (1.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7289s / 87.0374 s
agent0:                 episode reward: -0.0676,                 loss: 0.1637
agent1:                 episode reward: 0.0676,                 loss: nan
Episode: 1521/101000 (1.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4667s / 88.5041 s
agent0:                 episode reward: -0.1043,                 loss: 0.1619
agent1:                 episode reward: 0.1043,                 loss: nan
Episode: 1541/101000 (1.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0474s / 90.5514 s
agent0:                 episode reward: -0.4588,                 loss: 0.1616
agent1:                 episode reward: 0.4588,                 loss: nan
Episode: 1561/101000 (1.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2357s / 92.7871 s
agent0:                 episode reward: -0.6164,                 loss: 0.1626
agent1:                 episode reward: 0.6164,                 loss: nan
Episode: 1581/101000 (1.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8781s / 94.6652 s
agent0:                 episode reward: -0.6395,                 loss: 0.1630
agent1:                 episode reward: 0.6395,                 loss: nan
Episode: 1601/101000 (1.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9631s / 96.6283 s
agent0:                 episode reward: -0.3766,                 loss: 0.1598
agent1:                 episode reward: 0.3766,                 loss: nan
Episode: 1621/101000 (1.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4529s / 98.0812 s
agent0:                 episode reward: -0.8846,                 loss: 0.1620
agent1:                 episode reward: 0.8846,                 loss: nan
Episode: 1641/101000 (1.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6371s / 99.7182 s
agent0:                 episode reward: -0.0741,                 loss: 0.1602
agent1:                 episode reward: 0.0741,                 loss: nan
Episode: 1661/101000 (1.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5081s / 101.2264 s
agent0:                 episode reward: -0.5451,                 loss: 0.1611
agent1:                 episode reward: 0.5451,                 loss: nan
Episode: 1681/101000 (1.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6517s / 102.8781 s
agent0:                 episode reward: -0.2733,                 loss: 0.1812
agent1:                 episode reward: 0.2733,                 loss: nan
Episode: 1701/101000 (1.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5477s / 104.4257 s
agent0:                 episode reward: -0.6702,                 loss: 0.1812
agent1:                 episode reward: 0.6702,                 loss: nan
Episode: 1721/101000 (1.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7213s / 106.1470 s
agent0:                 episode reward: -0.4913,                 loss: 0.1787
agent1:                 episode reward: 0.4913,                 loss: nan
Episode: 1741/101000 (1.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7222s / 107.8693 s
agent0:                 episode reward: -0.7187,                 loss: 0.1819
agent1:                 episode reward: 0.7187,                 loss: nan
Episode: 1761/101000 (1.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6676s / 109.5369 s
agent0:                 episode reward: -0.3313,                 loss: 0.1790
agent1:                 episode reward: 0.3313,                 loss: nan
Episode: 1781/101000 (1.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6560s / 111.1929 s
agent0:                 episode reward: -0.6126,                 loss: 0.1773
agent1:                 episode reward: 0.6126,                 loss: nan
Episode: 1801/101000 (1.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7740s / 112.9669 s
agent0:                 episode reward: 0.0737,                 loss: 0.1776
agent1:                 episode reward: -0.0737,                 loss: nan
Episode: 1821/101000 (1.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4962s / 114.4631 s
agent0:                 episode reward: -0.8476,                 loss: 0.1793
agent1:                 episode reward: 0.8476,                 loss: nan
Episode: 1841/101000 (1.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7431s / 116.2062 s
agent0:                 episode reward: -0.7548,                 loss: 0.1782
agent1:                 episode reward: 0.7548,                 loss: nan
Episode: 1861/101000 (1.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2447s / 117.4509 s
agent0:                 episode reward: -0.8202,                 loss: 0.1786
agent1:                 episode reward: 0.8202,                 loss: nan
Episode: 1881/101000 (1.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5220s / 118.9729 s
agent0:                 episode reward: -0.7091,                 loss: 0.1789
agent1:                 episode reward: 0.7091,                 loss: nan
Episode: 1901/101000 (1.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4063s / 120.3791 s
agent0:                 episode reward: -0.7200,                 loss: 0.1798
agent1:                 episode reward: 0.7200,                 loss: nan
Episode: 1921/101000 (1.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7987s / 122.1778 s
agent0:                 episode reward: -0.3323,                 loss: 0.1788
agent1:                 episode reward: 0.3323,                 loss: nan
Episode: 1941/101000 (1.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7102s / 123.8880 s
agent0:                 episode reward: -0.8256,                 loss: 0.1773
agent1:                 episode reward: 0.8256,                 loss: nan
Episode: 1961/101000 (1.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4803s / 125.3683 s
agent0:                 episode reward: -0.6029,                 loss: 0.1811
agent1:                 episode reward: 0.6029,                 loss: nan
Episode: 1981/101000 (1.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8077s / 127.1760 s
agent0:                 episode reward: -0.5636,                 loss: 0.1795
agent1:                 episode reward: 0.5636,                 loss: nan
Episode: 2001/101000 (1.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4136s / 128.5896 s
agent0:                 episode reward: -0.5997,                 loss: 0.1827
agent1:                 episode reward: 0.5997,                 loss: nan
Episode: 2021/101000 (2.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 0.9564s / 129.5460 s
agent0:                 episode reward: -0.3336,                 loss: 0.1916
agent1:                 episode reward: 0.3336,                 loss: nan
Episode: 2041/101000 (2.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7223s / 131.2683 s
agent0:                 episode reward: -0.7379,                 loss: 0.1922
agent1:                 episode reward: 0.7379,                 loss: nan
Episode: 2061/101000 (2.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8190s / 133.0873 s
agent0:                 episode reward: -0.6684,                 loss: 0.1890
agent1:                 episode reward: 0.6684,                 loss: nan
Episode: 2081/101000 (2.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9281s / 135.0155 s
agent0:                 episode reward: -0.6755,                 loss: 0.1886
agent1:                 episode reward: 0.6755,                 loss: nan
Episode: 2101/101000 (2.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7169s / 136.7323 s
agent0:                 episode reward: -0.5077,                 loss: 0.1890
agent1:                 episode reward: 0.5077,                 loss: nan
Episode: 2121/101000 (2.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6025s / 138.3348 s
agent0:                 episode reward: -0.6220,                 loss: 0.1895
agent1:                 episode reward: 0.6220,                 loss: nan
Episode: 2141/101000 (2.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4142s / 139.7490 s
agent0:                 episode reward: -0.1477,                 loss: 0.1907
agent1:                 episode reward: 0.1477,                 loss: nan
Episode: 2161/101000 (2.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5532s / 141.3022 s
agent0:                 episode reward: -0.1027,                 loss: 0.1888
agent1:                 episode reward: 0.1027,                 loss: nan
Episode: 2181/101000 (2.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9297s / 143.2319 s
agent0:                 episode reward: -0.8056,                 loss: 0.1894
agent1:                 episode reward: 0.8056,                 loss: nan
Episode: 2201/101000 (2.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6671s / 144.8989 s
agent0:                 episode reward: -0.6849,                 loss: 0.1902
agent1:                 episode reward: 0.6849,                 loss: nan
Episode: 2221/101000 (2.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7932s / 146.6921 s
agent0:                 episode reward: -0.4795,                 loss: 0.1892
agent1:                 episode reward: 0.4795,                 loss: nan
Episode: 2241/101000 (2.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6393s / 148.3314 s
agent0:                 episode reward: -0.2189,                 loss: 0.1910
agent1:                 episode reward: 0.2189,                 loss: nan
Episode: 2261/101000 (2.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5157s / 149.8471 s
agent0:                 episode reward: -0.5334,                 loss: 0.1891
agent1:                 episode reward: 0.5334,                 loss: nan
Episode: 2281/101000 (2.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4820s / 151.3291 s
agent0:                 episode reward: -0.6229,                 loss: 0.1903
agent1:                 episode reward: 0.6229,                 loss: nan
Episode: 2301/101000 (2.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4262s / 152.7553 s
agent0:                 episode reward: -0.7607,                 loss: 0.1906
agent1:                 episode reward: 0.7607,                 loss: nan
Episode: 2321/101000 (2.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0165s / 154.7718 s
agent0:                 episode reward: -0.8235,                 loss: 0.1903
agent1:                 episode reward: 0.8235,                 loss: nan
Episode: 2341/101000 (2.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7282s / 156.5000 s
agent0:                 episode reward: -0.5725,                 loss: 0.1966
agent1:                 episode reward: 0.5725,                 loss: nan
Episode: 2361/101000 (2.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3672s / 157.8671 s
agent0:                 episode reward: -0.5658,                 loss: 0.1985
agent1:                 episode reward: 0.5658,                 loss: nan
Episode: 2381/101000 (2.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6502s / 159.5173 s
agent0:                 episode reward: -0.6018,                 loss: 0.1986
agent1:                 episode reward: 0.6018,                 loss: nan
Episode: 2401/101000 (2.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5018s / 161.0191 s
agent0:                 episode reward: -0.4756,                 loss: 0.1976
agent1:                 episode reward: 0.4756,                 loss: nan
Episode: 2421/101000 (2.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2787s / 162.2977 s
agent0:                 episode reward: -0.3865,                 loss: 0.1999
agent1:                 episode reward: 0.3865,                 loss: nan
Episode: 2441/101000 (2.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6081s / 163.9058 s
agent0:                 episode reward: -0.8878,                 loss: 0.1971
agent1:                 episode reward: 0.8878,                 loss: nan
Episode: 2461/101000 (2.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3152s / 165.2210 s
agent0:                 episode reward: -0.5088,                 loss: 0.1982
agent1:                 episode reward: 0.5088,                 loss: nan
Episode: 2481/101000 (2.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1617s / 166.3827 s
agent0:                 episode reward: -0.7477,                 loss: 0.1992
agent1:                 episode reward: 0.7477,                 loss: nan
Episode: 2501/101000 (2.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9041s / 168.2868 s
agent0:                 episode reward: -0.5846,                 loss: 0.1970
agent1:                 episode reward: 0.5846,                 loss: nan
Episode: 2521/101000 (2.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3878s / 169.6746 s
agent0:                 episode reward: -0.7587,                 loss: 0.1963
agent1:                 episode reward: 0.7587,                 loss: nan
Episode: 2541/101000 (2.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3480s / 171.0226 s
agent0:                 episode reward: -0.7124,                 loss: 0.1964
agent1:                 episode reward: 0.7124,                 loss: nan
Episode: 2561/101000 (2.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6842s / 172.7067 s
agent0:                 episode reward: -0.1332,                 loss: 0.1961
agent1:                 episode reward: 0.1332,                 loss: nan
Episode: 2581/101000 (2.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6243s / 174.3310 s
agent0:                 episode reward: -0.5009,                 loss: 0.1958
agent1:                 episode reward: 0.5009,                 loss: nan
Episode: 2601/101000 (2.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6396s / 175.9706 s
agent0:                 episode reward: -0.5881,                 loss: 0.1960
agent1:                 episode reward: 0.5881,                 loss: nan
Episode: 2621/101000 (2.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5886s / 177.5593 s
agent0:                 episode reward: -0.4847,                 loss: 0.1973
agent1:                 episode reward: 0.4847,                 loss: nan
Episode: 2641/101000 (2.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8233s / 179.3826 s
agent0:                 episode reward: -0.1238,                 loss: 0.1987
agent1:                 episode reward: 0.1238,                 loss: nan
Episode: 2661/101000 (2.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4005s / 180.7831 s
agent0:                 episode reward: -0.0057,                 loss: 0.1948
agent1:                 episode reward: 0.0057,                 loss: nan
Episode: 2681/101000 (2.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4012s / 182.1844 s
agent0:                 episode reward: -0.2495,                 loss: 0.2009
agent1:                 episode reward: 0.2495,                 loss: nan
Episode: 2701/101000 (2.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8186s / 184.0030 s
agent0:                 episode reward: -0.4242,                 loss: 0.2009
agent1:                 episode reward: 0.4242,                 loss: nan
Episode: 2721/101000 (2.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7608s / 185.7637 s
agent0:                 episode reward: -0.2913,                 loss: 0.2011
agent1:                 episode reward: 0.2913,                 loss: nan
Episode: 2741/101000 (2.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6468s / 187.4105 s
agent0:                 episode reward: -0.7495,                 loss: 0.2036
agent1:                 episode reward: 0.7495,                 loss: nan
Episode: 2761/101000 (2.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8412s / 189.2518 s
agent0:                 episode reward: -0.1776,                 loss: 0.2027
agent1:                 episode reward: 0.1776,                 loss: nan
Episode: 2781/101000 (2.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3500s / 191.6018 s
agent0:                 episode reward: -0.4321,                 loss: 0.2000
agent1:                 episode reward: 0.4321,                 loss: nan
Episode: 2801/101000 (2.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8783s / 193.4800 s
agent0:                 episode reward: -0.8298,                 loss: 0.2016
agent1:                 episode reward: 0.8298,                 loss: nan
Episode: 2821/101000 (2.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2916s / 194.7717 s
agent0:                 episode reward: -0.4288,                 loss: 0.2057
agent1:                 episode reward: 0.4288,                 loss: nan
Episode: 2841/101000 (2.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5375s / 196.3092 s
agent0:                 episode reward: -0.3930,                 loss: 0.1983
agent1:                 episode reward: 0.3930,                 loss: nan
Episode: 2861/101000 (2.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6139s / 197.9231 s
agent0:                 episode reward: -0.2856,                 loss: 0.2001
agent1:                 episode reward: 0.2856,                 loss: nan
Episode: 2881/101000 (2.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6811s / 199.6042 s
agent0:                 episode reward: -0.5854,                 loss: 0.2013
agent1:                 episode reward: 0.5854,                 loss: nan
Episode: 2901/101000 (2.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0254s / 201.6295 s
agent0:                 episode reward: -0.0982,                 loss: 0.2004
agent1:                 episode reward: 0.0982,                 loss: nan
Episode: 2921/101000 (2.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8723s / 203.5019 s
agent0:                 episode reward: -0.7589,                 loss: 0.2043
agent1:                 episode reward: 0.7589,                 loss: nan
Episode: 2941/101000 (2.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5029s / 205.0047 s
agent0:                 episode reward: -0.3297,                 loss: 0.2014
agent1:                 episode reward: 0.3297,                 loss: nan
Episode: 2961/101000 (2.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8142s / 206.8189 s
agent0:                 episode reward: -0.7001,                 loss: 0.2010
agent1:                 episode reward: 0.7001,                 loss: nan
Episode: 2981/101000 (2.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6662s / 208.4851 s
agent0:                 episode reward: -0.5650,                 loss: 0.2017
agent1:                 episode reward: 0.5650,                 loss: nan
Episode: 3001/101000 (2.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4158s / 209.9008 s
agent0:                 episode reward: -0.3842,                 loss: 0.2014
agent1:                 episode reward: 0.3842,                 loss: nan
Episode: 3021/101000 (2.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7038s / 211.6047 s
agent0:                 episode reward: -0.0247,                 loss: 0.1965
agent1:                 episode reward: 0.0247,                 loss: nan
Episode: 3041/101000 (3.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7749s / 213.3796 s
agent0:                 episode reward: -0.3134,                 loss: 0.1959
agent1:                 episode reward: 0.3134,                 loss: nan
Episode: 3061/101000 (3.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2898s / 214.6694 s
agent0:                 episode reward: -0.6270,                 loss: 0.1998
agent1:                 episode reward: 0.6270,                 loss: nan
Episode: 3081/101000 (3.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3477s / 216.0170 s
agent0:                 episode reward: -0.2253,                 loss: 0.1989
agent1:                 episode reward: 0.2253,                 loss: nan
Episode: 3101/101000 (3.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5556s / 217.5727 s
agent0:                 episode reward: -0.3436,                 loss: 0.1959
agent1:                 episode reward: 0.3436,                 loss: nan
Episode: 3121/101000 (3.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5010s / 219.0737 s
agent0:                 episode reward: -0.5430,                 loss: 0.1992
agent1:                 episode reward: 0.5430,                 loss: nan
Episode: 3141/101000 (3.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4990s / 220.5727 s
agent0:                 episode reward: -0.1955,                 loss: 0.1985
agent1:                 episode reward: 0.1955,                 loss: nan
Episode: 3161/101000 (3.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6594s / 222.2321 s
agent0:                 episode reward: -0.3766,                 loss: 0.1975
agent1:                 episode reward: 0.3766,                 loss: nan
Episode: 3181/101000 (3.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6664s / 223.8985 s
agent0:                 episode reward: -0.5820,                 loss: 0.1992
agent1:                 episode reward: 0.5820,                 loss: nan
Episode: 3201/101000 (3.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6147s / 225.5133 s
agent0:                 episode reward: -0.3425,                 loss: 0.1965
agent1:                 episode reward: 0.3425,                 loss: nan
Episode: 3221/101000 (3.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4344s / 226.9477 s
agent0:                 episode reward: -0.4134,                 loss: 0.1955
agent1:                 episode reward: 0.4134,                 loss: nan
Episode: 3241/101000 (3.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5320s / 228.4798 s
agent0:                 episode reward: -0.0208,                 loss: 0.1997
agent1:                 episode reward: 0.0208,                 loss: nan
Episode: 3261/101000 (3.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5747s / 230.0545 s
agent0:                 episode reward: -0.2803,                 loss: 0.1977
agent1:                 episode reward: 0.2803,                 loss: nan
Episode: 3281/101000 (3.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8873s / 231.9418 s
agent0:                 episode reward: -0.6642,                 loss: 0.1967
agent1:                 episode reward: 0.6642,                 loss: nan
Episode: 3301/101000 (3.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7131s / 233.6549 s
agent0:                 episode reward: 0.1994,                 loss: 0.1980
agent1:                 episode reward: -0.1994,                 loss: 0.1255
Score delta: 1.675535295358657, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/2864_0.
Episode: 3321/101000 (3.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6182s / 235.2731 s
agent0:                 episode reward: -0.0456,                 loss: nan
agent1:                 episode reward: 0.0456,                 loss: 0.1236
Episode: 3341/101000 (3.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7950s / 237.0681 s
agent0:                 episode reward: -0.2796,                 loss: nan
agent1:                 episode reward: 0.2796,                 loss: 0.1231
Episode: 3361/101000 (3.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7013s / 238.7694 s
agent0:                 episode reward: -0.4313,                 loss: 0.1977
agent1:                 episode reward: 0.4313,                 loss: 0.1244
Score delta: 1.6598966312717311, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/2917_1.
Episode: 3381/101000 (3.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7419s / 240.5112 s
agent0:                 episode reward: -0.0320,                 loss: 0.1945
agent1:                 episode reward: 0.0320,                 loss: nan
Episode: 3401/101000 (3.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6142s / 242.1254 s
agent0:                 episode reward: -0.7894,                 loss: 0.1984
agent1:                 episode reward: 0.7894,                 loss: nan
Episode: 3421/101000 (3.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4150s / 243.5404 s
agent0:                 episode reward: -0.3952,                 loss: 0.1964
agent1:                 episode reward: 0.3952,                 loss: nan
Episode: 3441/101000 (3.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4405s / 244.9809 s
agent0:                 episode reward: -0.0157,                 loss: 0.1945
agent1:                 episode reward: 0.0157,                 loss: nan
Episode: 3461/101000 (3.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4217s / 246.4026 s
agent0:                 episode reward: -0.5567,                 loss: 0.1943
agent1:                 episode reward: 0.5567,                 loss: nan
Episode: 3481/101000 (3.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7469s / 248.1495 s
agent0:                 episode reward: -0.3950,                 loss: 0.1972
agent1:                 episode reward: 0.3950,                 loss: nan
Episode: 3501/101000 (3.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7948s / 249.9444 s
agent0:                 episode reward: -0.3408,                 loss: 0.1955
agent1:                 episode reward: 0.3408,                 loss: nan
Episode: 3521/101000 (3.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8144s / 251.7588 s
agent0:                 episode reward: -0.5378,                 loss: 0.1961
agent1:                 episode reward: 0.5378,                 loss: nan
Episode: 3541/101000 (3.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3698s / 253.1285 s
agent0:                 episode reward: -0.3271,                 loss: 0.1921
agent1:                 episode reward: 0.3271,                 loss: nan
Episode: 3561/101000 (3.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4582s / 254.5867 s
agent0:                 episode reward: -0.8354,                 loss: 0.1955
agent1:                 episode reward: 0.8354,                 loss: nan
Episode: 3581/101000 (3.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6845s / 256.2712 s
agent0:                 episode reward: -0.4361,                 loss: 0.1976
agent1:                 episode reward: 0.4361,                 loss: nan
Episode: 3601/101000 (3.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6926s / 257.9639 s
agent0:                 episode reward: -1.1377,                 loss: 0.1905
agent1:                 episode reward: 1.1377,                 loss: nan
Episode: 3621/101000 (3.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7986s / 259.7624 s
agent0:                 episode reward: -0.5706,                 loss: 0.1951
agent1:                 episode reward: 0.5706,                 loss: nan
Episode: 3641/101000 (3.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8872s / 261.6496 s
agent0:                 episode reward: -0.5200,                 loss: 0.1933
agent1:                 episode reward: 0.5200,                 loss: nan
Episode: 3661/101000 (3.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8221s / 263.4717 s
agent0:                 episode reward: 0.0164,                 loss: 0.1962
agent1:                 episode reward: -0.0164,                 loss: nan
Episode: 3681/101000 (3.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8724s / 265.3441 s
agent0:                 episode reward: -0.4884,                 loss: 0.1933
agent1:                 episode reward: 0.4884,                 loss: nan
Episode: 3701/101000 (3.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0119s / 267.3560 s
agent0:                 episode reward: -0.4215,                 loss: 0.1936
agent1:                 episode reward: 0.4215,                 loss: nan
Episode: 3721/101000 (3.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0383s / 269.3943 s
agent0:                 episode reward: -0.6013,                 loss: 0.1987
agent1:                 episode reward: 0.6013,                 loss: nan
Episode: 3741/101000 (3.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6480s / 271.0423 s
agent0:                 episode reward: -0.4007,                 loss: 0.1977
agent1:                 episode reward: 0.4007,                 loss: nan
Episode: 3761/101000 (3.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9007s / 272.9430 s
agent0:                 episode reward: -0.5664,                 loss: 0.2014
agent1:                 episode reward: 0.5664,                 loss: nan
Episode: 3781/101000 (3.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8490s / 274.7921 s
agent0:                 episode reward: -0.1422,                 loss: 0.2005
agent1:                 episode reward: 0.1422,                 loss: nan
Episode: 3801/101000 (3.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4938s / 276.2858 s
agent0:                 episode reward: -0.2333,                 loss: 0.2009
agent1:                 episode reward: 0.2333,                 loss: nan
Episode: 3821/101000 (3.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4124s / 277.6982 s
agent0:                 episode reward: -0.1617,                 loss: 0.2008
agent1:                 episode reward: 0.1617,                 loss: nan
Episode: 3841/101000 (3.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1460s / 279.8442 s
agent0:                 episode reward: -0.3722,                 loss: 0.1992
agent1:                 episode reward: 0.3722,                 loss: nan
Episode: 3861/101000 (3.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7290s / 281.5732 s
agent0:                 episode reward: -0.4688,                 loss: 0.1984
agent1:                 episode reward: 0.4688,                 loss: nan
Episode: 3881/101000 (3.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6820s / 283.2552 s
agent0:                 episode reward: 0.0800,                 loss: 0.2010
agent1:                 episode reward: -0.0800,                 loss: 0.1229
Score delta: 1.6543351591704512, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/3440_0.
Episode: 3901/101000 (3.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5058s / 284.7610 s
agent0:                 episode reward: 0.3118,                 loss: nan
agent1:                 episode reward: -0.3118,                 loss: 0.1225
Episode: 3921/101000 (3.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5077s / 286.2687 s
agent0:                 episode reward: 0.1298,                 loss: nan
agent1:                 episode reward: -0.1298,                 loss: 0.1258
Episode: 3941/101000 (3.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8507s / 288.1194 s
agent0:                 episode reward: 0.1875,                 loss: nan
agent1:                 episode reward: -0.1875,                 loss: 0.1254
Episode: 3961/101000 (3.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5062s / 289.6256 s
agent0:                 episode reward: -0.2501,                 loss: nan
agent1:                 episode reward: 0.2501,                 loss: 0.1242
Episode: 3981/101000 (3.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5102s / 291.1358 s
agent0:                 episode reward: -0.8572,                 loss: 0.2023
agent1:                 episode reward: 0.8572,                 loss: 0.1256
Score delta: 1.5858913149711902, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/3546_1.
Episode: 4001/101000 (3.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5710s / 292.7068 s
agent0:                 episode reward: -0.0148,                 loss: 0.2019
agent1:                 episode reward: 0.0148,                 loss: nan
Episode: 4021/101000 (3.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5658s / 294.2725 s
agent0:                 episode reward: -0.6975,                 loss: 0.2032
agent1:                 episode reward: 0.6975,                 loss: nan
Episode: 4041/101000 (4.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8079s / 296.0805 s
agent0:                 episode reward: 0.1534,                 loss: 0.2026
agent1:                 episode reward: -0.1534,                 loss: nan
Episode: 4061/101000 (4.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5365s / 297.6169 s
agent0:                 episode reward: 0.4224,                 loss: 0.2004
agent1:                 episode reward: -0.4224,                 loss: 0.1339
Score delta: 1.9383847735236102, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/3622_0.
Episode: 4081/101000 (4.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8871s / 299.5040 s
agent0:                 episode reward: -0.0729,                 loss: nan
agent1:                 episode reward: 0.0729,                 loss: 0.1321
Episode: 4101/101000 (4.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5592s / 301.0632 s
agent0:                 episode reward: -0.3270,                 loss: nan
agent1:                 episode reward: 0.3270,                 loss: 0.1338
Episode: 4121/101000 (4.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2846s / 302.3478 s
agent0:                 episode reward: -0.0699,                 loss: nan
agent1:                 episode reward: 0.0699,                 loss: 0.1311
Episode: 4141/101000 (4.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8141s / 304.1619 s
agent0:                 episode reward: 0.1940,                 loss: nan
agent1:                 episode reward: -0.1940,                 loss: 0.1330
Episode: 4161/101000 (4.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9135s / 306.0754 s
agent0:                 episode reward: 0.2189,                 loss: nan
agent1:                 episode reward: -0.2189,                 loss: 0.1316
Episode: 4181/101000 (4.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3222s / 307.3976 s
agent0:                 episode reward: 0.0095,                 loss: nan
agent1:                 episode reward: -0.0095,                 loss: 0.1314
Episode: 4201/101000 (4.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3605s / 308.7581 s
agent0:                 episode reward: 0.3461,                 loss: nan
agent1:                 episode reward: -0.3461,                 loss: 0.1326
Episode: 4221/101000 (4.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6214s / 310.3796 s
agent0:                 episode reward: 0.2181,                 loss: nan
agent1:                 episode reward: -0.2181,                 loss: 0.1328
Episode: 4241/101000 (4.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2599s / 311.6395 s
agent0:                 episode reward: 0.1398,                 loss: nan
agent1:                 episode reward: -0.1398,                 loss: 0.1332
Episode: 4261/101000 (4.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6757s / 313.3152 s
agent0:                 episode reward: -0.4638,                 loss: nan
agent1:                 episode reward: 0.4638,                 loss: 0.1338
Episode: 4281/101000 (4.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7517s / 315.0669 s
agent0:                 episode reward: -0.2404,                 loss: 0.2024
agent1:                 episode reward: 0.2404,                 loss: 0.1341
Score delta: 1.5626823982254385, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/3837_1.
Episode: 4301/101000 (4.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3660s / 316.4329 s
agent0:                 episode reward: -0.1646,                 loss: 0.2039
agent1:                 episode reward: 0.1646,                 loss: nan
Episode: 4321/101000 (4.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5313s / 317.9641 s
agent0:                 episode reward: -0.1259,                 loss: 0.2037
agent1:                 episode reward: 0.1259,                 loss: nan
Episode: 4341/101000 (4.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4776s / 319.4417 s
agent0:                 episode reward: -0.3937,                 loss: 0.2016
agent1:                 episode reward: 0.3937,                 loss: nan
Episode: 4361/101000 (4.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6812s / 321.1229 s
agent0:                 episode reward: -0.7240,                 loss: 0.2025
agent1:                 episode reward: 0.7240,                 loss: nan
Episode: 4381/101000 (4.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3605s / 322.4834 s
agent0:                 episode reward: -0.2144,                 loss: 0.1990
agent1:                 episode reward: 0.2144,                 loss: nan
Episode: 4401/101000 (4.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4823s / 323.9657 s
agent0:                 episode reward: -0.0280,                 loss: 0.2019
agent1:                 episode reward: 0.0280,                 loss: 0.1423
Score delta: 1.6186593781004912, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/3959_0.
Episode: 4421/101000 (4.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6548s / 325.6205 s
agent0:                 episode reward: -0.3029,                 loss: nan
agent1:                 episode reward: 0.3029,                 loss: 0.1396
Episode: 4441/101000 (4.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6445s / 327.2651 s
agent0:                 episode reward: -0.0133,                 loss: nan
agent1:                 episode reward: 0.0133,                 loss: 0.1394
Episode: 4461/101000 (4.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2067s / 329.4718 s
agent0:                 episode reward: 0.1334,                 loss: nan
agent1:                 episode reward: -0.1334,                 loss: 0.1509
Episode: 4481/101000 (4.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8389s / 331.3106 s
agent0:                 episode reward: 0.2568,                 loss: nan
agent1:                 episode reward: -0.2568,                 loss: 0.1506
Episode: 4501/101000 (4.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3940s / 332.7047 s
agent0:                 episode reward: 0.1070,                 loss: nan
agent1:                 episode reward: -0.1070,                 loss: 0.1503
Episode: 4521/101000 (4.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7164s / 334.4211 s
agent0:                 episode reward: 0.3501,                 loss: nan
agent1:                 episode reward: -0.3501,                 loss: 0.1517
Episode: 4541/101000 (4.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1738s / 335.5949 s
agent0:                 episode reward: 0.2739,                 loss: nan
agent1:                 episode reward: -0.2739,                 loss: 0.1533
Episode: 4561/101000 (4.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6296s / 337.2245 s
agent0:                 episode reward: 0.1293,                 loss: nan
agent1:                 episode reward: -0.1293,                 loss: 0.1532
Episode: 4581/101000 (4.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3707s / 338.5952 s
agent0:                 episode reward: -0.3630,                 loss: nan
agent1:                 episode reward: 0.3630,                 loss: 0.1545
Episode: 4601/101000 (4.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4778s / 340.0730 s
agent0:                 episode reward: 0.2362,                 loss: nan
agent1:                 episode reward: -0.2362,                 loss: 0.1538
Episode: 4621/101000 (4.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2972s / 341.3702 s
agent0:                 episode reward: 0.0572,                 loss: nan
agent1:                 episode reward: -0.0572,                 loss: 0.1534
Episode: 4641/101000 (4.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0807s / 343.4509 s
agent0:                 episode reward: -0.1243,                 loss: nan
agent1:                 episode reward: 0.1243,                 loss: 0.1546
Episode: 4661/101000 (4.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6744s / 345.1253 s
agent0:                 episode reward: -0.4827,                 loss: nan
agent1:                 episode reward: 0.4827,                 loss: 0.1548
Episode: 4681/101000 (4.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6156s / 346.7409 s
agent0:                 episode reward: -0.2347,                 loss: nan
agent1:                 episode reward: 0.2347,                 loss: 0.1561
Episode: 4701/101000 (4.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0430s / 348.7839 s
agent0:                 episode reward: -0.4455,                 loss: nan
agent1:                 episode reward: 0.4455,                 loss: 0.1571
Episode: 4721/101000 (4.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5187s / 350.3026 s
agent0:                 episode reward: -0.7654,                 loss: 0.2001
agent1:                 episode reward: 0.7654,                 loss: 0.1536
Score delta: 1.7826111754691432, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/4278_1.
Episode: 4741/101000 (4.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8175s / 352.1201 s
agent0:                 episode reward: -0.4178,                 loss: 0.1981
agent1:                 episode reward: 0.4178,                 loss: nan
Episode: 4761/101000 (4.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6150s / 353.7351 s
agent0:                 episode reward: -0.6119,                 loss: 0.1985
agent1:                 episode reward: 0.6119,                 loss: nan
Episode: 4781/101000 (4.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5699s / 355.3050 s
agent0:                 episode reward: 0.0739,                 loss: 0.1981
agent1:                 episode reward: -0.0739,                 loss: nan
Episode: 4801/101000 (4.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8045s / 357.1095 s
agent0:                 episode reward: 0.2164,                 loss: 0.1992
agent1:                 episode reward: -0.2164,                 loss: 0.1575
Score delta: 1.531947886239537, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/4372_0.
Episode: 4821/101000 (4.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4562s / 358.5656 s
agent0:                 episode reward: -0.2071,                 loss: nan
agent1:                 episode reward: 0.2071,                 loss: 0.1543
Episode: 4841/101000 (4.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5144s / 360.0800 s
agent0:                 episode reward: -0.2403,                 loss: nan
agent1:                 episode reward: 0.2403,                 loss: 0.1542
Episode: 4861/101000 (4.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5611s / 361.6411 s
agent0:                 episode reward: 0.1196,                 loss: nan
agent1:                 episode reward: -0.1196,                 loss: 0.1540
Episode: 4881/101000 (4.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8474s / 363.4885 s
agent0:                 episode reward: -0.3050,                 loss: nan
agent1:                 episode reward: 0.3050,                 loss: 0.1544
Episode: 4901/101000 (4.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1098s / 365.5983 s
agent0:                 episode reward: -0.7304,                 loss: 0.1809
agent1:                 episode reward: 0.7304,                 loss: 0.1539
Score delta: 1.5351987535845975, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/4463_1.
Episode: 4921/101000 (4.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0084s / 367.6067 s
agent0:                 episode reward: -0.9447,                 loss: 0.1661
agent1:                 episode reward: 0.9447,                 loss: nan
Episode: 4941/101000 (4.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3573s / 368.9641 s
agent0:                 episode reward: -0.3754,                 loss: 0.1616
agent1:                 episode reward: 0.3754,                 loss: nan
Episode: 4961/101000 (4.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8743s / 370.8384 s
agent0:                 episode reward: -0.5246,                 loss: 0.1590
agent1:                 episode reward: 0.5246,                 loss: nan
Episode: 4981/101000 (4.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2511s / 373.0895 s
agent0:                 episode reward: -0.9483,                 loss: 0.1555
agent1:                 episode reward: 0.9483,                 loss: nan
Episode: 5001/101000 (4.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6609s / 374.7504 s
agent0:                 episode reward: -0.3736,                 loss: 0.1541
agent1:                 episode reward: 0.3736,                 loss: nan
Episode: 5021/101000 (4.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4338s / 376.1842 s
agent0:                 episode reward: -0.4585,                 loss: 0.1532
agent1:                 episode reward: 0.4585,                 loss: nan
Episode: 5041/101000 (4.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7349s / 377.9191 s
agent0:                 episode reward: -0.1888,                 loss: 0.1506
agent1:                 episode reward: 0.1888,                 loss: nan
Episode: 5061/101000 (5.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0497s / 379.9687 s
agent0:                 episode reward: -0.5597,                 loss: 0.1501
agent1:                 episode reward: 0.5597,                 loss: nan
Episode: 5081/101000 (5.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7373s / 381.7060 s
agent0:                 episode reward: -0.5205,                 loss: 0.1503
agent1:                 episode reward: 0.5205,                 loss: nan
Episode: 5101/101000 (5.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0435s / 382.7495 s
agent0:                 episode reward: -0.9414,                 loss: 0.1466
agent1:                 episode reward: 0.9414,                 loss: nan
Episode: 5121/101000 (5.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3602s / 384.1097 s
agent0:                 episode reward: -0.6687,                 loss: 0.1489
agent1:                 episode reward: 0.6687,                 loss: nan
Episode: 5141/101000 (5.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2042s / 385.3138 s
agent0:                 episode reward: -0.3024,                 loss: 0.1492
agent1:                 episode reward: 0.3024,                 loss: nan
Episode: 5161/101000 (5.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7125s / 387.0263 s
agent0:                 episode reward: -0.3594,                 loss: 0.1478
agent1:                 episode reward: 0.3594,                 loss: nan
Episode: 5181/101000 (5.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8117s / 388.8381 s
agent0:                 episode reward: -0.7134,                 loss: 0.1455
agent1:                 episode reward: 0.7134,                 loss: nan
Episode: 5201/101000 (5.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3541s / 390.1921 s
agent0:                 episode reward: -0.5215,                 loss: 0.1425
agent1:                 episode reward: 0.5215,                 loss: nan
Episode: 5221/101000 (5.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2914s / 391.4835 s
agent0:                 episode reward: -0.7045,                 loss: 0.1414
agent1:                 episode reward: 0.7045,                 loss: nan
Episode: 5241/101000 (5.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2018s / 392.6853 s
agent0:                 episode reward: -0.5653,                 loss: 0.1382
agent1:                 episode reward: 0.5653,                 loss: nan
Episode: 5261/101000 (5.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9122s / 394.5976 s
agent0:                 episode reward: -0.3940,                 loss: 0.1368
agent1:                 episode reward: 0.3940,                 loss: nan
Episode: 5281/101000 (5.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7269s / 396.3245 s
agent0:                 episode reward: -0.3718,                 loss: 0.1329
agent1:                 episode reward: 0.3718,                 loss: nan
Episode: 5301/101000 (5.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1052s / 398.4298 s
agent0:                 episode reward: -0.4951,                 loss: 0.1342
agent1:                 episode reward: 0.4951,                 loss: nan
Episode: 5321/101000 (5.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8094s / 400.2392 s
agent0:                 episode reward: -0.4094,                 loss: 0.1304
agent1:                 episode reward: 0.4094,                 loss: nan
Episode: 5341/101000 (5.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0779s / 402.3171 s
agent0:                 episode reward: -0.4838,                 loss: 0.1297
agent1:                 episode reward: 0.4838,                 loss: nan
Episode: 5361/101000 (5.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7505s / 404.0676 s
agent0:                 episode reward: -0.3751,                 loss: 0.1265
agent1:                 episode reward: 0.3751,                 loss: nan
Episode: 5381/101000 (5.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6058s / 405.6734 s
agent0:                 episode reward: -0.3463,                 loss: 0.1248
agent1:                 episode reward: 0.3463,                 loss: nan
Episode: 5401/101000 (5.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5119s / 407.1853 s
agent0:                 episode reward: -0.6248,                 loss: 0.1233
agent1:                 episode reward: 0.6248,                 loss: nan
Episode: 5421/101000 (5.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9228s / 409.1081 s
agent0:                 episode reward: -0.1441,                 loss: 0.1213
agent1:                 episode reward: 0.1441,                 loss: nan
Episode: 5441/101000 (5.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2882s / 410.3963 s
agent0:                 episode reward: -0.1323,                 loss: 0.1201
agent1:                 episode reward: 0.1323,                 loss: nan
Episode: 5461/101000 (5.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0307s / 412.4269 s
agent0:                 episode reward: -0.4050,                 loss: 0.1485
agent1:                 episode reward: 0.4050,                 loss: nan
Episode: 5481/101000 (5.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2118s / 414.6387 s
agent0:                 episode reward: -0.6872,                 loss: 0.1547
agent1:                 episode reward: 0.6872,                 loss: nan
Episode: 5501/101000 (5.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4684s / 416.1071 s
agent0:                 episode reward: -0.0640,                 loss: 0.1535
agent1:                 episode reward: 0.0640,                 loss: nan
Episode: 5521/101000 (5.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8604s / 417.9676 s
agent0:                 episode reward: -0.3288,                 loss: 0.1532
agent1:                 episode reward: 0.3288,                 loss: nan
Episode: 5541/101000 (5.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6110s / 419.5786 s
agent0:                 episode reward: -0.8628,                 loss: 0.1523
agent1:                 episode reward: 0.8628,                 loss: nan
Episode: 5561/101000 (5.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6774s / 421.2560 s
agent0:                 episode reward: -0.8077,                 loss: 0.1500
agent1:                 episode reward: 0.8077,                 loss: nan
Episode: 5581/101000 (5.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7122s / 422.9682 s
agent0:                 episode reward: -0.4967,                 loss: 0.1518
agent1:                 episode reward: 0.4967,                 loss: nan
Episode: 5601/101000 (5.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8083s / 424.7766 s
agent0:                 episode reward: -0.3532,                 loss: 0.1504
agent1:                 episode reward: 0.3532,                 loss: nan
Episode: 5621/101000 (5.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7750s / 426.5516 s
agent0:                 episode reward: -0.4979,                 loss: 0.1497
agent1:                 episode reward: 0.4979,                 loss: nan
Episode: 5641/101000 (5.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4271s / 427.9787 s
agent0:                 episode reward: -0.6735,                 loss: 0.1501
agent1:                 episode reward: 0.6735,                 loss: nan
Episode: 5661/101000 (5.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4446s / 429.4233 s
agent0:                 episode reward: -0.6934,                 loss: 0.1517
agent1:                 episode reward: 0.6934,                 loss: nan
Episode: 5681/101000 (5.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9155s / 431.3387 s
agent0:                 episode reward: -0.1321,                 loss: 0.1502
agent1:                 episode reward: 0.1321,                 loss: nan
Episode: 5701/101000 (5.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5314s / 432.8701 s
agent0:                 episode reward: -0.8577,                 loss: 0.1500
agent1:                 episode reward: 0.8577,                 loss: nan
Episode: 5721/101000 (5.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6440s / 434.5142 s
agent0:                 episode reward: -0.9933,                 loss: 0.1514
agent1:                 episode reward: 0.9933,                 loss: nan
Episode: 5741/101000 (5.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8606s / 436.3748 s
agent0:                 episode reward: -0.4930,                 loss: 0.1486
agent1:                 episode reward: 0.4930,                 loss: nan
Episode: 5761/101000 (5.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4897s / 437.8645 s
agent0:                 episode reward: -0.6592,                 loss: 0.1506
agent1:                 episode reward: 0.6592,                 loss: nan
Episode: 5781/101000 (5.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7132s / 439.5777 s
agent0:                 episode reward: 0.0430,                 loss: 0.1552
agent1:                 episode reward: -0.0430,                 loss: nan
Episode: 5801/101000 (5.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9989s / 441.5766 s
agent0:                 episode reward: -0.4054,                 loss: 0.1845
agent1:                 episode reward: 0.4054,                 loss: nan
Episode: 5821/101000 (5.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8600s / 443.4366 s
agent0:                 episode reward: -0.4805,                 loss: 0.1854
agent1:                 episode reward: 0.4805,                 loss: nan
Episode: 5841/101000 (5.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8303s / 445.2670 s
agent0:                 episode reward: -0.1731,                 loss: 0.1849
agent1:                 episode reward: 0.1731,                 loss: nan
Episode: 5861/101000 (5.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6667s / 446.9337 s
agent0:                 episode reward: -0.4159,                 loss: 0.1841
agent1:                 episode reward: 0.4159,                 loss: nan
Episode: 5881/101000 (5.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7437s / 448.6774 s
agent0:                 episode reward: -0.5306,                 loss: 0.1829
agent1:                 episode reward: 0.5306,                 loss: nan
Episode: 5901/101000 (5.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4266s / 450.1041 s
agent0:                 episode reward: -0.4451,                 loss: 0.1823
agent1:                 episode reward: 0.4451,                 loss: nan
Episode: 5921/101000 (5.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7787s / 451.8828 s
agent0:                 episode reward: -0.7003,                 loss: 0.1840
agent1:                 episode reward: 0.7003,                 loss: nan
Episode: 5941/101000 (5.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2010s / 454.0838 s
agent0:                 episode reward: -0.7608,                 loss: 0.1848
agent1:                 episode reward: 0.7608,                 loss: nan
Episode: 5961/101000 (5.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0617s / 456.1455 s
agent0:                 episode reward: -0.2810,                 loss: 0.1849
agent1:                 episode reward: 0.2810,                 loss: nan
Episode: 5981/101000 (5.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5832s / 457.7287 s
agent0:                 episode reward: -0.3842,                 loss: 0.1826
agent1:                 episode reward: 0.3842,                 loss: nan
Episode: 6001/101000 (5.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5733s / 459.3020 s
agent0:                 episode reward: -0.1865,                 loss: 0.1829
agent1:                 episode reward: 0.1865,                 loss: nan
Episode: 6021/101000 (5.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5053s / 460.8073 s
agent0:                 episode reward: -0.2961,                 loss: 0.1876
agent1:                 episode reward: 0.2961,                 loss: nan
Episode: 6041/101000 (5.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0386s / 462.8459 s
agent0:                 episode reward: 0.1567,                 loss: 0.1852
agent1:                 episode reward: -0.1567,                 loss: nan
Episode: 6061/101000 (6.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6998s / 464.5457 s
agent0:                 episode reward: -0.2000,                 loss: 0.1849
agent1:                 episode reward: 0.2000,                 loss: nan
Episode: 6081/101000 (6.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7873s / 466.3330 s
agent0:                 episode reward: -0.1796,                 loss: 0.1839
agent1:                 episode reward: 0.1796,                 loss: nan
Episode: 6101/101000 (6.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7515s / 468.0845 s
agent0:                 episode reward: -0.4657,                 loss: 0.1849
agent1:                 episode reward: 0.4657,                 loss: nan
Episode: 6121/101000 (6.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3413s / 469.4258 s
agent0:                 episode reward: -0.0947,                 loss: 0.1888
agent1:                 episode reward: 0.0947,                 loss: nan
Episode: 6141/101000 (6.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6368s / 471.0625 s
agent0:                 episode reward: -0.3000,                 loss: 0.1903
agent1:                 episode reward: 0.3000,                 loss: nan
Episode: 6161/101000 (6.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4983s / 472.5609 s
agent0:                 episode reward: -0.2590,                 loss: 0.1914
agent1:                 episode reward: 0.2590,                 loss: nan
Episode: 6181/101000 (6.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6440s / 474.2049 s
agent0:                 episode reward: -0.3656,                 loss: 0.1906
agent1:                 episode reward: 0.3656,                 loss: nan
Episode: 6201/101000 (6.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3111s / 475.5160 s
agent0:                 episode reward: -0.3386,                 loss: 0.1924
agent1:                 episode reward: 0.3386,                 loss: nan
Episode: 6221/101000 (6.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6959s / 477.2119 s
agent0:                 episode reward: 0.0927,                 loss: 0.1904
agent1:                 episode reward: -0.0927,                 loss: nan
Episode: 6241/101000 (6.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8135s / 479.0253 s
agent0:                 episode reward: 0.1232,                 loss: 0.1922
agent1:                 episode reward: -0.1232,                 loss: nan
Episode: 6261/101000 (6.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5430s / 480.5683 s
agent0:                 episode reward: -0.3764,                 loss: 0.1926
agent1:                 episode reward: 0.3764,                 loss: nan
Episode: 6281/101000 (6.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5540s / 483.1223 s
agent0:                 episode reward: -0.4143,                 loss: 0.1912
agent1:                 episode reward: 0.4143,                 loss: nan
Episode: 6301/101000 (6.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1123s / 485.2346 s
agent0:                 episode reward: -0.6223,                 loss: 0.1925
agent1:                 episode reward: 0.6223,                 loss: nan
Episode: 6321/101000 (6.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9454s / 487.1800 s
agent0:                 episode reward: -0.5799,                 loss: 0.1936
agent1:                 episode reward: 0.5799,                 loss: nan
Episode: 6341/101000 (6.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4436s / 488.6235 s
agent0:                 episode reward: -0.1269,                 loss: 0.1924
agent1:                 episode reward: 0.1269,                 loss: nan
Episode: 6361/101000 (6.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8803s / 490.5039 s
agent0:                 episode reward: -0.7321,                 loss: 0.1928
agent1:                 episode reward: 0.7321,                 loss: nan
Episode: 6381/101000 (6.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6752s / 492.1790 s
agent0:                 episode reward: -0.0354,                 loss: 0.1918
agent1:                 episode reward: 0.0354,                 loss: nan
Episode: 6401/101000 (6.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6356s / 493.8146 s
agent0:                 episode reward: -0.3578,                 loss: 0.1927
agent1:                 episode reward: 0.3578,                 loss: nan
Episode: 6421/101000 (6.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1573s / 494.9720 s
agent0:                 episode reward: -0.2554,                 loss: 0.1920
agent1:                 episode reward: 0.2554,                 loss: nan
Episode: 6441/101000 (6.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7251s / 496.6971 s
agent0:                 episode reward: -0.0743,                 loss: 0.1926
agent1:                 episode reward: 0.0743,                 loss: nan
Episode: 6461/101000 (6.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4381s / 498.1352 s
agent0:                 episode reward: -0.2057,                 loss: 0.1930
agent1:                 episode reward: 0.2057,                 loss: nan
Episode: 6481/101000 (6.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4824s / 499.6176 s
agent0:                 episode reward: -0.3173,                 loss: 0.1943
agent1:                 episode reward: 0.3173,                 loss: nan
Episode: 6501/101000 (6.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8471s / 501.4647 s
agent0:                 episode reward: -0.1048,                 loss: 0.1945
agent1:                 episode reward: 0.1048,                 loss: nan
Episode: 6521/101000 (6.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5552s / 503.0199 s
agent0:                 episode reward: -0.1703,                 loss: 0.1949
agent1:                 episode reward: 0.1703,                 loss: nan
Episode: 6541/101000 (6.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8346s / 504.8545 s
agent0:                 episode reward: -0.4397,                 loss: 0.1939
agent1:                 episode reward: 0.4397,                 loss: nan
Episode: 6561/101000 (6.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0827s / 506.9371 s
agent0:                 episode reward: -0.2014,                 loss: 0.1942
agent1:                 episode reward: 0.2014,                 loss: nan
Episode: 6581/101000 (6.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5704s / 508.5076 s
agent0:                 episode reward: -0.4366,                 loss: 0.1957
agent1:                 episode reward: 0.4366,                 loss: nan
Episode: 6601/101000 (6.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7215s / 510.2291 s
agent0:                 episode reward: -0.5419,                 loss: 0.1923
agent1:                 episode reward: 0.5419,                 loss: nan
Episode: 6621/101000 (6.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7861s / 512.0152 s
agent0:                 episode reward: -0.2392,                 loss: 0.1957
agent1:                 episode reward: 0.2392,                 loss: nan
Episode: 6641/101000 (6.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8220s / 513.8372 s
agent0:                 episode reward: -0.4947,                 loss: 0.1941
agent1:                 episode reward: 0.4947,                 loss: nan
Episode: 6661/101000 (6.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6366s / 515.4738 s
agent0:                 episode reward: -0.5176,                 loss: 0.1958
agent1:                 episode reward: 0.5176,                 loss: nan
Episode: 6681/101000 (6.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3093s / 516.7831 s
agent0:                 episode reward: -0.6464,                 loss: 0.1938
agent1:                 episode reward: 0.6464,                 loss: nan
Episode: 6701/101000 (6.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8796s / 518.6627 s
agent0:                 episode reward: 0.0013,                 loss: 0.1914
agent1:                 episode reward: -0.0013,                 loss: nan
Episode: 6721/101000 (6.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8131s / 520.4758 s
agent0:                 episode reward: 0.1296,                 loss: 0.1929
agent1:                 episode reward: -0.1296,                 loss: nan
Episode: 6741/101000 (6.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2027s / 522.6785 s
agent0:                 episode reward: -0.3325,                 loss: 0.1955
agent1:                 episode reward: 0.3325,                 loss: nan
Episode: 6761/101000 (6.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6736s / 524.3522 s
agent0:                 episode reward: -0.0715,                 loss: 0.1945
agent1:                 episode reward: 0.0715,                 loss: nan
Episode: 6781/101000 (6.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8118s / 526.1640 s
agent0:                 episode reward: -0.3833,                 loss: 0.1936
agent1:                 episode reward: 0.3833,                 loss: nan
Episode: 6801/101000 (6.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7315s / 527.8955 s
agent0:                 episode reward: -0.6517,                 loss: 0.1850
agent1:                 episode reward: 0.6517,                 loss: nan
Episode: 6821/101000 (6.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6495s / 529.5450 s
agent0:                 episode reward: 0.1268,                 loss: 0.1894
agent1:                 episode reward: -0.1268,                 loss: 0.1532
Score delta: 1.6292636536503138, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/6390_0.
Episode: 6841/101000 (6.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7312s / 531.2762 s
agent0:                 episode reward: -0.4482,                 loss: nan
agent1:                 episode reward: 0.4482,                 loss: 0.1560
Episode: 6861/101000 (6.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4992s / 532.7754 s
agent0:                 episode reward: -0.0088,                 loss: nan
agent1:                 episode reward: 0.0088,                 loss: 0.1572
Episode: 6881/101000 (6.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8134s / 534.5887 s
agent0:                 episode reward: -0.3581,                 loss: nan
agent1:                 episode reward: 0.3581,                 loss: 0.1560
Episode: 6901/101000 (6.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8786s / 536.4673 s
agent0:                 episode reward: -0.4660,                 loss: 0.2197
agent1:                 episode reward: 0.4660,                 loss: 0.1572
Score delta: 1.5300439431089665, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/6465_1.
Episode: 6921/101000 (6.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3327s / 538.7999 s
agent0:                 episode reward: -0.5671,                 loss: 0.2149
agent1:                 episode reward: 0.5671,                 loss: nan
Episode: 6941/101000 (6.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0843s / 540.8842 s
agent0:                 episode reward: -0.4270,                 loss: 0.2144
agent1:                 episode reward: 0.4270,                 loss: nan
Episode: 6961/101000 (6.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7959s / 542.6801 s
agent0:                 episode reward: -0.3320,                 loss: 0.2174
agent1:                 episode reward: 0.3320,                 loss: nan
Episode: 6981/101000 (6.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6925s / 544.3726 s
agent0:                 episode reward: -0.3037,                 loss: 0.2165
agent1:                 episode reward: 0.3037,                 loss: nan
Episode: 7001/101000 (6.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5093s / 545.8819 s
agent0:                 episode reward: -0.4294,                 loss: 0.2196
agent1:                 episode reward: 0.4294,                 loss: nan
Episode: 7021/101000 (6.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8419s / 547.7238 s
agent0:                 episode reward: -0.2627,                 loss: 0.2195
agent1:                 episode reward: 0.2627,                 loss: nan
Episode: 7041/101000 (6.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3437s / 550.0675 s
agent0:                 episode reward: -0.3452,                 loss: 0.2179
agent1:                 episode reward: 0.3452,                 loss: nan
Episode: 7061/101000 (6.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6258s / 551.6933 s
agent0:                 episode reward: -0.1076,                 loss: 0.2149
agent1:                 episode reward: 0.1076,                 loss: nan
Episode: 7081/101000 (7.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8396s / 553.5329 s
agent0:                 episode reward: -0.4046,                 loss: 0.2189
agent1:                 episode reward: 0.4046,                 loss: nan
Episode: 7101/101000 (7.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3925s / 554.9254 s
agent0:                 episode reward: -0.2923,                 loss: 0.2167
agent1:                 episode reward: 0.2923,                 loss: nan
Episode: 7121/101000 (7.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1402s / 557.0656 s
agent0:                 episode reward: -0.3152,                 loss: 0.2190
agent1:                 episode reward: 0.3152,                 loss: nan
Episode: 7141/101000 (7.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6320s / 558.6976 s
agent0:                 episode reward: -0.3646,                 loss: 0.2159
agent1:                 episode reward: 0.3646,                 loss: nan
Episode: 7161/101000 (7.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7628s / 560.4604 s
agent0:                 episode reward: -0.4340,                 loss: 0.2185
agent1:                 episode reward: 0.4340,                 loss: nan
Episode: 7181/101000 (7.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6657s / 562.1261 s
agent0:                 episode reward: -0.2327,                 loss: 0.2196
agent1:                 episode reward: 0.2327,                 loss: nan
Episode: 7201/101000 (7.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0141s / 564.1401 s
agent0:                 episode reward: -0.3906,                 loss: 0.1987
agent1:                 episode reward: 0.3906,                 loss: nan
Episode: 7221/101000 (7.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8604s / 566.0006 s
agent0:                 episode reward: -0.6466,                 loss: 0.1883
agent1:                 episode reward: 0.6466,                 loss: nan
Episode: 7241/101000 (7.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8064s / 567.8069 s
agent0:                 episode reward: 0.1148,                 loss: 0.1894
agent1:                 episode reward: -0.1148,                 loss: nan
Episode: 7261/101000 (7.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3053s / 570.1122 s
agent0:                 episode reward: -0.3144,                 loss: 0.1888
agent1:                 episode reward: 0.3144,                 loss: nan
Episode: 7281/101000 (7.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9257s / 572.0379 s
agent0:                 episode reward: -0.4713,                 loss: 0.1884
agent1:                 episode reward: 0.4713,                 loss: nan
Episode: 7301/101000 (7.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1095s / 574.1474 s
agent0:                 episode reward: -0.4556,                 loss: 0.1884
agent1:                 episode reward: 0.4556,                 loss: nan
Episode: 7321/101000 (7.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7430s / 575.8904 s
agent0:                 episode reward: -0.2963,                 loss: 0.1908
agent1:                 episode reward: 0.2963,                 loss: nan
Episode: 7341/101000 (7.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2904s / 578.1808 s
agent0:                 episode reward: -0.6097,                 loss: 0.1892
agent1:                 episode reward: 0.6097,                 loss: nan
Episode: 7361/101000 (7.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6393s / 579.8201 s
agent0:                 episode reward: -0.7165,                 loss: 0.1892
agent1:                 episode reward: 0.7165,                 loss: nan
Episode: 7381/101000 (7.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8781s / 581.6982 s
agent0:                 episode reward: -0.2353,                 loss: 0.1887
agent1:                 episode reward: 0.2353,                 loss: nan
Episode: 7401/101000 (7.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8780s / 583.5761 s
agent0:                 episode reward: -0.3311,                 loss: 0.1901
agent1:                 episode reward: 0.3311,                 loss: nan
Episode: 7421/101000 (7.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4262s / 585.0023 s
agent0:                 episode reward: -0.2472,                 loss: 0.1907
agent1:                 episode reward: 0.2472,                 loss: nan
Episode: 7441/101000 (7.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8503s / 586.8527 s
agent0:                 episode reward: -0.5610,                 loss: 0.1899
agent1:                 episode reward: 0.5610,                 loss: nan
Episode: 7461/101000 (7.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9361s / 588.7888 s
agent0:                 episode reward: -0.3323,                 loss: 0.1906
agent1:                 episode reward: 0.3323,                 loss: nan
Episode: 7481/101000 (7.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1419s / 590.9307 s
agent0:                 episode reward: 0.4841,                 loss: 0.1914
agent1:                 episode reward: -0.4841,                 loss: 0.1482
Score delta: 1.5819850633407921, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/7044_0.
Episode: 7501/101000 (7.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5150s / 592.4457 s
agent0:                 episode reward: 0.0287,                 loss: nan
agent1:                 episode reward: -0.0287,                 loss: 0.1426
Episode: 7521/101000 (7.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6581s / 594.1038 s
agent0:                 episode reward: -0.2063,                 loss: nan
agent1:                 episode reward: 0.2063,                 loss: 0.1426
Episode: 7541/101000 (7.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3992s / 596.5031 s
agent0:                 episode reward: 0.0210,                 loss: nan
agent1:                 episode reward: -0.0210,                 loss: 0.1387
Episode: 7561/101000 (7.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8185s / 598.3216 s
agent0:                 episode reward: -0.1260,                 loss: nan
agent1:                 episode reward: 0.1260,                 loss: 0.1406
Episode: 7581/101000 (7.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8184s / 600.1400 s
agent0:                 episode reward: 0.0830,                 loss: nan
agent1:                 episode reward: -0.0830,                 loss: 0.1406
Episode: 7601/101000 (7.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5644s / 601.7043 s
agent0:                 episode reward: -0.7780,                 loss: 0.1916
agent1:                 episode reward: 0.7780,                 loss: 0.1398
Score delta: 1.7087053081059367, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/7164_1.
Episode: 7621/101000 (7.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6773s / 603.3816 s
agent0:                 episode reward: -0.0787,                 loss: 0.1887
agent1:                 episode reward: 0.0787,                 loss: nan
Episode: 7641/101000 (7.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3703s / 604.7520 s
agent0:                 episode reward: -0.2975,                 loss: 0.1881
agent1:                 episode reward: 0.2975,                 loss: nan
Episode: 7661/101000 (7.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8645s / 606.6165 s
agent0:                 episode reward: -0.0590,                 loss: 0.1818
agent1:                 episode reward: 0.0590,                 loss: nan
Episode: 7681/101000 (7.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6622s / 608.2787 s
agent0:                 episode reward: -0.9778,                 loss: 0.1815
agent1:                 episode reward: 0.9778,                 loss: nan
Episode: 7701/101000 (7.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0336s / 610.3123 s
agent0:                 episode reward: -0.1007,                 loss: 0.1814
agent1:                 episode reward: 0.1007,                 loss: nan
Episode: 7721/101000 (7.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8709s / 612.1832 s
agent0:                 episode reward: -0.8570,                 loss: 0.1819
agent1:                 episode reward: 0.8570,                 loss: nan
Episode: 7741/101000 (7.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2557s / 613.4389 s
agent0:                 episode reward: -0.2264,                 loss: 0.1825
agent1:                 episode reward: 0.2264,                 loss: nan
Episode: 7761/101000 (7.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6667s / 615.1056 s
agent0:                 episode reward: -0.6666,                 loss: 0.1809
agent1:                 episode reward: 0.6666,                 loss: nan
Episode: 7781/101000 (7.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8276s / 616.9332 s
agent0:                 episode reward: -0.4985,                 loss: 0.1821
agent1:                 episode reward: 0.4985,                 loss: nan
Episode: 7801/101000 (7.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7950s / 618.7282 s
agent0:                 episode reward: -0.2751,                 loss: 0.1816
agent1:                 episode reward: 0.2751,                 loss: nan
Episode: 7821/101000 (7.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8182s / 620.5464 s
agent0:                 episode reward: -0.1170,                 loss: 0.1835
agent1:                 episode reward: 0.1170,                 loss: nan
Episode: 7841/101000 (7.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8034s / 622.3498 s
agent0:                 episode reward: -0.7111,                 loss: 0.1843
agent1:                 episode reward: 0.7111,                 loss: nan
Episode: 7861/101000 (7.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1294s / 624.4792 s
agent0:                 episode reward: -0.6162,                 loss: 0.1834
agent1:                 episode reward: 0.6162,                 loss: nan
Episode: 7881/101000 (7.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3462s / 625.8254 s
agent0:                 episode reward: -0.2902,                 loss: 0.1811
agent1:                 episode reward: 0.2902,                 loss: nan
Episode: 7901/101000 (7.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9698s / 627.7952 s
agent0:                 episode reward: -0.1478,                 loss: 0.1817
agent1:                 episode reward: 0.1478,                 loss: nan
Episode: 7921/101000 (7.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9480s / 629.7432 s
agent0:                 episode reward: -0.1849,                 loss: 0.1801
agent1:                 episode reward: 0.1849,                 loss: nan
Episode: 7941/101000 (7.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8832s / 631.6264 s
agent0:                 episode reward: -0.4539,                 loss: 0.1826
agent1:                 episode reward: 0.4539,                 loss: nan
Episode: 7961/101000 (7.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8931s / 633.5195 s
agent0:                 episode reward: -0.6803,                 loss: 0.1842
agent1:                 episode reward: 0.6803,                 loss: nan
Episode: 7981/101000 (7.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0172s / 635.5367 s
agent0:                 episode reward: -0.4663,                 loss: 0.1803
agent1:                 episode reward: 0.4663,                 loss: nan
Episode: 8001/101000 (7.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6756s / 637.2123 s
agent0:                 episode reward: -0.2086,                 loss: 0.1779
agent1:                 episode reward: 0.2086,                 loss: nan
Episode: 8021/101000 (7.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6334s / 638.8457 s
agent0:                 episode reward: -0.1332,                 loss: 0.1756
agent1:                 episode reward: 0.1332,                 loss: nan
Episode: 8041/101000 (7.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9965s / 640.8423 s
agent0:                 episode reward: -0.0271,                 loss: 0.1775
agent1:                 episode reward: 0.0271,                 loss: nan
Episode: 8061/101000 (7.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0690s / 642.9112 s
agent0:                 episode reward: -0.5664,                 loss: 0.1757
agent1:                 episode reward: 0.5664,                 loss: nan
Episode: 8081/101000 (8.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4250s / 645.3362 s
agent0:                 episode reward: -0.3441,                 loss: 0.1763
agent1:                 episode reward: 0.3441,                 loss: nan
Episode: 8101/101000 (8.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7105s / 647.0467 s
agent0:                 episode reward: -0.5518,                 loss: 0.1767
agent1:                 episode reward: 0.5518,                 loss: nan
Episode: 8121/101000 (8.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5684s / 648.6151 s
agent0:                 episode reward: 0.1168,                 loss: 0.1760
agent1:                 episode reward: -0.1168,                 loss: nan
Episode: 8141/101000 (8.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1787s / 650.7938 s
agent0:                 episode reward: -0.1667,                 loss: 0.1796
agent1:                 episode reward: 0.1667,                 loss: nan
Episode: 8161/101000 (8.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7876s / 652.5814 s
agent0:                 episode reward: -0.1684,                 loss: 0.1791
agent1:                 episode reward: 0.1684,                 loss: nan
Episode: 8181/101000 (8.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0766s / 654.6581 s
agent0:                 episode reward: -0.0181,                 loss: 0.1757
agent1:                 episode reward: 0.0181,                 loss: nan
Episode: 8201/101000 (8.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5069s / 656.1650 s
agent0:                 episode reward: -0.4274,                 loss: 0.1801
agent1:                 episode reward: 0.4274,                 loss: nan
Episode: 8221/101000 (8.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8406s / 658.0055 s
agent0:                 episode reward: -0.8476,                 loss: 0.1784
agent1:                 episode reward: 0.8476,                 loss: nan
Episode: 8241/101000 (8.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7590s / 659.7645 s
agent0:                 episode reward: -0.4149,                 loss: 0.1783
agent1:                 episode reward: 0.4149,                 loss: nan
Episode: 8261/101000 (8.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1372s / 661.9017 s
agent0:                 episode reward: -0.7876,                 loss: 0.1799
agent1:                 episode reward: 0.7876,                 loss: nan
Episode: 8281/101000 (8.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9495s / 663.8512 s
agent0:                 episode reward: -0.6393,                 loss: 0.1789
agent1:                 episode reward: 0.6393,                 loss: nan
Episode: 8301/101000 (8.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5418s / 665.3930 s
agent0:                 episode reward: -0.3337,                 loss: 0.1793
agent1:                 episode reward: 0.3337,                 loss: nan
Episode: 8321/101000 (8.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5752s / 666.9682 s
agent0:                 episode reward: -0.5152,                 loss: 0.1807
agent1:                 episode reward: 0.5152,                 loss: nan
Episode: 8341/101000 (8.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9798s / 668.9480 s
agent0:                 episode reward: -0.2892,                 loss: 0.1776
agent1:                 episode reward: 0.2892,                 loss: nan
Episode: 8361/101000 (8.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5002s / 670.4482 s
agent0:                 episode reward: -0.0234,                 loss: 0.1810
agent1:                 episode reward: 0.0234,                 loss: nan
Episode: 8381/101000 (8.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8941s / 672.3423 s
agent0:                 episode reward: -0.4718,                 loss: 0.1799
agent1:                 episode reward: 0.4718,                 loss: nan
Episode: 8401/101000 (8.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6871s / 674.0294 s
agent0:                 episode reward: -0.5847,                 loss: 0.1779
agent1:                 episode reward: 0.5847,                 loss: nan
Episode: 8421/101000 (8.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7961s / 675.8255 s
agent0:                 episode reward: -0.4184,                 loss: 0.1810
agent1:                 episode reward: 0.4184,                 loss: nan
Episode: 8441/101000 (8.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3536s / 677.1792 s
agent0:                 episode reward: -0.1348,                 loss: 0.1817
agent1:                 episode reward: 0.1348,                 loss: nan
Episode: 8461/101000 (8.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7476s / 678.9268 s
agent0:                 episode reward: -0.2437,                 loss: 0.1804
agent1:                 episode reward: 0.2437,                 loss: nan
Episode: 8481/101000 (8.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2000s / 681.1268 s
agent0:                 episode reward: -0.1651,                 loss: 0.1790
agent1:                 episode reward: 0.1651,                 loss: nan
Episode: 8501/101000 (8.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6050s / 682.7318 s
agent0:                 episode reward: -0.2934,                 loss: 0.1775
agent1:                 episode reward: 0.2934,                 loss: nan
Episode: 8521/101000 (8.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7224s / 684.4541 s
agent0:                 episode reward: -0.2486,                 loss: 0.1788
agent1:                 episode reward: 0.2486,                 loss: nan
Episode: 8541/101000 (8.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3157s / 686.7698 s
agent0:                 episode reward: -0.5027,                 loss: 0.1808
agent1:                 episode reward: 0.5027,                 loss: nan
Episode: 8561/101000 (8.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2132s / 688.9830 s
agent0:                 episode reward: -0.1936,                 loss: 0.1800
agent1:                 episode reward: 0.1936,                 loss: nan
Episode: 8581/101000 (8.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0539s / 691.0369 s
agent0:                 episode reward: -0.7758,                 loss: 0.1812
agent1:                 episode reward: 0.7758,                 loss: nan
Episode: 8601/101000 (8.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0771s / 693.1140 s
agent0:                 episode reward: -0.2700,                 loss: 0.1812
agent1:                 episode reward: 0.2700,                 loss: nan
Episode: 8621/101000 (8.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4795s / 695.5935 s
agent0:                 episode reward: 0.1776,                 loss: 0.1808
agent1:                 episode reward: -0.1776,                 loss: nan
Episode: 8641/101000 (8.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0804s / 697.6739 s
agent0:                 episode reward: -0.4245,                 loss: 0.1811
agent1:                 episode reward: 0.4245,                 loss: nan
Episode: 8661/101000 (8.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4914s / 700.1653 s
agent0:                 episode reward: -0.1299,                 loss: 0.1749
agent1:                 episode reward: 0.1299,                 loss: nan
Episode: 8681/101000 (8.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6289s / 701.7942 s
agent0:                 episode reward: -0.3181,                 loss: 0.1764
agent1:                 episode reward: 0.3181,                 loss: nan
Episode: 8701/101000 (8.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8995s / 703.6936 s
agent0:                 episode reward: -0.8037,                 loss: 0.1748
agent1:                 episode reward: 0.8037,                 loss: nan
Episode: 8721/101000 (8.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8551s / 705.5487 s
agent0:                 episode reward: -0.2816,                 loss: 0.1776
agent1:                 episode reward: 0.2816,                 loss: nan
Episode: 8741/101000 (8.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0826s / 707.6313 s
agent0:                 episode reward: -0.2065,                 loss: 0.1764
agent1:                 episode reward: 0.2065,                 loss: nan
Episode: 8761/101000 (8.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2651s / 709.8964 s
agent0:                 episode reward: -0.2926,                 loss: 0.1774
agent1:                 episode reward: 0.2926,                 loss: nan
Episode: 8781/101000 (8.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5831s / 711.4794 s
agent0:                 episode reward: -0.1578,                 loss: 0.1767
agent1:                 episode reward: 0.1578,                 loss: nan
Episode: 8801/101000 (8.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3383s / 713.8177 s
agent0:                 episode reward: -0.3061,                 loss: 0.1771
agent1:                 episode reward: 0.3061,                 loss: nan
Episode: 8821/101000 (8.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1169s / 715.9347 s
agent0:                 episode reward: -0.6646,                 loss: 0.1751
agent1:                 episode reward: 0.6646,                 loss: nan
Episode: 8841/101000 (8.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1610s / 718.0957 s
agent0:                 episode reward: -0.1470,                 loss: 0.1759
agent1:                 episode reward: 0.1470,                 loss: nan
Episode: 8861/101000 (8.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1567s / 720.2524 s
agent0:                 episode reward: 0.1637,                 loss: 0.1793
agent1:                 episode reward: -0.1637,                 loss: nan
Episode: 8881/101000 (8.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3591s / 722.6115 s
agent0:                 episode reward: -0.1560,                 loss: 0.1777
agent1:                 episode reward: 0.1560,                 loss: nan
Episode: 8901/101000 (8.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5412s / 724.1527 s
agent0:                 episode reward: -0.3154,                 loss: 0.1762
agent1:                 episode reward: 0.3154,                 loss: nan
Episode: 8921/101000 (8.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4565s / 725.6092 s
agent0:                 episode reward: -0.6143,                 loss: 0.1781
agent1:                 episode reward: 0.6143,                 loss: nan
Episode: 8941/101000 (8.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9996s / 727.6088 s
agent0:                 episode reward: -0.6150,                 loss: 0.1792
agent1:                 episode reward: 0.6150,                 loss: nan
Episode: 8961/101000 (8.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7501s / 729.3588 s
agent0:                 episode reward: -0.6866,                 loss: 0.1779
agent1:                 episode reward: 0.6866,                 loss: nan
Episode: 8981/101000 (8.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9083s / 731.2671 s
agent0:                 episode reward: -0.4312,                 loss: 0.1749
agent1:                 episode reward: 0.4312,                 loss: nan
Episode: 9001/101000 (8.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7300s / 732.9971 s
agent0:                 episode reward: -0.1834,                 loss: 0.1776
agent1:                 episode reward: 0.1834,                 loss: nan
Episode: 9021/101000 (8.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0670s / 735.0641 s
agent0:                 episode reward: 0.0566,                 loss: 0.1769
agent1:                 episode reward: -0.0566,                 loss: nan
Episode: 9041/101000 (8.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1612s / 737.2254 s
agent0:                 episode reward: -0.2071,                 loss: 0.1761
agent1:                 episode reward: 0.2071,                 loss: nan
Episode: 9061/101000 (8.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8423s / 739.0677 s
agent0:                 episode reward: -0.3996,                 loss: 0.1761
agent1:                 episode reward: 0.3996,                 loss: nan
Episode: 9081/101000 (8.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8338s / 740.9015 s
agent0:                 episode reward: -0.3425,                 loss: 0.1769
agent1:                 episode reward: 0.3425,                 loss: nan
Episode: 9101/101000 (9.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0691s / 742.9706 s
agent0:                 episode reward: -0.2900,                 loss: 0.1774
agent1:                 episode reward: 0.2900,                 loss: nan
Episode: 9121/101000 (9.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8162s / 744.7868 s
agent0:                 episode reward: -0.3636,                 loss: 0.1785
agent1:                 episode reward: 0.3636,                 loss: nan
Episode: 9141/101000 (9.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9060s / 746.6928 s
agent0:                 episode reward: -0.7056,                 loss: 0.1774
agent1:                 episode reward: 0.7056,                 loss: nan
Episode: 9161/101000 (9.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8795s / 748.5723 s
agent0:                 episode reward: -0.2217,                 loss: 0.1769
agent1:                 episode reward: 0.2217,                 loss: nan
Episode: 9181/101000 (9.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3785s / 749.9509 s
agent0:                 episode reward: -0.6561,                 loss: 0.1777
agent1:                 episode reward: 0.6561,                 loss: nan
Episode: 9201/101000 (9.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8627s / 751.8136 s
agent0:                 episode reward: -0.6644,                 loss: 0.1750
agent1:                 episode reward: 0.6644,                 loss: nan
Episode: 9221/101000 (9.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5105s / 753.3241 s
agent0:                 episode reward: -0.2442,                 loss: 0.1760
agent1:                 episode reward: 0.2442,                 loss: nan
Episode: 9241/101000 (9.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8653s / 755.1894 s
agent0:                 episode reward: -0.2308,                 loss: 0.1766
agent1:                 episode reward: 0.2308,                 loss: nan
Episode: 9261/101000 (9.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8111s / 757.0005 s
agent0:                 episode reward: -0.2146,                 loss: 0.1771
agent1:                 episode reward: 0.2146,                 loss: nan
Episode: 9281/101000 (9.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5607s / 758.5612 s
agent0:                 episode reward: 0.1127,                 loss: 0.1765
agent1:                 episode reward: -0.1127,                 loss: nan
Episode: 9301/101000 (9.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0591s / 760.6203 s
agent0:                 episode reward: -0.2608,                 loss: 0.1762
agent1:                 episode reward: 0.2608,                 loss: nan
Episode: 9321/101000 (9.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3470s / 761.9674 s
agent0:                 episode reward: -0.2568,                 loss: 0.1730
agent1:                 episode reward: 0.2568,                 loss: nan
Episode: 9341/101000 (9.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8032s / 763.7706 s
agent0:                 episode reward: -0.3388,                 loss: 0.1720
agent1:                 episode reward: 0.3388,                 loss: nan
Episode: 9361/101000 (9.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2808s / 766.0514 s
agent0:                 episode reward: -0.5017,                 loss: 0.1730
agent1:                 episode reward: 0.5017,                 loss: nan
Episode: 9381/101000 (9.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5844s / 767.6358 s
agent0:                 episode reward: -0.0755,                 loss: 0.1740
agent1:                 episode reward: 0.0755,                 loss: nan
Episode: 9401/101000 (9.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0072s / 769.6430 s
agent0:                 episode reward: 0.0164,                 loss: 0.1740
agent1:                 episode reward: -0.0164,                 loss: nan
Episode: 9421/101000 (9.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9794s / 771.6224 s
agent0:                 episode reward: -0.1605,                 loss: 0.1726
agent1:                 episode reward: 0.1605,                 loss: nan
Episode: 9441/101000 (9.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4346s / 773.0569 s
agent0:                 episode reward: -0.0080,                 loss: 0.1751
agent1:                 episode reward: 0.0080,                 loss: 0.1561
Score delta: 1.7167769495385277, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/8999_0.
Episode: 9461/101000 (9.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3618s / 774.4188 s
agent0:                 episode reward: -0.2639,                 loss: 0.2375
agent1:                 episode reward: 0.2639,                 loss: 0.1557
Score delta: 1.7000731160122118, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/9020_1.
Episode: 9481/101000 (9.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3626s / 775.7813 s
agent0:                 episode reward: -0.3000,                 loss: 0.2356
agent1:                 episode reward: 0.3000,                 loss: nan
Episode: 9501/101000 (9.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5243s / 777.3057 s
agent0:                 episode reward: -0.7516,                 loss: 0.2359
agent1:                 episode reward: 0.7516,                 loss: nan
Episode: 9521/101000 (9.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8147s / 779.1204 s
agent0:                 episode reward: -0.2827,                 loss: 0.2358
agent1:                 episode reward: 0.2827,                 loss: nan
Episode: 9541/101000 (9.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2944s / 781.4148 s
agent0:                 episode reward: -0.0404,                 loss: 0.2402
agent1:                 episode reward: 0.0404,                 loss: nan
Episode: 9561/101000 (9.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2258s / 783.6406 s
agent0:                 episode reward: -0.2454,                 loss: 0.2348
agent1:                 episode reward: 0.2454,                 loss: nan
Episode: 9581/101000 (9.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9444s / 785.5850 s
agent0:                 episode reward: -0.7220,                 loss: 0.2341
agent1:                 episode reward: 0.7220,                 loss: nan
Episode: 9601/101000 (9.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5872s / 787.1722 s
agent0:                 episode reward: -0.2103,                 loss: 0.2365
agent1:                 episode reward: 0.2103,                 loss: nan
Episode: 9621/101000 (9.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1515s / 789.3237 s
agent0:                 episode reward: -0.5917,                 loss: 0.2380
agent1:                 episode reward: 0.5917,                 loss: nan
Episode: 9641/101000 (9.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1833s / 791.5070 s
agent0:                 episode reward: -0.0766,                 loss: 0.2359
agent1:                 episode reward: 0.0766,                 loss: nan
Episode: 9661/101000 (9.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2125s / 793.7195 s
agent0:                 episode reward: -0.1635,                 loss: 0.2337
agent1:                 episode reward: 0.1635,                 loss: nan
Episode: 9681/101000 (9.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1995s / 795.9189 s
agent0:                 episode reward: -0.3367,                 loss: 0.1895
agent1:                 episode reward: 0.3367,                 loss: nan
Episode: 9701/101000 (9.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9119s / 797.8309 s
agent0:                 episode reward: 0.2181,                 loss: 0.1918
agent1:                 episode reward: -0.2181,                 loss: nan
Episode: 9721/101000 (9.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0319s / 799.8627 s
agent0:                 episode reward: -0.1546,                 loss: 0.1903
agent1:                 episode reward: 0.1546,                 loss: nan
Episode: 9741/101000 (9.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7752s / 801.6380 s
agent0:                 episode reward: -0.5104,                 loss: 0.1905
agent1:                 episode reward: 0.5104,                 loss: nan
Episode: 9761/101000 (9.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2077s / 803.8456 s
agent0:                 episode reward: -0.8888,                 loss: 0.1924
agent1:                 episode reward: 0.8888,                 loss: nan
Episode: 9781/101000 (9.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6852s / 805.5308 s
agent0:                 episode reward: -0.3659,                 loss: 0.1902
agent1:                 episode reward: 0.3659,                 loss: nan
Episode: 9801/101000 (9.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0933s / 807.6241 s
agent0:                 episode reward: -0.4866,                 loss: 0.1906
agent1:                 episode reward: 0.4866,                 loss: nan
Episode: 9821/101000 (9.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7122s / 809.3363 s
agent0:                 episode reward: -0.4977,                 loss: 0.1890
agent1:                 episode reward: 0.4977,                 loss: nan
Episode: 9841/101000 (9.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3258s / 810.6621 s
agent0:                 episode reward: -0.4801,                 loss: 0.1903
agent1:                 episode reward: 0.4801,                 loss: nan
Episode: 9861/101000 (9.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8538s / 812.5159 s
agent0:                 episode reward: 0.1845,                 loss: 0.1871
agent1:                 episode reward: -0.1845,                 loss: nan
Episode: 9881/101000 (9.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1825s / 814.6984 s
agent0:                 episode reward: -0.9342,                 loss: 0.1880
agent1:                 episode reward: 0.9342,                 loss: nan
Episode: 9901/101000 (9.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5042s / 816.2026 s
agent0:                 episode reward: -0.5444,                 loss: 0.1912
agent1:                 episode reward: 0.5444,                 loss: nan
Episode: 9921/101000 (9.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1617s / 818.3643 s
agent0:                 episode reward: -0.6811,                 loss: 0.1895
agent1:                 episode reward: 0.6811,                 loss: nan
Episode: 9941/101000 (9.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7776s / 820.1419 s
agent0:                 episode reward: -0.5050,                 loss: 0.1890
agent1:                 episode reward: 0.5050,                 loss: nan
Episode: 9961/101000 (9.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8538s / 821.9957 s
agent0:                 episode reward: -0.8281,                 loss: 0.1925
agent1:                 episode reward: 0.8281,                 loss: nan
Episode: 9981/101000 (9.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1137s / 824.1094 s
agent0:                 episode reward: -0.3676,                 loss: 0.1900
agent1:                 episode reward: 0.3676,                 loss: nan
Episode: 10001/101000 (9.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6905s / 825.8000 s
agent0:                 episode reward: -0.2134,                 loss: 0.1852
agent1:                 episode reward: 0.2134,                 loss: nan
Episode: 10021/101000 (9.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8526s / 827.6526 s
agent0:                 episode reward: -0.2498,                 loss: 0.1791
agent1:                 episode reward: 0.2498,                 loss: nan
Episode: 10041/101000 (9.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5610s / 829.2136 s
agent0:                 episode reward: -0.5115,                 loss: 0.1799
agent1:                 episode reward: 0.5115,                 loss: nan
Episode: 10061/101000 (9.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8903s / 831.1039 s
agent0:                 episode reward: -0.1449,                 loss: 0.1793
agent1:                 episode reward: 0.1449,                 loss: nan
Episode: 10081/101000 (9.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6544s / 832.7583 s
agent0:                 episode reward: -0.1150,                 loss: 0.1783
agent1:                 episode reward: 0.1150,                 loss: nan
Episode: 10101/101000 (10.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9962s / 834.7546 s
agent0:                 episode reward: -0.2774,                 loss: 0.1761
agent1:                 episode reward: 0.2774,                 loss: nan
Episode: 10121/101000 (10.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3474s / 837.1019 s
agent0:                 episode reward: -0.0439,                 loss: 0.1801
agent1:                 episode reward: 0.0439,                 loss: nan
Episode: 10141/101000 (10.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9744s / 839.0763 s
agent0:                 episode reward: -0.7494,                 loss: 0.1794
agent1:                 episode reward: 0.7494,                 loss: nan
Episode: 10161/101000 (10.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8005s / 840.8767 s
agent0:                 episode reward: -0.6422,                 loss: 0.1809
agent1:                 episode reward: 0.6422,                 loss: nan
Episode: 10181/101000 (10.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1739s / 843.0507 s
agent0:                 episode reward: -0.1034,                 loss: 0.1794
agent1:                 episode reward: 0.1034,                 loss: nan
Episode: 10201/101000 (10.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0637s / 845.1144 s
agent0:                 episode reward: -0.2534,                 loss: 0.1807
agent1:                 episode reward: 0.2534,                 loss: nan
Episode: 10221/101000 (10.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8586s / 846.9729 s
agent0:                 episode reward: -0.6513,                 loss: 0.1809
agent1:                 episode reward: 0.6513,                 loss: nan
Episode: 10241/101000 (10.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8546s / 848.8275 s
agent0:                 episode reward: -0.3972,                 loss: 0.1803
agent1:                 episode reward: 0.3972,                 loss: nan
Episode: 10261/101000 (10.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0394s / 850.8669 s
agent0:                 episode reward: -0.5094,                 loss: 0.1786
agent1:                 episode reward: 0.5094,                 loss: nan
Episode: 10281/101000 (10.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6567s / 852.5236 s
agent0:                 episode reward: -0.0789,                 loss: 0.1800
agent1:                 episode reward: 0.0789,                 loss: nan
Episode: 10301/101000 (10.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3551s / 854.8788 s
agent0:                 episode reward: -0.2087,                 loss: 0.1786
agent1:                 episode reward: 0.2087,                 loss: nan
Episode: 10321/101000 (10.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1249s / 857.0036 s
agent0:                 episode reward: -0.2004,                 loss: 0.1803
agent1:                 episode reward: 0.2004,                 loss: nan
Episode: 10341/101000 (10.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9726s / 858.9763 s
agent0:                 episode reward: -0.2100,                 loss: 0.1735
agent1:                 episode reward: 0.2100,                 loss: nan
Episode: 10361/101000 (10.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1107s / 861.0870 s
agent0:                 episode reward: -0.1531,                 loss: 0.1744
agent1:                 episode reward: 0.1531,                 loss: nan
Episode: 10381/101000 (10.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0032s / 863.0902 s
agent0:                 episode reward: -0.4820,                 loss: 0.1717
agent1:                 episode reward: 0.4820,                 loss: nan
Episode: 10401/101000 (10.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9913s / 865.0816 s
agent0:                 episode reward: -0.3526,                 loss: 0.1730
agent1:                 episode reward: 0.3526,                 loss: nan
Episode: 10421/101000 (10.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3867s / 866.4683 s
agent0:                 episode reward: -0.3205,                 loss: 0.1747
agent1:                 episode reward: 0.3205,                 loss: nan
Episode: 10441/101000 (10.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8042s / 868.2725 s
agent0:                 episode reward: 0.0250,                 loss: 0.1746
agent1:                 episode reward: -0.0250,                 loss: nan
Episode: 10461/101000 (10.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6901s / 869.9626 s
agent0:                 episode reward: -0.0523,                 loss: 0.1736
agent1:                 episode reward: 0.0523,                 loss: nan
Episode: 10481/101000 (10.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1255s / 872.0881 s
agent0:                 episode reward: -0.1282,                 loss: 0.1729
agent1:                 episode reward: 0.1282,                 loss: nan
Episode: 10501/101000 (10.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8142s / 873.9023 s
agent0:                 episode reward: -0.2078,                 loss: 0.1761
agent1:                 episode reward: 0.2078,                 loss: nan
Episode: 10521/101000 (10.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3470s / 876.2493 s
agent0:                 episode reward: -0.7288,                 loss: 0.1758
agent1:                 episode reward: 0.7288,                 loss: nan
Episode: 10541/101000 (10.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6949s / 877.9442 s
agent0:                 episode reward: 0.0730,                 loss: 0.1751
agent1:                 episode reward: -0.0730,                 loss: nan
Episode: 10561/101000 (10.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8743s / 879.8185 s
agent0:                 episode reward: -0.6597,                 loss: 0.1743
agent1:                 episode reward: 0.6597,                 loss: nan
Episode: 10581/101000 (10.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8126s / 881.6311 s
agent0:                 episode reward: -0.4749,                 loss: 0.1738
agent1:                 episode reward: 0.4749,                 loss: nan
Episode: 10601/101000 (10.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5164s / 883.1475 s
agent0:                 episode reward: 0.0510,                 loss: 0.1754
agent1:                 episode reward: -0.0510,                 loss: nan
Episode: 10621/101000 (10.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9301s / 885.0777 s
agent0:                 episode reward: -0.4084,                 loss: 0.1748
agent1:                 episode reward: 0.4084,                 loss: nan
Episode: 10641/101000 (10.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3006s / 887.3782 s
agent0:                 episode reward: -0.1613,                 loss: 0.1752
agent1:                 episode reward: 0.1613,                 loss: nan
Episode: 10661/101000 (10.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1699s / 889.5482 s
agent0:                 episode reward: -0.6150,                 loss: 0.1738
agent1:                 episode reward: 0.6150,                 loss: nan
Episode: 10681/101000 (10.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8684s / 891.4166 s
agent0:                 episode reward: -0.7170,                 loss: 0.1743
agent1:                 episode reward: 0.7170,                 loss: nan
Episode: 10701/101000 (10.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4939s / 892.9105 s
agent0:                 episode reward: -0.2022,                 loss: 0.1715
agent1:                 episode reward: 0.2022,                 loss: nan
Episode: 10721/101000 (10.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5189s / 894.4294 s
agent0:                 episode reward: -0.4087,                 loss: 0.1750
agent1:                 episode reward: 0.4087,                 loss: nan
Episode: 10741/101000 (10.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6974s / 896.1268 s
agent0:                 episode reward: -0.2504,                 loss: 0.1728
agent1:                 episode reward: 0.2504,                 loss: nan
Episode: 10761/101000 (10.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9792s / 898.1060 s
agent0:                 episode reward: -0.2752,                 loss: 0.1716
agent1:                 episode reward: 0.2752,                 loss: nan
Episode: 10781/101000 (10.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9585s / 900.0645 s
agent0:                 episode reward: -0.2465,                 loss: 0.1743
agent1:                 episode reward: 0.2465,                 loss: nan
Episode: 10801/101000 (10.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4899s / 902.5544 s
agent0:                 episode reward: -0.3928,                 loss: 0.1745
agent1:                 episode reward: 0.3928,                 loss: nan
Episode: 10821/101000 (10.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1451s / 904.6995 s
agent0:                 episode reward: -0.5183,                 loss: 0.1734
agent1:                 episode reward: 0.5183,                 loss: nan
Episode: 10841/101000 (10.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1694s / 906.8689 s
agent0:                 episode reward: -0.3425,                 loss: 0.1724
agent1:                 episode reward: 0.3425,                 loss: nan
Episode: 10861/101000 (10.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1915s / 909.0604 s
agent0:                 episode reward: 0.2158,                 loss: 0.1723
agent1:                 episode reward: -0.2158,                 loss: nan
Episode: 10881/101000 (10.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0328s / 911.0932 s
agent0:                 episode reward: -0.0239,                 loss: 0.1758
agent1:                 episode reward: 0.0239,                 loss: nan
Score delta: 1.571014566216648, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/10455_0.
Episode: 10901/101000 (10.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9174s / 913.0106 s
agent0:                 episode reward: -0.4478,                 loss: nan
agent1:                 episode reward: 0.4478,                 loss: 0.1558
Episode: 10921/101000 (10.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1849s / 915.1955 s
agent0:                 episode reward: -0.4436,                 loss: nan
agent1:                 episode reward: 0.4436,                 loss: 0.1558
Episode: 10941/101000 (10.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9656s / 917.1611 s
agent0:                 episode reward: 0.0339,                 loss: nan
agent1:                 episode reward: -0.0339,                 loss: 0.1547
Episode: 10961/101000 (10.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6971s / 918.8582 s
agent0:                 episode reward: -0.2288,                 loss: nan
agent1:                 episode reward: 0.2288,                 loss: 0.1547
Episode: 10981/101000 (10.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0592s / 920.9173 s
agent0:                 episode reward: -0.5105,                 loss: nan
agent1:                 episode reward: 0.5105,                 loss: 0.1536
Episode: 11001/101000 (10.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0004s / 922.9177 s
agent0:                 episode reward: -0.4338,                 loss: 0.2382
agent1:                 episode reward: 0.4338,                 loss: 0.1524
Score delta: 1.8081328314368394, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/10557_1.
Episode: 11021/101000 (10.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4163s / 925.3340 s
agent0:                 episode reward: -0.5237,                 loss: 0.2355
agent1:                 episode reward: 0.5237,                 loss: nan
Episode: 11041/101000 (10.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8414s / 927.1754 s
agent0:                 episode reward: -0.1316,                 loss: 0.2354
agent1:                 episode reward: 0.1316,                 loss: nan
Episode: 11061/101000 (10.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4213s / 929.5967 s
agent0:                 episode reward: 0.0606,                 loss: 0.2344
agent1:                 episode reward: -0.0606,                 loss: nan
Episode: 11081/101000 (10.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6178s / 931.2145 s
agent0:                 episode reward: -0.5948,                 loss: 0.2356
agent1:                 episode reward: 0.5948,                 loss: nan
Episode: 11101/101000 (10.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0013s / 933.2159 s
agent0:                 episode reward: -0.2821,                 loss: 0.2204
agent1:                 episode reward: 0.2821,                 loss: nan
Episode: 11121/101000 (11.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1409s / 935.3568 s
agent0:                 episode reward: -0.3287,                 loss: 0.1902
agent1:                 episode reward: 0.3287,                 loss: nan
Episode: 11141/101000 (11.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2213s / 937.5781 s
agent0:                 episode reward: -0.5972,                 loss: 0.1865
agent1:                 episode reward: 0.5972,                 loss: nan
Episode: 11161/101000 (11.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9059s / 939.4839 s
agent0:                 episode reward: -0.4645,                 loss: 0.1867
agent1:                 episode reward: 0.4645,                 loss: nan
Episode: 11181/101000 (11.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9748s / 941.4588 s
agent0:                 episode reward: -0.3057,                 loss: 0.1873
agent1:                 episode reward: 0.3057,                 loss: nan
Episode: 11201/101000 (11.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7109s / 943.1697 s
agent0:                 episode reward: -0.6354,                 loss: 0.1876
agent1:                 episode reward: 0.6354,                 loss: nan
Episode: 11221/101000 (11.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0633s / 945.2330 s
agent0:                 episode reward: -0.3699,                 loss: 0.1878
agent1:                 episode reward: 0.3699,                 loss: nan
Episode: 11241/101000 (11.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7531s / 946.9861 s
agent0:                 episode reward: -0.1561,                 loss: 0.1878
agent1:                 episode reward: 0.1561,                 loss: nan
Episode: 11261/101000 (11.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7632s / 948.7493 s
agent0:                 episode reward: -0.5717,                 loss: 0.1883
agent1:                 episode reward: 0.5717,                 loss: nan
Episode: 11281/101000 (11.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7734s / 950.5228 s
agent0:                 episode reward: -0.3176,                 loss: 0.1907
agent1:                 episode reward: 0.3176,                 loss: nan
Episode: 11301/101000 (11.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2898s / 952.8126 s
agent0:                 episode reward: -0.3665,                 loss: 0.1882
agent1:                 episode reward: 0.3665,                 loss: nan
Episode: 11321/101000 (11.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0419s / 954.8545 s
agent0:                 episode reward: -0.4358,                 loss: 0.1885
agent1:                 episode reward: 0.4358,                 loss: nan
Episode: 11341/101000 (11.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0969s / 956.9514 s
agent0:                 episode reward: -0.0062,                 loss: 0.1860
agent1:                 episode reward: 0.0062,                 loss: nan
Episode: 11361/101000 (11.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4198s / 959.3712 s
agent0:                 episode reward: -0.0455,                 loss: 0.1881
agent1:                 episode reward: 0.0455,                 loss: nan
Episode: 11381/101000 (11.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0285s / 961.3997 s
agent0:                 episode reward: -0.5575,                 loss: 0.1880
agent1:                 episode reward: 0.5575,                 loss: nan
Episode: 11401/101000 (11.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5854s / 962.9851 s
agent0:                 episode reward: -0.3968,                 loss: 0.1870
agent1:                 episode reward: 0.3968,                 loss: nan
Episode: 11421/101000 (11.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7306s / 964.7157 s
agent0:                 episode reward: -0.4351,                 loss: 0.1886
agent1:                 episode reward: 0.4351,                 loss: nan
Episode: 11441/101000 (11.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5740s / 967.2898 s
agent0:                 episode reward: -0.0007,                 loss: 0.1791
agent1:                 episode reward: 0.0007,                 loss: nan
Episode: 11461/101000 (11.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7420s / 969.0318 s
agent0:                 episode reward: -0.8576,                 loss: 0.1725
agent1:                 episode reward: 0.8576,                 loss: nan
Episode: 11481/101000 (11.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4706s / 971.5024 s
agent0:                 episode reward: -0.4186,                 loss: 0.1723
agent1:                 episode reward: 0.4186,                 loss: nan
Episode: 11501/101000 (11.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6471s / 973.1495 s
agent0:                 episode reward: -0.5959,                 loss: 0.1737
agent1:                 episode reward: 0.5959,                 loss: nan
Episode: 11521/101000 (11.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2032s / 975.3527 s
agent0:                 episode reward: -0.7418,                 loss: 0.1727
agent1:                 episode reward: 0.7418,                 loss: nan
Episode: 11541/101000 (11.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9895s / 977.3422 s
agent0:                 episode reward: -0.9424,                 loss: 0.1738
agent1:                 episode reward: 0.9424,                 loss: nan
Episode: 11561/101000 (11.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4187s / 979.7609 s
agent0:                 episode reward: -0.3952,                 loss: 0.1730
agent1:                 episode reward: 0.3952,                 loss: nan
Episode: 11581/101000 (11.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2489s / 982.0098 s
agent0:                 episode reward: -0.3418,                 loss: 0.1722
agent1:                 episode reward: 0.3418,                 loss: nan
Episode: 11601/101000 (11.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0873s / 984.0971 s
agent0:                 episode reward: -0.5069,                 loss: 0.1732
agent1:                 episode reward: 0.5069,                 loss: nan
Episode: 11621/101000 (11.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6440s / 985.7411 s
agent0:                 episode reward: -0.1251,                 loss: 0.1740
agent1:                 episode reward: 0.1251,                 loss: nan
Episode: 11641/101000 (11.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1883s / 987.9294 s
agent0:                 episode reward: -0.5651,                 loss: 0.1737
agent1:                 episode reward: 0.5651,                 loss: nan
Episode: 11661/101000 (11.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9923s / 989.9217 s
agent0:                 episode reward: -0.4404,                 loss: 0.1727
agent1:                 episode reward: 0.4404,                 loss: nan
Episode: 11681/101000 (11.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0606s / 991.9824 s
agent0:                 episode reward: -0.2377,                 loss: 0.1742
agent1:                 episode reward: 0.2377,                 loss: nan
Episode: 11701/101000 (11.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6279s / 993.6103 s
agent0:                 episode reward: -0.4945,                 loss: 0.1728
agent1:                 episode reward: 0.4945,                 loss: nan
Episode: 11721/101000 (11.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8327s / 995.4429 s
agent0:                 episode reward: -0.5447,                 loss: 0.1724
agent1:                 episode reward: 0.5447,                 loss: nan
Episode: 11741/101000 (11.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9231s / 997.3661 s
agent0:                 episode reward: -0.5669,                 loss: 0.1737
agent1:                 episode reward: 0.5669,                 loss: nan
Episode: 11761/101000 (11.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2428s / 999.6089 s
agent0:                 episode reward: -0.1543,                 loss: 0.1724
agent1:                 episode reward: 0.1543,                 loss: nan
Episode: 11781/101000 (11.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7945s / 1001.4033 s
agent0:                 episode reward: -0.5546,                 loss: 0.1673
agent1:                 episode reward: 0.5546,                 loss: nan
Episode: 11801/101000 (11.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9438s / 1003.3472 s
agent0:                 episode reward: -0.2942,                 loss: 0.1675
agent1:                 episode reward: 0.2942,                 loss: nan
Episode: 11821/101000 (11.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1079s / 1005.4551 s
agent0:                 episode reward: -0.1519,                 loss: 0.1662
agent1:                 episode reward: 0.1519,                 loss: nan
Episode: 11841/101000 (11.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0429s / 1007.4980 s
agent0:                 episode reward: -0.5816,                 loss: 0.1655
agent1:                 episode reward: 0.5816,                 loss: nan
Episode: 11861/101000 (11.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9941s / 1009.4921 s
agent0:                 episode reward: -0.3628,                 loss: 0.1661
agent1:                 episode reward: 0.3628,                 loss: nan
Episode: 11881/101000 (11.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9523s / 1011.4444 s
agent0:                 episode reward: -0.4065,                 loss: 0.1674
agent1:                 episode reward: 0.4065,                 loss: nan
Episode: 11901/101000 (11.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8515s / 1013.2959 s
agent0:                 episode reward: -0.4610,                 loss: 0.1661
agent1:                 episode reward: 0.4610,                 loss: nan
Episode: 11921/101000 (11.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3264s / 1015.6224 s
agent0:                 episode reward: -0.6460,                 loss: 0.1656
agent1:                 episode reward: 0.6460,                 loss: nan
Episode: 11941/101000 (11.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1795s / 1017.8018 s
agent0:                 episode reward: 0.1104,                 loss: 0.1665
agent1:                 episode reward: -0.1104,                 loss: nan
Episode: 11961/101000 (11.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6554s / 1020.4572 s
agent0:                 episode reward: -0.5288,                 loss: 0.1662
agent1:                 episode reward: 0.5288,                 loss: nan
Episode: 11981/101000 (11.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2252s / 1022.6824 s
agent0:                 episode reward: -0.2480,                 loss: 0.1651
agent1:                 episode reward: 0.2480,                 loss: nan
Episode: 12001/101000 (11.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5899s / 1024.2723 s
agent0:                 episode reward: -0.8095,                 loss: 0.1685
agent1:                 episode reward: 0.8095,                 loss: nan
Episode: 12021/101000 (11.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9888s / 1026.2611 s
agent0:                 episode reward: 0.3733,                 loss: 0.1673
agent1:                 episode reward: -0.3733,                 loss: nan
Episode: 12041/101000 (11.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0974s / 1028.3584 s
agent0:                 episode reward: -0.3036,                 loss: 0.1680
agent1:                 episode reward: 0.3036,                 loss: nan
Episode: 12061/101000 (11.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6799s / 1030.0383 s
agent0:                 episode reward: 0.1327,                 loss: 0.1676
agent1:                 episode reward: -0.1327,                 loss: nan
Episode: 12081/101000 (11.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4940s / 1031.5324 s
agent0:                 episode reward: 0.1512,                 loss: 0.1698
agent1:                 episode reward: -0.1512,                 loss: 0.1469
Score delta: 1.7388820165069272, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/11638_0.
Episode: 12101/101000 (11.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8841s / 1033.4165 s
agent0:                 episode reward: -0.1937,                 loss: nan
agent1:                 episode reward: 0.1937,                 loss: 0.1423
Episode: 12121/101000 (12.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4090s / 1034.8255 s
agent0:                 episode reward: -0.3542,                 loss: 0.1691
agent1:                 episode reward: 0.3542,                 loss: 0.1400
Score delta: 1.679014438657039, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/11694_1.
Episode: 12141/101000 (12.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3105s / 1037.1360 s
agent0:                 episode reward: -0.5799,                 loss: 0.1669
agent1:                 episode reward: 0.5799,                 loss: nan
Episode: 12161/101000 (12.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0901s / 1039.2260 s
agent0:                 episode reward: -0.1623,                 loss: 0.1699
agent1:                 episode reward: 0.1623,                 loss: nan
Episode: 12181/101000 (12.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9093s / 1041.1353 s
agent0:                 episode reward: -0.1714,                 loss: 0.1708
agent1:                 episode reward: 0.1714,                 loss: nan
Episode: 12201/101000 (12.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3394s / 1043.4747 s
agent0:                 episode reward: -0.2286,                 loss: 0.1706
agent1:                 episode reward: 0.2286,                 loss: nan
Episode: 12221/101000 (12.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9194s / 1045.3942 s
agent0:                 episode reward: -0.2185,                 loss: 0.1698
agent1:                 episode reward: 0.2185,                 loss: nan
Episode: 12241/101000 (12.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1915s / 1047.5857 s
agent0:                 episode reward: -0.1611,                 loss: 0.1690
agent1:                 episode reward: 0.1611,                 loss: nan
Episode: 12261/101000 (12.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6913s / 1049.2770 s
agent0:                 episode reward: -0.3967,                 loss: 0.1696
agent1:                 episode reward: 0.3967,                 loss: nan
Episode: 12281/101000 (12.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5579s / 1050.8349 s
agent0:                 episode reward: -0.1117,                 loss: 0.1687
agent1:                 episode reward: 0.1117,                 loss: nan
Episode: 12301/101000 (12.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9800s / 1052.8149 s
agent0:                 episode reward: 0.0044,                 loss: 0.1683
agent1:                 episode reward: -0.0044,                 loss: nan
Episode: 12321/101000 (12.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9969s / 1054.8118 s
agent0:                 episode reward: -0.4568,                 loss: 0.1697
agent1:                 episode reward: 0.4568,                 loss: nan
Episode: 12341/101000 (12.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0490s / 1056.8608 s
agent0:                 episode reward: -0.4365,                 loss: 0.1705
agent1:                 episode reward: 0.4365,                 loss: nan
Episode: 12361/101000 (12.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8211s / 1058.6819 s
agent0:                 episode reward: 0.3671,                 loss: 0.1705
agent1:                 episode reward: -0.3671,                 loss: nan
Episode: 12381/101000 (12.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7418s / 1060.4237 s
agent0:                 episode reward: -0.0808,                 loss: 0.1733
agent1:                 episode reward: 0.0808,                 loss: 0.1568
Score delta: 2.025440803617096, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/11936_0.
Episode: 12401/101000 (12.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8290s / 1062.2527 s
agent0:                 episode reward: -0.7833,                 loss: 0.2456
agent1:                 episode reward: 0.7833,                 loss: 0.1591
Score delta: 1.6971422895570285, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/11966_1.
Episode: 12421/101000 (12.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7214s / 1063.9741 s
agent0:                 episode reward: -0.6267,                 loss: 0.2410
agent1:                 episode reward: 0.6267,                 loss: nan
Episode: 12441/101000 (12.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0859s / 1066.0600 s
agent0:                 episode reward: -0.2532,                 loss: 0.2400
agent1:                 episode reward: 0.2532,                 loss: nan
Episode: 12461/101000 (12.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9765s / 1068.0365 s
agent0:                 episode reward: -0.1327,                 loss: 0.2381
agent1:                 episode reward: 0.1327,                 loss: nan
Episode: 12481/101000 (12.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3245s / 1070.3610 s
agent0:                 episode reward: -0.0326,                 loss: 0.2382
agent1:                 episode reward: 0.0326,                 loss: nan
Episode: 12501/101000 (12.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3672s / 1072.7282 s
agent0:                 episode reward: -0.4100,                 loss: 0.2384
agent1:                 episode reward: 0.4100,                 loss: nan
Episode: 12521/101000 (12.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9473s / 1074.6755 s
agent0:                 episode reward: -0.6459,                 loss: 0.2212
agent1:                 episode reward: 0.6459,                 loss: nan
Episode: 12541/101000 (12.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2118s / 1076.8873 s
agent0:                 episode reward: -0.5442,                 loss: 0.1792
agent1:                 episode reward: 0.5442,                 loss: nan
Episode: 12561/101000 (12.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6082s / 1078.4955 s
agent0:                 episode reward: -0.3855,                 loss: 0.1807
agent1:                 episode reward: 0.3855,                 loss: nan
Episode: 12581/101000 (12.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2739s / 1080.7694 s
agent0:                 episode reward: -0.5336,                 loss: 0.1818
agent1:                 episode reward: 0.5336,                 loss: nan
Episode: 12601/101000 (12.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1475s / 1082.9169 s
agent0:                 episode reward: -0.1680,                 loss: 0.1795
agent1:                 episode reward: 0.1680,                 loss: nan
Episode: 12621/101000 (12.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2844s / 1085.2013 s
agent0:                 episode reward: -0.5495,                 loss: 0.1798
agent1:                 episode reward: 0.5495,                 loss: nan
Episode: 12641/101000 (12.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9091s / 1087.1104 s
agent0:                 episode reward: -0.6396,                 loss: 0.1811
agent1:                 episode reward: 0.6396,                 loss: nan
Episode: 12661/101000 (12.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1505s / 1089.2609 s
agent0:                 episode reward: -0.1471,                 loss: 0.1793
agent1:                 episode reward: 0.1471,                 loss: nan
Episode: 12681/101000 (12.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0305s / 1091.2913 s
agent0:                 episode reward: -0.4635,                 loss: 0.1779
agent1:                 episode reward: 0.4635,                 loss: nan
Episode: 12701/101000 (12.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2146s / 1093.5059 s
agent0:                 episode reward: -0.4949,                 loss: 0.1802
agent1:                 episode reward: 0.4949,                 loss: nan
Episode: 12721/101000 (12.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2700s / 1095.7759 s
agent0:                 episode reward: -0.2397,                 loss: 0.1791
agent1:                 episode reward: 0.2397,                 loss: nan
Episode: 12741/101000 (12.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9651s / 1097.7410 s
agent0:                 episode reward: -0.4275,                 loss: 0.1771
agent1:                 episode reward: 0.4275,                 loss: nan
Episode: 12761/101000 (12.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2556s / 1099.9966 s
agent0:                 episode reward: -0.7633,                 loss: 0.1783
agent1:                 episode reward: 0.7633,                 loss: nan
Episode: 12781/101000 (12.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3052s / 1102.3019 s
agent0:                 episode reward: -0.1110,                 loss: 0.1784
agent1:                 episode reward: 0.1110,                 loss: nan
Episode: 12801/101000 (12.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1483s / 1104.4501 s
agent0:                 episode reward: -0.1901,                 loss: 0.1786
agent1:                 episode reward: 0.1901,                 loss: nan
Episode: 12821/101000 (12.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8600s / 1106.3101 s
agent0:                 episode reward: -0.0237,                 loss: 0.1793
agent1:                 episode reward: 0.0237,                 loss: nan
Episode: 12841/101000 (12.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9150s / 1108.2252 s
agent0:                 episode reward: -0.5730,                 loss: 0.1798
agent1:                 episode reward: 0.5730,                 loss: nan
Episode: 12861/101000 (12.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3718s / 1110.5970 s
agent0:                 episode reward: -0.1892,                 loss: 0.1730
agent1:                 episode reward: 0.1892,                 loss: nan
Episode: 12881/101000 (12.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1892s / 1112.7861 s
agent0:                 episode reward: -0.4106,                 loss: 0.1737
agent1:                 episode reward: 0.4106,                 loss: nan
Episode: 12901/101000 (12.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7038s / 1114.4899 s
agent0:                 episode reward: -0.2541,                 loss: 0.1711
agent1:                 episode reward: 0.2541,                 loss: nan
Episode: 12921/101000 (12.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9264s / 1116.4163 s
agent0:                 episode reward: -0.8482,                 loss: 0.1731
agent1:                 episode reward: 0.8482,                 loss: nan
Episode: 12941/101000 (12.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9504s / 1118.3667 s
agent0:                 episode reward: -0.2973,                 loss: 0.1713
agent1:                 episode reward: 0.2973,                 loss: nan
Episode: 12961/101000 (12.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2133s / 1120.5800 s
agent0:                 episode reward: -0.2518,                 loss: 0.1714
agent1:                 episode reward: 0.2518,                 loss: nan
Episode: 12981/101000 (12.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1200s / 1122.7001 s
agent0:                 episode reward: -0.4296,                 loss: 0.1716
agent1:                 episode reward: 0.4296,                 loss: nan
Episode: 13001/101000 (12.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1140s / 1124.8141 s
agent0:                 episode reward: -0.3682,                 loss: 0.1722
agent1:                 episode reward: 0.3682,                 loss: nan
Episode: 13021/101000 (12.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5303s / 1126.3444 s
agent0:                 episode reward: -0.2447,                 loss: 0.1701
agent1:                 episode reward: 0.2447,                 loss: nan
Episode: 13041/101000 (12.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8230s / 1128.1675 s
agent0:                 episode reward: 0.0247,                 loss: 0.1711
agent1:                 episode reward: -0.0247,                 loss: nan
Episode: 13061/101000 (12.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6166s / 1129.7841 s
agent0:                 episode reward: -0.3979,                 loss: 0.1701
agent1:                 episode reward: 0.3979,                 loss: nan
Episode: 13081/101000 (12.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7385s / 1131.5226 s
agent0:                 episode reward: -0.4823,                 loss: 0.1699
agent1:                 episode reward: 0.4823,                 loss: nan
Episode: 13101/101000 (12.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2149s / 1133.7375 s
agent0:                 episode reward: -0.1444,                 loss: 0.1711
agent1:                 episode reward: 0.1444,                 loss: nan
Episode: 13121/101000 (12.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7352s / 1135.4727 s
agent0:                 episode reward: -0.5483,                 loss: 0.1717
agent1:                 episode reward: 0.5483,                 loss: nan
Episode: 13141/101000 (13.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9867s / 1137.4594 s
agent0:                 episode reward: -0.0740,                 loss: 0.1719
agent1:                 episode reward: 0.0740,                 loss: nan
Episode: 13161/101000 (13.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2743s / 1139.7338 s
agent0:                 episode reward: -0.0316,                 loss: 0.1703
agent1:                 episode reward: 0.0316,                 loss: nan
Episode: 13181/101000 (13.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7757s / 1141.5095 s
agent0:                 episode reward: -0.2999,                 loss: 0.1715
agent1:                 episode reward: 0.2999,                 loss: nan
Episode: 13201/101000 (13.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3767s / 1143.8862 s
agent0:                 episode reward: -0.1523,                 loss: 0.1671
agent1:                 episode reward: 0.1523,                 loss: nan
Episode: 13221/101000 (13.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0820s / 1145.9682 s
agent0:                 episode reward: -0.6173,                 loss: 0.1669
agent1:                 episode reward: 0.6173,                 loss: nan
Episode: 13241/101000 (13.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9942s / 1147.9624 s
agent0:                 episode reward: -0.6413,                 loss: 0.1649
agent1:                 episode reward: 0.6413,                 loss: nan
Episode: 13261/101000 (13.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7698s / 1149.7322 s
agent0:                 episode reward: -0.3840,                 loss: 0.1651
agent1:                 episode reward: 0.3840,                 loss: nan
Episode: 13281/101000 (13.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2181s / 1151.9502 s
agent0:                 episode reward: -0.5307,                 loss: 0.1660
agent1:                 episode reward: 0.5307,                 loss: nan
Episode: 13301/101000 (13.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9254s / 1153.8757 s
agent0:                 episode reward: -0.1758,                 loss: 0.1641
agent1:                 episode reward: 0.1758,                 loss: nan
Episode: 13321/101000 (13.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4955s / 1155.3711 s
agent0:                 episode reward: -0.4840,                 loss: 0.1649
agent1:                 episode reward: 0.4840,                 loss: nan
Episode: 13341/101000 (13.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1481s / 1157.5193 s
agent0:                 episode reward: -0.3066,                 loss: 0.1647
agent1:                 episode reward: 0.3066,                 loss: nan
Episode: 13361/101000 (13.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7134s / 1160.2327 s
agent0:                 episode reward: -0.6958,                 loss: 0.1643
agent1:                 episode reward: 0.6958,                 loss: nan
Episode: 13381/101000 (13.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2145s / 1162.4472 s
agent0:                 episode reward: 0.3198,                 loss: 0.1665
agent1:                 episode reward: -0.3198,                 loss: nan
Episode: 13401/101000 (13.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9817s / 1164.4288 s
agent0:                 episode reward: -0.6434,                 loss: 0.1683
agent1:                 episode reward: 0.6434,                 loss: 0.1596
Score delta: 1.5717115747251804, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/12961_0.
Episode: 13421/101000 (13.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9588s / 1166.3876 s
agent0:                 episode reward: -0.2383,                 loss: 0.1737
agent1:                 episode reward: 0.2383,                 loss: 0.1576
Score delta: 1.7305533133785098, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/12987_1.
Episode: 13441/101000 (13.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6150s / 1168.0026 s
agent0:                 episode reward: -0.2201,                 loss: 0.1735
agent1:                 episode reward: 0.2201,                 loss: nan
Episode: 13461/101000 (13.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9813s / 1169.9839 s
agent0:                 episode reward: -0.0236,                 loss: 0.1732
agent1:                 episode reward: 0.0236,                 loss: nan
Episode: 13481/101000 (13.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4335s / 1172.4174 s
agent0:                 episode reward: -0.2374,                 loss: 0.1731
agent1:                 episode reward: 0.2374,                 loss: nan
Episode: 13501/101000 (13.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5483s / 1174.9656 s
agent0:                 episode reward: -0.1974,                 loss: 0.1726
agent1:                 episode reward: 0.1974,                 loss: nan
Episode: 13521/101000 (13.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0341s / 1176.9998 s
agent0:                 episode reward: -0.5656,                 loss: 0.1728
agent1:                 episode reward: 0.5656,                 loss: nan
Episode: 13541/101000 (13.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8062s / 1178.8060 s
agent0:                 episode reward: -0.7636,                 loss: 0.1719
agent1:                 episode reward: 0.7636,                 loss: nan
Episode: 13561/101000 (13.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2460s / 1181.0520 s
agent0:                 episode reward: -0.1089,                 loss: 0.1665
agent1:                 episode reward: 0.1089,                 loss: nan
Episode: 13581/101000 (13.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2044s / 1183.2564 s
agent0:                 episode reward: 0.2153,                 loss: 0.1651
agent1:                 episode reward: -0.2153,                 loss: nan
Episode: 13601/101000 (13.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9273s / 1185.1837 s
agent0:                 episode reward: -0.4071,                 loss: 0.1644
agent1:                 episode reward: 0.4071,                 loss: nan
Episode: 13621/101000 (13.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9788s / 1187.1625 s
agent0:                 episode reward: -0.4970,                 loss: 0.1660
agent1:                 episode reward: 0.4970,                 loss: nan
Episode: 13641/101000 (13.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6047s / 1188.7672 s
agent0:                 episode reward: -0.5518,                 loss: 0.1659
agent1:                 episode reward: 0.5518,                 loss: nan
Episode: 13661/101000 (13.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4236s / 1191.1908 s
agent0:                 episode reward: -0.5979,                 loss: 0.1664
agent1:                 episode reward: 0.5979,                 loss: nan
Episode: 13681/101000 (13.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3766s / 1193.5674 s
agent0:                 episode reward: -0.8178,                 loss: 0.1662
agent1:                 episode reward: 0.8178,                 loss: nan
Episode: 13701/101000 (13.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6682s / 1195.2356 s
agent0:                 episode reward: -0.7246,                 loss: 0.1658
agent1:                 episode reward: 0.7246,                 loss: nan
Episode: 13721/101000 (13.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9547s / 1198.1902 s
agent0:                 episode reward: -0.4914,                 loss: 0.1659
agent1:                 episode reward: 0.4914,                 loss: nan
Episode: 13741/101000 (13.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0598s / 1200.2501 s
agent0:                 episode reward: -0.4363,                 loss: 0.1651
agent1:                 episode reward: 0.4363,                 loss: nan
Episode: 13761/101000 (13.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2673s / 1202.5173 s
agent0:                 episode reward: -0.4773,                 loss: 0.1650
agent1:                 episode reward: 0.4773,                 loss: nan
Episode: 13781/101000 (13.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3374s / 1204.8547 s
agent0:                 episode reward: -0.5204,                 loss: 0.1647
agent1:                 episode reward: 0.5204,                 loss: nan
Episode: 13801/101000 (13.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6357s / 1206.4905 s
agent0:                 episode reward: -0.6183,                 loss: 0.1659
agent1:                 episode reward: 0.6183,                 loss: nan
Episode: 13821/101000 (13.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9743s / 1208.4648 s
agent0:                 episode reward: -0.2210,                 loss: 0.1655
agent1:                 episode reward: 0.2210,                 loss: nan
Episode: 13841/101000 (13.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3827s / 1210.8474 s
agent0:                 episode reward: -0.4053,                 loss: 0.1661
agent1:                 episode reward: 0.4053,                 loss: nan
Episode: 13861/101000 (13.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9325s / 1212.7799 s
agent0:                 episode reward: -0.3476,                 loss: 0.1653
agent1:                 episode reward: 0.3476,                 loss: nan
Episode: 13881/101000 (13.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8087s / 1214.5886 s
agent0:                 episode reward: -0.6903,                 loss: 0.1664
agent1:                 episode reward: 0.6903,                 loss: nan
Episode: 13901/101000 (13.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3423s / 1216.9309 s
agent0:                 episode reward: -0.4918,                 loss: 0.1698
agent1:                 episode reward: 0.4918,                 loss: nan
Episode: 13921/101000 (13.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1594s / 1219.0903 s
agent0:                 episode reward: -0.7434,                 loss: 0.1724
agent1:                 episode reward: 0.7434,                 loss: nan
Episode: 13941/101000 (13.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9937s / 1221.0841 s
agent0:                 episode reward: -0.3961,                 loss: 0.1698
agent1:                 episode reward: 0.3961,                 loss: nan
Episode: 13961/101000 (13.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1649s / 1223.2489 s
agent0:                 episode reward: 0.0465,                 loss: 0.1705
agent1:                 episode reward: -0.0465,                 loss: nan
Episode: 13981/101000 (13.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0174s / 1225.2663 s
agent0:                 episode reward: -0.1196,                 loss: 0.1699
agent1:                 episode reward: 0.1196,                 loss: nan
Episode: 14001/101000 (13.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1623s / 1227.4286 s
agent0:                 episode reward: -0.3605,                 loss: 0.1692
agent1:                 episode reward: 0.3605,                 loss: nan
Episode: 14021/101000 (13.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0697s / 1229.4983 s
agent0:                 episode reward: -0.2080,                 loss: 0.1715
agent1:                 episode reward: 0.2080,                 loss: nan
Episode: 14041/101000 (13.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0751s / 1231.5734 s
agent0:                 episode reward: -0.3704,                 loss: 0.1710
agent1:                 episode reward: 0.3704,                 loss: nan
Episode: 14061/101000 (13.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2149s / 1233.7883 s
agent0:                 episode reward: -0.0033,                 loss: 0.1708
agent1:                 episode reward: 0.0033,                 loss: nan
Episode: 14081/101000 (13.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1451s / 1235.9334 s
agent0:                 episode reward: -0.3594,                 loss: 0.1706
agent1:                 episode reward: 0.3594,                 loss: nan
Episode: 14101/101000 (13.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9466s / 1237.8800 s
agent0:                 episode reward: -0.2197,                 loss: 0.1703
agent1:                 episode reward: 0.2197,                 loss: nan
Episode: 14121/101000 (13.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2952s / 1240.1751 s
agent0:                 episode reward: -0.5256,                 loss: 0.1698
agent1:                 episode reward: 0.5256,                 loss: nan
Episode: 14141/101000 (14.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9733s / 1242.1484 s
agent0:                 episode reward: -0.2604,                 loss: 0.1679
agent1:                 episode reward: 0.2604,                 loss: nan
Episode: 14161/101000 (14.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0583s / 1244.2067 s
agent0:                 episode reward: -0.3003,                 loss: 0.1717
agent1:                 episode reward: 0.3003,                 loss: nan
Episode: 14181/101000 (14.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7172s / 1245.9239 s
agent0:                 episode reward: -0.4040,                 loss: 0.1709
agent1:                 episode reward: 0.4040,                 loss: nan
Episode: 14201/101000 (14.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7143s / 1247.6382 s
agent0:                 episode reward: -0.1282,                 loss: 0.1707
agent1:                 episode reward: 0.1282,                 loss: nan
Episode: 14221/101000 (14.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0924s / 1249.7306 s
agent0:                 episode reward: -0.5438,                 loss: 0.1658
agent1:                 episode reward: 0.5438,                 loss: nan
Episode: 14241/101000 (14.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1544s / 1251.8850 s
agent0:                 episode reward: -0.3590,                 loss: 0.1641
agent1:                 episode reward: 0.3590,                 loss: nan
Episode: 14261/101000 (14.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0953s / 1253.9804 s
agent0:                 episode reward: -0.4369,                 loss: 0.1635
agent1:                 episode reward: 0.4369,                 loss: nan
Episode: 14281/101000 (14.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3403s / 1256.3207 s
agent0:                 episode reward: -0.4230,                 loss: 0.1641
agent1:                 episode reward: 0.4230,                 loss: nan
Episode: 14301/101000 (14.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0871s / 1258.4078 s
agent0:                 episode reward: -0.4092,                 loss: 0.1642
agent1:                 episode reward: 0.4092,                 loss: nan
Episode: 14321/101000 (14.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5129s / 1260.9208 s
agent0:                 episode reward: -0.1974,                 loss: 0.1649
agent1:                 episode reward: 0.1974,                 loss: nan
Episode: 14341/101000 (14.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9419s / 1262.8626 s
agent0:                 episode reward: -0.1405,                 loss: 0.1645
agent1:                 episode reward: 0.1405,                 loss: nan
Episode: 14361/101000 (14.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9948s / 1264.8575 s
agent0:                 episode reward: -0.3474,                 loss: 0.1638
agent1:                 episode reward: 0.3474,                 loss: nan
Episode: 14381/101000 (14.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0576s / 1266.9151 s
agent0:                 episode reward: -0.0599,                 loss: 0.1645
agent1:                 episode reward: 0.0599,                 loss: nan
Episode: 14401/101000 (14.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4087s / 1269.3238 s
agent0:                 episode reward: -0.3726,                 loss: 0.1644
agent1:                 episode reward: 0.3726,                 loss: nan
Episode: 14421/101000 (14.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9697s / 1271.2935 s
agent0:                 episode reward: -0.1698,                 loss: 0.1631
agent1:                 episode reward: 0.1698,                 loss: nan
Episode: 14441/101000 (14.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4302s / 1273.7237 s
agent0:                 episode reward: -0.6393,                 loss: 0.1640
agent1:                 episode reward: 0.6393,                 loss: nan
Episode: 14461/101000 (14.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0439s / 1275.7676 s
agent0:                 episode reward: -0.4476,                 loss: 0.1628
agent1:                 episode reward: 0.4476,                 loss: nan
Episode: 14481/101000 (14.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9446s / 1277.7121 s
agent0:                 episode reward: -0.2732,                 loss: 0.1630
agent1:                 episode reward: 0.2732,                 loss: nan
Episode: 14501/101000 (14.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0684s / 1279.7805 s
agent0:                 episode reward: -0.3677,                 loss: 0.1623
agent1:                 episode reward: 0.3677,                 loss: nan
Episode: 14521/101000 (14.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3044s / 1282.0849 s
agent0:                 episode reward: -0.2346,                 loss: 0.1632
agent1:                 episode reward: 0.2346,                 loss: nan
Episode: 14541/101000 (14.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6460s / 1284.7309 s
agent0:                 episode reward: -0.1811,                 loss: 0.1647
agent1:                 episode reward: 0.1811,                 loss: nan
Episode: 14561/101000 (14.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2925s / 1287.0234 s
agent0:                 episode reward: -0.1896,                 loss: 0.1618
agent1:                 episode reward: 0.1896,                 loss: nan
Episode: 14581/101000 (14.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3570s / 1289.3804 s
agent0:                 episode reward: -0.2455,                 loss: 0.1616
agent1:                 episode reward: 0.2455,                 loss: nan
Episode: 14601/101000 (14.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4106s / 1291.7910 s
agent0:                 episode reward: -0.3189,                 loss: 0.1616
agent1:                 episode reward: 0.3189,                 loss: nan
Episode: 14621/101000 (14.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0088s / 1293.7998 s
agent0:                 episode reward: -0.6419,                 loss: 0.1624
agent1:                 episode reward: 0.6419,                 loss: nan
Episode: 14641/101000 (14.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5998s / 1295.3996 s
agent0:                 episode reward: -0.3998,                 loss: 0.1605
agent1:                 episode reward: 0.3998,                 loss: nan
Episode: 14661/101000 (14.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6620s / 1298.0616 s
agent0:                 episode reward: -0.5325,                 loss: 0.1598
agent1:                 episode reward: 0.5325,                 loss: nan
Episode: 14681/101000 (14.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7004s / 1300.7620 s
agent0:                 episode reward: -0.2520,                 loss: 0.1598
agent1:                 episode reward: 0.2520,                 loss: nan
Episode: 14701/101000 (14.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9875s / 1302.7495 s
agent0:                 episode reward: -0.6901,                 loss: 0.1619
agent1:                 episode reward: 0.6901,                 loss: nan
Episode: 14721/101000 (14.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2331s / 1304.9826 s
agent0:                 episode reward: -0.7890,                 loss: 0.1603
agent1:                 episode reward: 0.7890,                 loss: nan
Episode: 14741/101000 (14.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5420s / 1307.5246 s
agent0:                 episode reward: -0.5273,                 loss: 0.1595
agent1:                 episode reward: 0.5273,                 loss: nan
Episode: 14761/101000 (14.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8750s / 1309.3995 s
agent0:                 episode reward: 0.0183,                 loss: 0.1598
agent1:                 episode reward: -0.0183,                 loss: nan
Episode: 14781/101000 (14.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0723s / 1311.4718 s
agent0:                 episode reward: -0.0219,                 loss: 0.1600
agent1:                 episode reward: 0.0219,                 loss: nan
Episode: 14801/101000 (14.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7445s / 1313.2163 s
agent0:                 episode reward: -0.6001,                 loss: 0.1592
agent1:                 episode reward: 0.6001,                 loss: nan
Episode: 14821/101000 (14.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0937s / 1315.3101 s
agent0:                 episode reward: -0.5892,                 loss: 0.1594
agent1:                 episode reward: 0.5892,                 loss: nan
Episode: 14841/101000 (14.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8714s / 1317.1815 s
agent0:                 episode reward: -0.2296,                 loss: 0.1616
agent1:                 episode reward: 0.2296,                 loss: nan
Episode: 14861/101000 (14.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7311s / 1318.9126 s
agent0:                 episode reward: -0.0760,                 loss: 0.1610
agent1:                 episode reward: 0.0760,                 loss: nan
Episode: 14881/101000 (14.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0406s / 1320.9532 s
agent0:                 episode reward: -0.2024,                 loss: 0.1612
agent1:                 episode reward: 0.2024,                 loss: nan
Episode: 14901/101000 (14.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1333s / 1323.0865 s
agent0:                 episode reward: -0.4507,                 loss: 0.1590
agent1:                 episode reward: 0.4507,                 loss: nan
Episode: 14921/101000 (14.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6984s / 1324.7849 s
agent0:                 episode reward: 0.1677,                 loss: 0.1574
agent1:                 episode reward: -0.1677,                 loss: 0.1435
Score delta: 1.528198261192392, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/14489_0.
Episode: 14941/101000 (14.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9633s / 1326.7481 s
agent0:                 episode reward: 0.1981,                 loss: nan
agent1:                 episode reward: -0.1981,                 loss: 0.1435
Episode: 14961/101000 (14.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0949s / 1328.8430 s
agent0:                 episode reward: -0.6947,                 loss: 0.2491
agent1:                 episode reward: 0.6947,                 loss: 0.1417
Score delta: 1.5756049366005103, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/14519_1.
Episode: 14981/101000 (14.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6688s / 1330.5118 s
agent0:                 episode reward: 0.1883,                 loss: 0.2452
agent1:                 episode reward: -0.1883,                 loss: nan
Episode: 15001/101000 (14.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8978s / 1332.4096 s
agent0:                 episode reward: -0.4808,                 loss: 0.2449
agent1:                 episode reward: 0.4808,                 loss: nan
Episode: 15021/101000 (14.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3856s / 1334.7953 s
agent0:                 episode reward: -0.4400,                 loss: 0.2444
agent1:                 episode reward: 0.4400,                 loss: nan
Episode: 15041/101000 (14.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8461s / 1336.6414 s
agent0:                 episode reward: -0.3393,                 loss: 0.2448
agent1:                 episode reward: 0.3393,                 loss: nan
Episode: 15061/101000 (14.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1380s / 1338.7794 s
agent0:                 episode reward: -0.3985,                 loss: 0.2459
agent1:                 episode reward: 0.3985,                 loss: nan
Episode: 15081/101000 (14.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0766s / 1340.8560 s
agent0:                 episode reward: -0.5227,                 loss: 0.2464
agent1:                 episode reward: 0.5227,                 loss: nan
Episode: 15101/101000 (14.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3757s / 1343.2317 s
agent0:                 episode reward: -0.5237,                 loss: 0.2451
agent1:                 episode reward: 0.5237,                 loss: nan
Episode: 15121/101000 (14.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1448s / 1345.3765 s
agent0:                 episode reward: -0.2377,                 loss: 0.2444
agent1:                 episode reward: 0.2377,                 loss: nan
Episode: 15141/101000 (14.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1070s / 1347.4835 s
agent0:                 episode reward: -0.3787,                 loss: 0.2457
agent1:                 episode reward: 0.3787,                 loss: nan
Episode: 15161/101000 (15.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4777s / 1349.9612 s
agent0:                 episode reward: -0.6904,                 loss: 0.2442
agent1:                 episode reward: 0.6904,                 loss: nan
Episode: 15181/101000 (15.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0789s / 1352.0401 s
agent0:                 episode reward: -0.3608,                 loss: 0.2460
agent1:                 episode reward: 0.3608,                 loss: nan
Episode: 15201/101000 (15.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2690s / 1354.3091 s
agent0:                 episode reward: -0.0414,                 loss: 0.2443
agent1:                 episode reward: 0.0414,                 loss: nan
Episode: 15221/101000 (15.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8266s / 1356.1357 s
agent0:                 episode reward: -0.5584,                 loss: 0.2441
agent1:                 episode reward: 0.5584,                 loss: nan
Episode: 15241/101000 (15.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2730s / 1358.4087 s
agent0:                 episode reward: -0.1693,                 loss: 0.2335
agent1:                 episode reward: 0.1693,                 loss: nan
Episode: 15261/101000 (15.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4726s / 1360.8813 s
agent0:                 episode reward: -0.7536,                 loss: 0.1724
agent1:                 episode reward: 0.7536,                 loss: nan
Episode: 15281/101000 (15.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2122s / 1363.0936 s
agent0:                 episode reward: -0.5105,                 loss: 0.1718
agent1:                 episode reward: 0.5105,                 loss: nan
Episode: 15301/101000 (15.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8634s / 1364.9570 s
agent0:                 episode reward: -0.4009,                 loss: 0.1715
agent1:                 episode reward: 0.4009,                 loss: nan
Episode: 15321/101000 (15.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0211s / 1366.9780 s
agent0:                 episode reward: -0.6364,                 loss: 0.1721
agent1:                 episode reward: 0.6364,                 loss: nan
Episode: 15341/101000 (15.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9077s / 1368.8857 s
agent0:                 episode reward: -0.3629,                 loss: 0.1714
agent1:                 episode reward: 0.3629,                 loss: nan
Episode: 15361/101000 (15.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1816s / 1371.0674 s
agent0:                 episode reward: 0.2114,                 loss: 0.1706
agent1:                 episode reward: -0.2114,                 loss: 0.1634
Score delta: 1.533421338818841, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/14933_0.
Episode: 15381/101000 (15.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0680s / 1373.1353 s
agent0:                 episode reward: -0.4145,                 loss: nan
agent1:                 episode reward: 0.4145,                 loss: 0.1489
Episode: 15401/101000 (15.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8921s / 1375.0274 s
agent0:                 episode reward: -0.5631,                 loss: 0.1653
agent1:                 episode reward: 0.5631,                 loss: 0.1435
Score delta: 1.5080811604906244, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/14973_1.
Episode: 15421/101000 (15.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7524s / 1376.7798 s
agent0:                 episode reward: -0.3904,                 loss: 0.1702
agent1:                 episode reward: 0.3904,                 loss: nan
Episode: 15441/101000 (15.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1344s / 1378.9142 s
agent0:                 episode reward: -0.2938,                 loss: 0.1698
agent1:                 episode reward: 0.2938,                 loss: nan
Episode: 15461/101000 (15.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2123s / 1381.1266 s
agent0:                 episode reward: -0.2777,                 loss: 0.1694
agent1:                 episode reward: 0.2777,                 loss: nan
Episode: 15481/101000 (15.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0699s / 1383.1964 s
agent0:                 episode reward: -0.5061,                 loss: 0.1699
agent1:                 episode reward: 0.5061,                 loss: nan
Episode: 15501/101000 (15.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0348s / 1385.2313 s
agent0:                 episode reward: -0.5124,                 loss: 0.1700
agent1:                 episode reward: 0.5124,                 loss: nan
Episode: 15521/101000 (15.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7835s / 1387.0148 s
agent0:                 episode reward: -0.4687,                 loss: 0.1695
agent1:                 episode reward: 0.4687,                 loss: nan
Episode: 15541/101000 (15.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3335s / 1389.3483 s
agent0:                 episode reward: -0.5268,                 loss: 0.1697
agent1:                 episode reward: 0.5268,                 loss: nan
Episode: 15561/101000 (15.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6042s / 1390.9525 s
agent0:                 episode reward: -0.7717,                 loss: 0.1689
agent1:                 episode reward: 0.7717,                 loss: nan
Episode: 15581/101000 (15.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9226s / 1392.8751 s
agent0:                 episode reward: -0.4838,                 loss: 0.1696
agent1:                 episode reward: 0.4838,                 loss: nan
Episode: 15601/101000 (15.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2549s / 1395.1300 s
agent0:                 episode reward: -0.2545,                 loss: 0.1698
agent1:                 episode reward: 0.2545,                 loss: nan
Episode: 15621/101000 (15.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3084s / 1397.4384 s
agent0:                 episode reward: -0.2353,                 loss: 0.1652
agent1:                 episode reward: 0.2353,                 loss: nan
Episode: 15641/101000 (15.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2016s / 1399.6400 s
agent0:                 episode reward: -0.5643,                 loss: 0.1629
agent1:                 episode reward: 0.5643,                 loss: nan
Episode: 15661/101000 (15.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8274s / 1401.4674 s
agent0:                 episode reward: -0.6722,                 loss: 0.1623
agent1:                 episode reward: 0.6722,                 loss: nan
Episode: 15681/101000 (15.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2483s / 1403.7156 s
agent0:                 episode reward: -0.4164,                 loss: 0.1625
agent1:                 episode reward: 0.4164,                 loss: nan
Episode: 15701/101000 (15.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1017s / 1405.8173 s
agent0:                 episode reward: 0.0629,                 loss: 0.1624
agent1:                 episode reward: -0.0629,                 loss: nan
Episode: 15721/101000 (15.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3119s / 1408.1291 s
agent0:                 episode reward: -0.4637,                 loss: 0.1628
agent1:                 episode reward: 0.4637,                 loss: nan
Episode: 15741/101000 (15.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0662s / 1410.1953 s
agent0:                 episode reward: 0.1579,                 loss: 0.1599
agent1:                 episode reward: -0.1579,                 loss: nan
Episode: 15761/101000 (15.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9509s / 1412.1463 s
agent0:                 episode reward: -0.3230,                 loss: 0.1598
agent1:                 episode reward: 0.3230,                 loss: 0.1530
Score delta: 1.5201663201460334, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/15318_0.
Episode: 15781/101000 (15.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8351s / 1413.9814 s
agent0:                 episode reward: -0.3077,                 loss: nan
agent1:                 episode reward: 0.3077,                 loss: 0.1549
Episode: 15801/101000 (15.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5182s / 1415.4996 s
agent0:                 episode reward: -0.4092,                 loss: nan
agent1:                 episode reward: 0.4092,                 loss: 0.1532
Episode: 15821/101000 (15.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9097s / 1417.4093 s
agent0:                 episode reward: -0.4978,                 loss: 0.2557
agent1:                 episode reward: 0.4978,                 loss: 0.1516
Score delta: 1.5932915356898287, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/15378_1.
Episode: 15841/101000 (15.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1931s / 1419.6023 s
agent0:                 episode reward: -0.3791,                 loss: 0.2506
agent1:                 episode reward: 0.3791,                 loss: nan
Episode: 15861/101000 (15.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2188s / 1421.8211 s
agent0:                 episode reward: -0.2974,                 loss: 0.2493
agent1:                 episode reward: 0.2974,                 loss: nan
Episode: 15881/101000 (15.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2026s / 1424.0237 s
agent0:                 episode reward: -0.3560,                 loss: 0.2490
agent1:                 episode reward: 0.3560,                 loss: nan
Episode: 15901/101000 (15.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9989s / 1426.0226 s
agent0:                 episode reward: -0.7403,                 loss: 0.2515
agent1:                 episode reward: 0.7403,                 loss: nan
Episode: 15921/101000 (15.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6112s / 1428.6338 s
agent0:                 episode reward: -0.6199,                 loss: 0.2494
agent1:                 episode reward: 0.6199,                 loss: nan
Episode: 15941/101000 (15.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3390s / 1430.9728 s
agent0:                 episode reward: -0.2006,                 loss: 0.2479
agent1:                 episode reward: 0.2006,                 loss: nan
Episode: 15961/101000 (15.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3342s / 1433.3070 s
agent0:                 episode reward: -0.3353,                 loss: 0.2497
agent1:                 episode reward: 0.3353,                 loss: nan
Episode: 15981/101000 (15.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8718s / 1435.1789 s
agent0:                 episode reward: -0.2147,                 loss: 0.2490
agent1:                 episode reward: 0.2147,                 loss: nan
Episode: 16001/101000 (15.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2594s / 1437.4382 s
agent0:                 episode reward: -0.5612,                 loss: 0.2503
agent1:                 episode reward: 0.5612,                 loss: nan
Episode: 16021/101000 (15.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0467s / 1439.4849 s
agent0:                 episode reward: -0.3234,                 loss: 0.1871
agent1:                 episode reward: 0.3234,                 loss: nan
Episode: 16041/101000 (15.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9464s / 1441.4314 s
agent0:                 episode reward: -0.3447,                 loss: 0.1712
agent1:                 episode reward: 0.3447,                 loss: nan
Episode: 16061/101000 (15.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0610s / 1443.4924 s
agent0:                 episode reward: -0.4457,                 loss: 0.1699
agent1:                 episode reward: 0.4457,                 loss: nan
Episode: 16081/101000 (15.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1662s / 1445.6585 s
agent0:                 episode reward: -0.3967,                 loss: 0.1693
agent1:                 episode reward: 0.3967,                 loss: nan
Episode: 16101/101000 (15.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1222s / 1447.7807 s
agent0:                 episode reward: -0.2212,                 loss: 0.1692
agent1:                 episode reward: 0.2212,                 loss: nan
Episode: 16121/101000 (15.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2023s / 1449.9831 s
agent0:                 episode reward: -0.3817,                 loss: 0.1706
agent1:                 episode reward: 0.3817,                 loss: nan
Episode: 16141/101000 (15.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2191s / 1452.2022 s
agent0:                 episode reward: -0.4240,                 loss: 0.1707
agent1:                 episode reward: 0.4240,                 loss: nan
Episode: 16161/101000 (16.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7320s / 1454.9342 s
agent0:                 episode reward: -0.5041,                 loss: 0.1690
agent1:                 episode reward: 0.5041,                 loss: nan
Episode: 16181/101000 (16.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3080s / 1457.2422 s
agent0:                 episode reward: -0.3392,                 loss: 0.1703
agent1:                 episode reward: 0.3392,                 loss: nan
Episode: 16201/101000 (16.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2710s / 1459.5131 s
agent0:                 episode reward: -0.1826,                 loss: 0.1699
agent1:                 episode reward: 0.1826,                 loss: nan
Episode: 16221/101000 (16.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5312s / 1461.0443 s
agent0:                 episode reward: -0.1502,                 loss: 0.1708
agent1:                 episode reward: 0.1502,                 loss: nan
Episode: 16241/101000 (16.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8024s / 1462.8467 s
agent0:                 episode reward: -0.5057,                 loss: 0.1701
agent1:                 episode reward: 0.5057,                 loss: nan
Episode: 16261/101000 (16.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5993s / 1465.4461 s
agent0:                 episode reward: -0.1025,                 loss: 0.1705
agent1:                 episode reward: 0.1025,                 loss: nan
Episode: 16281/101000 (16.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7054s / 1467.1514 s
agent0:                 episode reward: -0.4458,                 loss: 0.1702
agent1:                 episode reward: 0.4458,                 loss: nan
Episode: 16301/101000 (16.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5816s / 1469.7330 s
agent0:                 episode reward: -0.6112,                 loss: 0.1683
agent1:                 episode reward: 0.6112,                 loss: nan
Episode: 16321/101000 (16.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0984s / 1471.8314 s
agent0:                 episode reward: -0.6227,                 loss: 0.1689
agent1:                 episode reward: 0.6227,                 loss: nan
Episode: 16341/101000 (16.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8105s / 1474.6419 s
agent0:                 episode reward: -0.1342,                 loss: 0.1686
agent1:                 episode reward: 0.1342,                 loss: nan
Episode: 16361/101000 (16.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2718s / 1476.9137 s
agent0:                 episode reward: -0.2677,                 loss: 0.1613
agent1:                 episode reward: 0.2677,                 loss: nan
Episode: 16381/101000 (16.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9705s / 1478.8842 s
agent0:                 episode reward: -0.0483,                 loss: 0.1611
agent1:                 episode reward: 0.0483,                 loss: nan
Episode: 16401/101000 (16.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1050s / 1480.9892 s
agent0:                 episode reward: -0.4442,                 loss: 0.1609
agent1:                 episode reward: 0.4442,                 loss: nan
Episode: 16421/101000 (16.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7443s / 1483.7335 s
agent0:                 episode reward: 0.0655,                 loss: 0.1615
agent1:                 episode reward: -0.0655,                 loss: nan
Episode: 16441/101000 (16.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0808s / 1485.8144 s
agent0:                 episode reward: -0.2871,                 loss: 0.1632
agent1:                 episode reward: 0.2871,                 loss: nan
Episode: 16461/101000 (16.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0806s / 1487.8950 s
agent0:                 episode reward: -0.5740,                 loss: 0.1613
agent1:                 episode reward: 0.5740,                 loss: nan
Episode: 16481/101000 (16.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7128s / 1489.6078 s
agent0:                 episode reward: -0.4318,                 loss: 0.1604
agent1:                 episode reward: 0.4318,                 loss: nan
Episode: 16501/101000 (16.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3324s / 1491.9402 s
agent0:                 episode reward: 0.1289,                 loss: 0.1601
agent1:                 episode reward: -0.1289,                 loss: 0.1553
Score delta: 1.517722820214298, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/16069_0.
Episode: 16521/101000 (16.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0276s / 1493.9678 s
agent0:                 episode reward: -0.0956,                 loss: nan
agent1:                 episode reward: 0.0956,                 loss: 0.1551
Episode: 16541/101000 (16.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0070s / 1495.9747 s
agent0:                 episode reward: -0.1390,                 loss: 0.2575
agent1:                 episode reward: 0.1390,                 loss: 0.1474
Score delta: 1.5168374290782285, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/16101_1.
Episode: 16561/101000 (16.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7754s / 1497.7502 s
agent0:                 episode reward: -0.5277,                 loss: 0.2497
agent1:                 episode reward: 0.5277,                 loss: nan
Episode: 16581/101000 (16.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7548s / 1499.5049 s
agent0:                 episode reward: 0.0504,                 loss: 0.2507
agent1:                 episode reward: -0.0504,                 loss: nan
Episode: 16601/101000 (16.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9293s / 1501.4342 s
agent0:                 episode reward: -0.3399,                 loss: 0.2467
agent1:                 episode reward: 0.3399,                 loss: 0.1607
Score delta: 1.5444304424563706, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/16161_0.
Episode: 16621/101000 (16.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8933s / 1503.3275 s
agent0:                 episode reward: -0.4456,                 loss: nan
agent1:                 episode reward: 0.4456,                 loss: 0.1588
Episode: 16641/101000 (16.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2481s / 1505.5756 s
agent0:                 episode reward: -0.2387,                 loss: nan
agent1:                 episode reward: 0.2387,                 loss: 0.1590
Episode: 16661/101000 (16.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0372s / 1507.6128 s
agent0:                 episode reward: -0.4021,                 loss: nan
agent1:                 episode reward: 0.4021,                 loss: 0.1561
Episode: 16681/101000 (16.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5251s / 1510.1379 s
agent0:                 episode reward: -0.6334,                 loss: 0.1690
agent1:                 episode reward: 0.6334,                 loss: 0.1524
Score delta: 1.8168292765669392, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/16244_1.
Episode: 16701/101000 (16.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9297s / 1514.0676 s
agent0:                 episode reward: -0.3277,                 loss: 0.1725
agent1:                 episode reward: 0.3277,                 loss: nan
Episode: 16721/101000 (16.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7294s / 1516.7970 s
agent0:                 episode reward: -0.5193,                 loss: 0.1722
agent1:                 episode reward: 0.5193,                 loss: nan
Episode: 16741/101000 (16.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7103s / 1518.5073 s
agent0:                 episode reward: -0.7284,                 loss: 0.1724
agent1:                 episode reward: 0.7284,                 loss: nan
Episode: 16761/101000 (16.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4490s / 1520.9563 s
agent0:                 episode reward: -0.4576,                 loss: 0.1717
agent1:                 episode reward: 0.4576,                 loss: nan
Episode: 16781/101000 (16.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9033s / 1523.8596 s
agent0:                 episode reward: -0.4385,                 loss: 0.1724
agent1:                 episode reward: 0.4385,                 loss: nan
Episode: 16801/101000 (16.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7178s / 1526.5775 s
agent0:                 episode reward: -0.3022,                 loss: 0.1659
agent1:                 episode reward: 0.3022,                 loss: nan
Episode: 16821/101000 (16.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7097s / 1528.2872 s
agent0:                 episode reward: -0.2492,                 loss: 0.1629
agent1:                 episode reward: 0.2492,                 loss: nan
Episode: 16841/101000 (16.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2899s / 1530.5771 s
agent0:                 episode reward: -0.4618,                 loss: 0.1621
agent1:                 episode reward: 0.4618,                 loss: nan
Episode: 16861/101000 (16.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9073s / 1532.4844 s
agent0:                 episode reward: -0.1471,                 loss: 0.1610
agent1:                 episode reward: 0.1471,                 loss: nan
Episode: 16881/101000 (16.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0204s / 1535.5048 s
agent0:                 episode reward: -0.8207,                 loss: 0.1616
agent1:                 episode reward: 0.8207,                 loss: nan
Episode: 16901/101000 (16.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4433s / 1537.9481 s
agent0:                 episode reward: -0.3189,                 loss: 0.1609
agent1:                 episode reward: 0.3189,                 loss: nan
Episode: 16921/101000 (16.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9330s / 1540.8811 s
agent0:                 episode reward: -0.7349,                 loss: 0.1611
agent1:                 episode reward: 0.7349,                 loss: nan
Episode: 16941/101000 (16.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5633s / 1543.4444 s
agent0:                 episode reward: -0.3820,                 loss: 0.1594
agent1:                 episode reward: 0.3820,                 loss: nan
Episode: 16961/101000 (16.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7220s / 1546.1664 s
agent0:                 episode reward: -0.7436,                 loss: 0.1605
agent1:                 episode reward: 0.7436,                 loss: nan
Episode: 16981/101000 (16.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5262s / 1548.6926 s
agent0:                 episode reward: -0.1544,                 loss: 0.1610
agent1:                 episode reward: 0.1544,                 loss: nan
Episode: 17001/101000 (16.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1422s / 1551.8348 s
agent0:                 episode reward: -0.1252,                 loss: 0.1605
agent1:                 episode reward: 0.1252,                 loss: nan
Episode: 17021/101000 (16.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6955s / 1555.5303 s
agent0:                 episode reward: -0.1052,                 loss: 0.1612
agent1:                 episode reward: 0.1052,                 loss: nan
Episode: 17041/101000 (16.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3581s / 1559.8885 s
agent0:                 episode reward: -0.3502,                 loss: 0.1617
agent1:                 episode reward: 0.3502,                 loss: nan
Episode: 17061/101000 (16.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3690s / 1563.2575 s
agent0:                 episode reward: -0.5310,                 loss: 0.1618
agent1:                 episode reward: 0.5310,                 loss: nan
Episode: 17081/101000 (16.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6614s / 1565.9189 s
agent0:                 episode reward: -0.5981,                 loss: 0.1601
agent1:                 episode reward: 0.5981,                 loss: nan
Episode: 17101/101000 (16.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9232s / 1568.8421 s
agent0:                 episode reward: -0.4757,                 loss: 0.1606
agent1:                 episode reward: 0.4757,                 loss: nan
Episode: 17121/101000 (16.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4649s / 1571.3070 s
agent0:                 episode reward: -0.5680,                 loss: 0.1606
agent1:                 episode reward: 0.5680,                 loss: nan
Episode: 17141/101000 (16.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5324s / 1573.8395 s
agent0:                 episode reward: -0.3318,                 loss: 0.1595
agent1:                 episode reward: 0.3318,                 loss: nan
Episode: 17161/101000 (16.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9716s / 1577.8110 s
agent0:                 episode reward: -0.6902,                 loss: 0.1597
agent1:                 episode reward: 0.6902,                 loss: nan
Episode: 17181/101000 (17.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8912s / 1581.7023 s
agent0:                 episode reward: -0.4691,                 loss: 0.1609
agent1:                 episode reward: 0.4691,                 loss: nan
Episode: 17201/101000 (17.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3055s / 1586.0078 s
agent0:                 episode reward: -0.6547,                 loss: 0.1585
agent1:                 episode reward: 0.6547,                 loss: nan
Episode: 17221/101000 (17.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0245s / 1590.0323 s
agent0:                 episode reward: 0.0336,                 loss: 0.1591
agent1:                 episode reward: -0.0336,                 loss: nan
Episode: 17241/101000 (17.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2741s / 1594.3063 s
agent0:                 episode reward: -0.1887,                 loss: 0.1600
agent1:                 episode reward: 0.1887,                 loss: nan
Episode: 17261/101000 (17.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3207s / 1598.6270 s
agent0:                 episode reward: -0.2848,                 loss: 0.1589
agent1:                 episode reward: 0.2848,                 loss: nan
Episode: 17281/101000 (17.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0725s / 1602.6995 s
agent0:                 episode reward: -0.3745,                 loss: 0.1601
agent1:                 episode reward: 0.3745,                 loss: nan
Episode: 17301/101000 (17.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9941s / 1606.6936 s
agent0:                 episode reward: -0.5415,                 loss: 0.1613
agent1:                 episode reward: 0.5415,                 loss: nan
Episode: 17321/101000 (17.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8166s / 1611.5102 s
agent0:                 episode reward: -0.4338,                 loss: 0.1605
agent1:                 episode reward: 0.4338,                 loss: nan
Episode: 17341/101000 (17.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3286s / 1614.8388 s
agent0:                 episode reward: -0.0737,                 loss: 0.1613
agent1:                 episode reward: 0.0737,                 loss: nan
Episode: 17361/101000 (17.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7940s / 1618.6328 s
agent0:                 episode reward: -0.2121,                 loss: 0.1618
agent1:                 episode reward: 0.2121,                 loss: nan
Episode: 17381/101000 (17.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0755s / 1622.7083 s
agent0:                 episode reward: -0.3412,                 loss: 0.1605
agent1:                 episode reward: 0.3412,                 loss: nan
Episode: 17401/101000 (17.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8021s / 1627.5104 s
agent0:                 episode reward: -0.6715,                 loss: 0.1615
agent1:                 episode reward: 0.6715,                 loss: nan
Episode: 17421/101000 (17.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1380s / 1634.6485 s
agent0:                 episode reward: -0.0736,                 loss: 0.1604
agent1:                 episode reward: 0.0736,                 loss: nan
Episode: 17441/101000 (17.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2298s / 1641.8782 s
agent0:                 episode reward: -0.0716,                 loss: 0.1599
agent1:                 episode reward: 0.0716,                 loss: nan
Episode: 17461/101000 (17.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0779s / 1646.9561 s
agent0:                 episode reward: -0.6215,                 loss: 0.1607
agent1:                 episode reward: 0.6215,                 loss: nan
Episode: 17481/101000 (17.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8153s / 1654.7713 s
agent0:                 episode reward: -0.3266,                 loss: 0.1608
agent1:                 episode reward: 0.3266,                 loss: nan
Episode: 17501/101000 (17.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6510s / 1661.4224 s
agent0:                 episode reward: -0.5335,                 loss: 0.1603
agent1:                 episode reward: 0.5335,                 loss: nan
Episode: 17521/101000 (17.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9422s / 1668.3645 s
agent0:                 episode reward: -0.2190,                 loss: 0.1594
agent1:                 episode reward: 0.2190,                 loss: nan
Episode: 17541/101000 (17.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0790s / 1675.4435 s
agent0:                 episode reward: -0.2598,                 loss: 0.1590
agent1:                 episode reward: 0.2598,                 loss: nan
Episode: 17561/101000 (17.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4003s / 1681.8438 s
agent0:                 episode reward: -0.5612,                 loss: 0.1604
agent1:                 episode reward: 0.5612,                 loss: nan
Episode: 17581/101000 (17.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0423s / 1688.8861 s
agent0:                 episode reward: -0.3346,                 loss: 0.1611
agent1:                 episode reward: 0.3346,                 loss: nan
Episode: 17601/101000 (17.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1709s / 1694.0570 s
agent0:                 episode reward: -0.3388,                 loss: 0.1597
agent1:                 episode reward: 0.3388,                 loss: nan
Episode: 17621/101000 (17.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0547s / 1701.1117 s
agent0:                 episode reward: -0.1172,                 loss: 0.1594
agent1:                 episode reward: 0.1172,                 loss: nan
Episode: 17641/101000 (17.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7391s / 1706.8508 s
agent0:                 episode reward: -0.2965,                 loss: 0.1611
agent1:                 episode reward: 0.2965,                 loss: nan
Episode: 17661/101000 (17.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0971s / 1710.9479 s
agent0:                 episode reward: -0.0531,                 loss: 0.1607
agent1:                 episode reward: 0.0531,                 loss: nan
Episode: 17681/101000 (17.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3908s / 1719.3386 s
agent0:                 episode reward: -0.3892,                 loss: 0.1596
agent1:                 episode reward: 0.3892,                 loss: nan
Episode: 17701/101000 (17.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2731s / 1725.6118 s
agent0:                 episode reward: -0.3546,                 loss: 0.1579
agent1:                 episode reward: 0.3546,                 loss: nan
Episode: 17721/101000 (17.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7641s / 1731.3759 s
agent0:                 episode reward: -0.1894,                 loss: 0.1596
agent1:                 episode reward: 0.1894,                 loss: nan
Episode: 17741/101000 (17.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1387s / 1737.5146 s
agent0:                 episode reward: -0.6199,                 loss: 0.1597
agent1:                 episode reward: 0.6199,                 loss: nan
Episode: 17761/101000 (17.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4808s / 1743.9954 s
agent0:                 episode reward: -0.3345,                 loss: 0.1597
agent1:                 episode reward: 0.3345,                 loss: nan
Episode: 17781/101000 (17.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2037s / 1750.1991 s
agent0:                 episode reward: -0.7597,                 loss: 0.1615
agent1:                 episode reward: 0.7597,                 loss: nan
Episode: 17801/101000 (17.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2862s / 1756.4853 s
agent0:                 episode reward: 0.1615,                 loss: 0.1654
agent1:                 episode reward: -0.1615,                 loss: nan
Episode: 17821/101000 (17.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1757s / 1763.6609 s
agent0:                 episode reward: -0.1569,                 loss: 0.1667
agent1:                 episode reward: 0.1569,                 loss: nan
Episode: 17841/101000 (17.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7931s / 1769.4540 s
agent0:                 episode reward: -0.7335,                 loss: 0.1669
agent1:                 episode reward: 0.7335,                 loss: nan
Episode: 17861/101000 (17.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1475s / 1775.6015 s
agent0:                 episode reward: -0.3599,                 loss: 0.1650
agent1:                 episode reward: 0.3599,                 loss: nan
Episode: 17881/101000 (17.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2725s / 1782.8740 s
agent0:                 episode reward: -0.6892,                 loss: 0.1665
agent1:                 episode reward: 0.6892,                 loss: nan
Episode: 17901/101000 (17.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7581s / 1789.6321 s
agent0:                 episode reward: -0.0155,                 loss: 0.1674
agent1:                 episode reward: 0.0155,                 loss: nan
Episode: 17921/101000 (17.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2385s / 1794.8706 s
agent0:                 episode reward: -0.3870,                 loss: 0.1653
agent1:                 episode reward: 0.3870,                 loss: nan
Episode: 17941/101000 (17.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8501s / 1798.7207 s
agent0:                 episode reward: -0.4472,                 loss: 0.1665
agent1:                 episode reward: 0.4472,                 loss: nan
Episode: 17961/101000 (17.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0373s / 1805.7580 s
agent0:                 episode reward: -0.4328,                 loss: 0.1680
agent1:                 episode reward: 0.4328,                 loss: nan
Episode: 17981/101000 (17.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6846s / 1811.4425 s
agent0:                 episode reward: -0.4375,                 loss: 0.1664
agent1:                 episode reward: 0.4375,                 loss: nan
Episode: 18001/101000 (17.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3288s / 1818.7713 s
agent0:                 episode reward: -0.2239,                 loss: 0.1661
agent1:                 episode reward: 0.2239,                 loss: nan
Episode: 18021/101000 (17.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5414s / 1823.3126 s
agent0:                 episode reward: -0.6693,                 loss: 0.1667
agent1:                 episode reward: 0.6693,                 loss: nan
Episode: 18041/101000 (17.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7512s / 1829.0639 s
agent0:                 episode reward: -0.6400,                 loss: 0.1660
agent1:                 episode reward: 0.6400,                 loss: nan
Episode: 18061/101000 (17.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2097s / 1835.2736 s
agent0:                 episode reward: -0.0667,                 loss: 0.1668
agent1:                 episode reward: 0.0667,                 loss: nan
Episode: 18081/101000 (17.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4371s / 1842.7107 s
agent0:                 episode reward: -0.5938,                 loss: 0.1652
agent1:                 episode reward: 0.5938,                 loss: nan
Episode: 18101/101000 (17.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5533s / 1848.2639 s
agent0:                 episode reward: -0.2207,                 loss: 0.1650
agent1:                 episode reward: 0.2207,                 loss: nan
Episode: 18121/101000 (17.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6100s / 1855.8739 s
agent0:                 episode reward: 0.2238,                 loss: 0.1652
agent1:                 episode reward: -0.2238,                 loss: nan
Episode: 18141/101000 (17.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4094s / 1861.2834 s
agent0:                 episode reward: -0.7569,                 loss: 0.1607
agent1:                 episode reward: 0.7569,                 loss: nan
Episode: 18161/101000 (17.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2875s / 1867.5709 s
agent0:                 episode reward: -0.2341,                 loss: 0.1601
agent1:                 episode reward: 0.2341,                 loss: nan
Episode: 18181/101000 (18.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8855s / 1872.4564 s
agent0:                 episode reward: -0.5916,                 loss: 0.1606
agent1:                 episode reward: 0.5916,                 loss: nan
Episode: 18201/101000 (18.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4116s / 1878.8680 s
agent0:                 episode reward: -0.3934,                 loss: 0.1580
agent1:                 episode reward: 0.3934,                 loss: nan
Episode: 18221/101000 (18.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6136s / 1884.4816 s
agent0:                 episode reward: -0.2103,                 loss: 0.1607
agent1:                 episode reward: 0.2103,                 loss: nan
Episode: 18241/101000 (18.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5307s / 1889.0123 s
agent0:                 episode reward: -0.0612,                 loss: 0.1624
agent1:                 episode reward: 0.0612,                 loss: nan
Episode: 18261/101000 (18.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8342s / 1896.8464 s
agent0:                 episode reward: -0.5404,                 loss: 0.1592
agent1:                 episode reward: 0.5404,                 loss: nan
Episode: 18281/101000 (18.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0266s / 1903.8730 s
agent0:                 episode reward: -0.2323,                 loss: 0.1614
agent1:                 episode reward: 0.2323,                 loss: nan
Episode: 18301/101000 (18.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9990s / 1909.8720 s
agent0:                 episode reward: 0.1142,                 loss: 0.1620
agent1:                 episode reward: -0.1142,                 loss: nan
Episode: 18321/101000 (18.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4405s / 1915.3126 s
agent0:                 episode reward: -0.6186,                 loss: 0.1629
agent1:                 episode reward: 0.6186,                 loss: nan
Episode: 18341/101000 (18.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8325s / 1923.1451 s
agent0:                 episode reward: -0.2600,                 loss: 0.1600
agent1:                 episode reward: 0.2600,                 loss: nan
Episode: 18361/101000 (18.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1718s / 1929.3169 s
agent0:                 episode reward: 0.2326,                 loss: 0.1599
agent1:                 episode reward: -0.2326,                 loss: nan
Episode: 18381/101000 (18.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7670s / 1936.0839 s
agent0:                 episode reward: -0.4941,                 loss: 0.1605
agent1:                 episode reward: 0.4941,                 loss: nan
Episode: 18401/101000 (18.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2451s / 1943.3290 s
agent0:                 episode reward: -0.5862,                 loss: 0.1602
agent1:                 episode reward: 0.5862,                 loss: nan
Episode: 18421/101000 (18.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5566s / 1948.8856 s
agent0:                 episode reward: 0.0319,                 loss: 0.1602
agent1:                 episode reward: -0.0319,                 loss: nan
Episode: 18441/101000 (18.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1408s / 1955.0264 s
agent0:                 episode reward: 0.0568,                 loss: 0.1623
agent1:                 episode reward: -0.0568,                 loss: nan
Episode: 18461/101000 (18.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2923s / 1960.3188 s
agent0:                 episode reward: -0.2881,                 loss: 0.1612
agent1:                 episode reward: 0.2881,                 loss: nan
Episode: 18481/101000 (18.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8072s / 1967.1260 s
agent0:                 episode reward: -0.6212,                 loss: 0.1602
agent1:                 episode reward: 0.6212,                 loss: nan
Episode: 18501/101000 (18.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8739s / 1972.9999 s
agent0:                 episode reward: -0.4908,                 loss: 0.1613
agent1:                 episode reward: 0.4908,                 loss: nan
Episode: 18521/101000 (18.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4593s / 1978.4592 s
agent0:                 episode reward: -0.2914,                 loss: 0.1623
agent1:                 episode reward: 0.2914,                 loss: nan
Episode: 18541/101000 (18.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5794s / 1985.0386 s
agent0:                 episode reward: -0.0269,                 loss: 0.1588
agent1:                 episode reward: 0.0269,                 loss: nan
Episode: 18561/101000 (18.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5798s / 1990.6183 s
agent0:                 episode reward: 0.1509,                 loss: 0.1615
agent1:                 episode reward: -0.1509,                 loss: nan
Episode: 18581/101000 (18.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4212s / 1997.0396 s
agent0:                 episode reward: -0.2701,                 loss: 0.1597
agent1:                 episode reward: 0.2701,                 loss: nan
Episode: 18601/101000 (18.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5710s / 2004.6105 s
agent0:                 episode reward: -0.2407,                 loss: 0.1604
agent1:                 episode reward: 0.2407,                 loss: nan
Episode: 18621/101000 (18.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0190s / 2011.6295 s
agent0:                 episode reward: -0.1741,                 loss: 0.1628
agent1:                 episode reward: 0.1741,                 loss: nan
Episode: 18641/101000 (18.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5693s / 2018.1989 s
agent0:                 episode reward: -0.2629,                 loss: 0.1605
agent1:                 episode reward: 0.2629,                 loss: nan
Episode: 18661/101000 (18.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3792s / 2024.5780 s
agent0:                 episode reward: -0.7651,                 loss: 0.1600
agent1:                 episode reward: 0.7651,                 loss: nan
Episode: 18681/101000 (18.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0292s / 2030.6073 s
agent0:                 episode reward: -0.5539,                 loss: 0.1603
agent1:                 episode reward: 0.5539,                 loss: nan
Episode: 18701/101000 (18.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1371s / 2038.7444 s
agent0:                 episode reward: 0.1010,                 loss: 0.1599
agent1:                 episode reward: -0.1010,                 loss: nan
Episode: 18721/101000 (18.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9362s / 2043.6806 s
agent0:                 episode reward: -0.3489,                 loss: 0.1616
agent1:                 episode reward: 0.3489,                 loss: nan
Episode: 18741/101000 (18.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8839s / 2051.5645 s
agent0:                 episode reward: -0.4195,                 loss: 0.1611
agent1:                 episode reward: 0.4195,                 loss: nan
Episode: 18761/101000 (18.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8500s / 2058.4145 s
agent0:                 episode reward: -0.1977,                 loss: 0.1616
agent1:                 episode reward: 0.1977,                 loss: nan
Episode: 18781/101000 (18.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9877s / 2065.4022 s
agent0:                 episode reward: -0.5925,                 loss: 0.1612
agent1:                 episode reward: 0.5925,                 loss: nan
Episode: 18801/101000 (18.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1683s / 2072.5704 s
agent0:                 episode reward: -0.4520,                 loss: 0.1600
agent1:                 episode reward: 0.4520,                 loss: nan
Episode: 18821/101000 (18.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6761s / 2081.2465 s
agent0:                 episode reward: -0.1651,                 loss: 0.1591
agent1:                 episode reward: 0.1651,                 loss: nan
Episode: 18841/101000 (18.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7147s / 2088.9612 s
agent0:                 episode reward: -0.5450,                 loss: 0.1602
agent1:                 episode reward: 0.5450,                 loss: nan
Episode: 18861/101000 (18.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6595s / 2097.6207 s
agent0:                 episode reward: -0.0906,                 loss: 0.1611
agent1:                 episode reward: 0.0906,                 loss: nan
Episode: 18881/101000 (18.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4270s / 2104.0477 s
agent0:                 episode reward: -0.1968,                 loss: 0.1586
agent1:                 episode reward: 0.1968,                 loss: nan
Episode: 18901/101000 (18.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3661s / 2108.4138 s
agent0:                 episode reward: -0.1916,                 loss: 0.1600
agent1:                 episode reward: 0.1916,                 loss: nan
Episode: 18921/101000 (18.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1094s / 2114.5232 s
agent0:                 episode reward: -0.0729,                 loss: 0.1576
agent1:                 episode reward: 0.0729,                 loss: nan
Episode: 18941/101000 (18.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1919s / 2120.7151 s
agent0:                 episode reward: -0.2609,                 loss: 0.1597
agent1:                 episode reward: 0.2609,                 loss: nan
Episode: 18961/101000 (18.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2230s / 2126.9381 s
agent0:                 episode reward: -0.3890,                 loss: 0.1590
agent1:                 episode reward: 0.3890,                 loss: nan
Episode: 18981/101000 (18.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5980s / 2133.5361 s
agent0:                 episode reward: -0.8076,                 loss: 0.1607
agent1:                 episode reward: 0.8076,                 loss: nan
Episode: 19001/101000 (18.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0836s / 2139.6197 s
agent0:                 episode reward: -0.3810,                 loss: 0.1596
agent1:                 episode reward: 0.3810,                 loss: nan
Episode: 19021/101000 (18.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5230s / 2146.1428 s
agent0:                 episode reward: -0.7213,                 loss: 0.1596
agent1:                 episode reward: 0.7213,                 loss: nan
Episode: 19041/101000 (18.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6661s / 2150.8089 s
agent0:                 episode reward: -0.2469,                 loss: 0.1607
agent1:                 episode reward: 0.2469,                 loss: nan
Episode: 19061/101000 (18.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9842s / 2156.7931 s
agent0:                 episode reward: -0.2761,                 loss: 0.1598
agent1:                 episode reward: 0.2761,                 loss: nan
Episode: 19081/101000 (18.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7852s / 2162.5783 s
agent0:                 episode reward: -0.0450,                 loss: 0.1599
agent1:                 episode reward: 0.0450,                 loss: nan
Episode: 19101/101000 (18.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4783s / 2168.0566 s
agent0:                 episode reward: -0.3240,                 loss: 0.1588
agent1:                 episode reward: 0.3240,                 loss: nan
Episode: 19121/101000 (18.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2927s / 2174.3493 s
agent0:                 episode reward: -0.8104,                 loss: 0.1603
agent1:                 episode reward: 0.8104,                 loss: nan
Episode: 19141/101000 (18.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8886s / 2183.2379 s
agent0:                 episode reward: -0.0763,                 loss: 0.1613
agent1:                 episode reward: 0.0763,                 loss: nan
Episode: 19161/101000 (18.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8501s / 2189.0879 s
agent0:                 episode reward: -0.2680,                 loss: 0.1620
agent1:                 episode reward: 0.2680,                 loss: nan
Episode: 19181/101000 (18.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2979s / 2195.3858 s
agent0:                 episode reward: -0.8622,                 loss: 0.1608
agent1:                 episode reward: 0.8622,                 loss: nan
Episode: 19201/101000 (19.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6818s / 2201.0676 s
agent0:                 episode reward: -0.4809,                 loss: 0.1620
agent1:                 episode reward: 0.4809,                 loss: nan
Episode: 19221/101000 (19.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8670s / 2208.9346 s
agent0:                 episode reward: -0.3312,                 loss: 0.1615
agent1:                 episode reward: 0.3312,                 loss: nan
Episode: 19241/101000 (19.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1676s / 2215.1022 s
agent0:                 episode reward: -0.4184,                 loss: 0.1615
agent1:                 episode reward: 0.4184,                 loss: nan
Episode: 19261/101000 (19.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1571s / 2222.2593 s
agent0:                 episode reward: -0.3895,                 loss: 0.1618
agent1:                 episode reward: 0.3895,                 loss: nan
Episode: 19281/101000 (19.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8811s / 2230.1404 s
agent0:                 episode reward: -0.4533,                 loss: 0.1621
agent1:                 episode reward: 0.4533,                 loss: nan
Episode: 19301/101000 (19.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6142s / 2237.7546 s
agent0:                 episode reward: -0.1764,                 loss: 0.1596
agent1:                 episode reward: 0.1764,                 loss: nan
Episode: 19321/101000 (19.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6807s / 2245.4353 s
agent0:                 episode reward: -0.1346,                 loss: 0.1595
agent1:                 episode reward: 0.1346,                 loss: nan
Episode: 19341/101000 (19.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9983s / 2253.4336 s
agent0:                 episode reward: -0.3224,                 loss: 0.1615
agent1:                 episode reward: 0.3224,                 loss: nan
Episode: 19361/101000 (19.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6283s / 2258.0619 s
agent0:                 episode reward: -0.2809,                 loss: 0.1612
agent1:                 episode reward: 0.2809,                 loss: nan
Episode: 19381/101000 (19.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4245s / 2266.4864 s
agent0:                 episode reward: -0.2613,                 loss: 0.1596
agent1:                 episode reward: 0.2613,                 loss: nan
Episode: 19401/101000 (19.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9039s / 2273.3903 s
agent0:                 episode reward: -0.4288,                 loss: 0.1602
agent1:                 episode reward: 0.4288,                 loss: nan
Episode: 19421/101000 (19.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7493s / 2280.1396 s
agent0:                 episode reward: -0.4972,                 loss: 0.1612
agent1:                 episode reward: 0.4972,                 loss: nan
Episode: 19441/101000 (19.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9260s / 2287.0656 s
agent0:                 episode reward: -0.3570,                 loss: 0.1625
agent1:                 episode reward: 0.3570,                 loss: nan
Episode: 19461/101000 (19.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9152s / 2290.9808 s
agent0:                 episode reward: -0.3578,                 loss: 0.1614
agent1:                 episode reward: 0.3578,                 loss: nan
Episode: 19481/101000 (19.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8440s / 2298.8248 s
agent0:                 episode reward: -0.5047,                 loss: 0.1625
agent1:                 episode reward: 0.5047,                 loss: nan
Episode: 19501/101000 (19.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9235s / 2305.7483 s
agent0:                 episode reward: -0.6567,                 loss: 0.1630
agent1:                 episode reward: 0.6567,                 loss: nan
Episode: 19521/101000 (19.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4652s / 2312.2135 s
agent0:                 episode reward: -0.4592,                 loss: 0.1602
agent1:                 episode reward: 0.4592,                 loss: nan
Episode: 19541/101000 (19.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8393s / 2317.0528 s
agent0:                 episode reward: -0.3545,                 loss: 0.1596
agent1:                 episode reward: 0.3545,                 loss: nan
Episode: 19561/101000 (19.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6646s / 2324.7174 s
agent0:                 episode reward: -0.5100,                 loss: 0.1594
agent1:                 episode reward: 0.5100,                 loss: nan
Episode: 19581/101000 (19.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9609s / 2329.6782 s
agent0:                 episode reward: -0.6811,                 loss: 0.1622
agent1:                 episode reward: 0.6811,                 loss: nan
Episode: 19601/101000 (19.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4104s / 2338.0886 s
agent0:                 episode reward: -0.3501,                 loss: 0.1616
agent1:                 episode reward: 0.3501,                 loss: nan
Episode: 19621/101000 (19.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5531s / 2346.6418 s
agent0:                 episode reward: -0.2280,                 loss: 0.1622
agent1:                 episode reward: 0.2280,                 loss: nan
Episode: 19641/101000 (19.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1013s / 2353.7430 s
agent0:                 episode reward: -0.1129,                 loss: 0.1615
agent1:                 episode reward: 0.1129,                 loss: nan
Episode: 19661/101000 (19.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3596s / 2360.1026 s
agent0:                 episode reward: -0.0305,                 loss: 0.1621
agent1:                 episode reward: 0.0305,                 loss: nan
Episode: 19681/101000 (19.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1848s / 2368.2874 s
agent0:                 episode reward: -0.5106,                 loss: 0.1621
agent1:                 episode reward: 0.5106,                 loss: nan
Episode: 19701/101000 (19.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4279s / 2376.7153 s
agent0:                 episode reward: -0.4984,                 loss: 0.1590
agent1:                 episode reward: 0.4984,                 loss: nan
Episode: 19721/101000 (19.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4160s / 2384.1314 s
agent0:                 episode reward: -0.2858,                 loss: 0.1612
agent1:                 episode reward: 0.2858,                 loss: nan
Episode: 19741/101000 (19.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6611s / 2390.7924 s
agent0:                 episode reward: -0.5038,                 loss: 0.1602
agent1:                 episode reward: 0.5038,                 loss: nan
Episode: 19761/101000 (19.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6831s / 2397.4755 s
agent0:                 episode reward: -0.4220,                 loss: 0.1607
agent1:                 episode reward: 0.4220,                 loss: nan
Episode: 19781/101000 (19.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7587s / 2404.2342 s
agent0:                 episode reward: -0.6204,                 loss: 0.1612
agent1:                 episode reward: 0.6204,                 loss: nan
Episode: 19801/101000 (19.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6898s / 2412.9240 s
agent0:                 episode reward: 0.2398,                 loss: 0.1648
agent1:                 episode reward: -0.2398,                 loss: nan
Episode: 19821/101000 (19.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3994s / 2420.3234 s
agent0:                 episode reward: -0.3856,                 loss: 0.1632
agent1:                 episode reward: 0.3856,                 loss: nan
Episode: 19841/101000 (19.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7303s / 2425.0538 s
agent0:                 episode reward: -0.3156,                 loss: 0.1628
agent1:                 episode reward: 0.3156,                 loss: nan
Episode: 19861/101000 (19.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6855s / 2431.7392 s
agent0:                 episode reward: -0.0562,                 loss: 0.1628
agent1:                 episode reward: 0.0562,                 loss: nan
Episode: 19881/101000 (19.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3264s / 2439.0657 s
agent0:                 episode reward: -0.1130,                 loss: 0.1632
agent1:                 episode reward: 0.1130,                 loss: nan
Episode: 19901/101000 (19.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3267s / 2444.3924 s
agent0:                 episode reward: -0.3212,                 loss: 0.1633
agent1:                 episode reward: 0.3212,                 loss: nan
Episode: 19921/101000 (19.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1055s / 2449.4979 s
agent0:                 episode reward: -0.4166,                 loss: 0.1633
agent1:                 episode reward: 0.4166,                 loss: nan
Episode: 19941/101000 (19.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0234s / 2456.5213 s
agent0:                 episode reward: -0.2488,                 loss: 0.1614
agent1:                 episode reward: 0.2488,                 loss: nan
Episode: 19961/101000 (19.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8171s / 2464.3384 s
agent0:                 episode reward: 0.1053,                 loss: 0.1624
agent1:                 episode reward: -0.1053,                 loss: 0.1576
Score delta: 1.6913701000541725, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/19524_0.
Episode: 19981/101000 (19.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4195s / 2469.7579 s
agent0:                 episode reward: 0.2385,                 loss: nan
agent1:                 episode reward: -0.2385,                 loss: 0.1472
Episode: 20001/101000 (19.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6222s / 2477.3801 s
agent0:                 episode reward: -0.0512,                 loss: nan
agent1:                 episode reward: 0.0512,                 loss: 0.1454
Episode: 20021/101000 (19.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5400s / 2483.9201 s
agent0:                 episode reward: -0.2326,                 loss: nan
agent1:                 episode reward: 0.2326,                 loss: 0.1438
Episode: 20041/101000 (19.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8257s / 2490.7458 s
agent0:                 episode reward: -0.2932,                 loss: nan
agent1:                 episode reward: 0.2932,                 loss: 0.1428
Episode: 20061/101000 (19.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7361s / 2496.4819 s
agent0:                 episode reward: -0.3981,                 loss: 0.2596
agent1:                 episode reward: 0.3981,                 loss: 0.1400
Score delta: 1.7481031677102592, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/19620_1.
Episode: 20081/101000 (19.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3037s / 2503.7855 s
agent0:                 episode reward: -0.5232,                 loss: 0.2545
agent1:                 episode reward: 0.5232,                 loss: nan
Episode: 20101/101000 (19.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2022s / 2510.9877 s
agent0:                 episode reward: -0.6566,                 loss: 0.2527
agent1:                 episode reward: 0.6566,                 loss: nan
Episode: 20121/101000 (19.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0083s / 2518.9960 s
agent0:                 episode reward: -0.0654,                 loss: 0.2527
agent1:                 episode reward: 0.0654,                 loss: nan
Episode: 20141/101000 (19.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6423s / 2527.6383 s
agent0:                 episode reward: -0.6915,                 loss: 0.2543
agent1:                 episode reward: 0.6915,                 loss: nan
Episode: 20161/101000 (19.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7014s / 2533.3397 s
agent0:                 episode reward: -0.7257,                 loss: 0.2563
agent1:                 episode reward: 0.7257,                 loss: nan
Episode: 20181/101000 (19.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2908s / 2540.6305 s
agent0:                 episode reward: -0.2622,                 loss: 0.2546
agent1:                 episode reward: 0.2622,                 loss: nan
Episode: 20201/101000 (20.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5048s / 2549.1354 s
agent0:                 episode reward: -0.6600,                 loss: 0.2554
agent1:                 episode reward: 0.6600,                 loss: nan
Episode: 20221/101000 (20.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0201s / 2555.1555 s
agent0:                 episode reward: 0.1001,                 loss: 0.2314
agent1:                 episode reward: -0.1001,                 loss: nan
Episode: 20241/101000 (20.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7564s / 2562.9119 s
agent0:                 episode reward: -0.2398,                 loss: 0.1688
agent1:                 episode reward: 0.2398,                 loss: nan
Episode: 20261/101000 (20.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0877s / 2569.9996 s
agent0:                 episode reward: -0.7957,                 loss: 0.1704
agent1:                 episode reward: 0.7957,                 loss: nan
Episode: 20281/101000 (20.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9612s / 2576.9609 s
agent0:                 episode reward: 0.1939,                 loss: 0.1690
agent1:                 episode reward: -0.1939,                 loss: nan
Episode: 20301/101000 (20.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9760s / 2585.9369 s
agent0:                 episode reward: -0.6877,                 loss: 0.1678
agent1:                 episode reward: 0.6877,                 loss: nan
Episode: 20321/101000 (20.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6080s / 2592.5449 s
agent0:                 episode reward: -0.3378,                 loss: 0.1693
agent1:                 episode reward: 0.3378,                 loss: nan
Episode: 20341/101000 (20.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8404s / 2597.3853 s
agent0:                 episode reward: -0.6265,                 loss: 0.1691
agent1:                 episode reward: 0.6265,                 loss: nan
Episode: 20361/101000 (20.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3291s / 2605.7144 s
agent0:                 episode reward: -0.3841,                 loss: 0.1687
agent1:                 episode reward: 0.3841,                 loss: nan
Episode: 20381/101000 (20.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5263s / 2615.2407 s
agent0:                 episode reward: -0.4751,                 loss: 0.1684
agent1:                 episode reward: 0.4751,                 loss: nan
Episode: 20401/101000 (20.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3804s / 2623.6211 s
agent0:                 episode reward: -0.0154,                 loss: 0.1706
agent1:                 episode reward: 0.0154,                 loss: 0.1514
Score delta: 1.8938315807939639, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/19970_0.
Episode: 20421/101000 (20.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4306s / 2630.0516 s
agent0:                 episode reward: -0.2914,                 loss: nan
agent1:                 episode reward: 0.2914,                 loss: 0.1540
Episode: 20441/101000 (20.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1400s / 2638.1917 s
agent0:                 episode reward: -0.7236,                 loss: 0.1645
agent1:                 episode reward: 0.7236,                 loss: 0.1533
Score delta: 1.5081910862396124, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/20000_1.
Episode: 20461/101000 (20.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3824s / 2644.5740 s
agent0:                 episode reward: -0.6929,                 loss: 0.1639
agent1:                 episode reward: 0.6929,                 loss: nan
Episode: 20481/101000 (20.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6329s / 2651.2069 s
agent0:                 episode reward: -0.3528,                 loss: 0.1649
agent1:                 episode reward: 0.3528,                 loss: nan
Episode: 20501/101000 (20.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7174s / 2655.9244 s
agent0:                 episode reward: -0.0774,                 loss: 0.1646
agent1:                 episode reward: 0.0774,                 loss: nan
Episode: 20521/101000 (20.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5278s / 2663.4521 s
agent0:                 episode reward: -0.1694,                 loss: 0.1669
agent1:                 episode reward: 0.1694,                 loss: nan
Episode: 20541/101000 (20.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6755s / 2669.1277 s
agent0:                 episode reward: -0.1597,                 loss: 0.1657
agent1:                 episode reward: 0.1597,                 loss: nan
Episode: 20561/101000 (20.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0623s / 2677.1900 s
agent0:                 episode reward: -0.3246,                 loss: 0.1644
agent1:                 episode reward: 0.3246,                 loss: nan
Episode: 20581/101000 (20.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9484s / 2683.1384 s
agent0:                 episode reward: 0.2209,                 loss: 0.1654
agent1:                 episode reward: -0.2209,                 loss: nan
Episode: 20601/101000 (20.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2401s / 2689.3785 s
agent0:                 episode reward: -0.4897,                 loss: 0.1614
agent1:                 episode reward: 0.4897,                 loss: nan
Episode: 20621/101000 (20.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5291s / 2696.9076 s
agent0:                 episode reward: 0.0512,                 loss: 0.1591
agent1:                 episode reward: -0.0512,                 loss: nan
Episode: 20641/101000 (20.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5896s / 2703.4972 s
agent0:                 episode reward: -0.3671,                 loss: 0.1585
agent1:                 episode reward: 0.3671,                 loss: nan
Episode: 20661/101000 (20.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7655s / 2710.2628 s
agent0:                 episode reward: 0.0174,                 loss: 0.1591
agent1:                 episode reward: -0.0174,                 loss: nan
Episode: 20681/101000 (20.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0337s / 2716.2965 s
agent0:                 episode reward: -0.1794,                 loss: 0.1579
agent1:                 episode reward: 0.1794,                 loss: nan
Episode: 20701/101000 (20.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7288s / 2721.0253 s
agent0:                 episode reward: -0.1346,                 loss: 0.1587
agent1:                 episode reward: 0.1346,                 loss: 0.1529
Score delta: 1.6979944677584045, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/20266_0.
Episode: 20721/101000 (20.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4386s / 2728.4639 s
agent0:                 episode reward: -0.2420,                 loss: nan
agent1:                 episode reward: 0.2420,                 loss: 0.1549
Episode: 20741/101000 (20.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4085s / 2735.8723 s
agent0:                 episode reward: -0.4679,                 loss: 0.1718
agent1:                 episode reward: 0.4679,                 loss: 0.1513
Score delta: 1.5642170032455476, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/20312_1.
Episode: 20761/101000 (20.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8147s / 2744.6870 s
agent0:                 episode reward: -0.3254,                 loss: 0.1672
agent1:                 episode reward: 0.3254,                 loss: nan
Episode: 20781/101000 (20.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6144s / 2752.3014 s
agent0:                 episode reward: -0.3452,                 loss: 0.1663
agent1:                 episode reward: 0.3452,                 loss: nan
Episode: 20801/101000 (20.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9868s / 2759.2882 s
agent0:                 episode reward: -0.2263,                 loss: 0.1662
agent1:                 episode reward: 0.2263,                 loss: nan
Episode: 20821/101000 (20.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2793s / 2766.5675 s
agent0:                 episode reward: -0.7040,                 loss: 0.1656
agent1:                 episode reward: 0.7040,                 loss: nan
Episode: 20841/101000 (20.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4679s / 2774.0354 s
agent0:                 episode reward: -0.3557,                 loss: 0.1657
agent1:                 episode reward: 0.3557,                 loss: nan
Episode: 20861/101000 (20.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0996s / 2780.1350 s
agent0:                 episode reward: -0.3846,                 loss: 0.1652
agent1:                 episode reward: 0.3846,                 loss: nan
Episode: 20881/101000 (20.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9515s / 2786.0866 s
agent0:                 episode reward: -0.2333,                 loss: 0.1661
agent1:                 episode reward: 0.2333,                 loss: nan
Episode: 20901/101000 (20.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8845s / 2792.9711 s
agent0:                 episode reward: -0.1476,                 loss: 0.1651
agent1:                 episode reward: 0.1476,                 loss: nan
Episode: 20921/101000 (20.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5887s / 2797.5598 s
agent0:                 episode reward: -0.2839,                 loss: 0.1646
agent1:                 episode reward: 0.2839,                 loss: nan
Episode: 20941/101000 (20.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3994s / 2804.9592 s
agent0:                 episode reward: -0.1925,                 loss: 0.1628
agent1:                 episode reward: 0.1925,                 loss: nan
Episode: 20961/101000 (20.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9749s / 2811.9341 s
agent0:                 episode reward: 0.1494,                 loss: 0.1658
agent1:                 episode reward: -0.1494,                 loss: 0.1504
Score delta: 1.7611474353952155, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/20520_0.
Episode: 20981/101000 (20.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6886s / 2818.6226 s
agent0:                 episode reward: -0.6033,                 loss: 0.1669
agent1:                 episode reward: 0.6033,                 loss: 0.1433
Score delta: 1.539376808587372, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/20552_1.
Episode: 21001/101000 (20.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7007s / 2825.3234 s
agent0:                 episode reward: -0.5153,                 loss: 0.1592
agent1:                 episode reward: 0.5153,                 loss: nan
Episode: 21021/101000 (20.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5944s / 2832.9178 s
agent0:                 episode reward: -0.3105,                 loss: 0.1564
agent1:                 episode reward: 0.3105,                 loss: nan
Episode: 21041/101000 (20.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4174s / 2839.3352 s
agent0:                 episode reward: -0.2002,                 loss: 0.1561
agent1:                 episode reward: 0.2002,                 loss: nan
Episode: 21061/101000 (20.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1483s / 2845.4835 s
agent0:                 episode reward: -0.4822,                 loss: 0.1578
agent1:                 episode reward: 0.4822,                 loss: nan
Episode: 21081/101000 (20.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7341s / 2851.2176 s
agent0:                 episode reward: -0.0432,                 loss: 0.1567
agent1:                 episode reward: 0.0432,                 loss: nan
Episode: 21101/101000 (20.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5854s / 2858.8030 s
agent0:                 episode reward: -0.1970,                 loss: 0.1559
agent1:                 episode reward: 0.1970,                 loss: nan
Episode: 21121/101000 (20.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6454s / 2865.4484 s
agent0:                 episode reward: -0.3227,                 loss: 0.1552
agent1:                 episode reward: 0.3227,                 loss: nan
Episode: 21141/101000 (20.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3058s / 2871.7542 s
agent0:                 episode reward: -0.0384,                 loss: 0.1568
agent1:                 episode reward: 0.0384,                 loss: nan
Episode: 21161/101000 (20.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2004s / 2877.9546 s
agent0:                 episode reward: -0.4110,                 loss: 0.1576
agent1:                 episode reward: 0.4110,                 loss: nan
Episode: 21181/101000 (20.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5984s / 2884.5529 s
agent0:                 episode reward: -0.0221,                 loss: 0.1543
agent1:                 episode reward: 0.0221,                 loss: nan
Episode: 21201/101000 (20.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9971s / 2890.5500 s
agent0:                 episode reward: 0.2662,                 loss: 0.1568
agent1:                 episode reward: -0.2662,                 loss: nan
Episode: 21221/101000 (21.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3404s / 2896.8904 s
agent0:                 episode reward: -0.1549,                 loss: 0.1562
agent1:                 episode reward: 0.1549,                 loss: nan
Episode: 21241/101000 (21.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1061s / 2902.9965 s
agent0:                 episode reward: -0.2463,                 loss: 0.1566
agent1:                 episode reward: 0.2463,                 loss: nan
Episode: 21261/101000 (21.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6323s / 2908.6288 s
agent0:                 episode reward: -0.5297,                 loss: 0.1542
agent1:                 episode reward: 0.5297,                 loss: 0.1383
Score delta: 1.6709426713700233, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/20820_0.
Episode: 21281/101000 (21.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8577s / 2915.4865 s
agent0:                 episode reward: -0.7210,                 loss: 0.1697
agent1:                 episode reward: 0.7210,                 loss: 0.1405
Score delta: 2.397097910695389, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/20844_1.
Episode: 21301/101000 (21.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1017s / 2922.5882 s
agent0:                 episode reward: 0.1508,                 loss: 0.1681
agent1:                 episode reward: -0.1508,                 loss: nan
Episode: 21321/101000 (21.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5352s / 2929.1234 s
agent0:                 episode reward: -0.4597,                 loss: 0.1697
agent1:                 episode reward: 0.4597,                 loss: nan
Episode: 21341/101000 (21.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3968s / 2935.5203 s
agent0:                 episode reward: -0.1062,                 loss: 0.1689
agent1:                 episode reward: 0.1062,                 loss: nan
Episode: 21361/101000 (21.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4801s / 2943.0004 s
agent0:                 episode reward: -0.6358,                 loss: 0.1626
agent1:                 episode reward: 0.6358,                 loss: nan
Episode: 21381/101000 (21.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3341s / 2951.3345 s
agent0:                 episode reward: -0.2835,                 loss: 0.1606
agent1:                 episode reward: 0.2835,                 loss: nan
Episode: 21401/101000 (21.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8880s / 2960.2226 s
agent0:                 episode reward: -0.0280,                 loss: 0.1604
agent1:                 episode reward: 0.0280,                 loss: nan
Episode: 21421/101000 (21.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8429s / 2968.0655 s
agent0:                 episode reward: -0.4731,                 loss: 0.1602
agent1:                 episode reward: 0.4731,                 loss: nan
Episode: 21441/101000 (21.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6571s / 2975.7226 s
agent0:                 episode reward: -0.2600,                 loss: 0.1597
agent1:                 episode reward: 0.2600,                 loss: nan
Episode: 21461/101000 (21.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8213s / 2981.5439 s
agent0:                 episode reward: -0.2977,                 loss: 0.1604
agent1:                 episode reward: 0.2977,                 loss: nan
Episode: 21481/101000 (21.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5732s / 2989.1172 s
agent0:                 episode reward: -0.2799,                 loss: 0.1585
agent1:                 episode reward: 0.2799,                 loss: nan
Episode: 21501/101000 (21.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0503s / 2997.1675 s
agent0:                 episode reward: -0.0428,                 loss: 0.1604
agent1:                 episode reward: 0.0428,                 loss: nan
Episode: 21521/101000 (21.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1140s / 3004.2815 s
agent0:                 episode reward: -0.0511,                 loss: 0.1608
agent1:                 episode reward: 0.0511,                 loss: nan
Episode: 21541/101000 (21.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1948s / 3008.4763 s
agent0:                 episode reward: -0.4267,                 loss: 0.1615
agent1:                 episode reward: 0.4267,                 loss: nan
Episode: 21561/101000 (21.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7002s / 3016.1765 s
agent0:                 episode reward: -0.0950,                 loss: 0.1611
agent1:                 episode reward: 0.0950,                 loss: nan
Episode: 21581/101000 (21.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5771s / 3023.7536 s
agent0:                 episode reward: -0.6148,                 loss: 0.1596
agent1:                 episode reward: 0.6148,                 loss: nan
Episode: 21601/101000 (21.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7370s / 3031.4906 s
agent0:                 episode reward: -0.3558,                 loss: 0.1596
agent1:                 episode reward: 0.3558,                 loss: nan
Episode: 21621/101000 (21.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8381s / 3037.3286 s
agent0:                 episode reward: -0.0336,                 loss: 0.1574
agent1:                 episode reward: 0.0336,                 loss: nan
Episode: 21641/101000 (21.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3120s / 3043.6406 s
agent0:                 episode reward: -0.2813,                 loss: 0.1608
agent1:                 episode reward: 0.2813,                 loss: nan
Episode: 21661/101000 (21.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2584s / 3050.8990 s
agent0:                 episode reward: -0.7533,                 loss: 0.1589
agent1:                 episode reward: 0.7533,                 loss: nan
Episode: 21681/101000 (21.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2236s / 3057.1226 s
agent0:                 episode reward: 0.0243,                 loss: 0.1618
agent1:                 episode reward: -0.0243,                 loss: nan
Episode: 21701/101000 (21.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8610s / 3062.9836 s
agent0:                 episode reward: 0.0451,                 loss: 0.1677
agent1:                 episode reward: -0.0451,                 loss: nan
Episode: 21721/101000 (21.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9873s / 3071.9709 s
agent0:                 episode reward: -0.1241,                 loss: 0.1676
agent1:                 episode reward: 0.1241,                 loss: nan
Episode: 21741/101000 (21.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7042s / 3078.6751 s
agent0:                 episode reward: -0.2735,                 loss: 0.1680
agent1:                 episode reward: 0.2735,                 loss: nan
Episode: 21761/101000 (21.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1420s / 3084.8171 s
agent0:                 episode reward: -0.3803,                 loss: 0.1681
agent1:                 episode reward: 0.3803,                 loss: nan
Episode: 21781/101000 (21.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5416s / 3091.3586 s
agent0:                 episode reward: -0.3305,                 loss: 0.1689
agent1:                 episode reward: 0.3305,                 loss: nan
Episode: 21801/101000 (21.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1289s / 3099.4875 s
agent0:                 episode reward: -0.0486,                 loss: 0.1676
agent1:                 episode reward: 0.0486,                 loss: nan
Episode: 21821/101000 (21.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9293s / 3108.4168 s
agent0:                 episode reward: -0.3804,                 loss: 0.1675
agent1:                 episode reward: 0.3804,                 loss: nan
Episode: 21841/101000 (21.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3054s / 3114.7222 s
agent0:                 episode reward: -0.4379,                 loss: 0.1673
agent1:                 episode reward: 0.4379,                 loss: nan
Episode: 21861/101000 (21.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3934s / 3121.1156 s
agent0:                 episode reward: 0.2697,                 loss: 0.1696
agent1:                 episode reward: -0.2697,                 loss: nan
Episode: 21881/101000 (21.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5263s / 3126.6419 s
agent0:                 episode reward: -0.3350,                 loss: 0.1662
agent1:                 episode reward: 0.3350,                 loss: nan
Episode: 21901/101000 (21.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8179s / 3135.4598 s
agent0:                 episode reward: -0.3753,                 loss: 0.1686
agent1:                 episode reward: 0.3753,                 loss: nan
Episode: 21921/101000 (21.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4542s / 3142.9140 s
agent0:                 episode reward: 0.0928,                 loss: 0.1677
agent1:                 episode reward: -0.0928,                 loss: nan
Episode: 21941/101000 (21.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0219s / 3150.9359 s
agent0:                 episode reward: -0.3890,                 loss: 0.1685
agent1:                 episode reward: 0.3890,                 loss: nan
Episode: 21961/101000 (21.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9579s / 3156.8938 s
agent0:                 episode reward: -0.3926,                 loss: 0.1688
agent1:                 episode reward: 0.3926,                 loss: nan
Episode: 21981/101000 (21.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3881s / 3163.2819 s
agent0:                 episode reward: -0.0705,                 loss: 0.1670
agent1:                 episode reward: 0.0705,                 loss: nan
Episode: 22001/101000 (21.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0351s / 3169.3169 s
agent0:                 episode reward: -0.0314,                 loss: 0.1677
agent1:                 episode reward: 0.0314,                 loss: nan
Episode: 22021/101000 (21.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7274s / 3177.0443 s
agent0:                 episode reward: 0.0323,                 loss: 0.1660
agent1:                 episode reward: -0.0323,                 loss: nan
Episode: 22041/101000 (21.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2242s / 3184.2685 s
agent0:                 episode reward: -0.0457,                 loss: 0.1629
agent1:                 episode reward: 0.0457,                 loss: nan
Episode: 22061/101000 (21.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3054s / 3191.5739 s
agent0:                 episode reward: -0.3024,                 loss: 0.1611
agent1:                 episode reward: 0.3024,                 loss: nan
Episode: 22081/101000 (21.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0727s / 3198.6466 s
agent0:                 episode reward: -0.5119,                 loss: 0.1615
agent1:                 episode reward: 0.5119,                 loss: nan
Episode: 22101/101000 (21.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3150s / 3204.9616 s
agent0:                 episode reward: -0.1338,                 loss: 0.1609
agent1:                 episode reward: 0.1338,                 loss: nan
Episode: 22121/101000 (21.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9691s / 3211.9307 s
agent0:                 episode reward: -0.5138,                 loss: 0.1620
agent1:                 episode reward: 0.5138,                 loss: nan
Episode: 22141/101000 (21.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8340s / 3218.7647 s
agent0:                 episode reward: 0.0170,                 loss: 0.1609
agent1:                 episode reward: -0.0170,                 loss: nan
Episode: 22161/101000 (21.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0417s / 3224.8064 s
agent0:                 episode reward: -0.2393,                 loss: 0.1630
agent1:                 episode reward: 0.2393,                 loss: nan
Episode: 22181/101000 (21.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0062s / 3231.8126 s
agent0:                 episode reward: -0.4026,                 loss: 0.1610
agent1:                 episode reward: 0.4026,                 loss: nan
Episode: 22201/101000 (21.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8215s / 3239.6341 s
agent0:                 episode reward: -0.3964,                 loss: 0.1611
agent1:                 episode reward: 0.3964,                 loss: nan
Episode: 22221/101000 (22.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1878s / 3246.8219 s
agent0:                 episode reward: -0.3827,                 loss: 0.1615
agent1:                 episode reward: 0.3827,                 loss: nan
Episode: 22241/101000 (22.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1330s / 3253.9550 s
agent0:                 episode reward: -0.3044,                 loss: 0.1616
agent1:                 episode reward: 0.3044,                 loss: nan
Episode: 22261/101000 (22.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2300s / 3261.1850 s
agent0:                 episode reward: -0.0426,                 loss: 0.1617
agent1:                 episode reward: 0.0426,                 loss: nan
Episode: 22281/101000 (22.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8819s / 3270.0669 s
agent0:                 episode reward: -0.5297,                 loss: 0.1627
agent1:                 episode reward: 0.5297,                 loss: nan
Episode: 22301/101000 (22.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8096s / 3276.8765 s
agent0:                 episode reward: -0.1730,                 loss: 0.1611
agent1:                 episode reward: 0.1730,                 loss: nan
Episode: 22321/101000 (22.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1316s / 3284.0081 s
agent0:                 episode reward: -0.1914,                 loss: 0.1605
agent1:                 episode reward: 0.1914,                 loss: nan
Episode: 22341/101000 (22.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6616s / 3289.6697 s
agent0:                 episode reward: -0.1909,                 loss: 0.1602
agent1:                 episode reward: 0.1909,                 loss: nan
Episode: 22361/101000 (22.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1320s / 3296.8016 s
agent0:                 episode reward: 0.3448,                 loss: 0.1674
agent1:                 episode reward: -0.3448,                 loss: nan
Episode: 22381/101000 (22.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3884s / 3303.1900 s
agent0:                 episode reward: -0.4017,                 loss: 0.1698
agent1:                 episode reward: 0.4017,                 loss: nan
Episode: 22401/101000 (22.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5015s / 3310.6915 s
agent0:                 episode reward: -0.1621,                 loss: 0.1711
agent1:                 episode reward: 0.1621,                 loss: nan
Episode: 22421/101000 (22.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7231s / 3319.4146 s
agent0:                 episode reward: -0.2425,                 loss: 0.1696
agent1:                 episode reward: 0.2425,                 loss: nan
Episode: 22441/101000 (22.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3604s / 3324.7750 s
agent0:                 episode reward: -0.1574,                 loss: 0.1701
agent1:                 episode reward: 0.1574,                 loss: nan
Episode: 22461/101000 (22.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6091s / 3331.3841 s
agent0:                 episode reward: -0.2186,                 loss: 0.1712
agent1:                 episode reward: 0.2186,                 loss: nan
Episode: 22481/101000 (22.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0309s / 3337.4149 s
agent0:                 episode reward: 0.0934,                 loss: 0.1694
agent1:                 episode reward: -0.0934,                 loss: nan
Episode: 22501/101000 (22.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0098s / 3345.4248 s
agent0:                 episode reward: -0.2422,                 loss: 0.1694
agent1:                 episode reward: 0.2422,                 loss: nan
Episode: 22521/101000 (22.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2503s / 3349.6750 s
agent0:                 episode reward: -0.1106,                 loss: 0.1707
agent1:                 episode reward: 0.1106,                 loss: nan
Episode: 22541/101000 (22.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2728s / 3356.9479 s
agent0:                 episode reward: -0.0512,                 loss: 0.1709
agent1:                 episode reward: 0.0512,                 loss: nan
Episode: 22561/101000 (22.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0824s / 3365.0303 s
agent0:                 episode reward: -0.3990,                 loss: 0.1696
agent1:                 episode reward: 0.3990,                 loss: nan
Episode: 22581/101000 (22.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7442s / 3373.7745 s
agent0:                 episode reward: 0.0199,                 loss: 0.1692
agent1:                 episode reward: -0.0199,                 loss: nan
Episode: 22601/101000 (22.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4221s / 3380.1966 s
agent0:                 episode reward: -0.0323,                 loss: 0.1709
agent1:                 episode reward: 0.0323,                 loss: nan
Episode: 22621/101000 (22.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1768s / 3388.3734 s
agent0:                 episode reward: -0.6129,                 loss: 0.1704
agent1:                 episode reward: 0.6129,                 loss: nan
Episode: 22641/101000 (22.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9154s / 3395.2888 s
agent0:                 episode reward: -0.0182,                 loss: 0.1710
agent1:                 episode reward: 0.0182,                 loss: nan
Episode: 22661/101000 (22.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0208s / 3400.3095 s
agent0:                 episode reward: -0.1321,                 loss: 0.1701
agent1:                 episode reward: 0.1321,                 loss: nan
Episode: 22681/101000 (22.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8090s / 3406.1185 s
agent0:                 episode reward: 0.0729,                 loss: 0.1695
agent1:                 episode reward: -0.0729,                 loss: nan
Episode: 22701/101000 (22.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2601s / 3413.3786 s
agent0:                 episode reward: -0.5292,                 loss: 0.1623
agent1:                 episode reward: 0.5292,                 loss: nan
Episode: 22721/101000 (22.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3970s / 3418.7756 s
agent0:                 episode reward: -0.6174,                 loss: 0.1634
agent1:                 episode reward: 0.6174,                 loss: nan
Episode: 22741/101000 (22.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0354s / 3425.8110 s
agent0:                 episode reward: 0.1669,                 loss: 0.1612
agent1:                 episode reward: -0.1669,                 loss: nan
Episode: 22761/101000 (22.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5485s / 3432.3595 s
agent0:                 episode reward: -0.0995,                 loss: 0.1615
agent1:                 episode reward: 0.0995,                 loss: nan
Episode: 22781/101000 (22.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7184s / 3441.0780 s
agent0:                 episode reward: 0.1805,                 loss: 0.1617
agent1:                 episode reward: -0.1805,                 loss: nan
Episode: 22801/101000 (22.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9878s / 3448.0657 s
agent0:                 episode reward: -0.5151,                 loss: 0.1623
agent1:                 episode reward: 0.5151,                 loss: nan
Episode: 22821/101000 (22.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9496s / 3455.0153 s
agent0:                 episode reward: -0.5450,                 loss: 0.1611
agent1:                 episode reward: 0.5450,                 loss: nan
Episode: 22841/101000 (22.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1031s / 3461.1184 s
agent0:                 episode reward: 0.2828,                 loss: 0.1608
agent1:                 episode reward: -0.2828,                 loss: nan
Episode: 22861/101000 (22.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2129s / 3467.3314 s
agent0:                 episode reward: -0.2273,                 loss: 0.1548
agent1:                 episode reward: 0.2273,                 loss: 0.1409
Score delta: 1.6662500194688217, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/22416_0.
Episode: 22881/101000 (22.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9031s / 3475.2345 s
agent0:                 episode reward: -0.6562,                 loss: 0.2298
agent1:                 episode reward: 0.6562,                 loss: 0.1406
Score delta: 1.7493205442824753, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/22449_1.
Episode: 22901/101000 (22.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0816s / 3482.3160 s
agent0:                 episode reward: -0.5597,                 loss: 0.2243
agent1:                 episode reward: 0.5597,                 loss: nan
Episode: 22921/101000 (22.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5226s / 3487.8386 s
agent0:                 episode reward: -0.3480,                 loss: 0.2229
agent1:                 episode reward: 0.3480,                 loss: nan
Episode: 22941/101000 (22.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1082s / 3494.9468 s
agent0:                 episode reward: -0.5054,                 loss: 0.2245
agent1:                 episode reward: 0.5054,                 loss: nan
Episode: 22961/101000 (22.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2198s / 3500.1666 s
agent0:                 episode reward: -0.1062,                 loss: 0.2237
agent1:                 episode reward: 0.1062,                 loss: nan
Episode: 22981/101000 (22.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6350s / 3507.8016 s
agent0:                 episode reward: -0.3445,                 loss: 0.2232
agent1:                 episode reward: 0.3445,                 loss: nan
Episode: 23001/101000 (22.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0794s / 3513.8810 s
agent0:                 episode reward: 0.0152,                 loss: 0.2215
agent1:                 episode reward: -0.0152,                 loss: nan
Episode: 23021/101000 (22.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0403s / 3522.9214 s
agent0:                 episode reward: -0.4643,                 loss: 0.2217
agent1:                 episode reward: 0.4643,                 loss: nan
Episode: 23041/101000 (22.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5964s / 3527.5177 s
agent0:                 episode reward: -0.4926,                 loss: 0.2226
agent1:                 episode reward: 0.4926,                 loss: nan
Episode: 23061/101000 (22.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2217s / 3534.7394 s
agent0:                 episode reward: -0.1970,                 loss: 0.1934
agent1:                 episode reward: 0.1970,                 loss: nan
Episode: 23081/101000 (22.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4592s / 3541.1986 s
agent0:                 episode reward: -0.3650,                 loss: 0.1781
agent1:                 episode reward: 0.3650,                 loss: nan
Episode: 23101/101000 (22.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2966s / 3547.4952 s
agent0:                 episode reward: -0.2417,                 loss: 0.1799
agent1:                 episode reward: 0.2417,                 loss: nan
Episode: 23121/101000 (22.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6505s / 3553.1458 s
agent0:                 episode reward: -0.4519,                 loss: 0.1794
agent1:                 episode reward: 0.4519,                 loss: nan
Episode: 23141/101000 (22.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1410s / 3560.2868 s
agent0:                 episode reward: -0.1477,                 loss: 0.1802
agent1:                 episode reward: 0.1477,                 loss: nan
Episode: 23161/101000 (22.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8466s / 3567.1334 s
agent0:                 episode reward: -0.1839,                 loss: 0.1785
agent1:                 episode reward: 0.1839,                 loss: nan
Episode: 23181/101000 (22.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7613s / 3573.8946 s
agent0:                 episode reward: -0.2152,                 loss: 0.1783
agent1:                 episode reward: 0.2152,                 loss: nan
Episode: 23201/101000 (22.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7636s / 3579.6583 s
agent0:                 episode reward: -0.2527,                 loss: 0.1787
agent1:                 episode reward: 0.2527,                 loss: nan
Episode: 23221/101000 (22.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1818s / 3585.8401 s
agent0:                 episode reward: -0.3989,                 loss: 0.1799
agent1:                 episode reward: 0.3989,                 loss: nan
Episode: 23241/101000 (23.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5007s / 3593.3408 s
agent0:                 episode reward: -0.2958,                 loss: 0.1790
agent1:                 episode reward: 0.2958,                 loss: nan
Episode: 23261/101000 (23.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6961s / 3600.0368 s
agent0:                 episode reward: 0.2148,                 loss: 0.1786
agent1:                 episode reward: -0.2148,                 loss: nan
Episode: 23281/101000 (23.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0017s / 3607.0386 s
agent0:                 episode reward: -0.3314,                 loss: 0.1777
agent1:                 episode reward: 0.3314,                 loss: 0.1527
Score delta: 1.7042766688877904, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/22841_0.
Episode: 23301/101000 (23.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3818s / 3614.4204 s
agent0:                 episode reward: -0.4451,                 loss: nan
agent1:                 episode reward: 0.4451,                 loss: 0.1518
Episode: 23321/101000 (23.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4215s / 3619.8419 s
agent0:                 episode reward: -0.3652,                 loss: nan
agent1:                 episode reward: 0.3652,                 loss: 0.1523
Episode: 23341/101000 (23.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6852s / 3625.5271 s
agent0:                 episode reward: -0.6478,                 loss: 0.1726
agent1:                 episode reward: 0.6478,                 loss: 0.1519
Score delta: 1.6617317465127996, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/22911_1.
Episode: 23361/101000 (23.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2226s / 3632.7497 s
agent0:                 episode reward: -0.3240,                 loss: 0.1756
agent1:                 episode reward: 0.3240,                 loss: nan
Episode: 23381/101000 (23.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6622s / 3638.4119 s
agent0:                 episode reward: -0.0683,                 loss: 0.1748
agent1:                 episode reward: 0.0683,                 loss: nan
Episode: 23401/101000 (23.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2989s / 3644.7108 s
agent0:                 episode reward: -0.2661,                 loss: 0.1747
agent1:                 episode reward: 0.2661,                 loss: nan
Episode: 23421/101000 (23.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8638s / 3651.5745 s
agent0:                 episode reward: -0.2524,                 loss: 0.1774
agent1:                 episode reward: 0.2524,                 loss: nan
Episode: 23441/101000 (23.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2119s / 3658.7864 s
agent0:                 episode reward: -0.2716,                 loss: 0.1766
agent1:                 episode reward: 0.2716,                 loss: nan
Episode: 23461/101000 (23.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8039s / 3667.5903 s
agent0:                 episode reward: -0.4350,                 loss: 0.1722
agent1:                 episode reward: 0.4350,                 loss: nan
Episode: 23481/101000 (23.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1275s / 3675.7178 s
agent0:                 episode reward: -0.0151,                 loss: 0.1669
agent1:                 episode reward: 0.0151,                 loss: nan
Episode: 23501/101000 (23.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2921s / 3681.0099 s
agent0:                 episode reward: 0.1623,                 loss: 0.1684
agent1:                 episode reward: -0.1623,                 loss: nan
Episode: 23521/101000 (23.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9775s / 3687.9875 s
agent0:                 episode reward: -0.5935,                 loss: 0.1672
agent1:                 episode reward: 0.5935,                 loss: nan
Episode: 23541/101000 (23.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6994s / 3696.6869 s
agent0:                 episode reward: -0.1815,                 loss: 0.1684
agent1:                 episode reward: 0.1815,                 loss: nan
Episode: 23561/101000 (23.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4563s / 3703.1431 s
agent0:                 episode reward: -0.3029,                 loss: 0.1688
agent1:                 episode reward: 0.3029,                 loss: nan
Episode: 23581/101000 (23.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6617s / 3710.8048 s
agent0:                 episode reward: -0.1011,                 loss: 0.1671
agent1:                 episode reward: 0.1011,                 loss: nan
Episode: 23601/101000 (23.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5640s / 3716.3688 s
agent0:                 episode reward: -0.1203,                 loss: 0.1673
agent1:                 episode reward: 0.1203,                 loss: nan
Episode: 23621/101000 (23.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7966s / 3724.1655 s
agent0:                 episode reward: -0.1553,                 loss: 0.1677
agent1:                 episode reward: 0.1553,                 loss: nan
Episode: 23641/101000 (23.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5400s / 3730.7054 s
agent0:                 episode reward: -0.1498,                 loss: 0.1696
agent1:                 episode reward: 0.1498,                 loss: nan
Episode: 23661/101000 (23.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4681s / 3738.1735 s
agent0:                 episode reward: 0.1475,                 loss: 0.1663
agent1:                 episode reward: -0.1475,                 loss: nan
Episode: 23681/101000 (23.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0584s / 3744.2319 s
agent0:                 episode reward: -0.2080,                 loss: 0.1671
agent1:                 episode reward: 0.2080,                 loss: nan
Episode: 23701/101000 (23.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5366s / 3752.7684 s
agent0:                 episode reward: -0.5963,                 loss: 0.1659
agent1:                 episode reward: 0.5963,                 loss: nan
Episode: 23721/101000 (23.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7928s / 3759.5612 s
agent0:                 episode reward: -0.3518,                 loss: 0.1668
agent1:                 episode reward: 0.3518,                 loss: nan
Episode: 23741/101000 (23.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8034s / 3766.3646 s
agent0:                 episode reward: -0.1299,                 loss: 0.1678
agent1:                 episode reward: 0.1299,                 loss: nan
Episode: 23761/101000 (23.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8528s / 3772.2174 s
agent0:                 episode reward: 0.2715,                 loss: 0.1684
agent1:                 episode reward: -0.2715,                 loss: nan
Episode: 23781/101000 (23.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6193s / 3780.8367 s
agent0:                 episode reward: -0.4950,                 loss: 0.1666
agent1:                 episode reward: 0.4950,                 loss: nan
Episode: 23801/101000 (23.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7779s / 3785.6145 s
agent0:                 episode reward: -0.0088,                 loss: 0.1699
agent1:                 episode reward: 0.0088,                 loss: nan
Episode: 23821/101000 (23.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0590s / 3792.6735 s
agent0:                 episode reward: -0.2947,                 loss: 0.1698
agent1:                 episode reward: 0.2947,                 loss: nan
Episode: 23841/101000 (23.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6268s / 3798.3003 s
agent0:                 episode reward: -0.2460,                 loss: 0.1711
agent1:                 episode reward: 0.2460,                 loss: nan
Episode: 23861/101000 (23.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4967s / 3806.7970 s
agent0:                 episode reward: 0.1234,                 loss: 0.1713
agent1:                 episode reward: -0.1234,                 loss: nan
Episode: 23881/101000 (23.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2719s / 3813.0689 s
agent0:                 episode reward: -0.3593,                 loss: 0.1692
agent1:                 episode reward: 0.3593,                 loss: nan
Episode: 23901/101000 (23.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8532s / 3821.9221 s
agent0:                 episode reward: -0.1218,                 loss: 0.1691
agent1:                 episode reward: 0.1218,                 loss: nan
Episode: 23921/101000 (23.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6711s / 3829.5932 s
agent0:                 episode reward: -0.1231,                 loss: 0.1672
agent1:                 episode reward: 0.1231,                 loss: nan
Episode: 23941/101000 (23.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1095s / 3836.7027 s
agent0:                 episode reward: -0.2847,                 loss: 0.1685
agent1:                 episode reward: 0.2847,                 loss: nan
Episode: 23961/101000 (23.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1256s / 3844.8283 s
agent0:                 episode reward: -0.1425,                 loss: 0.1692
agent1:                 episode reward: 0.1425,                 loss: nan
Episode: 23981/101000 (23.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9049s / 3850.7332 s
agent0:                 episode reward: 0.1180,                 loss: 0.1700
agent1:                 episode reward: -0.1180,                 loss: nan
Episode: 24001/101000 (23.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9894s / 3858.7226 s
agent0:                 episode reward: 0.3511,                 loss: 0.1684
agent1:                 episode reward: -0.3511,                 loss: nan
Episode: 24021/101000 (23.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0723s / 3865.7949 s
agent0:                 episode reward: 0.2086,                 loss: 0.1697
agent1:                 episode reward: -0.2086,                 loss: 0.1592
Score delta: 1.522799564524688, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/23586_0.
Episode: 24041/101000 (23.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5899s / 3871.3848 s
agent0:                 episode reward: -0.3339,                 loss: nan
agent1:                 episode reward: 0.3339,                 loss: 0.1596
Episode: 24061/101000 (23.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5030s / 3875.8878 s
agent0:                 episode reward: -0.5547,                 loss: 0.1656
agent1:                 episode reward: 0.5547,                 loss: 0.1598
Score delta: 1.611201264505685, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/23630_1.
Episode: 24081/101000 (23.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8041s / 3881.6918 s
agent0:                 episode reward: -0.3098,                 loss: 0.1700
agent1:                 episode reward: 0.3098,                 loss: nan
Episode: 24101/101000 (23.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8279s / 3886.5197 s
agent0:                 episode reward: -0.0760,                 loss: 0.1688
agent1:                 episode reward: 0.0760,                 loss: nan
Episode: 24121/101000 (23.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7259s / 3891.2456 s
agent0:                 episode reward: -0.1985,                 loss: 0.1690
agent1:                 episode reward: 0.1985,                 loss: nan
Episode: 24141/101000 (23.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3173s / 3897.5629 s
agent0:                 episode reward: -0.6738,                 loss: 0.1692
agent1:                 episode reward: 0.6738,                 loss: nan
Episode: 24161/101000 (23.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9651s / 3904.5280 s
agent0:                 episode reward: -0.1488,                 loss: 0.1683
agent1:                 episode reward: 0.1488,                 loss: nan
Episode: 24181/101000 (23.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4370s / 3910.9649 s
agent0:                 episode reward: -0.1687,                 loss: 0.1702
agent1:                 episode reward: 0.1687,                 loss: nan
Episode: 24201/101000 (23.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0761s / 3919.0410 s
agent0:                 episode reward: -0.1575,                 loss: 0.1684
agent1:                 episode reward: 0.1575,                 loss: nan
Episode: 24221/101000 (23.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1935s / 3926.2345 s
agent0:                 episode reward: -0.5462,                 loss: 0.1668
agent1:                 episode reward: 0.5462,                 loss: nan
Episode: 24241/101000 (24.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3714s / 3934.6059 s
agent0:                 episode reward: -0.1713,                 loss: 0.1685
agent1:                 episode reward: 0.1713,                 loss: nan
Episode: 24261/101000 (24.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3747s / 3940.9806 s
agent0:                 episode reward: -0.0654,                 loss: 0.1687
agent1:                 episode reward: 0.0654,                 loss: nan
Episode: 24281/101000 (24.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9494s / 3948.9300 s
agent0:                 episode reward: -0.0242,                 loss: 0.1673
agent1:                 episode reward: 0.0242,                 loss: nan
Episode: 24301/101000 (24.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9112s / 3955.8412 s
agent0:                 episode reward: -0.2509,                 loss: 0.1678
agent1:                 episode reward: 0.2509,                 loss: nan
Episode: 24321/101000 (24.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7242s / 3963.5654 s
agent0:                 episode reward: -0.0396,                 loss: 0.1674
agent1:                 episode reward: 0.0396,                 loss: nan
Episode: 24341/101000 (24.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5789s / 3971.1443 s
agent0:                 episode reward: -0.2587,                 loss: 0.1671
agent1:                 episode reward: 0.2587,                 loss: nan
Episode: 24361/101000 (24.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7853s / 3977.9296 s
agent0:                 episode reward: -0.4633,                 loss: 0.1671
agent1:                 episode reward: 0.4633,                 loss: nan
Episode: 24381/101000 (24.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6871s / 3985.6167 s
agent0:                 episode reward: -0.0360,                 loss: 0.1676
agent1:                 episode reward: 0.0360,                 loss: nan
Episode: 24401/101000 (24.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9921s / 3992.6088 s
agent0:                 episode reward: -0.0991,                 loss: 0.1677
agent1:                 episode reward: 0.0991,                 loss: nan
Episode: 24421/101000 (24.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9719s / 3997.5807 s
agent0:                 episode reward: -0.2605,                 loss: 0.1675
agent1:                 episode reward: 0.2605,                 loss: nan
Episode: 24441/101000 (24.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4178s / 4002.9984 s
agent0:                 episode reward: -0.1231,                 loss: 0.1675
agent1:                 episode reward: 0.1231,                 loss: nan
Episode: 24461/101000 (24.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2225s / 4010.2210 s
agent0:                 episode reward: 0.2457,                 loss: 0.1668
agent1:                 episode reward: -0.2457,                 loss: nan
Episode: 24481/101000 (24.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1297s / 4018.3507 s
agent0:                 episode reward: -0.3897,                 loss: 0.1680
agent1:                 episode reward: 0.3897,                 loss: nan
Episode: 24501/101000 (24.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7662s / 4024.1169 s
agent0:                 episode reward: -0.5797,                 loss: 0.1703
agent1:                 episode reward: 0.5797,                 loss: nan
Episode: 24521/101000 (24.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1369s / 4030.2539 s
agent0:                 episode reward: 0.0575,                 loss: 0.1765
agent1:                 episode reward: -0.0575,                 loss: nan
Episode: 24541/101000 (24.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8206s / 4036.0744 s
agent0:                 episode reward: 0.1705,                 loss: 0.1740
agent1:                 episode reward: -0.1705,                 loss: nan
Episode: 24561/101000 (24.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5851s / 4042.6596 s
agent0:                 episode reward: -0.3498,                 loss: 0.1751
agent1:                 episode reward: 0.3498,                 loss: nan
Episode: 24581/101000 (24.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5561s / 4050.2156 s
agent0:                 episode reward: -0.1485,                 loss: 0.1770
agent1:                 episode reward: 0.1485,                 loss: nan
Episode: 24601/101000 (24.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7990s / 4058.0146 s
agent0:                 episode reward: 0.0902,                 loss: 0.1758
agent1:                 episode reward: -0.0902,                 loss: nan
Episode: 24621/101000 (24.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8809s / 4066.8955 s
agent0:                 episode reward: -0.3427,                 loss: 0.1762
agent1:                 episode reward: 0.3427,                 loss: nan
Episode: 24641/101000 (24.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2133s / 4074.1088 s
agent0:                 episode reward: 0.1272,                 loss: 0.1738
agent1:                 episode reward: -0.1272,                 loss: nan
Episode: 24661/101000 (24.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1098s / 4081.2186 s
agent0:                 episode reward: -0.3481,                 loss: 0.1751
agent1:                 episode reward: 0.3481,                 loss: nan
Episode: 24681/101000 (24.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0577s / 4090.2763 s
agent0:                 episode reward: 0.0127,                 loss: 0.1756
agent1:                 episode reward: -0.0127,                 loss: nan
Episode: 24701/101000 (24.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1699s / 4098.4462 s
agent0:                 episode reward: -0.0727,                 loss: 0.1751
agent1:                 episode reward: 0.0727,                 loss: nan
Episode: 24721/101000 (24.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1567s / 4105.6029 s
agent0:                 episode reward: -0.5625,                 loss: 0.1768
agent1:                 episode reward: 0.5625,                 loss: nan
Episode: 24741/101000 (24.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8380s / 4114.4409 s
agent0:                 episode reward: 0.2410,                 loss: 0.1742
agent1:                 episode reward: -0.2410,                 loss: nan
Episode: 24761/101000 (24.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7117s / 4121.1526 s
agent0:                 episode reward: -0.0221,                 loss: 0.1757
agent1:                 episode reward: 0.0221,                 loss: nan
Episode: 24781/101000 (24.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1469s / 4129.2995 s
agent0:                 episode reward: -0.3161,                 loss: 0.1759
agent1:                 episode reward: 0.3161,                 loss: nan
Episode: 24801/101000 (24.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0735s / 4137.3730 s
agent0:                 episode reward: -0.2296,                 loss: 0.1750
agent1:                 episode reward: 0.2296,                 loss: nan
Episode: 24821/101000 (24.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7683s / 4144.1413 s
agent0:                 episode reward: -0.1295,                 loss: 0.1793
agent1:                 episode reward: 0.1295,                 loss: 0.1517
Score delta: 1.6192353688373768, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/24381_0.
Episode: 24841/101000 (24.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0973s / 4151.2385 s
agent0:                 episode reward: -0.5339,                 loss: 0.2911
agent1:                 episode reward: 0.5339,                 loss: 0.1498
Score delta: 1.5842104481877186, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/24412_1.
Episode: 24861/101000 (24.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4470s / 4158.6855 s
agent0:                 episode reward: -0.5424,                 loss: 0.2494
agent1:                 episode reward: 0.5424,                 loss: nan
Episode: 24881/101000 (24.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1959s / 4165.8814 s
agent0:                 episode reward: -0.8682,                 loss: 0.1754
agent1:                 episode reward: 0.8682,                 loss: nan
Episode: 24901/101000 (24.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6272s / 4173.5086 s
agent0:                 episode reward: 0.1416,                 loss: 0.1725
agent1:                 episode reward: -0.1416,                 loss: nan
Episode: 24921/101000 (24.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1859s / 4180.6945 s
agent0:                 episode reward: -0.3485,                 loss: 0.1734
agent1:                 episode reward: 0.3485,                 loss: nan
Episode: 24941/101000 (24.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8791s / 4185.5736 s
agent0:                 episode reward: -0.8210,                 loss: 0.1729
agent1:                 episode reward: 0.8210,                 loss: nan
Episode: 24961/101000 (24.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3072s / 4192.8808 s
agent0:                 episode reward: -0.1954,                 loss: 0.1725
agent1:                 episode reward: 0.1954,                 loss: nan
Episode: 24981/101000 (24.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9097s / 4199.7905 s
agent0:                 episode reward: -0.3912,                 loss: 0.1732
agent1:                 episode reward: 0.3912,                 loss: nan
Episode: 25001/101000 (24.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5281s / 4206.3186 s
agent0:                 episode reward: -0.1216,                 loss: 0.1698
agent1:                 episode reward: 0.1216,                 loss: nan
Episode: 25021/101000 (24.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4973s / 4211.8159 s
agent0:                 episode reward: -0.0124,                 loss: 0.1719
agent1:                 episode reward: 0.0124,                 loss: nan
Episode: 25041/101000 (24.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7440s / 4217.5599 s
agent0:                 episode reward: -0.2753,                 loss: 0.1707
agent1:                 episode reward: 0.2753,                 loss: nan
Episode: 25061/101000 (24.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1817s / 4223.7416 s
agent0:                 episode reward: -0.7236,                 loss: 0.1714
agent1:                 episode reward: 0.7236,                 loss: nan
Episode: 25081/101000 (24.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5057s / 4229.2473 s
agent0:                 episode reward: -0.5882,                 loss: 0.1712
agent1:                 episode reward: 0.5882,                 loss: nan
Episode: 25101/101000 (24.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0362s / 4236.2836 s
agent0:                 episode reward: -0.3731,                 loss: 0.1705
agent1:                 episode reward: 0.3731,                 loss: nan
Episode: 25121/101000 (24.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8682s / 4244.1518 s
agent0:                 episode reward: -0.0819,                 loss: 0.1718
agent1:                 episode reward: 0.0819,                 loss: nan
Episode: 25141/101000 (24.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8434s / 4250.9952 s
agent0:                 episode reward: -0.5421,                 loss: 0.1710
agent1:                 episode reward: 0.5421,                 loss: nan
Episode: 25161/101000 (24.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6651s / 4258.6603 s
agent0:                 episode reward: -0.2658,                 loss: 0.1723
agent1:                 episode reward: 0.2658,                 loss: nan
Episode: 25181/101000 (24.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3080s / 4265.9684 s
agent0:                 episode reward: -0.5255,                 loss: 0.1705
agent1:                 episode reward: 0.5255,                 loss: nan
Episode: 25201/101000 (24.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5745s / 4272.5429 s
agent0:                 episode reward: -0.6412,                 loss: 0.1710
agent1:                 episode reward: 0.6412,                 loss: nan
Episode: 25221/101000 (24.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7550s / 4280.2979 s
agent0:                 episode reward: -0.4719,                 loss: 0.1702
agent1:                 episode reward: 0.4719,                 loss: nan
Episode: 25241/101000 (24.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7080s / 4288.0059 s
agent0:                 episode reward: -0.2350,                 loss: 0.1684
agent1:                 episode reward: 0.2350,                 loss: nan
Episode: 25261/101000 (25.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9584s / 4294.9643 s
agent0:                 episode reward: -0.1665,                 loss: 0.1703
agent1:                 episode reward: 0.1665,                 loss: nan
Episode: 25281/101000 (25.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3889s / 4301.3532 s
agent0:                 episode reward: -0.2091,                 loss: 0.1706
agent1:                 episode reward: 0.2091,                 loss: nan
Episode: 25301/101000 (25.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1269s / 4306.4801 s
agent0:                 episode reward: -0.0914,                 loss: 0.1677
agent1:                 episode reward: 0.0914,                 loss: nan
Episode: 25321/101000 (25.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2840s / 4313.7641 s
agent0:                 episode reward: -0.2400,                 loss: 0.1708
agent1:                 episode reward: 0.2400,                 loss: nan
Episode: 25341/101000 (25.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5458s / 4322.3099 s
agent0:                 episode reward: -0.3070,                 loss: 0.1698
agent1:                 episode reward: 0.3070,                 loss: nan
Episode: 25361/101000 (25.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6410s / 4329.9509 s
agent0:                 episode reward: -0.2942,                 loss: 0.1697
agent1:                 episode reward: 0.2942,                 loss: nan
Episode: 25381/101000 (25.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4210s / 4337.3720 s
agent0:                 episode reward: -0.2308,                 loss: 0.1670
agent1:                 episode reward: 0.2308,                 loss: nan
Episode: 25401/101000 (25.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5596s / 4341.9316 s
agent0:                 episode reward: -0.1556,                 loss: 0.1698
agent1:                 episode reward: 0.1556,                 loss: nan
Episode: 25421/101000 (25.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1241s / 4350.0557 s
agent0:                 episode reward: 0.3309,                 loss: 0.1693
agent1:                 episode reward: -0.3309,                 loss: nan
Episode: 25441/101000 (25.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6431s / 4357.6988 s
agent0:                 episode reward: -0.1605,                 loss: 0.1696
agent1:                 episode reward: 0.1605,                 loss: nan
Episode: 25461/101000 (25.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8772s / 4362.5759 s
agent0:                 episode reward: 0.2501,                 loss: 0.1694
agent1:                 episode reward: -0.2501,                 loss: nan
Episode: 25481/101000 (25.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5919s / 4369.1679 s
agent0:                 episode reward: -0.2174,                 loss: 0.1687
agent1:                 episode reward: 0.2174,                 loss: nan
Episode: 25501/101000 (25.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5032s / 4374.6711 s
agent0:                 episode reward: -0.3301,                 loss: 0.1699
agent1:                 episode reward: 0.3301,                 loss: nan
Episode: 25521/101000 (25.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5275s / 4383.1987 s
agent0:                 episode reward: -0.4126,                 loss: 0.1685
agent1:                 episode reward: 0.4126,                 loss: nan
Episode: 25541/101000 (25.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8844s / 4389.0831 s
agent0:                 episode reward: -0.6521,                 loss: 0.1689
agent1:                 episode reward: 0.6521,                 loss: nan
Episode: 25561/101000 (25.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3192s / 4396.4023 s
agent0:                 episode reward: -0.0059,                 loss: 0.1696
agent1:                 episode reward: 0.0059,                 loss: nan
Episode: 25581/101000 (25.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0625s / 4402.4648 s
agent0:                 episode reward: -0.1982,                 loss: 0.1680
agent1:                 episode reward: 0.1982,                 loss: nan
Episode: 25601/101000 (25.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9844s / 4410.4492 s
agent0:                 episode reward: -0.3375,                 loss: 0.1704
agent1:                 episode reward: 0.3375,                 loss: nan
Episode: 25621/101000 (25.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7716s / 4416.2208 s
agent0:                 episode reward: -0.1942,                 loss: 0.1697
agent1:                 episode reward: 0.1942,                 loss: nan
Episode: 25641/101000 (25.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2052s / 4420.4260 s
agent0:                 episode reward: -0.1033,                 loss: 0.1706
agent1:                 episode reward: 0.1033,                 loss: nan
Episode: 25661/101000 (25.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2364s / 4428.6623 s
agent0:                 episode reward: -0.2021,                 loss: 0.1699
agent1:                 episode reward: 0.2021,                 loss: nan
Episode: 25681/101000 (25.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3675s / 4437.0299 s
agent0:                 episode reward: 0.1204,                 loss: 0.1688
agent1:                 episode reward: -0.1204,                 loss: nan
Episode: 25701/101000 (25.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2424s / 4444.2722 s
agent0:                 episode reward: -0.3139,                 loss: 0.1697
agent1:                 episode reward: 0.3139,                 loss: nan
Episode: 25721/101000 (25.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5748s / 4448.8470 s
agent0:                 episode reward: -0.2106,                 loss: 0.1717
agent1:                 episode reward: 0.2106,                 loss: nan
Episode: 25741/101000 (25.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1235s / 4455.9705 s
agent0:                 episode reward: 0.0658,                 loss: 0.1672
agent1:                 episode reward: -0.0658,                 loss: nan
Episode: 25761/101000 (25.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7219s / 4464.6924 s
agent0:                 episode reward: -0.1758,                 loss: 0.1703
agent1:                 episode reward: 0.1758,                 loss: nan
Episode: 25781/101000 (25.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5320s / 4473.2245 s
agent0:                 episode reward: -0.7502,                 loss: 0.1680
agent1:                 episode reward: 0.7502,                 loss: nan
Episode: 25801/101000 (25.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2977s / 4481.5222 s
agent0:                 episode reward: -0.0512,                 loss: 0.1680
agent1:                 episode reward: 0.0512,                 loss: nan
Episode: 25821/101000 (25.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2475s / 4488.7697 s
agent0:                 episode reward: 0.3212,                 loss: 0.1669
agent1:                 episode reward: -0.3212,                 loss: nan
Episode: 25841/101000 (25.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6252s / 4496.3949 s
agent0:                 episode reward: -0.2813,                 loss: 0.1666
agent1:                 episode reward: 0.2813,                 loss: 0.1600
Score delta: 1.6885148924823714, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/25398_0.
Episode: 25861/101000 (25.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5918s / 4501.9867 s
agent0:                 episode reward: -0.4092,                 loss: 0.1649
agent1:                 episode reward: 0.4092,                 loss: 0.1606
Score delta: 1.5432636484409386, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/25427_1.
Episode: 25881/101000 (25.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0241s / 4508.0108 s
agent0:                 episode reward: -0.5194,                 loss: 0.1666
agent1:                 episode reward: 0.5194,                 loss: nan
Episode: 25901/101000 (25.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4698s / 4515.4806 s
agent0:                 episode reward: -0.2293,                 loss: 0.1692
agent1:                 episode reward: 0.2293,                 loss: nan
Episode: 25921/101000 (25.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9819s / 4523.4625 s
agent0:                 episode reward: -0.3432,                 loss: 0.1723
agent1:                 episode reward: 0.3432,                 loss: nan
Episode: 25941/101000 (25.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8851s / 4529.3476 s
agent0:                 episode reward: -0.1634,                 loss: 0.1685
agent1:                 episode reward: 0.1634,                 loss: nan
Episode: 25961/101000 (25.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3968s / 4535.7444 s
agent0:                 episode reward: 0.2746,                 loss: 0.1697
agent1:                 episode reward: -0.2746,                 loss: nan
Episode: 25981/101000 (25.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4915s / 4544.2360 s
agent0:                 episode reward: -0.1414,                 loss: 0.1720
agent1:                 episode reward: 0.1414,                 loss: nan
Episode: 26001/101000 (25.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5529s / 4550.7889 s
agent0:                 episode reward: 0.2965,                 loss: 0.1720
agent1:                 episode reward: -0.2965,                 loss: nan
Episode: 26021/101000 (25.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4426s / 4559.2315 s
agent0:                 episode reward: -0.3233,                 loss: 0.1712
agent1:                 episode reward: 0.3233,                 loss: 0.1592
Score delta: 1.605836830432492, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/25580_0.
Episode: 26041/101000 (25.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4250s / 4567.6565 s
agent0:                 episode reward: -0.6554,                 loss: nan
agent1:                 episode reward: 0.6554,                 loss: 0.1592
Episode: 26061/101000 (25.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1797s / 4573.8362 s
agent0:                 episode reward: -0.3170,                 loss: 0.1767
agent1:                 episode reward: 0.3170,                 loss: 0.1648
Score delta: 1.652088420152582, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/25616_1.
Episode: 26081/101000 (25.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0476s / 4579.8838 s
agent0:                 episode reward: -0.3758,                 loss: 0.1754
agent1:                 episode reward: 0.3758,                 loss: nan
Episode: 26101/101000 (25.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7554s / 4586.6393 s
agent0:                 episode reward: -0.4639,                 loss: 0.1753
agent1:                 episode reward: 0.4639,                 loss: nan
Episode: 26121/101000 (25.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7115s / 4595.3508 s
agent0:                 episode reward: 0.0004,                 loss: 0.1742
agent1:                 episode reward: -0.0004,                 loss: nan
Episode: 26141/101000 (25.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6358s / 4601.9866 s
agent0:                 episode reward: -0.5335,                 loss: 0.1782
agent1:                 episode reward: 0.5335,                 loss: nan
Episode: 26161/101000 (25.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6950s / 4610.6817 s
agent0:                 episode reward: -0.2471,                 loss: 0.1746
agent1:                 episode reward: 0.2471,                 loss: nan
Episode: 26181/101000 (25.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3527s / 4618.0344 s
agent0:                 episode reward: 0.1163,                 loss: 0.1741
agent1:                 episode reward: -0.1163,                 loss: nan
Episode: 26201/101000 (25.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2254s / 4626.2598 s
agent0:                 episode reward: -0.4884,                 loss: 0.1744
agent1:                 episode reward: 0.4884,                 loss: nan
Episode: 26221/101000 (25.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0369s / 4634.2967 s
agent0:                 episode reward: -0.4000,                 loss: 0.1771
agent1:                 episode reward: 0.4000,                 loss: nan
Episode: 26241/101000 (25.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9919s / 4641.2886 s
agent0:                 episode reward: 0.0791,                 loss: 0.1767
agent1:                 episode reward: -0.0791,                 loss: nan
Episode: 26261/101000 (26.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4683s / 4648.7569 s
agent0:                 episode reward: -0.0717,                 loss: 0.1738
agent1:                 episode reward: 0.0717,                 loss: nan
Episode: 26281/101000 (26.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4156s / 4655.1725 s
agent0:                 episode reward: 0.0077,                 loss: 0.1703
agent1:                 episode reward: -0.0077,                 loss: nan
Episode: 26301/101000 (26.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6635s / 4662.8360 s
agent0:                 episode reward: -0.5171,                 loss: 0.1689
agent1:                 episode reward: 0.5171,                 loss: nan
Episode: 26321/101000 (26.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0193s / 4670.8553 s
agent0:                 episode reward: -0.2448,                 loss: 0.1701
agent1:                 episode reward: 0.2448,                 loss: nan
Episode: 26341/101000 (26.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5914s / 4679.4467 s
agent0:                 episode reward: -0.2105,                 loss: 0.1684
agent1:                 episode reward: 0.2105,                 loss: nan
Episode: 26361/101000 (26.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2236s / 4686.6703 s
agent0:                 episode reward: -0.0929,                 loss: 0.1688
agent1:                 episode reward: 0.0929,                 loss: nan
Episode: 26381/101000 (26.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9096s / 4695.5799 s
agent0:                 episode reward: -0.0600,                 loss: 0.1696
agent1:                 episode reward: 0.0600,                 loss: nan
Episode: 26401/101000 (26.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8262s / 4701.4061 s
agent0:                 episode reward: 0.1092,                 loss: 0.1703
agent1:                 episode reward: -0.1092,                 loss: 0.1656
Score delta: 1.5072202008047033, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/25973_0.
Episode: 26421/101000 (26.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9521s / 4707.3582 s
agent0:                 episode reward: -0.1962,                 loss: nan
agent1:                 episode reward: 0.1962,                 loss: 0.1611
Episode: 26441/101000 (26.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2877s / 4714.6459 s
agent0:                 episode reward: -0.2660,                 loss: nan
agent1:                 episode reward: 0.2660,                 loss: 0.1597
Episode: 26461/101000 (26.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8138s / 4721.4597 s
agent0:                 episode reward: -0.5031,                 loss: nan
agent1:                 episode reward: 0.5031,                 loss: 0.1593
Episode: 26481/101000 (26.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0680s / 4726.5277 s
agent0:                 episode reward: -0.1058,                 loss: nan
agent1:                 episode reward: 0.1058,                 loss: 0.1596
Episode: 26501/101000 (26.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6120s / 4735.1397 s
agent0:                 episode reward: -0.6115,                 loss: 0.2824
agent1:                 episode reward: 0.6115,                 loss: 0.1597
Score delta: 1.770658562611199, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/26070_1.
Episode: 26521/101000 (26.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1991s / 4742.3388 s
agent0:                 episode reward: -0.3648,                 loss: 0.2564
agent1:                 episode reward: 0.3648,                 loss: nan
Episode: 26541/101000 (26.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4481s / 4750.7868 s
agent0:                 episode reward: -0.1554,                 loss: 0.2538
agent1:                 episode reward: 0.1554,                 loss: nan
Episode: 26561/101000 (26.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0545s / 4755.8413 s
agent0:                 episode reward: 0.1400,                 loss: 0.2577
agent1:                 episode reward: -0.1400,                 loss: 0.1394
Score delta: 1.5229174733891093, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/26126_0.
Episode: 26581/101000 (26.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6303s / 4761.4716 s
agent0:                 episode reward: -0.3331,                 loss: nan
agent1:                 episode reward: 0.3331,                 loss: 0.1385
Episode: 26601/101000 (26.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7268s / 4769.1984 s
agent0:                 episode reward: -0.3589,                 loss: 0.1549
agent1:                 episode reward: 0.3589,                 loss: 0.1373
Score delta: 1.5233737586667437, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/26171_1.
Episode: 26621/101000 (26.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3654s / 4777.5638 s
agent0:                 episode reward: -0.5944,                 loss: 0.1556
agent1:                 episode reward: 0.5944,                 loss: nan
Episode: 26641/101000 (26.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6739s / 4785.2377 s
agent0:                 episode reward: -0.1068,                 loss: 0.1558
agent1:                 episode reward: 0.1068,                 loss: nan
Episode: 26661/101000 (26.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9727s / 4792.2104 s
agent0:                 episode reward: -0.4251,                 loss: 0.1574
agent1:                 episode reward: 0.4251,                 loss: nan
Episode: 26681/101000 (26.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5857s / 4800.7961 s
agent0:                 episode reward: -0.0102,                 loss: 0.1576
agent1:                 episode reward: 0.0102,                 loss: nan
Episode: 26701/101000 (26.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2474s / 4809.0434 s
agent0:                 episode reward: -0.1705,                 loss: 0.1569
agent1:                 episode reward: 0.1705,                 loss: nan
Episode: 26721/101000 (26.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6591s / 4816.7025 s
agent0:                 episode reward: 0.1959,                 loss: 0.1565
agent1:                 episode reward: -0.1959,                 loss: nan
Episode: 26741/101000 (26.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7175s / 4824.4200 s
agent0:                 episode reward: -0.4133,                 loss: 0.1659
agent1:                 episode reward: 0.4133,                 loss: nan
Episode: 26761/101000 (26.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2574s / 4831.6774 s
agent0:                 episode reward: 0.0742,                 loss: 0.1781
agent1:                 episode reward: -0.0742,                 loss: nan
Episode: 26781/101000 (26.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0828s / 4838.7602 s
agent0:                 episode reward: -0.1490,                 loss: 0.1779
agent1:                 episode reward: 0.1490,                 loss: nan
Episode: 26801/101000 (26.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7823s / 4845.5425 s
agent0:                 episode reward: 0.1700,                 loss: 0.1782
agent1:                 episode reward: -0.1700,                 loss: nan
Episode: 26821/101000 (26.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3836s / 4852.9261 s
agent0:                 episode reward: -0.2585,                 loss: 0.1806
agent1:                 episode reward: 0.2585,                 loss: nan
Episode: 26841/101000 (26.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7245s / 4858.6506 s
agent0:                 episode reward: -0.3312,                 loss: 0.1788
agent1:                 episode reward: 0.3312,                 loss: nan
Episode: 26861/101000 (26.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3473s / 4865.9979 s
agent0:                 episode reward: -0.0074,                 loss: 0.1797
agent1:                 episode reward: 0.0074,                 loss: nan
Episode: 26881/101000 (26.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7003s / 4873.6983 s
agent0:                 episode reward: -0.1595,                 loss: 0.1793
agent1:                 episode reward: 0.1595,                 loss: nan
Episode: 26901/101000 (26.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9721s / 4879.6704 s
agent0:                 episode reward: -0.6719,                 loss: 0.1787
agent1:                 episode reward: 0.6719,                 loss: nan
Episode: 26921/101000 (26.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1842s / 4887.8546 s
agent0:                 episode reward: -0.2189,                 loss: 0.1795
agent1:                 episode reward: 0.2189,                 loss: nan
Episode: 26941/101000 (26.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9991s / 4896.8537 s
agent0:                 episode reward: -0.4911,                 loss: 0.1798
agent1:                 episode reward: 0.4911,                 loss: nan
Episode: 26961/101000 (26.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5769s / 4902.4305 s
agent0:                 episode reward: -0.4906,                 loss: 0.1792
agent1:                 episode reward: 0.4906,                 loss: nan
Episode: 26981/101000 (26.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1021s / 4911.5327 s
agent0:                 episode reward: -0.0777,                 loss: 0.1800
agent1:                 episode reward: 0.0777,                 loss: nan
Episode: 27001/101000 (26.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3970s / 4919.9296 s
agent0:                 episode reward: -0.1566,                 loss: 0.1803
agent1:                 episode reward: 0.1566,                 loss: nan
Episode: 27021/101000 (26.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0827s / 4928.0123 s
agent0:                 episode reward: -0.1754,                 loss: 0.1781
agent1:                 episode reward: 0.1754,                 loss: nan
Episode: 27041/101000 (26.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2315s / 4934.2438 s
agent0:                 episode reward: -0.2160,                 loss: 0.1804
agent1:                 episode reward: 0.2160,                 loss: nan
Episode: 27061/101000 (26.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0776s / 4942.3214 s
agent0:                 episode reward: -0.2781,                 loss: 0.1793
agent1:                 episode reward: 0.2781,                 loss: nan
Episode: 27081/101000 (26.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4677s / 4949.7891 s
agent0:                 episode reward: -0.1297,                 loss: 0.1713
agent1:                 episode reward: 0.1297,                 loss: nan
Episode: 27101/101000 (26.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9034s / 4956.6925 s
agent0:                 episode reward: 0.0310,                 loss: 0.1719
agent1:                 episode reward: -0.0310,                 loss: nan
Episode: 27121/101000 (26.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9012s / 4965.5937 s
agent0:                 episode reward: -0.1262,                 loss: 0.1721
agent1:                 episode reward: 0.1262,                 loss: nan
Episode: 27141/101000 (26.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6569s / 4974.2506 s
agent0:                 episode reward: 0.0615,                 loss: 0.1728
agent1:                 episode reward: -0.0615,                 loss: nan
Episode: 27161/101000 (26.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7800s / 4982.0305 s
agent0:                 episode reward: 0.0415,                 loss: 0.1701
agent1:                 episode reward: -0.0415,                 loss: nan
Episode: 27181/101000 (26.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6389s / 4989.6694 s
agent0:                 episode reward: -0.2102,                 loss: 0.1718
agent1:                 episode reward: 0.2102,                 loss: nan
Episode: 27201/101000 (26.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7366s / 4996.4060 s
agent0:                 episode reward: -0.4306,                 loss: 0.1712
agent1:                 episode reward: 0.4306,                 loss: nan
Episode: 27221/101000 (26.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8925s / 5004.2985 s
agent0:                 episode reward: -0.0102,                 loss: 0.1696
agent1:                 episode reward: 0.0102,                 loss: nan
Episode: 27241/101000 (26.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1675s / 5009.4660 s
agent0:                 episode reward: 0.1517,                 loss: 0.1705
agent1:                 episode reward: -0.1517,                 loss: nan
Episode: 27261/101000 (26.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0996s / 5016.5656 s
agent0:                 episode reward: -0.0734,                 loss: 0.1703
agent1:                 episode reward: 0.0734,                 loss: nan
Episode: 27281/101000 (27.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3692s / 5022.9348 s
agent0:                 episode reward: -0.1659,                 loss: 0.1703
agent1:                 episode reward: 0.1659,                 loss: nan
Episode: 27301/101000 (27.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3365s / 5029.2713 s
agent0:                 episode reward: -0.4056,                 loss: 0.1714
agent1:                 episode reward: 0.4056,                 loss: nan
Episode: 27321/101000 (27.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1707s / 5035.4420 s
agent0:                 episode reward: -0.1120,                 loss: 0.1703
agent1:                 episode reward: 0.1120,                 loss: nan
Episode: 27341/101000 (27.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7871s / 5042.2292 s
agent0:                 episode reward: -0.2411,                 loss: 0.1701
agent1:                 episode reward: 0.2411,                 loss: nan
Episode: 27361/101000 (27.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6100s / 5047.8392 s
agent0:                 episode reward: -0.5647,                 loss: 0.1699
agent1:                 episode reward: 0.5647,                 loss: nan
Episode: 27381/101000 (27.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0770s / 5054.9162 s
agent0:                 episode reward: 0.2296,                 loss: 0.1741
agent1:                 episode reward: -0.2296,                 loss: 0.1614
Score delta: 1.5400202169791846, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/26948_0.
Episode: 27401/101000 (27.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9170s / 5063.8332 s
agent0:                 episode reward: -0.3489,                 loss: nan
agent1:                 episode reward: 0.3489,                 loss: 0.1597
Episode: 27421/101000 (27.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4294s / 5069.2626 s
agent0:                 episode reward: -0.5602,                 loss: nan
agent1:                 episode reward: 0.5602,                 loss: 0.1543
Episode: 27441/101000 (27.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1904s / 5076.4530 s
agent0:                 episode reward: -0.2669,                 loss: nan
agent1:                 episode reward: 0.2669,                 loss: 0.1519
Episode: 27461/101000 (27.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1617s / 5083.6147 s
agent0:                 episode reward: -0.3479,                 loss: nan
agent1:                 episode reward: 0.3479,                 loss: 0.1521
Episode: 27481/101000 (27.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6822s / 5092.2969 s
agent0:                 episode reward: -0.3290,                 loss: nan
agent1:                 episode reward: 0.3290,                 loss: 0.1528
Episode: 27501/101000 (27.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5011s / 5098.7980 s
agent0:                 episode reward: -0.5840,                 loss: 0.2045
agent1:                 episode reward: 0.5840,                 loss: 0.1516
Score delta: 1.6258997258456156, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/27074_1.
Episode: 27521/101000 (27.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5316s / 5104.3296 s
agent0:                 episode reward: -0.0363,                 loss: 0.1918
agent1:                 episode reward: 0.0363,                 loss: nan
Episode: 27541/101000 (27.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1852s / 5112.5148 s
agent0:                 episode reward: -0.2072,                 loss: 0.1686
agent1:                 episode reward: 0.2072,                 loss: nan
Episode: 27561/101000 (27.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4755s / 5120.9902 s
agent0:                 episode reward: 0.4970,                 loss: 0.1639
agent1:                 episode reward: -0.4970,                 loss: nan
Episode: 27581/101000 (27.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5499s / 5128.5401 s
agent0:                 episode reward: -0.2058,                 loss: 0.1655
agent1:                 episode reward: 0.2058,                 loss: nan
Episode: 27601/101000 (27.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2943s / 5137.8344 s
agent0:                 episode reward: -0.4628,                 loss: 0.1630
agent1:                 episode reward: 0.4628,                 loss: nan
Episode: 27621/101000 (27.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5431s / 5145.3775 s
agent0:                 episode reward: 0.0887,                 loss: 0.1638
agent1:                 episode reward: -0.0887,                 loss: nan
Episode: 27641/101000 (27.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0210s / 5152.3986 s
agent0:                 episode reward: 0.0159,                 loss: 0.1622
agent1:                 episode reward: -0.0159,                 loss: nan
Episode: 27661/101000 (27.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0668s / 5160.4654 s
agent0:                 episode reward: -0.1441,                 loss: 0.1617
agent1:                 episode reward: 0.1441,                 loss: nan
Episode: 27681/101000 (27.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2656s / 5169.7309 s
agent0:                 episode reward: -0.1166,                 loss: 0.1631
agent1:                 episode reward: 0.1166,                 loss: nan
Episode: 27701/101000 (27.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7341s / 5178.4650 s
agent0:                 episode reward: -0.2085,                 loss: 0.1632
agent1:                 episode reward: 0.2085,                 loss: nan
Episode: 27721/101000 (27.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2377s / 5188.7027 s
agent0:                 episode reward: -0.5221,                 loss: 0.1630
agent1:                 episode reward: 0.5221,                 loss: nan
Episode: 27741/101000 (27.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4242s / 5195.1269 s
agent0:                 episode reward: 0.0586,                 loss: 0.1640
agent1:                 episode reward: -0.0586,                 loss: nan
Episode: 27761/101000 (27.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6739s / 5204.8008 s
agent0:                 episode reward: -0.2665,                 loss: 0.1637
agent1:                 episode reward: 0.2665,                 loss: nan
Episode: 27781/101000 (27.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9873s / 5212.7880 s
agent0:                 episode reward: -0.4494,                 loss: 0.1635
agent1:                 episode reward: 0.4494,                 loss: nan
Episode: 27801/101000 (27.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9670s / 5219.7551 s
agent0:                 episode reward: 0.3132,                 loss: 0.1643
agent1:                 episode reward: -0.3132,                 loss: nan
Episode: 27821/101000 (27.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4025s / 5225.1575 s
agent0:                 episode reward: -0.2991,                 loss: 0.1631
agent1:                 episode reward: 0.2991,                 loss: nan
Episode: 27841/101000 (27.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8252s / 5232.9827 s
agent0:                 episode reward: -0.2110,                 loss: 0.1650
agent1:                 episode reward: 0.2110,                 loss: nan
Episode: 27861/101000 (27.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1590s / 5240.1417 s
agent0:                 episode reward: 0.2130,                 loss: 0.1646
agent1:                 episode reward: -0.2130,                 loss: nan
Episode: 27881/101000 (27.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6712s / 5249.8129 s
agent0:                 episode reward: -0.6532,                 loss: 0.1736
agent1:                 episode reward: 0.6532,                 loss: nan
Episode: 27901/101000 (27.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4154s / 5256.2283 s
agent0:                 episode reward: -0.0079,                 loss: 0.1753
agent1:                 episode reward: 0.0079,                 loss: nan
Episode: 27921/101000 (27.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0524s / 5262.2807 s
agent0:                 episode reward: -0.5066,                 loss: 0.1736
agent1:                 episode reward: 0.5066,                 loss: nan
Episode: 27941/101000 (27.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1164s / 5270.3972 s
agent0:                 episode reward: 0.3421,                 loss: 0.1735
agent1:                 episode reward: -0.3421,                 loss: nan
Episode: 27961/101000 (27.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6339s / 5279.0310 s
agent0:                 episode reward: -0.2959,                 loss: 0.1733
agent1:                 episode reward: 0.2959,                 loss: nan
Episode: 27981/101000 (27.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0301s / 5288.0611 s
agent0:                 episode reward: -0.3157,                 loss: 0.1749
agent1:                 episode reward: 0.3157,                 loss: nan
Episode: 28001/101000 (27.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0993s / 5297.1604 s
agent0:                 episode reward: -0.2544,                 loss: 0.1748
agent1:                 episode reward: 0.2544,                 loss: nan
Episode: 28021/101000 (27.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1970s / 5302.3574 s
agent0:                 episode reward: -0.2882,                 loss: 0.1740
agent1:                 episode reward: 0.2882,                 loss: nan
Episode: 28041/101000 (27.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8712s / 5312.2286 s
agent0:                 episode reward: -0.2371,                 loss: 0.1752
agent1:                 episode reward: 0.2371,                 loss: nan
Episode: 28061/101000 (27.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8145s / 5321.0431 s
agent0:                 episode reward: -0.4054,                 loss: 0.1749
agent1:                 episode reward: 0.4054,                 loss: nan
Episode: 28081/101000 (27.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3084s / 5327.3514 s
agent0:                 episode reward: -0.2632,                 loss: 0.1745
agent1:                 episode reward: 0.2632,                 loss: nan
Episode: 28101/101000 (27.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3588s / 5334.7102 s
agent0:                 episode reward: -0.0892,                 loss: 0.1753
agent1:                 episode reward: 0.0892,                 loss: nan
Episode: 28121/101000 (27.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5015s / 5344.2117 s
agent0:                 episode reward: -0.2275,                 loss: 0.1752
agent1:                 episode reward: 0.2275,                 loss: nan
Episode: 28141/101000 (27.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6649s / 5352.8765 s
agent0:                 episode reward: -0.3553,                 loss: 0.1735
agent1:                 episode reward: 0.3553,                 loss: nan
Episode: 28161/101000 (27.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7613s / 5358.6378 s
agent0:                 episode reward: 0.2597,                 loss: 0.1729
agent1:                 episode reward: -0.2597,                 loss: nan
Episode: 28181/101000 (27.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1012s / 5365.7391 s
agent0:                 episode reward: -0.0323,                 loss: 0.1751
agent1:                 episode reward: 0.0323,                 loss: nan
Episode: 28201/101000 (27.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8767s / 5372.6158 s
agent0:                 episode reward: -0.1838,                 loss: 0.1724
agent1:                 episode reward: 0.1838,                 loss: nan
Episode: 28221/101000 (27.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8389s / 5380.4547 s
agent0:                 episode reward: -0.0510,                 loss: 0.1741
agent1:                 episode reward: 0.0510,                 loss: nan
Episode: 28241/101000 (27.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3684s / 5387.8231 s
agent0:                 episode reward: -0.0188,                 loss: 0.1728
agent1:                 episode reward: 0.0188,                 loss: nan
Episode: 28261/101000 (27.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7717s / 5395.5948 s
agent0:                 episode reward: -0.1596,                 loss: 0.1706
agent1:                 episode reward: 0.1596,                 loss: nan
Episode: 28281/101000 (28.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2754s / 5401.8702 s
agent0:                 episode reward: -0.0687,                 loss: 0.1737
agent1:                 episode reward: 0.0687,                 loss: nan
Episode: 28301/101000 (28.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1621s / 5410.0323 s
agent0:                 episode reward: 0.0948,                 loss: 0.1702
agent1:                 episode reward: -0.0948,                 loss: nan
Episode: 28321/101000 (28.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8410s / 5415.8733 s
agent0:                 episode reward: -0.1568,                 loss: 0.1740
agent1:                 episode reward: 0.1568,                 loss: nan
Episode: 28341/101000 (28.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1674s / 5424.0407 s
agent0:                 episode reward: -0.5175,                 loss: 0.1734
agent1:                 episode reward: 0.5175,                 loss: nan
Episode: 28361/101000 (28.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3079s / 5431.3485 s
agent0:                 episode reward: -0.2696,                 loss: 0.1721
agent1:                 episode reward: 0.2696,                 loss: nan
Episode: 28381/101000 (28.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3495s / 5439.6980 s
agent0:                 episode reward: -0.1151,                 loss: 0.1730
agent1:                 episode reward: 0.1151,                 loss: nan
Episode: 28401/101000 (28.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8493s / 5448.5474 s
agent0:                 episode reward: -0.5257,                 loss: 0.1720
agent1:                 episode reward: 0.5257,                 loss: nan
Episode: 28421/101000 (28.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4409s / 5453.9883 s
agent0:                 episode reward: -0.1563,                 loss: 0.1718
agent1:                 episode reward: 0.1563,                 loss: nan
Episode: 28441/101000 (28.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9037s / 5461.8920 s
agent0:                 episode reward: 0.1615,                 loss: 0.1721
agent1:                 episode reward: -0.1615,                 loss: nan
Episode: 28461/101000 (28.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2317s / 5470.1237 s
agent0:                 episode reward: -0.2578,                 loss: 0.1703
agent1:                 episode reward: 0.2578,                 loss: nan
Episode: 28481/101000 (28.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1453s / 5476.2690 s
agent0:                 episode reward: -0.4803,                 loss: 0.1713
agent1:                 episode reward: 0.4803,                 loss: nan
Episode: 28501/101000 (28.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2048s / 5483.4738 s
agent0:                 episode reward: -0.4117,                 loss: 0.1700
agent1:                 episode reward: 0.4117,                 loss: nan
Episode: 28521/101000 (28.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6048s / 5492.0785 s
agent0:                 episode reward: 0.2779,                 loss: 0.1727
agent1:                 episode reward: -0.2779,                 loss: nan
Episode: 28541/101000 (28.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8807s / 5500.9593 s
agent0:                 episode reward: -0.3888,                 loss: 0.1698
agent1:                 episode reward: 0.3888,                 loss: nan
Episode: 28561/101000 (28.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8906s / 5508.8499 s
agent0:                 episode reward: -0.0036,                 loss: 0.1676
agent1:                 episode reward: 0.0036,                 loss: nan
Episode: 28581/101000 (28.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2090s / 5517.0589 s
agent0:                 episode reward: -0.5114,                 loss: 0.1678
agent1:                 episode reward: 0.5114,                 loss: nan
Episode: 28601/101000 (28.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8167s / 5524.8756 s
agent0:                 episode reward: -0.0662,                 loss: 0.1686
agent1:                 episode reward: 0.0662,                 loss: nan
Episode: 28621/101000 (28.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5287s / 5532.4043 s
agent0:                 episode reward: -0.3153,                 loss: 0.1667
agent1:                 episode reward: 0.3153,                 loss: nan
Episode: 28641/101000 (28.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2063s / 5539.6106 s
agent0:                 episode reward: -0.1089,                 loss: 0.1674
agent1:                 episode reward: 0.1089,                 loss: nan
Episode: 28661/101000 (28.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4885s / 5545.0991 s
agent0:                 episode reward: -0.1761,                 loss: 0.1675
agent1:                 episode reward: 0.1761,                 loss: nan
Episode: 28681/101000 (28.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1919s / 5552.2910 s
agent0:                 episode reward: -0.2309,                 loss: 0.1684
agent1:                 episode reward: 0.2309,                 loss: nan
Episode: 28701/101000 (28.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2067s / 5558.4977 s
agent0:                 episode reward: -0.2955,                 loss: 0.1669
agent1:                 episode reward: 0.2955,                 loss: nan
Episode: 28721/101000 (28.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0015s / 5566.4992 s
agent0:                 episode reward: -0.5411,                 loss: 0.1675
agent1:                 episode reward: 0.5411,                 loss: nan
Episode: 28741/101000 (28.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8043s / 5574.3035 s
agent0:                 episode reward: -0.5010,                 loss: 0.1686
agent1:                 episode reward: 0.5010,                 loss: nan
Episode: 28761/101000 (28.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1189s / 5581.4224 s
agent0:                 episode reward: -0.0129,                 loss: 0.1672
agent1:                 episode reward: 0.0129,                 loss: nan
Episode: 28781/101000 (28.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6678s / 5589.0902 s
agent0:                 episode reward: -0.3657,                 loss: 0.1679
agent1:                 episode reward: 0.3657,                 loss: nan
Episode: 28801/101000 (28.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2196s / 5595.3098 s
agent0:                 episode reward: -0.3320,                 loss: 0.1675
agent1:                 episode reward: 0.3320,                 loss: nan
Episode: 28821/101000 (28.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5511s / 5601.8609 s
agent0:                 episode reward: -0.0639,                 loss: 0.1678
agent1:                 episode reward: 0.0639,                 loss: nan
Episode: 28841/101000 (28.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1268s / 5609.9877 s
agent0:                 episode reward: -0.3591,                 loss: 0.1698
agent1:                 episode reward: 0.3591,                 loss: nan
Episode: 28861/101000 (28.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7759s / 5617.7636 s
agent0:                 episode reward: -0.2310,                 loss: 0.1692
agent1:                 episode reward: 0.2310,                 loss: nan
Episode: 28881/101000 (28.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4229s / 5625.1865 s
agent0:                 episode reward: 0.0950,                 loss: 0.1728
agent1:                 episode reward: -0.0950,                 loss: nan
Episode: 28901/101000 (28.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9269s / 5632.1134 s
agent0:                 episode reward: -0.3493,                 loss: 0.1723
agent1:                 episode reward: 0.3493,                 loss: nan
Episode: 28921/101000 (28.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6970s / 5638.8104 s
agent0:                 episode reward: -0.4887,                 loss: 0.1744
agent1:                 episode reward: 0.4887,                 loss: nan
Episode: 28941/101000 (28.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4248s / 5645.2352 s
agent0:                 episode reward: -0.5457,                 loss: 0.1735
agent1:                 episode reward: 0.5457,                 loss: nan
Episode: 28961/101000 (28.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7287s / 5653.9639 s
agent0:                 episode reward: -0.1262,                 loss: 0.1737
agent1:                 episode reward: 0.1262,                 loss: nan
Episode: 28981/101000 (28.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3334s / 5661.2973 s
agent0:                 episode reward: -0.0786,                 loss: 0.1732
agent1:                 episode reward: 0.0786,                 loss: nan
Episode: 29001/101000 (28.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2945s / 5669.5918 s
agent0:                 episode reward: -0.5179,                 loss: 0.1732
agent1:                 episode reward: 0.5179,                 loss: nan
Episode: 29021/101000 (28.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6717s / 5674.2635 s
agent0:                 episode reward: -0.0617,                 loss: 0.1736
agent1:                 episode reward: 0.0617,                 loss: nan
Episode: 29041/101000 (28.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4067s / 5682.6702 s
agent0:                 episode reward: 0.0249,                 loss: 0.1703
agent1:                 episode reward: -0.0249,                 loss: nan
Episode: 29061/101000 (28.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0907s / 5688.7609 s
agent0:                 episode reward: 0.4229,                 loss: 0.1732
agent1:                 episode reward: -0.4229,                 loss: nan
Episode: 29081/101000 (28.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9001s / 5696.6610 s
agent0:                 episode reward: -0.6386,                 loss: 0.1724
agent1:                 episode reward: 0.6386,                 loss: nan
Episode: 29101/101000 (28.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5895s / 5706.2505 s
agent0:                 episode reward: -0.1074,                 loss: 0.1725
agent1:                 episode reward: 0.1074,                 loss: nan
Episode: 29121/101000 (28.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5120s / 5713.7626 s
agent0:                 episode reward: -0.6971,                 loss: 0.1726
agent1:                 episode reward: 0.6971,                 loss: nan
Episode: 29141/101000 (28.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4522s / 5720.2148 s
agent0:                 episode reward: 0.0671,                 loss: 0.1741
agent1:                 episode reward: -0.0671,                 loss: nan
Episode: 29161/101000 (28.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8551s / 5729.0699 s
agent0:                 episode reward: 0.0383,                 loss: 0.1733
agent1:                 episode reward: -0.0383,                 loss: nan
Episode: 29181/101000 (28.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9466s / 5736.0165 s
agent0:                 episode reward: -0.3049,                 loss: 0.1739
agent1:                 episode reward: 0.3049,                 loss: nan
Episode: 29201/101000 (28.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9725s / 5742.9890 s
agent0:                 episode reward: -0.0779,                 loss: 0.1745
agent1:                 episode reward: 0.0779,                 loss: nan
Episode: 29221/101000 (28.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5133s / 5750.5023 s
agent0:                 episode reward: -0.4430,                 loss: 0.1726
agent1:                 episode reward: 0.4430,                 loss: nan
Episode: 29241/101000 (28.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7799s / 5759.2822 s
agent0:                 episode reward: -0.1973,                 loss: 0.1721
agent1:                 episode reward: 0.1973,                 loss: nan
Episode: 29261/101000 (28.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2002s / 5769.4824 s
agent0:                 episode reward: -0.3705,                 loss: 0.1723
agent1:                 episode reward: 0.3705,                 loss: nan
Episode: 29281/101000 (28.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8763s / 5777.3588 s
agent0:                 episode reward: -0.4958,                 loss: 0.1729
agent1:                 episode reward: 0.4958,                 loss: nan
Episode: 29301/101000 (29.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1853s / 5784.5441 s
agent0:                 episode reward: -0.4364,                 loss: 0.1738
agent1:                 episode reward: 0.4364,                 loss: nan
Episode: 29321/101000 (29.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4871s / 5793.0312 s
agent0:                 episode reward: 0.0005,                 loss: 0.1748
agent1:                 episode reward: -0.0005,                 loss: nan
Episode: 29341/101000 (29.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3274s / 5801.3586 s
agent0:                 episode reward: -0.2194,                 loss: 0.1720
agent1:                 episode reward: 0.2194,                 loss: nan
Episode: 29361/101000 (29.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5830s / 5806.9416 s
agent0:                 episode reward: -0.1494,                 loss: 0.1731
agent1:                 episode reward: 0.1494,                 loss: nan
Episode: 29381/101000 (29.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2095s / 5814.1511 s
agent0:                 episode reward: 0.0620,                 loss: 0.1746
agent1:                 episode reward: -0.0620,                 loss: nan
Episode: 29401/101000 (29.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3959s / 5821.5470 s
agent0:                 episode reward: -0.3202,                 loss: 0.1716
agent1:                 episode reward: 0.3202,                 loss: nan
Episode: 29421/101000 (29.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5252s / 5830.0722 s
agent0:                 episode reward: -0.2288,                 loss: 0.1740
agent1:                 episode reward: 0.2288,                 loss: nan
Episode: 29441/101000 (29.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4672s / 5836.5394 s
agent0:                 episode reward: 0.6218,                 loss: 0.1742
agent1:                 episode reward: -0.6218,                 loss: nan
Score delta: 1.6591793069383243, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/29015_0.
Episode: 29461/101000 (29.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9807s / 5843.5201 s
agent0:                 episode reward: -0.1829,                 loss: nan
agent1:                 episode reward: 0.1829,                 loss: 0.1582
Episode: 29481/101000 (29.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7554s / 5850.2755 s
agent0:                 episode reward: -0.4130,                 loss: 0.1726
agent1:                 episode reward: 0.4130,                 loss: 0.1602
Score delta: 1.688336243768869, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/29049_1.
Episode: 29501/101000 (29.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1961s / 5856.4716 s
agent0:                 episode reward: 0.0397,                 loss: 0.1703
agent1:                 episode reward: -0.0397,                 loss: nan
Episode: 29521/101000 (29.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6747s / 5862.1462 s
agent0:                 episode reward: -0.4435,                 loss: 0.1697
agent1:                 episode reward: 0.4435,                 loss: nan
Episode: 29541/101000 (29.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5183s / 5869.6646 s
agent0:                 episode reward: -0.0938,                 loss: 0.1712
agent1:                 episode reward: 0.0938,                 loss: nan
Episode: 29561/101000 (29.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8706s / 5874.5352 s
agent0:                 episode reward: -0.6916,                 loss: 0.1715
agent1:                 episode reward: 0.6916,                 loss: nan
Episode: 29581/101000 (29.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4421s / 5880.9773 s
agent0:                 episode reward: -0.1672,                 loss: 0.1731
agent1:                 episode reward: 0.1672,                 loss: nan
Episode: 29601/101000 (29.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7575s / 5885.7348 s
agent0:                 episode reward: -0.3723,                 loss: 0.1781
agent1:                 episode reward: 0.3723,                 loss: nan
Episode: 29621/101000 (29.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4592s / 5889.1940 s
agent0:                 episode reward: -0.1228,                 loss: 0.1770
agent1:                 episode reward: 0.1228,                 loss: nan
Episode: 29641/101000 (29.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1608s / 5898.3547 s
agent0:                 episode reward: -0.1608,                 loss: 0.1769
agent1:                 episode reward: 0.1608,                 loss: nan
Episode: 29661/101000 (29.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2726s / 5908.6274 s
agent0:                 episode reward: -0.1564,                 loss: 0.1767
agent1:                 episode reward: 0.1564,                 loss: nan
Episode: 29681/101000 (29.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7618s / 5914.3892 s
agent0:                 episode reward: -0.4003,                 loss: 0.1758
agent1:                 episode reward: 0.4003,                 loss: nan
Episode: 29701/101000 (29.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3016s / 5923.6908 s
agent0:                 episode reward: -0.0841,                 loss: 0.1759
agent1:                 episode reward: 0.0841,                 loss: nan
Episode: 29721/101000 (29.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5437s / 5932.2345 s
agent0:                 episode reward: -0.2898,                 loss: 0.1766
agent1:                 episode reward: 0.2898,                 loss: nan
Episode: 29741/101000 (29.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8599s / 5939.0944 s
agent0:                 episode reward: 0.3424,                 loss: 0.1777
agent1:                 episode reward: -0.3424,                 loss: 0.1620
Score delta: 1.6630818747432374, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/29305_0.
Episode: 29761/101000 (29.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2624s / 5946.3568 s
agent0:                 episode reward: -0.6330,                 loss: nan
agent1:                 episode reward: 0.6330,                 loss: 0.1578
Episode: 29781/101000 (29.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7625s / 5953.1193 s
agent0:                 episode reward: -0.5874,                 loss: 0.2642
agent1:                 episode reward: 0.5874,                 loss: 0.1597
Score delta: 2.2784964702208512, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/29340_1.
Episode: 29801/101000 (29.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3739s / 5961.4933 s
agent0:                 episode reward: 0.0895,                 loss: 0.2549
agent1:                 episode reward: -0.0895,                 loss: nan
Episode: 29821/101000 (29.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1012s / 5969.5945 s
agent0:                 episode reward: -0.4936,                 loss: 0.2538
agent1:                 episode reward: 0.4936,                 loss: nan
Episode: 29841/101000 (29.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0301s / 5977.6245 s
agent0:                 episode reward: -0.2118,                 loss: 0.2541
agent1:                 episode reward: 0.2118,                 loss: nan
Episode: 29861/101000 (29.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2389s / 5985.8634 s
agent0:                 episode reward: 0.0717,                 loss: 0.2548
agent1:                 episode reward: -0.0717,                 loss: nan
Episode: 29881/101000 (29.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2291s / 5993.0925 s
agent0:                 episode reward: -0.0547,                 loss: 0.2535
agent1:                 episode reward: 0.0547,                 loss: nan
Episode: 29901/101000 (29.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4887s / 6002.5812 s
agent0:                 episode reward: -0.3473,                 loss: 0.2525
agent1:                 episode reward: 0.3473,                 loss: nan
Episode: 29921/101000 (29.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5984s / 6013.1796 s
agent0:                 episode reward: -0.4068,                 loss: 0.2521
agent1:                 episode reward: 0.4068,                 loss: nan
Episode: 29941/101000 (29.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5629s / 6019.7425 s
agent0:                 episode reward: -0.1992,                 loss: 0.2004
agent1:                 episode reward: 0.1992,                 loss: nan
Episode: 29961/101000 (29.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5281s / 6028.2707 s
agent0:                 episode reward: -0.2303,                 loss: 0.1694
agent1:                 episode reward: 0.2303,                 loss: nan
Episode: 29981/101000 (29.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6900s / 6034.9606 s
agent0:                 episode reward: -0.3910,                 loss: 0.1686
agent1:                 episode reward: 0.3910,                 loss: nan
Episode: 30001/101000 (29.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2243s / 6041.1850 s
agent0:                 episode reward: 0.1858,                 loss: 0.1694
agent1:                 episode reward: -0.1858,                 loss: nan
Episode: 30021/101000 (29.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5302s / 6050.7152 s
agent0:                 episode reward: -0.2098,                 loss: 0.1691
agent1:                 episode reward: 0.2098,                 loss: nan
Episode: 30041/101000 (29.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5748s / 6058.2900 s
agent0:                 episode reward: -0.0815,                 loss: 0.1671
agent1:                 episode reward: 0.0815,                 loss: nan
Episode: 30061/101000 (29.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8467s / 6067.1367 s
agent0:                 episode reward: -0.5908,                 loss: 0.1685
agent1:                 episode reward: 0.5908,                 loss: nan
Episode: 30081/101000 (29.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6678s / 6076.8046 s
agent0:                 episode reward: -0.1820,                 loss: 0.1707
agent1:                 episode reward: 0.1820,                 loss: nan
Episode: 30101/101000 (29.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4929s / 6085.2975 s
agent0:                 episode reward: -0.4008,                 loss: 0.1678
agent1:                 episode reward: 0.4008,                 loss: nan
Episode: 30121/101000 (29.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6728s / 6094.9703 s
agent0:                 episode reward: -0.0835,                 loss: 0.1693
agent1:                 episode reward: 0.0835,                 loss: nan
Episode: 30141/101000 (29.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0950s / 6103.0654 s
agent0:                 episode reward: -0.2958,                 loss: 0.1697
agent1:                 episode reward: 0.2958,                 loss: nan
Episode: 30161/101000 (29.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7278s / 6111.7932 s
agent0:                 episode reward: -0.6714,                 loss: 0.1689
agent1:                 episode reward: 0.6714,                 loss: nan
Episode: 30181/101000 (29.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5188s / 6121.3119 s
agent0:                 episode reward: 0.3255,                 loss: 0.1680
agent1:                 episode reward: -0.3255,                 loss: nan
Episode: 30201/101000 (29.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6873s / 6128.9992 s
agent0:                 episode reward: -0.4050,                 loss: 0.1689
agent1:                 episode reward: 0.4050,                 loss: nan
Episode: 30221/101000 (29.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5657s / 6137.5649 s
agent0:                 episode reward: -0.4467,                 loss: 0.1690
agent1:                 episode reward: 0.4467,                 loss: nan
Episode: 30241/101000 (29.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4765s / 6143.0415 s
agent0:                 episode reward: -0.3172,                 loss: 0.1666
agent1:                 episode reward: 0.3172,                 loss: nan
Episode: 30261/101000 (29.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4251s / 6152.4666 s
agent0:                 episode reward: -0.1811,                 loss: 0.1681
agent1:                 episode reward: 0.1811,                 loss: nan
Episode: 30281/101000 (29.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2727s / 6159.7393 s
agent0:                 episode reward: 0.0388,                 loss: 0.1756
agent1:                 episode reward: -0.0388,                 loss: nan
Episode: 30301/101000 (30.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4090s / 6167.1483 s
agent0:                 episode reward: -0.4682,                 loss: 0.1752
agent1:                 episode reward: 0.4682,                 loss: nan
Episode: 30321/101000 (30.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5090s / 6175.6573 s
agent0:                 episode reward: -0.3363,                 loss: 0.1755
agent1:                 episode reward: 0.3363,                 loss: nan
Episode: 30341/101000 (30.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4069s / 6182.0642 s
agent0:                 episode reward: -0.1929,                 loss: 0.1741
agent1:                 episode reward: 0.1929,                 loss: nan
Episode: 30361/101000 (30.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7667s / 6189.8309 s
agent0:                 episode reward: -0.2051,                 loss: 0.1729
agent1:                 episode reward: 0.2051,                 loss: nan
Episode: 30381/101000 (30.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9715s / 6195.8024 s
agent0:                 episode reward: -0.0321,                 loss: 0.1749
agent1:                 episode reward: 0.0321,                 loss: nan
Episode: 30401/101000 (30.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0489s / 6204.8513 s
agent0:                 episode reward: -0.0584,                 loss: 0.1751
agent1:                 episode reward: 0.0584,                 loss: nan
Episode: 30421/101000 (30.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2352s / 6212.0865 s
agent0:                 episode reward: -0.1510,                 loss: 0.1751
agent1:                 episode reward: 0.1510,                 loss: nan
Episode: 30441/101000 (30.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4277s / 6220.5143 s
agent0:                 episode reward: -0.2841,                 loss: 0.1720
agent1:                 episode reward: 0.2841,                 loss: nan
Episode: 30461/101000 (30.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2778s / 6227.7920 s
agent0:                 episode reward: -0.5182,                 loss: 0.1745
agent1:                 episode reward: 0.5182,                 loss: nan
Episode: 30481/101000 (30.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6283s / 6236.4203 s
agent0:                 episode reward: 0.0408,                 loss: 0.1738
agent1:                 episode reward: -0.0408,                 loss: nan
Episode: 30501/101000 (30.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1143s / 6245.5346 s
agent0:                 episode reward: -0.0134,                 loss: 0.1743
agent1:                 episode reward: 0.0134,                 loss: nan
Episode: 30521/101000 (30.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2323s / 6251.7668 s
agent0:                 episode reward: 0.2046,                 loss: 0.1743
agent1:                 episode reward: -0.2046,                 loss: nan
Episode: 30541/101000 (30.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2209s / 6258.9878 s
agent0:                 episode reward: -0.1828,                 loss: 0.1763
agent1:                 episode reward: 0.1828,                 loss: nan
Episode: 30561/101000 (30.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6058s / 6265.5936 s
agent0:                 episode reward: 0.1518,                 loss: 0.1732
agent1:                 episode reward: -0.1518,                 loss: nan
Episode: 30581/101000 (30.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1826s / 6273.7762 s
agent0:                 episode reward: 0.0666,                 loss: 0.1738
agent1:                 episode reward: -0.0666,                 loss: nan
Episode: 30601/101000 (30.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5464s / 6284.3225 s
agent0:                 episode reward: -0.3133,                 loss: 0.1748
agent1:                 episode reward: 0.3133,                 loss: nan
Episode: 30621/101000 (30.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8914s / 6292.2140 s
agent0:                 episode reward: -0.3907,                 loss: 0.1744
agent1:                 episode reward: 0.3907,                 loss: nan
Episode: 30641/101000 (30.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3515s / 6299.5655 s
agent0:                 episode reward: -0.1133,                 loss: 0.1778
agent1:                 episode reward: 0.1133,                 loss: nan
Episode: 30661/101000 (30.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8017s / 6306.3672 s
agent0:                 episode reward: -0.3075,                 loss: 0.1761
agent1:                 episode reward: 0.3075,                 loss: nan
Episode: 30681/101000 (30.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1073s / 6313.4745 s
agent0:                 episode reward: -0.4413,                 loss: 0.1762
agent1:                 episode reward: 0.4413,                 loss: nan
Episode: 30701/101000 (30.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5935s / 6322.0680 s
agent0:                 episode reward: -0.2585,                 loss: 0.1759
agent1:                 episode reward: 0.2585,                 loss: nan
Episode: 30721/101000 (30.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9324s / 6330.0004 s
agent0:                 episode reward: -0.2097,                 loss: 0.1743
agent1:                 episode reward: 0.2097,                 loss: nan
Episode: 30741/101000 (30.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5365s / 6336.5369 s
agent0:                 episode reward: -0.1963,                 loss: 0.1764
agent1:                 episode reward: 0.1963,                 loss: nan
Episode: 30761/101000 (30.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8491s / 6345.3860 s
agent0:                 episode reward: -0.3601,                 loss: 0.1754
agent1:                 episode reward: 0.3601,                 loss: nan
Episode: 30781/101000 (30.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6988s / 6353.0849 s
agent0:                 episode reward: 0.7399,                 loss: 0.1752
agent1:                 episode reward: -0.7399,                 loss: 0.1759
Score delta: 1.601914324286399, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/30348_0.
Episode: 30801/101000 (30.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3961s / 6359.4810 s
agent0:                 episode reward: -0.3446,                 loss: nan
agent1:                 episode reward: 0.3446,                 loss: 0.1594
Episode: 30821/101000 (30.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2492s / 6365.7303 s
agent0:                 episode reward: -0.6375,                 loss: nan
agent1:                 episode reward: 0.6375,                 loss: 0.1575
Score delta: 2.094154257090303, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/30395_1.
Episode: 30841/101000 (30.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5982s / 6374.3285 s
agent0:                 episode reward: -0.0864,                 loss: 0.1973
agent1:                 episode reward: 0.0864,                 loss: nan
Episode: 30861/101000 (30.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8978s / 6380.2263 s
agent0:                 episode reward: -0.0377,                 loss: 0.1945
agent1:                 episode reward: 0.0377,                 loss: nan
Episode: 30881/101000 (30.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8419s / 6386.0682 s
agent0:                 episode reward: -0.3901,                 loss: 0.1926
agent1:                 episode reward: 0.3901,                 loss: nan
Episode: 30901/101000 (30.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 11.0057s / 6397.0739 s
agent0:                 episode reward: -0.7491,                 loss: 0.1933
agent1:                 episode reward: 0.7491,                 loss: nan
Episode: 30921/101000 (30.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8403s / 6403.9142 s
agent0:                 episode reward: 0.0732,                 loss: 0.1933
agent1:                 episode reward: -0.0732,                 loss: nan
Episode: 30941/101000 (30.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5852s / 6412.4993 s
agent0:                 episode reward: -0.1418,                 loss: 0.1937
agent1:                 episode reward: 0.1418,                 loss: nan
Episode: 30961/101000 (30.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5456s / 6418.0450 s
agent0:                 episode reward: -0.3359,                 loss: 0.1940
agent1:                 episode reward: 0.3359,                 loss: nan
Episode: 30981/101000 (30.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0745s / 6425.1195 s
agent0:                 episode reward: -0.0743,                 loss: 0.1897
agent1:                 episode reward: 0.0743,                 loss: nan
Episode: 31001/101000 (30.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0625s / 6434.1820 s
agent0:                 episode reward: -0.4553,                 loss: 0.1763
agent1:                 episode reward: 0.4553,                 loss: nan
Episode: 31021/101000 (30.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5471s / 6440.7291 s
agent0:                 episode reward: -0.3549,                 loss: 0.1758
agent1:                 episode reward: 0.3549,                 loss: nan
Episode: 31041/101000 (30.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3926s / 6447.1217 s
agent0:                 episode reward: -0.5012,                 loss: 0.1762
agent1:                 episode reward: 0.5012,                 loss: nan
Episode: 31061/101000 (30.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9009s / 6456.0226 s
agent0:                 episode reward: -0.1628,                 loss: 0.1790
agent1:                 episode reward: 0.1628,                 loss: nan
Episode: 31081/101000 (30.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4129s / 6462.4355 s
agent0:                 episode reward: -0.1644,                 loss: 0.1757
agent1:                 episode reward: 0.1644,                 loss: nan
Episode: 31101/101000 (30.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0862s / 6470.5217 s
agent0:                 episode reward: -0.1359,                 loss: 0.1756
agent1:                 episode reward: 0.1359,                 loss: nan
Episode: 31121/101000 (30.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8518s / 6479.3735 s
agent0:                 episode reward: 0.2367,                 loss: 0.1753
agent1:                 episode reward: -0.2367,                 loss: nan
Episode: 31141/101000 (30.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3194s / 6488.6929 s
agent0:                 episode reward: -0.1616,                 loss: 0.1757
agent1:                 episode reward: 0.1616,                 loss: nan
Episode: 31161/101000 (30.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0859s / 6496.7788 s
agent0:                 episode reward: -0.1647,                 loss: 0.1763
agent1:                 episode reward: 0.1647,                 loss: nan
Episode: 31181/101000 (30.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6160s / 6504.3948 s
agent0:                 episode reward: -0.5827,                 loss: 0.1755
agent1:                 episode reward: 0.5827,                 loss: nan
Episode: 31201/101000 (30.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3250s / 6511.7198 s
agent0:                 episode reward: 0.0825,                 loss: 0.1758
agent1:                 episode reward: -0.0825,                 loss: nan
Episode: 31221/101000 (30.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2188s / 6519.9385 s
agent0:                 episode reward: -0.3545,                 loss: 0.1753
agent1:                 episode reward: 0.3545,                 loss: nan
Episode: 31241/101000 (30.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8937s / 6526.8322 s
agent0:                 episode reward: -0.4277,                 loss: 0.1767
agent1:                 episode reward: 0.4277,                 loss: nan
Episode: 31261/101000 (30.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2167s / 6534.0489 s
agent0:                 episode reward: -0.1265,                 loss: 0.1763
agent1:                 episode reward: 0.1265,                 loss: nan
Episode: 31281/101000 (30.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4949s / 6541.5437 s
agent0:                 episode reward: 0.0986,                 loss: 0.1781
agent1:                 episode reward: -0.0986,                 loss: nan
Episode: 31301/101000 (30.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8494s / 6550.3932 s
agent0:                 episode reward: -0.2331,                 loss: 0.1776
agent1:                 episode reward: 0.2331,                 loss: nan
Episode: 31321/101000 (31.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5957s / 6557.9888 s
agent0:                 episode reward: -0.0551,                 loss: 0.1765
agent1:                 episode reward: 0.0551,                 loss: nan
Episode: 31341/101000 (31.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4806s / 6565.4695 s
agent0:                 episode reward: 0.4513,                 loss: 0.1769
agent1:                 episode reward: -0.4513,                 loss: nan
Episode: 31361/101000 (31.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7221s / 6572.1915 s
agent0:                 episode reward: -0.0994,                 loss: 0.1768
agent1:                 episode reward: 0.0994,                 loss: nan
Episode: 31381/101000 (31.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0761s / 6579.2677 s
agent0:                 episode reward: 0.3718,                 loss: 0.1771
agent1:                 episode reward: -0.3718,                 loss: nan
Episode: 31401/101000 (31.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7530s / 6586.0207 s
agent0:                 episode reward: -0.0086,                 loss: 0.1775
agent1:                 episode reward: 0.0086,                 loss: nan
Episode: 31421/101000 (31.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0079s / 6594.0286 s
agent0:                 episode reward: 0.0858,                 loss: 0.1761
agent1:                 episode reward: -0.0858,                 loss: nan
Episode: 31441/101000 (31.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9121s / 6601.9407 s
agent0:                 episode reward: 0.2063,                 loss: 0.1780
agent1:                 episode reward: -0.2063,                 loss: nan
Episode: 31461/101000 (31.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1004s / 6611.0410 s
agent0:                 episode reward: 0.1289,                 loss: 0.1771
agent1:                 episode reward: -0.1289,                 loss: nan
Episode: 31481/101000 (31.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7868s / 6620.8278 s
agent0:                 episode reward: -0.8363,                 loss: 0.1778
agent1:                 episode reward: 0.8363,                 loss: nan
Episode: 31501/101000 (31.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8246s / 6629.6524 s
agent0:                 episode reward: -0.6419,                 loss: 0.1738
agent1:                 episode reward: 0.6419,                 loss: nan
Episode: 31521/101000 (31.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4611s / 6637.1135 s
agent0:                 episode reward: -0.9115,                 loss: 0.1771
agent1:                 episode reward: 0.9115,                 loss: nan
Episode: 31541/101000 (31.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6423s / 6646.7558 s
agent0:                 episode reward: -0.2451,                 loss: 0.1742
agent1:                 episode reward: 0.2451,                 loss: nan
Episode: 31561/101000 (31.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1272s / 6654.8830 s
agent0:                 episode reward: -0.3731,                 loss: 0.1758
agent1:                 episode reward: 0.3731,                 loss: nan
Episode: 31581/101000 (31.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7864s / 6663.6694 s
agent0:                 episode reward: -0.0893,                 loss: 0.1760
agent1:                 episode reward: 0.0893,                 loss: nan
Episode: 31601/101000 (31.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8433s / 6668.5127 s
agent0:                 episode reward: -0.1261,                 loss: 0.1768
agent1:                 episode reward: 0.1261,                 loss: nan
Episode: 31621/101000 (31.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8675s / 6675.3802 s
agent0:                 episode reward: -0.1190,                 loss: 0.1760
agent1:                 episode reward: 0.1190,                 loss: nan
Episode: 31641/101000 (31.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1104s / 6683.4906 s
agent0:                 episode reward: -0.0107,                 loss: 0.1746
agent1:                 episode reward: 0.0107,                 loss: nan
Episode: 31661/101000 (31.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0366s / 6691.5272 s
agent0:                 episode reward: 0.0428,                 loss: 0.1777
agent1:                 episode reward: -0.0428,                 loss: nan
Episode: 31681/101000 (31.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0034s / 6699.5306 s
agent0:                 episode reward: -0.0923,                 loss: 0.1796
agent1:                 episode reward: 0.0923,                 loss: nan
Episode: 31701/101000 (31.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1656s / 6707.6961 s
agent0:                 episode reward: -0.0491,                 loss: 0.1810
agent1:                 episode reward: 0.0491,                 loss: nan
Episode: 31721/101000 (31.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9822s / 6714.6783 s
agent0:                 episode reward: -0.2336,                 loss: 0.1817
agent1:                 episode reward: 0.2336,                 loss: nan
Episode: 31741/101000 (31.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0604s / 6720.7387 s
agent0:                 episode reward: -0.2791,                 loss: 0.1779
agent1:                 episode reward: 0.2791,                 loss: nan
Episode: 31761/101000 (31.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1767s / 6727.9154 s
agent0:                 episode reward: -0.1816,                 loss: 0.1793
agent1:                 episode reward: 0.1816,                 loss: nan
Episode: 31781/101000 (31.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3949s / 6735.3103 s
agent0:                 episode reward: -0.1509,                 loss: 0.1764
agent1:                 episode reward: 0.1509,                 loss: nan
Episode: 31801/101000 (31.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3684s / 6742.6787 s
agent0:                 episode reward: -0.0813,                 loss: 0.1795
agent1:                 episode reward: 0.0813,                 loss: nan
Episode: 31821/101000 (31.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4776s / 6751.1563 s
agent0:                 episode reward: -0.1591,                 loss: 0.1805
agent1:                 episode reward: 0.1591,                 loss: nan
Episode: 31841/101000 (31.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1595s / 6758.3158 s
agent0:                 episode reward: -0.3164,                 loss: 0.1799
agent1:                 episode reward: 0.3164,                 loss: nan
Episode: 31861/101000 (31.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6527s / 6766.9685 s
agent0:                 episode reward: -0.0456,                 loss: 0.1790
agent1:                 episode reward: 0.0456,                 loss: nan
Episode: 31881/101000 (31.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2391s / 6775.2076 s
agent0:                 episode reward: 0.1374,                 loss: 0.1793
agent1:                 episode reward: -0.1374,                 loss: nan
Episode: 31901/101000 (31.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2078s / 6783.4154 s
agent0:                 episode reward: -0.0147,                 loss: 0.1796
agent1:                 episode reward: 0.0147,                 loss: nan
Episode: 31921/101000 (31.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4437s / 6790.8591 s
agent0:                 episode reward: -0.3018,                 loss: 0.1793
agent1:                 episode reward: 0.3018,                 loss: nan
Episode: 31941/101000 (31.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4198s / 6799.2788 s
agent0:                 episode reward: 0.2007,                 loss: 0.1792
agent1:                 episode reward: -0.2007,                 loss: nan
Episode: 31961/101000 (31.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7495s / 6806.0284 s
agent0:                 episode reward: -0.1032,                 loss: 0.1785
agent1:                 episode reward: 0.1032,                 loss: nan
Episode: 31981/101000 (31.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4754s / 6814.5037 s
agent0:                 episode reward: -0.6472,                 loss: 0.1774
agent1:                 episode reward: 0.6472,                 loss: nan
Episode: 32001/101000 (31.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7561s / 6821.2598 s
agent0:                 episode reward: 0.0083,                 loss: 0.1685
agent1:                 episode reward: -0.0083,                 loss: nan
Episode: 32021/101000 (31.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6485s / 6829.9083 s
agent0:                 episode reward: -0.0004,                 loss: 0.1672
agent1:                 episode reward: 0.0004,                 loss: nan
Episode: 32041/101000 (31.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8537s / 6836.7620 s
agent0:                 episode reward: -0.1432,                 loss: 0.1688
agent1:                 episode reward: 0.1432,                 loss: nan
Episode: 32061/101000 (31.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4655s / 6844.2276 s
agent0:                 episode reward: -0.2013,                 loss: 0.1684
agent1:                 episode reward: 0.2013,                 loss: nan
Episode: 32081/101000 (31.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4962s / 6853.7238 s
agent0:                 episode reward: 0.3415,                 loss: 0.1673
agent1:                 episode reward: -0.3415,                 loss: nan
Episode: 32101/101000 (31.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7867s / 6861.5104 s
agent0:                 episode reward: -0.1774,                 loss: 0.1677
agent1:                 episode reward: 0.1774,                 loss: nan
Episode: 32121/101000 (31.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8645s / 6868.3750 s
agent0:                 episode reward: -0.3817,                 loss: 0.1689
agent1:                 episode reward: 0.3817,                 loss: nan
Episode: 32141/101000 (31.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0376s / 6876.4126 s
agent0:                 episode reward: -0.0139,                 loss: 0.1677
agent1:                 episode reward: 0.0139,                 loss: nan
Episode: 32161/101000 (31.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7932s / 6883.2058 s
agent0:                 episode reward: 0.1156,                 loss: 0.1705
agent1:                 episode reward: -0.1156,                 loss: nan
Episode: 32181/101000 (31.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5334s / 6890.7392 s
agent0:                 episode reward: 0.0197,                 loss: 0.1682
agent1:                 episode reward: -0.0197,                 loss: nan
Episode: 32201/101000 (31.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0272s / 6898.7664 s
agent0:                 episode reward: 0.2671,                 loss: 0.1673
agent1:                 episode reward: -0.2671,                 loss: nan
Score delta: 1.697261891136409, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/31775_0.
Episode: 32221/101000 (31.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4652s / 6904.2316 s
agent0:                 episode reward: -0.8468,                 loss: nan
agent1:                 episode reward: 0.8468,                 loss: 0.1587
Episode: 32241/101000 (31.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2348s / 6912.4664 s
agent0:                 episode reward: -0.3837,                 loss: nan
agent1:                 episode reward: 0.3837,                 loss: 0.1583
Episode: 32261/101000 (31.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4077s / 6919.8741 s
agent0:                 episode reward: -0.4066,                 loss: 0.1775
agent1:                 episode reward: 0.4066,                 loss: 0.1592
Score delta: 1.5786690125916811, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/31825_1.
Episode: 32281/101000 (31.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4839s / 6925.3579 s
agent0:                 episode reward: -0.0925,                 loss: 0.1725
agent1:                 episode reward: 0.0925,                 loss: nan
Episode: 32301/101000 (31.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2906s / 6933.6486 s
agent0:                 episode reward: 0.0809,                 loss: 0.1751
agent1:                 episode reward: -0.0809,                 loss: nan
Episode: 32321/101000 (32.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1719s / 6940.8204 s
agent0:                 episode reward: -0.0785,                 loss: 0.1770
agent1:                 episode reward: 0.0785,                 loss: nan
Episode: 32341/101000 (32.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9604s / 6949.7808 s
agent0:                 episode reward: -0.2092,                 loss: 0.1757
agent1:                 episode reward: 0.2092,                 loss: nan
Episode: 32361/101000 (32.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7769s / 6957.5577 s
agent0:                 episode reward: 0.0016,                 loss: 0.1765
agent1:                 episode reward: -0.0016,                 loss: nan
Episode: 32381/101000 (32.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0489s / 6966.6066 s
agent0:                 episode reward: 0.1862,                 loss: 0.1767
agent1:                 episode reward: -0.1862,                 loss: nan
Episode: 32401/101000 (32.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3372s / 6976.9438 s
agent0:                 episode reward: 0.2384,                 loss: 0.1761
agent1:                 episode reward: -0.2384,                 loss: nan
Episode: 32421/101000 (32.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3397s / 6986.2835 s
agent0:                 episode reward: -0.1192,                 loss: 0.1761
agent1:                 episode reward: 0.1192,                 loss: nan
Episode: 32441/101000 (32.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9077s / 6994.1912 s
agent0:                 episode reward: 0.1376,                 loss: 0.1763
agent1:                 episode reward: -0.1376,                 loss: nan
Episode: 32461/101000 (32.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1348s / 7002.3260 s
agent0:                 episode reward: 0.0935,                 loss: 0.1754
agent1:                 episode reward: -0.0935,                 loss: nan
Episode: 32481/101000 (32.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5947s / 7008.9207 s
agent0:                 episode reward: 0.1065,                 loss: 0.1803
agent1:                 episode reward: -0.1065,                 loss: nan
Episode: 32501/101000 (32.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7927s / 7016.7134 s
agent0:                 episode reward: 0.0156,                 loss: 0.1757
agent1:                 episode reward: -0.0156,                 loss: nan
Episode: 32521/101000 (32.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1706s / 7023.8839 s
agent0:                 episode reward: -0.1649,                 loss: 0.1763
agent1:                 episode reward: 0.1649,                 loss: nan
Episode: 32541/101000 (32.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2233s / 7030.1072 s
agent0:                 episode reward: -0.4666,                 loss: 0.1772
agent1:                 episode reward: 0.4666,                 loss: nan
Episode: 32561/101000 (32.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2317s / 7038.3389 s
agent0:                 episode reward: 0.1601,                 loss: 0.1752
agent1:                 episode reward: -0.1601,                 loss: nan
Episode: 32581/101000 (32.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6341s / 7045.9730 s
agent0:                 episode reward: 0.1772,                 loss: 0.1753
agent1:                 episode reward: -0.1772,                 loss: nan
Episode: 32601/101000 (32.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9531s / 7052.9261 s
agent0:                 episode reward: -0.0652,                 loss: 0.1771
agent1:                 episode reward: 0.0652,                 loss: nan
Episode: 32621/101000 (32.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2651s / 7059.1912 s
agent0:                 episode reward: 0.1904,                 loss: 0.1771
agent1:                 episode reward: -0.1904,                 loss: nan
Episode: 32641/101000 (32.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7030s / 7066.8941 s
agent0:                 episode reward: 0.2633,                 loss: 0.1773
agent1:                 episode reward: -0.2633,                 loss: nan
Episode: 32661/101000 (32.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3166s / 7075.2108 s
agent0:                 episode reward: -0.0535,                 loss: 0.1771
agent1:                 episode reward: 0.0535,                 loss: nan
Episode: 32681/101000 (32.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2583s / 7085.4691 s
agent0:                 episode reward: -0.3458,                 loss: 0.1778
agent1:                 episode reward: 0.3458,                 loss: nan
Episode: 32701/101000 (32.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5865s / 7093.0555 s
agent0:                 episode reward: -0.5403,                 loss: 0.1742
agent1:                 episode reward: 0.5403,                 loss: nan
Episode: 32721/101000 (32.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2340s / 7099.2896 s
agent0:                 episode reward: -0.2723,                 loss: 0.1678
agent1:                 episode reward: 0.2723,                 loss: nan
Episode: 32741/101000 (32.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9248s / 7108.2144 s
agent0:                 episode reward: 0.1409,                 loss: 0.1681
agent1:                 episode reward: -0.1409,                 loss: nan
Episode: 32761/101000 (32.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2752s / 7115.4896 s
agent0:                 episode reward: -0.2173,                 loss: 0.1683
agent1:                 episode reward: 0.2173,                 loss: nan
Episode: 32781/101000 (32.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6529s / 7123.1425 s
agent0:                 episode reward: -0.1792,                 loss: 0.1670
agent1:                 episode reward: 0.1792,                 loss: nan
Episode: 32801/101000 (32.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5681s / 7130.7106 s
agent0:                 episode reward: -0.2817,                 loss: 0.1674
agent1:                 episode reward: 0.2817,                 loss: nan
Episode: 32821/101000 (32.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6392s / 7138.3498 s
agent0:                 episode reward: -0.2667,                 loss: 0.1683
agent1:                 episode reward: 0.2667,                 loss: nan
Episode: 32841/101000 (32.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5987s / 7146.9485 s
agent0:                 episode reward: -0.1399,                 loss: 0.1675
agent1:                 episode reward: 0.1399,                 loss: nan
Episode: 32861/101000 (32.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3159s / 7154.2644 s
agent0:                 episode reward: -0.3287,                 loss: 0.1664
agent1:                 episode reward: 0.3287,                 loss: nan
Episode: 32881/101000 (32.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9117s / 7161.1761 s
agent0:                 episode reward: -0.1550,                 loss: 0.1670
agent1:                 episode reward: 0.1550,                 loss: nan
Episode: 32901/101000 (32.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8253s / 7170.0014 s
agent0:                 episode reward: 0.2068,                 loss: 0.1673
agent1:                 episode reward: -0.2068,                 loss: nan
Episode: 32921/101000 (32.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1448s / 7178.1462 s
agent0:                 episode reward: 0.2518,                 loss: 0.1674
agent1:                 episode reward: -0.2518,                 loss: nan
Episode: 32941/101000 (32.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8756s / 7188.0218 s
agent0:                 episode reward: 0.2663,                 loss: 0.1665
agent1:                 episode reward: -0.2663,                 loss: nan
Episode: 32961/101000 (32.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1839s / 7195.2057 s
agent0:                 episode reward: 0.1839,                 loss: 0.1663
agent1:                 episode reward: -0.1839,                 loss: nan
Episode: 32981/101000 (32.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9652s / 7205.1709 s
agent0:                 episode reward: -0.3576,                 loss: 0.1680
agent1:                 episode reward: 0.3576,                 loss: nan
Episode: 33001/101000 (32.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6287s / 7213.7995 s
agent0:                 episode reward: 0.0720,                 loss: 0.1664
agent1:                 episode reward: -0.0720,                 loss: nan
Episode: 33021/101000 (32.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8132s / 7223.6127 s
agent0:                 episode reward: -0.0027,                 loss: 0.1692
agent1:                 episode reward: 0.0027,                 loss: nan
Episode: 33041/101000 (32.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6726s / 7230.2853 s
agent0:                 episode reward: -0.1250,                 loss: 0.1708
agent1:                 episode reward: 0.1250,                 loss: nan
Episode: 33061/101000 (32.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2618s / 7237.5471 s
agent0:                 episode reward: 0.2254,                 loss: 0.1711
agent1:                 episode reward: -0.2254,                 loss: nan
Episode: 33081/101000 (32.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1717s / 7244.7188 s
agent0:                 episode reward: -0.0138,                 loss: 0.1703
agent1:                 episode reward: 0.0138,                 loss: nan
Episode: 33101/101000 (32.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5870s / 7253.3058 s
agent0:                 episode reward: -0.2215,                 loss: 0.1722
agent1:                 episode reward: 0.2215,                 loss: nan
Episode: 33121/101000 (32.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4896s / 7261.7954 s
agent0:                 episode reward: -0.2872,                 loss: 0.1706
agent1:                 episode reward: 0.2872,                 loss: nan
Episode: 33141/101000 (32.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7851s / 7269.5805 s
agent0:                 episode reward: -0.0540,                 loss: 0.1697
agent1:                 episode reward: 0.0540,                 loss: nan
Episode: 33161/101000 (32.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6553s / 7279.2358 s
agent0:                 episode reward: 0.2423,                 loss: 0.1700
agent1:                 episode reward: -0.2423,                 loss: nan
Episode: 33181/101000 (32.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3738s / 7287.6096 s
agent0:                 episode reward: -0.1152,                 loss: 0.1727
agent1:                 episode reward: 0.1152,                 loss: nan
Episode: 33201/101000 (32.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4424s / 7295.0521 s
agent0:                 episode reward: -0.0832,                 loss: 0.1697
agent1:                 episode reward: 0.0832,                 loss: nan
Episode: 33221/101000 (32.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3222s / 7302.3742 s
agent0:                 episode reward: -0.0646,                 loss: 0.1722
agent1:                 episode reward: 0.0646,                 loss: nan
Episode: 33241/101000 (32.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8803s / 7309.2545 s
agent0:                 episode reward: -0.0606,                 loss: 0.1711
agent1:                 episode reward: 0.0606,                 loss: nan
Episode: 33261/101000 (32.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7693s / 7317.0238 s
agent0:                 episode reward: -0.1270,                 loss: 0.1699
agent1:                 episode reward: 0.1270,                 loss: nan
Episode: 33281/101000 (32.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5499s / 7326.5736 s
agent0:                 episode reward: -0.2779,                 loss: 0.1710
agent1:                 episode reward: 0.2779,                 loss: nan
Episode: 33301/101000 (32.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1507s / 7332.7243 s
agent0:                 episode reward: -0.3557,                 loss: 0.1723
agent1:                 episode reward: 0.3557,                 loss: nan
Episode: 33321/101000 (32.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3909s / 7339.1152 s
agent0:                 episode reward: 0.3669,                 loss: 0.1712
agent1:                 episode reward: -0.3669,                 loss: 0.1363
Score delta: 1.8737888268790368, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/32886_0.
Episode: 33341/101000 (33.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4182s / 7347.5334 s
agent0:                 episode reward: -0.5420,                 loss: nan
agent1:                 episode reward: 0.5420,                 loss: 0.1372
Episode: 33361/101000 (33.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2131s / 7355.7466 s
agent0:                 episode reward: -0.4495,                 loss: nan
agent1:                 episode reward: 0.4495,                 loss: 0.1372
Episode: 33381/101000 (33.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0577s / 7361.8042 s
agent0:                 episode reward: -0.4613,                 loss: nan
agent1:                 episode reward: 0.4613,                 loss: 0.1366
Episode: 33401/101000 (33.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5795s / 7369.3837 s
agent0:                 episode reward: -0.4563,                 loss: 0.1747
agent1:                 episode reward: 0.4563,                 loss: 0.1528
Score delta: 1.8611285923956498, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/32968_1.
Episode: 33421/101000 (33.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7548s / 7376.1385 s
agent0:                 episode reward: 0.0394,                 loss: 0.1752
agent1:                 episode reward: -0.0394,                 loss: nan
Episode: 33441/101000 (33.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8675s / 7381.0060 s
agent0:                 episode reward: -0.2822,                 loss: 0.1749
agent1:                 episode reward: 0.2822,                 loss: nan
Episode: 33461/101000 (33.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4042s / 7389.4102 s
agent0:                 episode reward: 0.2707,                 loss: 0.1714
agent1:                 episode reward: -0.2707,                 loss: nan
Episode: 33481/101000 (33.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3889s / 7397.7991 s
agent0:                 episode reward: -0.0383,                 loss: 0.1716
agent1:                 episode reward: 0.0383,                 loss: nan
Episode: 33501/101000 (33.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8337s / 7405.6328 s
agent0:                 episode reward: -0.1130,                 loss: 0.1736
agent1:                 episode reward: 0.1130,                 loss: nan
Episode: 33521/101000 (33.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1296s / 7412.7624 s
agent0:                 episode reward: -0.3158,                 loss: 0.1718
agent1:                 episode reward: 0.3158,                 loss: nan
Episode: 33541/101000 (33.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5499s / 7422.3123 s
agent0:                 episode reward: -0.1962,                 loss: 0.1723
agent1:                 episode reward: 0.1962,                 loss: nan
Episode: 33561/101000 (33.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0768s / 7429.3891 s
agent0:                 episode reward: 0.1532,                 loss: 0.1729
agent1:                 episode reward: -0.1532,                 loss: nan
Episode: 33581/101000 (33.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9751s / 7435.3642 s
agent0:                 episode reward: 0.0894,                 loss: 0.1717
agent1:                 episode reward: -0.0894,                 loss: nan
Episode: 33601/101000 (33.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7569s / 7442.1211 s
agent0:                 episode reward: 0.2190,                 loss: 0.1715
agent1:                 episode reward: -0.2190,                 loss: nan
Episode: 33621/101000 (33.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3196s / 7449.4406 s
agent0:                 episode reward: -0.0851,                 loss: 0.1729
agent1:                 episode reward: 0.0851,                 loss: nan
Episode: 33641/101000 (33.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1401s / 7455.5807 s
agent0:                 episode reward: -0.1180,                 loss: 0.1729
agent1:                 episode reward: 0.1180,                 loss: nan
Episode: 33661/101000 (33.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1296s / 7464.7103 s
agent0:                 episode reward: 0.1991,                 loss: 0.1737
agent1:                 episode reward: -0.1991,                 loss: nan
Episode: 33681/101000 (33.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3303s / 7474.0406 s
agent0:                 episode reward: -0.1595,                 loss: 0.1740
agent1:                 episode reward: 0.1595,                 loss: nan
Episode: 33701/101000 (33.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3112s / 7482.3518 s
agent0:                 episode reward: -0.0180,                 loss: 0.1712
agent1:                 episode reward: 0.0180,                 loss: nan
Episode: 33721/101000 (33.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1480s / 7491.4998 s
agent0:                 episode reward: 0.2069,                 loss: 0.1717
agent1:                 episode reward: -0.2069,                 loss: nan
Episode: 33741/101000 (33.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6816s / 7501.1814 s
agent0:                 episode reward: -0.1679,                 loss: 0.1724
agent1:                 episode reward: 0.1679,                 loss: nan
Episode: 33761/101000 (33.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2676s / 7510.4490 s
agent0:                 episode reward: 0.3895,                 loss: 0.1711
agent1:                 episode reward: -0.3895,                 loss: nan
Episode: 33781/101000 (33.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3411s / 7517.7901 s
agent0:                 episode reward: -0.0795,                 loss: 0.1762
agent1:                 episode reward: 0.0795,                 loss: nan
Episode: 33801/101000 (33.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1124s / 7525.9025 s
agent0:                 episode reward: -0.1724,                 loss: 0.1830
agent1:                 episode reward: 0.1724,                 loss: nan
Episode: 33821/101000 (33.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4526s / 7532.3552 s
agent0:                 episode reward: 0.1943,                 loss: 0.1826
agent1:                 episode reward: -0.1943,                 loss: nan
Episode: 33841/101000 (33.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5754s / 7540.9306 s
agent0:                 episode reward: 0.1229,                 loss: 0.1831
agent1:                 episode reward: -0.1229,                 loss: nan
Episode: 33861/101000 (33.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8649s / 7548.7955 s
agent0:                 episode reward: 0.0393,                 loss: 0.1837
agent1:                 episode reward: -0.0393,                 loss: nan
Episode: 33881/101000 (33.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1580s / 7555.9534 s
agent0:                 episode reward: -0.6572,                 loss: 0.1825
agent1:                 episode reward: 0.6572,                 loss: nan
Episode: 33901/101000 (33.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8857s / 7563.8392 s
agent0:                 episode reward: 0.0176,                 loss: 0.1839
agent1:                 episode reward: -0.0176,                 loss: nan
Episode: 33921/101000 (33.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7956s / 7568.6348 s
agent0:                 episode reward: 0.3251,                 loss: 0.1819
agent1:                 episode reward: -0.3251,                 loss: nan
Episode: 33941/101000 (33.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5327s / 7575.1675 s
agent0:                 episode reward: 0.1260,                 loss: 0.1824
agent1:                 episode reward: -0.1260,                 loss: nan
Episode: 33961/101000 (33.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3433s / 7582.5108 s
agent0:                 episode reward: 0.2769,                 loss: 0.1831
agent1:                 episode reward: -0.2769,                 loss: nan
Episode: 33981/101000 (33.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0469s / 7588.5577 s
agent0:                 episode reward: -0.2350,                 loss: 0.1830
agent1:                 episode reward: 0.2350,                 loss: nan
Episode: 34001/101000 (33.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8904s / 7597.4481 s
agent0:                 episode reward: 0.3993,                 loss: 0.1813
agent1:                 episode reward: -0.3993,                 loss: nan
Episode: 34021/101000 (33.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6092s / 7606.0573 s
agent0:                 episode reward: 0.3129,                 loss: 0.1840
agent1:                 episode reward: -0.3129,                 loss: nan
Episode: 34041/101000 (33.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6120s / 7612.6693 s
agent0:                 episode reward: 0.2842,                 loss: 0.1843
agent1:                 episode reward: -0.2842,                 loss: nan
Episode: 34061/101000 (33.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9132s / 7620.5824 s
agent0:                 episode reward: 0.0893,                 loss: 0.1814
agent1:                 episode reward: -0.0893,                 loss: nan
Episode: 34081/101000 (33.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4705s / 7628.0530 s
agent0:                 episode reward: -0.0962,                 loss: 0.1821
agent1:                 episode reward: 0.0962,                 loss: nan
Episode: 34101/101000 (33.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5197s / 7636.5726 s
agent0:                 episode reward: -0.1020,                 loss: 0.1831
agent1:                 episode reward: 0.1020,                 loss: nan
Episode: 34121/101000 (33.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6019s / 7644.1745 s
agent0:                 episode reward: -0.1777,                 loss: 0.1768
agent1:                 episode reward: 0.1777,                 loss: nan
Episode: 34141/101000 (33.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9759s / 7652.1505 s
agent0:                 episode reward: 0.1900,                 loss: 0.1732
agent1:                 episode reward: -0.1900,                 loss: nan
Episode: 34161/101000 (33.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5702s / 7659.7207 s
agent0:                 episode reward: 0.2040,                 loss: 0.1749
agent1:                 episode reward: -0.2040,                 loss: nan
Episode: 34181/101000 (33.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3787s / 7667.0993 s
agent0:                 episode reward: -0.0779,                 loss: 0.1739
agent1:                 episode reward: 0.0779,                 loss: nan
Episode: 34201/101000 (33.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 11.2687s / 7678.3680 s
agent0:                 episode reward: 0.0831,                 loss: 0.1752
agent1:                 episode reward: -0.0831,                 loss: nan
Episode: 34221/101000 (33.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5894s / 7685.9574 s
agent0:                 episode reward: 0.0263,                 loss: 0.1749
agent1:                 episode reward: -0.0263,                 loss: nan
Episode: 34241/101000 (33.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5331s / 7693.4905 s
agent0:                 episode reward: -0.0110,                 loss: 0.1738
agent1:                 episode reward: 0.0110,                 loss: nan
Episode: 34261/101000 (33.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2799s / 7701.7704 s
agent0:                 episode reward: -0.1063,                 loss: 0.1752
agent1:                 episode reward: 0.1063,                 loss: nan
Episode: 34281/101000 (33.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8133s / 7711.5837 s
agent0:                 episode reward: 0.1630,                 loss: 0.1735
agent1:                 episode reward: -0.1630,                 loss: nan
Episode: 34301/101000 (33.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5908s / 7720.1746 s
agent0:                 episode reward: -0.4553,                 loss: 0.1762
agent1:                 episode reward: 0.4553,                 loss: nan
Episode: 34321/101000 (33.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7485s / 7728.9231 s
agent0:                 episode reward: -0.2809,                 loss: 0.1745
agent1:                 episode reward: 0.2809,                 loss: nan
Episode: 34341/101000 (34.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4050s / 7737.3281 s
agent0:                 episode reward: -0.0445,                 loss: 0.1742
agent1:                 episode reward: 0.0445,                 loss: nan
Episode: 34361/101000 (34.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7014s / 7743.0294 s
agent0:                 episode reward: 0.0949,                 loss: 0.1741
agent1:                 episode reward: -0.0949,                 loss: nan
Episode: 34381/101000 (34.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2985s / 7751.3280 s
agent0:                 episode reward: 0.3033,                 loss: 0.1741
agent1:                 episode reward: -0.3033,                 loss: nan
Episode: 34401/101000 (34.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7654s / 7761.0934 s
agent0:                 episode reward: -0.0520,                 loss: 0.1749
agent1:                 episode reward: 0.0520,                 loss: nan
Episode: 34421/101000 (34.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3181s / 7768.4115 s
agent0:                 episode reward: 0.1268,                 loss: 0.1730
agent1:                 episode reward: -0.1268,                 loss: 0.1505
Score delta: 1.6456386290414975, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/33989_0.
Episode: 34441/101000 (34.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9191s / 7775.3307 s
agent0:                 episode reward: -0.4502,                 loss: nan
agent1:                 episode reward: 0.4502,                 loss: 0.1491
Episode: 34461/101000 (34.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0596s / 7781.3903 s
agent0:                 episode reward: -0.3030,                 loss: nan
agent1:                 episode reward: 0.3030,                 loss: 0.1458
Episode: 34481/101000 (34.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4099s / 7787.8002 s
agent0:                 episode reward: -0.2499,                 loss: nan
agent1:                 episode reward: 0.2499,                 loss: 0.1487
Episode: 34501/101000 (34.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9743s / 7795.7745 s
agent0:                 episode reward: -0.3859,                 loss: 0.2721
agent1:                 episode reward: 0.3859,                 loss: 0.1460
Score delta: 1.7110586170855595, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/34065_1.
Episode: 34521/101000 (34.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7090s / 7804.4835 s
agent0:                 episode reward: -0.0796,                 loss: 0.2365
agent1:                 episode reward: 0.0796,                 loss: nan
Episode: 34541/101000 (34.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1558s / 7812.6393 s
agent0:                 episode reward: -0.3739,                 loss: 0.1701
agent1:                 episode reward: 0.3739,                 loss: nan
Episode: 34561/101000 (34.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2552s / 7820.8945 s
agent0:                 episode reward: -0.1489,                 loss: 0.1690
agent1:                 episode reward: 0.1489,                 loss: nan
Episode: 34581/101000 (34.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3392s / 7829.2338 s
agent0:                 episode reward: -0.3095,                 loss: 0.1685
agent1:                 episode reward: 0.3095,                 loss: nan
Episode: 34601/101000 (34.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8117s / 7840.0455 s
agent0:                 episode reward: -0.1258,                 loss: 0.1675
agent1:                 episode reward: 0.1258,                 loss: nan
Episode: 34621/101000 (34.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1586s / 7848.2042 s
agent0:                 episode reward: -0.0656,                 loss: 0.1662
agent1:                 episode reward: 0.0656,                 loss: nan
Episode: 34641/101000 (34.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9361s / 7857.1403 s
agent0:                 episode reward: -0.5375,                 loss: 0.1669
agent1:                 episode reward: 0.5375,                 loss: nan
Episode: 34661/101000 (34.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2547s / 7863.3950 s
agent0:                 episode reward: -0.1142,                 loss: 0.1678
agent1:                 episode reward: 0.1142,                 loss: nan
Episode: 34681/101000 (34.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0848s / 7870.4798 s
agent0:                 episode reward: -0.2394,                 loss: 0.1673
agent1:                 episode reward: 0.2394,                 loss: nan
Episode: 34701/101000 (34.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5928s / 7876.0726 s
agent0:                 episode reward: -0.3214,                 loss: 0.1663
agent1:                 episode reward: 0.3214,                 loss: nan
Episode: 34721/101000 (34.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4696s / 7885.5423 s
agent0:                 episode reward: -0.0609,                 loss: 0.1676
agent1:                 episode reward: 0.0609,                 loss: nan
Episode: 34741/101000 (34.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4387s / 7893.9809 s
agent0:                 episode reward: -0.1397,                 loss: 0.1655
agent1:                 episode reward: 0.1397,                 loss: nan
Episode: 34761/101000 (34.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7347s / 7901.7156 s
agent0:                 episode reward: -0.4775,                 loss: 0.1664
agent1:                 episode reward: 0.4775,                 loss: nan
Episode: 34781/101000 (34.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7872s / 7909.5028 s
agent0:                 episode reward: -0.2709,                 loss: 0.1664
agent1:                 episode reward: 0.2709,                 loss: nan
Episode: 34801/101000 (34.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1030s / 7915.6058 s
agent0:                 episode reward: 0.0804,                 loss: 0.1665
agent1:                 episode reward: -0.0804,                 loss: nan
Episode: 34821/101000 (34.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7895s / 7923.3953 s
agent0:                 episode reward: -0.4336,                 loss: 0.1671
agent1:                 episode reward: 0.4336,                 loss: nan
Episode: 34841/101000 (34.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2551s / 7929.6504 s
agent0:                 episode reward: -0.3885,                 loss: 0.1650
agent1:                 episode reward: 0.3885,                 loss: nan
Episode: 34861/101000 (34.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8864s / 7939.5368 s
agent0:                 episode reward: -0.0005,                 loss: 0.1752
agent1:                 episode reward: 0.0005,                 loss: nan
Episode: 34881/101000 (34.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9828s / 7946.5196 s
agent0:                 episode reward: -0.1166,                 loss: 0.1798
agent1:                 episode reward: 0.1166,                 loss: nan
Episode: 34901/101000 (34.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5481s / 7954.0677 s
agent0:                 episode reward: -0.1115,                 loss: 0.1815
agent1:                 episode reward: 0.1115,                 loss: nan
Episode: 34921/101000 (34.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9368s / 7963.0045 s
agent0:                 episode reward: -0.0126,                 loss: 0.1795
agent1:                 episode reward: 0.0126,                 loss: nan
Episode: 34941/101000 (34.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5326s / 7970.5371 s
agent0:                 episode reward: 0.1755,                 loss: 0.1809
agent1:                 episode reward: -0.1755,                 loss: nan
Episode: 34961/101000 (34.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5156s / 7977.0527 s
agent0:                 episode reward: 0.2560,                 loss: 0.1789
agent1:                 episode reward: -0.2560,                 loss: nan
Episode: 34981/101000 (34.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5994s / 7986.6521 s
agent0:                 episode reward: 0.2254,                 loss: 0.1802
agent1:                 episode reward: -0.2254,                 loss: 0.1618
Score delta: 1.5502580082752735, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/34548_0.
Episode: 35001/101000 (34.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9782s / 7991.6303 s
agent0:                 episode reward: -0.1695,                 loss: nan
agent1:                 episode reward: 0.1695,                 loss: 0.1603
Episode: 35021/101000 (34.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7831s / 7998.4135 s
agent0:                 episode reward: -0.0007,                 loss: nan
agent1:                 episode reward: 0.0007,                 loss: 0.1577
Episode: 35041/101000 (34.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6813s / 8005.0947 s
agent0:                 episode reward: 0.0315,                 loss: nan
agent1:                 episode reward: -0.0315,                 loss: 0.1593
Episode: 35061/101000 (34.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3639s / 8012.4587 s
agent0:                 episode reward: -0.1509,                 loss: nan
agent1:                 episode reward: 0.1509,                 loss: 0.1579
Episode: 35081/101000 (34.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7879s / 8020.2465 s
agent0:                 episode reward: -0.1605,                 loss: nan
agent1:                 episode reward: 0.1605,                 loss: 0.1582
Episode: 35101/101000 (34.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3285s / 8027.5750 s
agent0:                 episode reward: -0.7414,                 loss: 0.1705
agent1:                 episode reward: 0.7414,                 loss: 0.1604
Score delta: 1.6931375305584688, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/34667_1.
Episode: 35121/101000 (34.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2008s / 8034.7758 s
agent0:                 episode reward: 0.1891,                 loss: 0.1697
agent1:                 episode reward: -0.1891,                 loss: nan
Episode: 35141/101000 (34.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8346s / 8041.6103 s
agent0:                 episode reward: -0.3971,                 loss: 0.1671
agent1:                 episode reward: 0.3971,                 loss: nan
Episode: 35161/101000 (34.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6482s / 8050.2585 s
agent0:                 episode reward: 0.0289,                 loss: 0.1671
agent1:                 episode reward: -0.0289,                 loss: nan
Episode: 35181/101000 (34.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6697s / 8058.9282 s
agent0:                 episode reward: 0.0899,                 loss: 0.1670
agent1:                 episode reward: -0.0899,                 loss: nan
Episode: 35201/101000 (34.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9031s / 8068.8313 s
agent0:                 episode reward: -0.1010,                 loss: 0.1680
agent1:                 episode reward: 0.1010,                 loss: nan
Episode: 35221/101000 (34.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6365s / 8077.4678 s
agent0:                 episode reward: -0.1767,                 loss: 0.1669
agent1:                 episode reward: 0.1767,                 loss: nan
Episode: 35241/101000 (34.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7734s / 8084.2412 s
agent0:                 episode reward: -0.2640,                 loss: 0.1681
agent1:                 episode reward: 0.2640,                 loss: nan
Episode: 35261/101000 (34.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2263s / 8089.4675 s
agent0:                 episode reward: -0.5517,                 loss: 0.1682
agent1:                 episode reward: 0.5517,                 loss: nan
Episode: 35281/101000 (34.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8207s / 8098.2881 s
agent0:                 episode reward: -0.5439,                 loss: 0.1673
agent1:                 episode reward: 0.5439,                 loss: nan
Episode: 35301/101000 (34.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9776s / 8107.2658 s
agent0:                 episode reward: 0.0188,                 loss: 0.1673
agent1:                 episode reward: -0.0188,                 loss: nan
Episode: 35321/101000 (34.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2604s / 8116.5262 s
agent0:                 episode reward: 0.1079,                 loss: 0.1762
agent1:                 episode reward: -0.1079,                 loss: 0.1418
Score delta: 1.6708598795481737, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/34892_0.
Episode: 35341/101000 (34.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5678s / 8126.0940 s
agent0:                 episode reward: 0.1140,                 loss: nan
agent1:                 episode reward: -0.1140,                 loss: 0.1384
Episode: 35361/101000 (35.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8606s / 8132.9546 s
agent0:                 episode reward: 0.1842,                 loss: nan
agent1:                 episode reward: -0.1842,                 loss: 0.1381
Episode: 35381/101000 (35.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4855s / 8141.4400 s
agent0:                 episode reward: -0.3497,                 loss: nan
agent1:                 episode reward: 0.3497,                 loss: 0.1371
Episode: 35401/101000 (35.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5428s / 8148.9829 s
agent0:                 episode reward: -0.4445,                 loss: 0.1723
agent1:                 episode reward: 0.4445,                 loss: 0.1384
Score delta: 1.5446594325172511, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/34969_1.
Episode: 35421/101000 (35.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8401s / 8156.8230 s
agent0:                 episode reward: 0.1561,                 loss: 0.1740
agent1:                 episode reward: -0.1561,                 loss: nan
Episode: 35441/101000 (35.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1332s / 8164.9562 s
agent0:                 episode reward: -0.0140,                 loss: 0.1730
agent1:                 episode reward: 0.0140,                 loss: nan
Episode: 35461/101000 (35.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9929s / 8172.9491 s
agent0:                 episode reward: 0.0203,                 loss: 0.1747
agent1:                 episode reward: -0.0203,                 loss: nan
Episode: 35481/101000 (35.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7160s / 8180.6652 s
agent0:                 episode reward: 0.1090,                 loss: 0.1752
agent1:                 episode reward: -0.1090,                 loss: nan
Episode: 35501/101000 (35.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8633s / 8188.5284 s
agent0:                 episode reward: 0.1578,                 loss: 0.1743
agent1:                 episode reward: -0.1578,                 loss: nan
Episode: 35521/101000 (35.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2202s / 8194.7486 s
agent0:                 episode reward: 0.1205,                 loss: 0.1757
agent1:                 episode reward: -0.1205,                 loss: nan
Episode: 35541/101000 (35.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7348s / 8203.4833 s
agent0:                 episode reward: 0.1447,                 loss: 0.1735
agent1:                 episode reward: -0.1447,                 loss: 0.1528
Score delta: 1.682458653779716, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/35114_0.
Episode: 35561/101000 (35.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3948s / 8208.8782 s
agent0:                 episode reward: -0.3426,                 loss: nan
agent1:                 episode reward: 0.3426,                 loss: 0.1520
Episode: 35581/101000 (35.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1372s / 8216.0154 s
agent0:                 episode reward: -0.3009,                 loss: nan
agent1:                 episode reward: 0.3009,                 loss: 0.1533
Episode: 35601/101000 (35.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8222s / 8223.8376 s
agent0:                 episode reward: -0.3983,                 loss: 0.1686
agent1:                 episode reward: 0.3983,                 loss: 0.1525
Score delta: 1.6586602683928404, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/35173_1.
Episode: 35621/101000 (35.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5197s / 8232.3573 s
agent0:                 episode reward: -0.4672,                 loss: 0.1672
agent1:                 episode reward: 0.4672,                 loss: nan
Episode: 35641/101000 (35.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3393s / 8241.6966 s
agent0:                 episode reward: 0.1561,                 loss: 0.1648
agent1:                 episode reward: -0.1561,                 loss: nan
Episode: 35661/101000 (35.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2525s / 8248.9491 s
agent0:                 episode reward: 0.3182,                 loss: 0.1659
agent1:                 episode reward: -0.3182,                 loss: nan
Episode: 35681/101000 (35.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1957s / 8255.1448 s
agent0:                 episode reward: -0.3239,                 loss: 0.1642
agent1:                 episode reward: 0.3239,                 loss: nan
Episode: 35701/101000 (35.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4277s / 8261.5725 s
agent0:                 episode reward: -0.2122,                 loss: 0.1664
agent1:                 episode reward: 0.2122,                 loss: nan
Episode: 35721/101000 (35.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7747s / 8271.3471 s
agent0:                 episode reward: -0.4412,                 loss: 0.1659
agent1:                 episode reward: 0.4412,                 loss: nan
Episode: 35741/101000 (35.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7034s / 8282.0505 s
agent0:                 episode reward: -0.0184,                 loss: 0.1643
agent1:                 episode reward: 0.0184,                 loss: nan
Episode: 35761/101000 (35.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8019s / 8290.8525 s
agent0:                 episode reward: 0.1260,                 loss: 0.1668
agent1:                 episode reward: -0.1260,                 loss: nan
Episode: 35781/101000 (35.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5686s / 8300.4210 s
agent0:                 episode reward: -0.0428,                 loss: 0.1735
agent1:                 episode reward: 0.0428,                 loss: nan
Episode: 35801/101000 (35.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2362s / 8310.6572 s
agent0:                 episode reward: -0.5642,                 loss: 0.1790
agent1:                 episode reward: 0.5642,                 loss: nan
Episode: 35821/101000 (35.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0992s / 8317.7564 s
agent0:                 episode reward: -0.0711,                 loss: 0.1785
agent1:                 episode reward: 0.0711,                 loss: nan
Episode: 35841/101000 (35.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7126s / 8325.4690 s
agent0:                 episode reward: -0.1510,                 loss: 0.1783
agent1:                 episode reward: 0.1510,                 loss: nan
Episode: 35861/101000 (35.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0814s / 8332.5505 s
agent0:                 episode reward: 0.1170,                 loss: 0.1786
agent1:                 episode reward: -0.1170,                 loss: 0.1574
Score delta: 1.6377345223016917, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/35425_0.
Episode: 35881/101000 (35.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0551s / 8338.6056 s
agent0:                 episode reward: 0.0039,                 loss: nan
agent1:                 episode reward: -0.0039,                 loss: 0.1563
Episode: 35901/101000 (35.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6403s / 8346.2459 s
agent0:                 episode reward: -0.3803,                 loss: nan
agent1:                 episode reward: 0.3803,                 loss: 0.1535
Episode: 35921/101000 (35.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0776s / 8355.3235 s
agent0:                 episode reward: -0.2133,                 loss: 0.2641
agent1:                 episode reward: 0.2133,                 loss: 0.1570
Score delta: 1.5229959356465816, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/35476_1.
Episode: 35941/101000 (35.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9949s / 8363.3185 s
agent0:                 episode reward: -0.3832,                 loss: 0.2517
agent1:                 episode reward: 0.3832,                 loss: nan
Episode: 35961/101000 (35.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8107s / 8371.1292 s
agent0:                 episode reward: -0.7211,                 loss: 0.2583
agent1:                 episode reward: 0.7211,                 loss: nan
Episode: 35981/101000 (35.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4535s / 8381.5827 s
agent0:                 episode reward: 0.0397,                 loss: 0.2549
agent1:                 episode reward: -0.0397,                 loss: nan
Episode: 36001/101000 (35.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8299s / 8389.4126 s
agent0:                 episode reward: -0.5978,                 loss: 0.2511
agent1:                 episode reward: 0.5978,                 loss: nan
Episode: 36021/101000 (35.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7171s / 8395.1297 s
agent0:                 episode reward: -0.1562,                 loss: 0.2510
agent1:                 episode reward: 0.1562,                 loss: nan
Episode: 36041/101000 (35.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6010s / 8403.7307 s
agent0:                 episode reward: -0.3591,                 loss: 0.2542
agent1:                 episode reward: 0.3591,                 loss: nan
Episode: 36061/101000 (35.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1600s / 8410.8906 s
agent0:                 episode reward: -0.1923,                 loss: 0.2534
agent1:                 episode reward: 0.1923,                 loss: nan
Episode: 36081/101000 (35.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9750s / 8418.8656 s
agent0:                 episode reward: -0.0473,                 loss: 0.2489
agent1:                 episode reward: 0.0473,                 loss: nan
Episode: 36101/101000 (35.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9591s / 8428.8247 s
agent0:                 episode reward: -0.2651,                 loss: 0.2506
agent1:                 episode reward: 0.2651,                 loss: nan
Episode: 36121/101000 (35.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3239s / 8435.1486 s
agent0:                 episode reward: -0.1448,                 loss: 0.2485
agent1:                 episode reward: 0.1448,                 loss: nan
Episode: 36141/101000 (35.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2966s / 8443.4452 s
agent0:                 episode reward: -0.1639,                 loss: 0.2483
agent1:                 episode reward: 0.1639,                 loss: nan
Episode: 36161/101000 (35.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7537s / 8452.1988 s
agent0:                 episode reward: -0.3384,                 loss: 0.2333
agent1:                 episode reward: 0.3384,                 loss: nan
Episode: 36181/101000 (35.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8348s / 8461.0336 s
agent0:                 episode reward: -0.5164,                 loss: 0.1799
agent1:                 episode reward: 0.5164,                 loss: nan
Episode: 36201/101000 (35.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3685s / 8469.4021 s
agent0:                 episode reward: -0.1079,                 loss: 0.1810
agent1:                 episode reward: 0.1079,                 loss: nan
Episode: 36221/101000 (35.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7767s / 8480.1788 s
agent0:                 episode reward: 0.1559,                 loss: 0.1759
agent1:                 episode reward: -0.1559,                 loss: nan
Episode: 36241/101000 (35.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5526s / 8488.7314 s
agent0:                 episode reward: -0.0884,                 loss: 0.1798
agent1:                 episode reward: 0.0884,                 loss: nan
Episode: 36261/101000 (35.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4197s / 8497.1512 s
agent0:                 episode reward: 0.3383,                 loss: 0.1804
agent1:                 episode reward: -0.3383,                 loss: nan
Score delta: 1.951522005119056, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/35835_0.
Episode: 36281/101000 (35.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0401s / 8506.1913 s
agent0:                 episode reward: -0.2583,                 loss: nan
agent1:                 episode reward: 0.2583,                 loss: 0.1380
Episode: 36301/101000 (35.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6280s / 8512.8193 s
agent0:                 episode reward: -0.2670,                 loss: nan
agent1:                 episode reward: 0.2670,                 loss: 0.1389
Episode: 36321/101000 (35.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6014s / 8519.4207 s
agent0:                 episode reward: -0.3544,                 loss: nan
agent1:                 episode reward: 0.3544,                 loss: 0.1370
Episode: 36341/101000 (35.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2642s / 8526.6849 s
agent0:                 episode reward: 0.0143,                 loss: nan
agent1:                 episode reward: -0.0143,                 loss: 0.1365
Episode: 36361/101000 (36.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9009s / 8533.5858 s
agent0:                 episode reward: -0.4166,                 loss: nan
agent1:                 episode reward: 0.4166,                 loss: 0.1378
Episode: 36381/101000 (36.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9766s / 8540.5624 s
agent0:                 episode reward: -0.4068,                 loss: nan
agent1:                 episode reward: 0.4068,                 loss: 0.1376
Episode: 36401/101000 (36.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2799s / 8547.8423 s
agent0:                 episode reward: -0.1584,                 loss: nan
agent1:                 episode reward: 0.1584,                 loss: 0.1377
Episode: 36421/101000 (36.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7040s / 8554.5463 s
agent0:                 episode reward: -0.1523,                 loss: 0.1701
agent1:                 episode reward: 0.1523,                 loss: 0.1347
Score delta: 1.6284475902004858, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/35978_1.
Episode: 36441/101000 (36.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3535s / 8561.8997 s
agent0:                 episode reward: -0.1144,                 loss: 0.1705
agent1:                 episode reward: 0.1144,                 loss: nan
Episode: 36461/101000 (36.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0315s / 8570.9313 s
agent0:                 episode reward: 0.0520,                 loss: 0.1706
agent1:                 episode reward: -0.0520,                 loss: nan
Episode: 36481/101000 (36.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8947s / 8577.8260 s
agent0:                 episode reward: 0.1070,                 loss: 0.1706
agent1:                 episode reward: -0.1070,                 loss: nan
Episode: 36501/101000 (36.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8374s / 8585.6634 s
agent0:                 episode reward: -0.1634,                 loss: 0.1702
agent1:                 episode reward: 0.1634,                 loss: 0.1584
Score delta: 1.6359341631740025, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/36060_0.
Episode: 36521/101000 (36.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1679s / 8593.8313 s
agent0:                 episode reward: -0.4001,                 loss: nan
agent1:                 episode reward: 0.4001,                 loss: 0.1595
Episode: 36541/101000 (36.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6517s / 8601.4829 s
agent0:                 episode reward: -0.1288,                 loss: nan
agent1:                 episode reward: 0.1288,                 loss: 0.1586
Episode: 36561/101000 (36.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0932s / 8608.5761 s
agent0:                 episode reward: -0.4243,                 loss: nan
agent1:                 episode reward: 0.4243,                 loss: 0.1583
Episode: 36581/101000 (36.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8460s / 8615.4221 s
agent0:                 episode reward: -0.4085,                 loss: nan
agent1:                 episode reward: 0.4085,                 loss: 0.1595
Episode: 36601/101000 (36.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2440s / 8622.6661 s
agent0:                 episode reward: -0.5615,                 loss: nan
agent1:                 episode reward: 0.5615,                 loss: 0.1610
Episode: 36621/101000 (36.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3966s / 8632.0627 s
agent0:                 episode reward: -0.1816,                 loss: 0.1949
agent1:                 episode reward: 0.1816,                 loss: 0.1548
Score delta: 1.5672502892130888, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/36176_1.
Episode: 36641/101000 (36.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1831s / 8640.2458 s
agent0:                 episode reward: -0.5444,                 loss: 0.1903
agent1:                 episode reward: 0.5444,                 loss: nan
Episode: 36661/101000 (36.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4927s / 8650.7385 s
agent0:                 episode reward: 0.1731,                 loss: 0.1877
agent1:                 episode reward: -0.1731,                 loss: nan
Episode: 36681/101000 (36.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7078s / 8657.4463 s
agent0:                 episode reward: -0.3970,                 loss: 0.1933
agent1:                 episode reward: 0.3970,                 loss: 0.1573
Score delta: 1.5137558833919473, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/36237_0.
Episode: 36701/101000 (36.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8695s / 8665.3158 s
agent0:                 episode reward: -0.2391,                 loss: nan
agent1:                 episode reward: 0.2391,                 loss: 0.1548
Episode: 36721/101000 (36.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6079s / 8672.9237 s
agent0:                 episode reward: -0.2046,                 loss: nan
agent1:                 episode reward: 0.2046,                 loss: 0.1590
Episode: 36741/101000 (36.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0998s / 8682.0235 s
agent0:                 episode reward: 0.1046,                 loss: nan
agent1:                 episode reward: -0.1046,                 loss: 0.1544
Episode: 36761/101000 (36.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8163s / 8690.8397 s
agent0:                 episode reward: -0.5750,                 loss: 0.1759
agent1:                 episode reward: 0.5750,                 loss: 0.1573
Score delta: 1.519068698411178, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/36332_1.
Episode: 36781/101000 (36.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9809s / 8699.8206 s
agent0:                 episode reward: -0.0020,                 loss: 0.1743
agent1:                 episode reward: 0.0020,                 loss: nan
Episode: 36801/101000 (36.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5888s / 8709.4094 s
agent0:                 episode reward: -0.3621,                 loss: 0.1744
agent1:                 episode reward: 0.3621,                 loss: nan
Episode: 36821/101000 (36.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3905s / 8716.7999 s
agent0:                 episode reward: 0.0250,                 loss: 0.1738
agent1:                 episode reward: -0.0250,                 loss: nan
Episode: 36841/101000 (36.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8864s / 8724.6863 s
agent0:                 episode reward: 0.2492,                 loss: 0.1737
agent1:                 episode reward: -0.2492,                 loss: nan
Episode: 36861/101000 (36.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6289s / 8731.3152 s
agent0:                 episode reward: -0.1415,                 loss: 0.1763
agent1:                 episode reward: 0.1415,                 loss: nan
Episode: 36881/101000 (36.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3884s / 8740.7036 s
agent0:                 episode reward: -0.2012,                 loss: 0.1771
agent1:                 episode reward: 0.2012,                 loss: nan
Episode: 36901/101000 (36.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7173s / 8750.4209 s
agent0:                 episode reward: -0.4323,                 loss: 0.1793
agent1:                 episode reward: 0.4323,                 loss: nan
Episode: 36921/101000 (36.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2630s / 8759.6839 s
agent0:                 episode reward: 0.1896,                 loss: 0.1757
agent1:                 episode reward: -0.1896,                 loss: nan
Episode: 36941/101000 (36.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3940s / 8769.0779 s
agent0:                 episode reward: -0.2659,                 loss: 0.1762
agent1:                 episode reward: 0.2659,                 loss: nan
Episode: 36961/101000 (36.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9669s / 8778.0448 s
agent0:                 episode reward: -0.4571,                 loss: 0.1756
agent1:                 episode reward: 0.4571,                 loss: nan
Episode: 36981/101000 (36.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1643s / 8784.2092 s
agent0:                 episode reward: 0.1764,                 loss: 0.1783
agent1:                 episode reward: -0.1764,                 loss: nan
Episode: 37001/101000 (36.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7769s / 8792.9860 s
agent0:                 episode reward: 0.0212,                 loss: 0.1771
agent1:                 episode reward: -0.0212,                 loss: nan
Episode: 37021/101000 (36.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6184s / 8801.6044 s
agent0:                 episode reward: -0.1391,                 loss: 0.1774
agent1:                 episode reward: 0.1391,                 loss: nan
Episode: 37041/101000 (36.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8382s / 8810.4427 s
agent0:                 episode reward: 0.1194,                 loss: 0.1772
agent1:                 episode reward: -0.1194,                 loss: nan
Episode: 37061/101000 (36.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4661s / 8818.9088 s
agent0:                 episode reward: -0.2339,                 loss: 0.1759
agent1:                 episode reward: 0.2339,                 loss: nan
Episode: 37081/101000 (36.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2249s / 8828.1337 s
agent0:                 episode reward: 0.1024,                 loss: 0.1765
agent1:                 episode reward: -0.1024,                 loss: nan
Episode: 37101/101000 (36.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1184s / 8835.2521 s
agent0:                 episode reward: 0.1366,                 loss: 0.1772
agent1:                 episode reward: -0.1366,                 loss: 0.1605
Score delta: 1.7992581869021802, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/36663_0.
Episode: 37121/101000 (36.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8607s / 8841.1128 s
agent0:                 episode reward: -0.2815,                 loss: nan
agent1:                 episode reward: 0.2815,                 loss: 0.1604
Episode: 37141/101000 (36.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7070s / 8847.8198 s
agent0:                 episode reward: -0.4060,                 loss: nan
agent1:                 episode reward: 0.4060,                 loss: 0.1608
Episode: 37161/101000 (36.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7668s / 8856.5866 s
agent0:                 episode reward: 0.0145,                 loss: 0.1744
agent1:                 episode reward: -0.0145,                 loss: 0.1602
Score delta: 1.6443821538861456, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/36720_1.
Episode: 37181/101000 (36.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2663s / 8866.8529 s
agent0:                 episode reward: -0.1079,                 loss: 0.1739
agent1:                 episode reward: 0.1079,                 loss: nan
Episode: 37201/101000 (36.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1860s / 8877.0389 s
agent0:                 episode reward: -0.0983,                 loss: 0.1737
agent1:                 episode reward: 0.0983,                 loss: nan
Episode: 37221/101000 (36.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8832s / 8884.9221 s
agent0:                 episode reward: 0.5707,                 loss: 0.1752
agent1:                 episode reward: -0.5707,                 loss: nan
Score delta: 1.598290907085957, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/36795_0.
Episode: 37241/101000 (36.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7139s / 8892.6359 s
agent0:                 episode reward: -0.3498,                 loss: nan
agent1:                 episode reward: 0.3498,                 loss: 0.1597
Episode: 37261/101000 (36.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7690s / 8899.4049 s
agent0:                 episode reward: -0.3476,                 loss: nan
agent1:                 episode reward: 0.3476,                 loss: 0.1612
Episode: 37281/101000 (36.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6219s / 8905.0268 s
agent0:                 episode reward: 0.1314,                 loss: nan
agent1:                 episode reward: -0.1314,                 loss: 0.1593
Episode: 37301/101000 (36.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1498s / 8913.1766 s
agent0:                 episode reward: -0.2547,                 loss: nan
agent1:                 episode reward: 0.2547,                 loss: 0.1622
Episode: 37321/101000 (36.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1869s / 8921.3635 s
agent0:                 episode reward: -0.2825,                 loss: 0.1696
agent1:                 episode reward: 0.2825,                 loss: 0.1607
Score delta: 1.8021822813544062, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/36881_1.
Episode: 37341/101000 (36.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5135s / 8928.8770 s
agent0:                 episode reward: -0.2804,                 loss: 0.1736
agent1:                 episode reward: 0.2804,                 loss: nan
Episode: 37361/101000 (36.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3335s / 8937.2105 s
agent0:                 episode reward: -0.0225,                 loss: 0.1729
agent1:                 episode reward: 0.0225,                 loss: nan
Episode: 37381/101000 (37.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0182s / 8944.2287 s
agent0:                 episode reward: -0.0504,                 loss: 0.1713
agent1:                 episode reward: 0.0504,                 loss: nan
Episode: 37401/101000 (37.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6428s / 8952.8716 s
agent0:                 episode reward: 0.2320,                 loss: 0.1726
agent1:                 episode reward: -0.2320,                 loss: nan
Episode: 37421/101000 (37.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9839s / 8960.8554 s
agent0:                 episode reward: -0.0310,                 loss: 0.1710
agent1:                 episode reward: 0.0310,                 loss: nan
Episode: 37441/101000 (37.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7403s / 8968.5958 s
agent0:                 episode reward: 0.0763,                 loss: 0.1712
agent1:                 episode reward: -0.0763,                 loss: nan
Episode: 37461/101000 (37.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1026s / 8976.6983 s
agent0:                 episode reward: 0.2558,                 loss: 0.1721
agent1:                 episode reward: -0.2558,                 loss: nan
Episode: 37481/101000 (37.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8403s / 8985.5386 s
agent0:                 episode reward: -0.0216,                 loss: 0.1726
agent1:                 episode reward: 0.0216,                 loss: nan
Episode: 37501/101000 (37.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6186s / 8992.1572 s
agent0:                 episode reward: 0.4178,                 loss: 0.1707
agent1:                 episode reward: -0.4178,                 loss: nan
Episode: 37521/101000 (37.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8269s / 9001.9841 s
agent0:                 episode reward: 0.0502,                 loss: 0.1731
agent1:                 episode reward: -0.0502,                 loss: nan
Episode: 37541/101000 (37.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0606s / 9008.0447 s
agent0:                 episode reward: -0.3231,                 loss: 0.1726
agent1:                 episode reward: 0.3231,                 loss: nan
Episode: 37561/101000 (37.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8061s / 9016.8508 s
agent0:                 episode reward: 0.4195,                 loss: 0.1719
agent1:                 episode reward: -0.4195,                 loss: nan
Episode: 37581/101000 (37.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4738s / 9026.3246 s
agent0:                 episode reward: -0.0027,                 loss: 0.1728
agent1:                 episode reward: 0.0027,                 loss: nan
Episode: 37601/101000 (37.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7733s / 9037.0979 s
agent0:                 episode reward: -0.1391,                 loss: 0.1725
agent1:                 episode reward: 0.1391,                 loss: nan
Episode: 37621/101000 (37.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1146s / 9047.2124 s
agent0:                 episode reward: -0.2777,                 loss: 0.1730
agent1:                 episode reward: 0.2777,                 loss: nan
Episode: 37641/101000 (37.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9856s / 9055.1980 s
agent0:                 episode reward: -0.0865,                 loss: 0.1722
agent1:                 episode reward: 0.0865,                 loss: nan
Episode: 37661/101000 (37.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0718s / 9064.2698 s
agent0:                 episode reward: -0.3329,                 loss: 0.1741
agent1:                 episode reward: 0.3329,                 loss: nan
Episode: 37681/101000 (37.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3767s / 9073.6465 s
agent0:                 episode reward: 0.4918,                 loss: 0.1763
agent1:                 episode reward: -0.4918,                 loss: nan
Episode: 37701/101000 (37.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9472s / 9080.5937 s
agent0:                 episode reward: -0.1191,                 loss: 0.1769
agent1:                 episode reward: 0.1191,                 loss: nan
Episode: 37721/101000 (37.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3125s / 9088.9062 s
agent0:                 episode reward: 0.0199,                 loss: 0.1769
agent1:                 episode reward: -0.0199,                 loss: nan
Episode: 37741/101000 (37.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2257s / 9097.1319 s
agent0:                 episode reward: -0.0325,                 loss: 0.1785
agent1:                 episode reward: 0.0325,                 loss: nan
Episode: 37761/101000 (37.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9106s / 9105.0425 s
agent0:                 episode reward: 0.0337,                 loss: 0.1766
agent1:                 episode reward: -0.0337,                 loss: nan
Episode: 37781/101000 (37.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4564s / 9114.4989 s
agent0:                 episode reward: 0.1392,                 loss: 0.1770
agent1:                 episode reward: -0.1392,                 loss: nan
Episode: 37801/101000 (37.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5107s / 9123.0095 s
agent0:                 episode reward: -0.0581,                 loss: 0.1777
agent1:                 episode reward: 0.0581,                 loss: nan
Episode: 37821/101000 (37.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1839s / 9131.1934 s
agent0:                 episode reward: 0.2073,                 loss: 0.1762
agent1:                 episode reward: -0.2073,                 loss: nan
Episode: 37841/101000 (37.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9792s / 9140.1726 s
agent0:                 episode reward: -0.2114,                 loss: 0.1799
agent1:                 episode reward: 0.2114,                 loss: nan
Episode: 37861/101000 (37.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8614s / 9150.0340 s
agent0:                 episode reward: -0.0249,                 loss: 0.1761
agent1:                 episode reward: 0.0249,                 loss: nan
Episode: 37881/101000 (37.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7170s / 9159.7510 s
agent0:                 episode reward: 0.3950,                 loss: 0.1783
agent1:                 episode reward: -0.3950,                 loss: nan
Episode: 37901/101000 (37.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5410s / 9170.2920 s
agent0:                 episode reward: -0.2783,                 loss: 0.1768
agent1:                 episode reward: 0.2783,                 loss: nan
Episode: 37921/101000 (37.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6363s / 9178.9283 s
agent0:                 episode reward: 0.3570,                 loss: 0.1766
agent1:                 episode reward: -0.3570,                 loss: 0.1402
Score delta: 1.682391443283769, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/37489_0.
Episode: 37941/101000 (37.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1420s / 9188.0703 s
agent0:                 episode reward: -0.2446,                 loss: nan
agent1:                 episode reward: 0.2446,                 loss: 0.1385
Episode: 37961/101000 (37.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6130s / 9194.6834 s
agent0:                 episode reward: -0.6665,                 loss: nan
agent1:                 episode reward: 0.6665,                 loss: 0.1389
Episode: 37981/101000 (37.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9602s / 9203.6435 s
agent0:                 episode reward: -0.3372,                 loss: 0.1749
agent1:                 episode reward: 0.3372,                 loss: 0.1413
Score delta: 2.0408017369256433, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/37538_1.
Episode: 38001/101000 (37.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2413s / 9210.8848 s
agent0:                 episode reward: 0.4593,                 loss: 0.1758
agent1:                 episode reward: -0.4593,                 loss: nan
Episode: 38021/101000 (37.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5261s / 9220.4108 s
agent0:                 episode reward: -0.0681,                 loss: 0.1763
agent1:                 episode reward: 0.0681,                 loss: nan
Episode: 38041/101000 (37.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2523s / 9226.6632 s
agent0:                 episode reward: 0.5044,                 loss: 0.1771
agent1:                 episode reward: -0.5044,                 loss: 0.1574
Score delta: 1.9713839422100885, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/37607_0.
Episode: 38061/101000 (37.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6104s / 9232.2736 s
agent0:                 episode reward: -0.1960,                 loss: nan
agent1:                 episode reward: 0.1960,                 loss: 0.1538
Episode: 38081/101000 (37.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0728s / 9241.3464 s
agent0:                 episode reward: -0.5690,                 loss: nan
agent1:                 episode reward: 0.5690,                 loss: 0.1533
Episode: 38101/101000 (37.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0648s / 9249.4112 s
agent0:                 episode reward: -0.2474,                 loss: nan
agent1:                 episode reward: 0.2474,                 loss: 0.1549
Episode: 38121/101000 (37.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8273s / 9257.2385 s
agent0:                 episode reward: -0.1944,                 loss: nan
agent1:                 episode reward: 0.1944,                 loss: 0.1580
Episode: 38141/101000 (37.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0993s / 9265.3378 s
agent0:                 episode reward: 0.0767,                 loss: nan
agent1:                 episode reward: -0.0767,                 loss: 0.1551
Episode: 38161/101000 (37.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9993s / 9272.3371 s
agent0:                 episode reward: -0.5540,                 loss: 0.1742
agent1:                 episode reward: 0.5540,                 loss: 0.1560
Score delta: 1.7582259942755545, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/37734_1.
Episode: 38181/101000 (37.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8505s / 9279.1876 s
agent0:                 episode reward: -0.0557,                 loss: 0.1765
agent1:                 episode reward: 0.0557,                 loss: nan
Episode: 38201/101000 (37.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8653s / 9288.0529 s
agent0:                 episode reward: -0.2493,                 loss: 0.1752
agent1:                 episode reward: 0.2493,                 loss: nan
Episode: 38221/101000 (37.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8349s / 9297.8879 s
agent0:                 episode reward: -0.1154,                 loss: 0.1759
agent1:                 episode reward: 0.1154,                 loss: nan
Episode: 38241/101000 (37.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1258s / 9307.0136 s
agent0:                 episode reward: 0.0862,                 loss: 0.1757
agent1:                 episode reward: -0.0862,                 loss: nan
Episode: 38261/101000 (37.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0538s / 9314.0674 s
agent0:                 episode reward: 0.2472,                 loss: 0.1770
agent1:                 episode reward: -0.2472,                 loss: 0.1473
Score delta: 1.5491562003540762, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/37829_0.
Episode: 38281/101000 (37.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1389s / 9323.2063 s
agent0:                 episode reward: -0.5696,                 loss: nan
agent1:                 episode reward: 0.5696,                 loss: 0.1416
Episode: 38301/101000 (37.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5992s / 9329.8055 s
agent0:                 episode reward: -0.0815,                 loss: nan
agent1:                 episode reward: 0.0815,                 loss: 0.1424
Episode: 38321/101000 (37.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7650s / 9334.5704 s
agent0:                 episode reward: 0.0528,                 loss: nan
agent1:                 episode reward: -0.0528,                 loss: 0.1392
Episode: 38341/101000 (37.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1029s / 9341.6733 s
agent0:                 episode reward: -0.2579,                 loss: 0.1765
agent1:                 episode reward: 0.2579,                 loss: 0.1376
Score delta: 1.5587140420738343, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/37906_1.
Episode: 38361/101000 (37.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6255s / 9350.2988 s
agent0:                 episode reward: -0.2160,                 loss: 0.1718
agent1:                 episode reward: 0.2160,                 loss: nan
Episode: 38381/101000 (38.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6699s / 9360.9687 s
agent0:                 episode reward: 0.0314,                 loss: 0.1729
agent1:                 episode reward: -0.0314,                 loss: nan
Episode: 38401/101000 (38.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6342s / 9370.6029 s
agent0:                 episode reward: -0.0386,                 loss: 0.1716
agent1:                 episode reward: 0.0386,                 loss: nan
Episode: 38421/101000 (38.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9026s / 9378.5055 s
agent0:                 episode reward: -0.2863,                 loss: 0.1721
agent1:                 episode reward: 0.2863,                 loss: nan
Episode: 38441/101000 (38.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9820s / 9387.4875 s
agent0:                 episode reward: 0.3305,                 loss: 0.1728
agent1:                 episode reward: -0.3305,                 loss: 0.1402
Score delta: 1.6495342597218479, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/38011_0.
Episode: 38461/101000 (38.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6849s / 9397.1724 s
agent0:                 episode reward: -0.3295,                 loss: nan
agent1:                 episode reward: 0.3295,                 loss: 0.1406
Episode: 38481/101000 (38.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9151s / 9404.0876 s
agent0:                 episode reward: -0.0901,                 loss: nan
agent1:                 episode reward: 0.0901,                 loss: 0.1393
Episode: 38501/101000 (38.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0761s / 9411.1637 s
agent0:                 episode reward: -0.2052,                 loss: nan
agent1:                 episode reward: 0.2052,                 loss: 0.1393
Episode: 38521/101000 (38.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6469s / 9420.8105 s
agent0:                 episode reward: -0.1133,                 loss: 0.1756
agent1:                 episode reward: 0.1133,                 loss: 0.1371
Score delta: 1.8136701610720043, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/38077_1.
Episode: 38541/101000 (38.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9650s / 9427.7756 s
agent0:                 episode reward: 0.2099,                 loss: 0.1772
agent1:                 episode reward: -0.2099,                 loss: nan
Episode: 38561/101000 (38.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7147s / 9435.4903 s
agent0:                 episode reward: 0.0656,                 loss: 0.1766
agent1:                 episode reward: -0.0656,                 loss: nan
Episode: 38581/101000 (38.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6269s / 9443.1171 s
agent0:                 episode reward: 0.1201,                 loss: 0.1762
agent1:                 episode reward: -0.1201,                 loss: nan
Episode: 38601/101000 (38.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0531s / 9450.1703 s
agent0:                 episode reward: -0.3635,                 loss: 0.1774
agent1:                 episode reward: 0.3635,                 loss: nan
Episode: 38621/101000 (38.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0972s / 9457.2674 s
agent0:                 episode reward: 0.0701,                 loss: 0.1779
agent1:                 episode reward: -0.0701,                 loss: nan
Episode: 38641/101000 (38.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8361s / 9465.1035 s
agent0:                 episode reward: 0.1148,                 loss: 0.1787
agent1:                 episode reward: -0.1148,                 loss: nan
Episode: 38661/101000 (38.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3084s / 9473.4119 s
agent0:                 episode reward: 0.1402,                 loss: 0.1777
agent1:                 episode reward: -0.1402,                 loss: nan
Episode: 38681/101000 (38.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9814s / 9481.3933 s
agent0:                 episode reward: 0.1807,                 loss: 0.1778
agent1:                 episode reward: -0.1807,                 loss: nan
Episode: 38701/101000 (38.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9908s / 9487.3841 s
agent0:                 episode reward: -0.1609,                 loss: 0.1801
agent1:                 episode reward: 0.1609,                 loss: nan
Episode: 38721/101000 (38.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7165s / 9495.1006 s
agent0:                 episode reward: 0.5773,                 loss: 0.1796
agent1:                 episode reward: -0.5773,                 loss: 0.1638
Score delta: 1.5608010562518362, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/38286_0.
Episode: 38741/101000 (38.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7889s / 9502.8895 s
agent0:                 episode reward: -0.5658,                 loss: nan
agent1:                 episode reward: 0.5658,                 loss: 0.1612
Episode: 38761/101000 (38.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9838s / 9510.8733 s
agent0:                 episode reward: -0.0919,                 loss: nan
agent1:                 episode reward: 0.0919,                 loss: 0.1618
Episode: 38781/101000 (38.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6664s / 9518.5397 s
agent0:                 episode reward: -0.4676,                 loss: nan
agent1:                 episode reward: 0.4676,                 loss: 0.1630
Episode: 38801/101000 (38.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1232s / 9528.6629 s
agent0:                 episode reward: 0.0931,                 loss: 0.2000
agent1:                 episode reward: -0.0931,                 loss: 0.1640
Score delta: 1.6406927209601534, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/38356_1.
Episode: 38821/101000 (38.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6639s / 9537.3268 s
agent0:                 episode reward: -0.1679,                 loss: 0.1938
agent1:                 episode reward: 0.1679,                 loss: nan
Episode: 38841/101000 (38.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9163s / 9547.2431 s
agent0:                 episode reward: -0.2944,                 loss: 0.1905
agent1:                 episode reward: 0.2944,                 loss: nan
Episode: 38861/101000 (38.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4297s / 9554.6728 s
agent0:                 episode reward: 0.1446,                 loss: 0.1906
agent1:                 episode reward: -0.1446,                 loss: nan
Episode: 38881/101000 (38.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6019s / 9564.2747 s
agent0:                 episode reward: 0.0475,                 loss: 0.1902
agent1:                 episode reward: -0.0475,                 loss: nan
Episode: 38901/101000 (38.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6002s / 9572.8749 s
agent0:                 episode reward: -0.1597,                 loss: 0.1909
agent1:                 episode reward: 0.1597,                 loss: nan
Episode: 38921/101000 (38.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2520s / 9581.1269 s
agent0:                 episode reward: -0.2498,                 loss: 0.1914
agent1:                 episode reward: 0.2498,                 loss: nan
Episode: 38941/101000 (38.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5629s / 9590.6898 s
agent0:                 episode reward: 0.3393,                 loss: 0.1908
agent1:                 episode reward: -0.3393,                 loss: nan
Episode: 38961/101000 (38.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6527s / 9600.3425 s
agent0:                 episode reward: 0.3016,                 loss: 0.1906
agent1:                 episode reward: -0.3016,                 loss: nan
Episode: 38981/101000 (38.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8087s / 9609.1512 s
agent0:                 episode reward: -0.0043,                 loss: 0.1909
agent1:                 episode reward: 0.0043,                 loss: nan
Episode: 39001/101000 (38.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7194s / 9617.8706 s
agent0:                 episode reward: 0.1972,                 loss: 0.1911
agent1:                 episode reward: -0.1972,                 loss: 0.1603
Score delta: 1.7353735740246765, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/38567_0.
Episode: 39021/101000 (38.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2974s / 9628.1680 s
agent0:                 episode reward: -0.1867,                 loss: nan
agent1:                 episode reward: 0.1867,                 loss: 0.1597
Episode: 39041/101000 (38.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8236s / 9635.9916 s
agent0:                 episode reward: -0.0166,                 loss: nan
agent1:                 episode reward: 0.0166,                 loss: 0.1612
Episode: 39061/101000 (38.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0693s / 9643.0609 s
agent0:                 episode reward: -0.5407,                 loss: nan
agent1:                 episode reward: 0.5407,                 loss: 0.1585
Score delta: 1.610542297817831, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/38635_1.
Episode: 39081/101000 (38.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3299s / 9651.3908 s
agent0:                 episode reward: -0.6206,                 loss: 0.1991
agent1:                 episode reward: 0.6206,                 loss: nan
Episode: 39101/101000 (38.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1865s / 9661.5773 s
agent0:                 episode reward: 0.3270,                 loss: 0.1931
agent1:                 episode reward: -0.3270,                 loss: nan
Episode: 39121/101000 (38.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4588s / 9671.0361 s
agent0:                 episode reward: 0.4064,                 loss: 0.1818
agent1:                 episode reward: -0.4064,                 loss: nan
Score delta: 1.579286819997789, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/38695_0.
Episode: 39141/101000 (38.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1636s / 9680.1997 s
agent0:                 episode reward: -0.4138,                 loss: nan
agent1:                 episode reward: 0.4138,                 loss: 0.1624
Episode: 39161/101000 (38.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9836s / 9687.1833 s
agent0:                 episode reward: 0.1282,                 loss: nan
agent1:                 episode reward: -0.1282,                 loss: 0.1622
Episode: 39181/101000 (38.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3324s / 9695.5157 s
agent0:                 episode reward: -0.5740,                 loss: nan
agent1:                 episode reward: 0.5740,                 loss: 0.1604
Episode: 39201/101000 (38.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4355s / 9701.9512 s
agent0:                 episode reward: -0.1228,                 loss: nan
agent1:                 episode reward: 0.1228,                 loss: 0.1619
Episode: 39221/101000 (38.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3793s / 9709.3305 s
agent0:                 episode reward: -0.0679,                 loss: nan
agent1:                 episode reward: 0.0679,                 loss: 0.1619
Episode: 39241/101000 (38.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6647s / 9717.9952 s
agent0:                 episode reward: -0.1819,                 loss: nan
agent1:                 episode reward: 0.1819,                 loss: 0.1608
Episode: 39261/101000 (38.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 11.4870s / 9729.4822 s
agent0:                 episode reward: -0.8421,                 loss: 0.1756
agent1:                 episode reward: 0.8421,                 loss: 0.1623
Score delta: 1.6177791391305454, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/38818_1.
Episode: 39281/101000 (38.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9112s / 9739.3934 s
agent0:                 episode reward: -0.1333,                 loss: 0.1747
agent1:                 episode reward: 0.1333,                 loss: nan
Episode: 39301/101000 (38.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3804s / 9747.7738 s
agent0:                 episode reward: 0.1406,                 loss: 0.1732
agent1:                 episode reward: -0.1406,                 loss: nan
Episode: 39321/101000 (38.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0648s / 9756.8386 s
agent0:                 episode reward: -0.3721,                 loss: 0.1733
agent1:                 episode reward: 0.3721,                 loss: nan
Episode: 39341/101000 (38.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2989s / 9764.1376 s
agent0:                 episode reward: 0.2652,                 loss: 0.1739
agent1:                 episode reward: -0.2652,                 loss: nan
Episode: 39361/101000 (38.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6416s / 9771.7792 s
agent0:                 episode reward: -0.3004,                 loss: 0.1740
agent1:                 episode reward: 0.3004,                 loss: nan
Episode: 39381/101000 (38.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1203s / 9779.8994 s
agent0:                 episode reward: -0.4199,                 loss: 0.1743
agent1:                 episode reward: 0.4199,                 loss: nan
Episode: 39401/101000 (39.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1177s / 9789.0172 s
agent0:                 episode reward: 0.2140,                 loss: 0.1734
agent1:                 episode reward: -0.2140,                 loss: nan
Episode: 39421/101000 (39.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2869s / 9798.3041 s
agent0:                 episode reward: 0.1605,                 loss: 0.1729
agent1:                 episode reward: -0.1605,                 loss: nan
Episode: 39441/101000 (39.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7343s / 9808.0384 s
agent0:                 episode reward: -0.3700,                 loss: 0.1731
agent1:                 episode reward: 0.3700,                 loss: nan
Episode: 39461/101000 (39.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8176s / 9816.8560 s
agent0:                 episode reward: 0.0537,                 loss: 0.1719
agent1:                 episode reward: -0.0537,                 loss: nan
Episode: 39481/101000 (39.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6824s / 9823.5383 s
agent0:                 episode reward: -0.2445,                 loss: 0.1702
agent1:                 episode reward: 0.2445,                 loss: nan
Episode: 39501/101000 (39.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8727s / 9832.4111 s
agent0:                 episode reward: 0.3242,                 loss: 0.1733
agent1:                 episode reward: -0.3242,                 loss: nan
Episode: 39521/101000 (39.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7354s / 9842.1465 s
agent0:                 episode reward: 0.3020,                 loss: 0.1744
agent1:                 episode reward: -0.3020,                 loss: nan
Episode: 39541/101000 (39.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9360s / 9851.0825 s
agent0:                 episode reward: 0.0370,                 loss: 0.1741
agent1:                 episode reward: -0.0370,                 loss: nan
Episode: 39561/101000 (39.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5867s / 9858.6692 s
agent0:                 episode reward: 0.2078,                 loss: 0.1754
agent1:                 episode reward: -0.2078,                 loss: 0.1654
Score delta: 1.6568812770413301, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/39124_0.
Episode: 39581/101000 (39.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6902s / 9866.3593 s
agent0:                 episode reward: -0.2935,                 loss: nan
agent1:                 episode reward: 0.2935,                 loss: 0.1635
Episode: 39601/101000 (39.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0995s / 9875.4588 s
agent0:                 episode reward: -0.2908,                 loss: nan
agent1:                 episode reward: 0.2908,                 loss: 0.1636
Episode: 39621/101000 (39.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9970s / 9884.4559 s
agent0:                 episode reward: -0.3336,                 loss: 0.1766
agent1:                 episode reward: 0.3336,                 loss: 0.1619
Score delta: 1.5080368078195208, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/39194_1.
Episode: 39641/101000 (39.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0263s / 9892.4821 s
agent0:                 episode reward: 0.1628,                 loss: 0.1729
agent1:                 episode reward: -0.1628,                 loss: nan
Episode: 39661/101000 (39.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9124s / 9901.3945 s
agent0:                 episode reward: -0.4780,                 loss: 0.1751
agent1:                 episode reward: 0.4780,                 loss: nan
Episode: 39681/101000 (39.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8665s / 9911.2610 s
agent0:                 episode reward: 0.3670,                 loss: 0.1756
agent1:                 episode reward: -0.3670,                 loss: nan
Episode: 39701/101000 (39.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7908s / 9920.0518 s
agent0:                 episode reward: 0.2515,                 loss: 0.1766
agent1:                 episode reward: -0.2515,                 loss: 0.1646
Score delta: 1.760672730659543, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/39268_0.
Episode: 39721/101000 (39.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7196s / 9926.7714 s
agent0:                 episode reward: 0.1491,                 loss: nan
agent1:                 episode reward: -0.1491,                 loss: 0.1633
Episode: 39741/101000 (39.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3697s / 9934.1411 s
agent0:                 episode reward: 0.0230,                 loss: nan
agent1:                 episode reward: -0.0230,                 loss: 0.1626
Episode: 39761/101000 (39.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7486s / 9942.8897 s
agent0:                 episode reward: 0.0471,                 loss: nan
agent1:                 episode reward: -0.0471,                 loss: 0.1633
Episode: 39781/101000 (39.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0688s / 9950.9585 s
agent0:                 episode reward: -0.1724,                 loss: nan
agent1:                 episode reward: 0.1724,                 loss: 0.1628
Episode: 39801/101000 (39.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9819s / 9958.9404 s
agent0:                 episode reward: -0.4201,                 loss: nan
agent1:                 episode reward: 0.4201,                 loss: 0.1636
Episode: 39821/101000 (39.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9500s / 9965.8904 s
agent0:                 episode reward: -0.0968,                 loss: 0.2240
agent1:                 episode reward: 0.0968,                 loss: 0.1630
Score delta: 1.8911719559354452, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/39376_1.
Episode: 39841/101000 (39.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9739s / 9974.8643 s
agent0:                 episode reward: -0.0656,                 loss: 0.2175
agent1:                 episode reward: 0.0656,                 loss: nan
Episode: 39861/101000 (39.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4880s / 9982.3523 s
agent0:                 episode reward: 0.1418,                 loss: 0.2193
agent1:                 episode reward: -0.1418,                 loss: nan
Episode: 39881/101000 (39.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6220s / 9990.9743 s
agent0:                 episode reward: 0.1064,                 loss: 0.2170
agent1:                 episode reward: -0.1064,                 loss: nan
Episode: 39901/101000 (39.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4110s / 9997.3853 s
agent0:                 episode reward: 0.3172,                 loss: 0.2181
agent1:                 episode reward: -0.3172,                 loss: nan
Episode: 39921/101000 (39.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4493s / 10006.8346 s
agent0:                 episode reward: 0.1644,                 loss: 0.2150
agent1:                 episode reward: -0.1644,                 loss: nan
Episode: 39941/101000 (39.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4999s / 10015.3345 s
agent0:                 episode reward: -0.0621,                 loss: 0.2161
agent1:                 episode reward: 0.0621,                 loss: nan
Episode: 39961/101000 (39.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8422s / 10024.1767 s
agent0:                 episode reward: -0.1562,                 loss: 0.2156
agent1:                 episode reward: 0.1562,                 loss: nan
Episode: 39981/101000 (39.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3154s / 10032.4920 s
agent0:                 episode reward: -0.0927,                 loss: 0.2161
agent1:                 episode reward: 0.0927,                 loss: nan
Episode: 40001/101000 (39.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5829s / 10040.0749 s
agent0:                 episode reward: 0.2170,                 loss: 0.2162
agent1:                 episode reward: -0.2170,                 loss: nan
Episode: 40021/101000 (39.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4813s / 10045.5562 s
agent0:                 episode reward: -0.1205,                 loss: 0.2156
agent1:                 episode reward: 0.1205,                 loss: nan
Episode: 40041/101000 (39.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1307s / 10054.6869 s
agent0:                 episode reward: -0.4172,                 loss: 0.2164
agent1:                 episode reward: 0.4172,                 loss: nan
Episode: 40061/101000 (39.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7735s / 10061.4605 s
agent0:                 episode reward: 0.2747,                 loss: 0.2133
agent1:                 episode reward: -0.2747,                 loss: 0.1597
Score delta: 1.546387592519281, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/39629_0.
Episode: 40081/101000 (39.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9803s / 10071.4408 s
agent0:                 episode reward: -0.1985,                 loss: nan
agent1:                 episode reward: 0.1985,                 loss: 0.1596
Episode: 40101/101000 (39.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4247s / 10079.8656 s
agent0:                 episode reward: 0.3290,                 loss: nan
agent1:                 episode reward: -0.3290,                 loss: 0.1574
Episode: 40121/101000 (39.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7126s / 10087.5782 s
agent0:                 episode reward: -0.2610,                 loss: nan
agent1:                 episode reward: 0.2610,                 loss: 0.1578
Episode: 40141/101000 (39.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2620s / 10093.8402 s
agent0:                 episode reward: -0.1269,                 loss: nan
agent1:                 episode reward: 0.1269,                 loss: 0.1582
Episode: 40161/101000 (39.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3372s / 10100.1773 s
agent0:                 episode reward: 0.0754,                 loss: nan
agent1:                 episode reward: -0.0754,                 loss: 0.1574
Episode: 40181/101000 (39.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9721s / 10108.1494 s
agent0:                 episode reward: -0.4186,                 loss: 0.1718
agent1:                 episode reward: 0.4186,                 loss: 0.1560
Score delta: 1.5278942764126615, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/39748_1.
Episode: 40201/101000 (39.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 11.1381s / 10119.2875 s
agent0:                 episode reward: 0.4992,                 loss: 0.1716
agent1:                 episode reward: -0.4992,                 loss: nan
Episode: 40221/101000 (39.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 11.5571s / 10130.8446 s
agent0:                 episode reward: 0.1719,                 loss: 0.1797
agent1:                 episode reward: -0.1719,                 loss: nan
Episode: 40241/101000 (39.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5068s / 10140.3514 s
agent0:                 episode reward: 0.0682,                 loss: 0.1797
agent1:                 episode reward: -0.0682,                 loss: nan
Episode: 40261/101000 (39.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9537s / 10147.3051 s
agent0:                 episode reward: 0.2873,                 loss: 0.1813
agent1:                 episode reward: -0.2873,                 loss: nan
Episode: 40281/101000 (39.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5641s / 10155.8692 s
agent0:                 episode reward: 0.0928,                 loss: 0.1800
agent1:                 episode reward: -0.0928,                 loss: nan
Episode: 40301/101000 (39.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7186s / 10160.5877 s
agent0:                 episode reward: 0.1712,                 loss: 0.1795
agent1:                 episode reward: -0.1712,                 loss: nan
Episode: 40321/101000 (39.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0132s / 10168.6010 s
agent0:                 episode reward: 0.4293,                 loss: 0.1786
agent1:                 episode reward: -0.4293,                 loss: nan
Episode: 40341/101000 (39.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2691s / 10177.8701 s
agent0:                 episode reward: 0.2631,                 loss: 0.1785
agent1:                 episode reward: -0.2631,                 loss: nan
Episode: 40361/101000 (39.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8967s / 10186.7667 s
agent0:                 episode reward: -0.5106,                 loss: 0.1787
agent1:                 episode reward: 0.5106,                 loss: nan
Episode: 40381/101000 (39.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1560s / 10194.9228 s
agent0:                 episode reward: 0.4004,                 loss: 0.1806
agent1:                 episode reward: -0.4004,                 loss: nan
Episode: 40401/101000 (40.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8282s / 10205.7509 s
agent0:                 episode reward: -0.2223,                 loss: 0.1809
agent1:                 episode reward: 0.2223,                 loss: nan
Episode: 40421/101000 (40.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5796s / 10213.3305 s
agent0:                 episode reward: -0.0678,                 loss: 0.1782
agent1:                 episode reward: 0.0678,                 loss: nan
Episode: 40441/101000 (40.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5153s / 10222.8459 s
agent0:                 episode reward: 0.1941,                 loss: 0.1780
agent1:                 episode reward: -0.1941,                 loss: nan
Episode: 40461/101000 (40.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3289s / 10230.1748 s
agent0:                 episode reward: 0.3026,                 loss: 0.1781
agent1:                 episode reward: -0.3026,                 loss: nan
Episode: 40481/101000 (40.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3838s / 10239.5585 s
agent0:                 episode reward: -0.0506,                 loss: 0.1790
agent1:                 episode reward: 0.0506,                 loss: nan
Episode: 40501/101000 (40.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3905s / 10247.9491 s
agent0:                 episode reward: -0.0211,                 loss: 0.1800
agent1:                 episode reward: 0.0211,                 loss: 0.1653
Score delta: 1.5743514537488177, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/40064_0.
Episode: 40521/101000 (40.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3057s / 10254.2547 s
agent0:                 episode reward: 0.0926,                 loss: nan
agent1:                 episode reward: -0.0926,                 loss: 0.1623
Episode: 40541/101000 (40.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1817s / 10262.4364 s
agent0:                 episode reward: -0.3050,                 loss: nan
agent1:                 episode reward: 0.3050,                 loss: 0.1615
Episode: 40561/101000 (40.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6739s / 10268.1102 s
agent0:                 episode reward: -0.0190,                 loss: nan
agent1:                 episode reward: 0.0190,                 loss: 0.1614
Episode: 40581/101000 (40.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3276s / 10275.4379 s
agent0:                 episode reward: -0.5850,                 loss: nan
agent1:                 episode reward: 0.5850,                 loss: 0.1599
Episode: 40601/101000 (40.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3968s / 10285.8347 s
agent0:                 episode reward: -0.4299,                 loss: 0.1985
agent1:                 episode reward: 0.4299,                 loss: 0.1619
Score delta: 1.7016578525419739, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/40158_1.
Episode: 40621/101000 (40.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2017s / 10295.0364 s
agent0:                 episode reward: 0.5929,                 loss: 0.1937
agent1:                 episode reward: -0.5929,                 loss: nan
Episode: 40641/101000 (40.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0866s / 10304.1229 s
agent0:                 episode reward: 0.1187,                 loss: 0.1770
agent1:                 episode reward: -0.1187,                 loss: nan
Episode: 40661/101000 (40.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2044s / 10312.3273 s
agent0:                 episode reward: -0.1437,                 loss: 0.1746
agent1:                 episode reward: 0.1437,                 loss: nan
Episode: 40681/101000 (40.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2979s / 10319.6252 s
agent0:                 episode reward: 0.1027,                 loss: 0.1748
agent1:                 episode reward: -0.1027,                 loss: nan
Episode: 40701/101000 (40.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8274s / 10327.4527 s
agent0:                 episode reward: -0.1592,                 loss: 0.1728
agent1:                 episode reward: 0.1592,                 loss: nan
Episode: 40721/101000 (40.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8828s / 10335.3355 s
agent0:                 episode reward: 0.0614,                 loss: 0.1730
agent1:                 episode reward: -0.0614,                 loss: nan
Episode: 40741/101000 (40.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2533s / 10344.5888 s
agent0:                 episode reward: 0.1973,                 loss: 0.1733
agent1:                 episode reward: -0.1973,                 loss: nan
Episode: 40761/101000 (40.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8825s / 10350.4713 s
agent0:                 episode reward: 0.2468,                 loss: 0.1723
agent1:                 episode reward: -0.2468,                 loss: nan
Episode: 40781/101000 (40.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0539s / 10359.5252 s
agent0:                 episode reward: -0.1919,                 loss: 0.1743
agent1:                 episode reward: 0.1919,                 loss: nan
Episode: 40801/101000 (40.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0558s / 10368.5810 s
agent0:                 episode reward: -0.0990,                 loss: 0.1732
agent1:                 episode reward: 0.0990,                 loss: nan
Episode: 40821/101000 (40.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9393s / 10378.5203 s
agent0:                 episode reward: 0.2306,                 loss: 0.1727
agent1:                 episode reward: -0.2306,                 loss: nan
Episode: 40841/101000 (40.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6794s / 10387.1996 s
agent0:                 episode reward: 0.1024,                 loss: 0.1749
agent1:                 episode reward: -0.1024,                 loss: nan
Episode: 40861/101000 (40.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5902s / 10395.7898 s
agent0:                 episode reward: 0.1596,                 loss: 0.1738
agent1:                 episode reward: -0.1596,                 loss: nan
Episode: 40881/101000 (40.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6569s / 10405.4467 s
agent0:                 episode reward: 0.0467,                 loss: 0.1737
agent1:                 episode reward: -0.0467,                 loss: nan
Episode: 40901/101000 (40.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5598s / 10416.0065 s
agent0:                 episode reward: 0.1769,                 loss: 0.1731
agent1:                 episode reward: -0.1769,                 loss: nan
Episode: 40921/101000 (40.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2250s / 10426.2315 s
agent0:                 episode reward: 0.3285,                 loss: 0.1721
agent1:                 episode reward: -0.3285,                 loss: nan
Episode: 40941/101000 (40.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9633s / 10434.1948 s
agent0:                 episode reward: -0.4511,                 loss: 0.1739
agent1:                 episode reward: 0.4511,                 loss: nan
Episode: 40961/101000 (40.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1413s / 10440.3361 s
agent0:                 episode reward: 0.2397,                 loss: 0.1780
agent1:                 episode reward: -0.2397,                 loss: nan
Episode: 40981/101000 (40.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6378s / 10449.9739 s
agent0:                 episode reward: 0.1273,                 loss: 0.1855
agent1:                 episode reward: -0.1273,                 loss: nan
Episode: 41001/101000 (40.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3318s / 10459.3057 s
agent0:                 episode reward: 0.1569,                 loss: 0.1847
agent1:                 episode reward: -0.1569,                 loss: nan
Episode: 41021/101000 (40.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4263s / 10467.7320 s
agent0:                 episode reward: 0.3257,                 loss: 0.1856
agent1:                 episode reward: -0.3257,                 loss: nan
Episode: 41041/101000 (40.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8679s / 10473.5999 s
agent0:                 episode reward: 0.0195,                 loss: 0.1834
agent1:                 episode reward: -0.0195,                 loss: 0.1599
Score delta: 1.5935047366844557, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/40599_0.
Episode: 41061/101000 (40.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0846s / 10482.6845 s
agent0:                 episode reward: 0.1056,                 loss: nan
agent1:                 episode reward: -0.1056,                 loss: 0.1626
Episode: 41081/101000 (40.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1469s / 10490.8314 s
agent0:                 episode reward: 0.0300,                 loss: nan
agent1:                 episode reward: -0.0300,                 loss: 0.1617
Episode: 41101/101000 (40.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5383s / 10495.3697 s
agent0:                 episode reward: -0.6833,                 loss: 0.1767
agent1:                 episode reward: 0.6833,                 loss: 0.1610
Score delta: 1.7405138131684184, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/40671_1.
Episode: 41121/101000 (40.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3469s / 10499.7166 s
agent0:                 episode reward: -0.1835,                 loss: 0.1746
agent1:                 episode reward: 0.1835,                 loss: nan
Episode: 41141/101000 (40.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0139s / 10508.7306 s
agent0:                 episode reward: -0.2211,                 loss: 0.1742
agent1:                 episode reward: 0.2211,                 loss: nan
Episode: 41161/101000 (40.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7958s / 10517.5264 s
agent0:                 episode reward: 0.4642,                 loss: 0.1767
agent1:                 episode reward: -0.4642,                 loss: nan
Episode: 41181/101000 (40.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1470s / 10524.6734 s
agent0:                 episode reward: -0.0697,                 loss: 0.1761
agent1:                 episode reward: 0.0697,                 loss: nan
Episode: 41201/101000 (40.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3968s / 10535.0702 s
agent0:                 episode reward: 0.3239,                 loss: 0.1753
agent1:                 episode reward: -0.3239,                 loss: nan
Episode: 41221/101000 (40.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2429s / 10545.3131 s
agent0:                 episode reward: -0.0253,                 loss: 0.1755
agent1:                 episode reward: 0.0253,                 loss: nan
Episode: 41241/101000 (40.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7648s / 10554.0779 s
agent0:                 episode reward: 0.5120,                 loss: 0.1762
agent1:                 episode reward: -0.5120,                 loss: 0.1612
Score delta: 1.623096970744152, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/40803_0.
Episode: 41261/101000 (40.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6548s / 10561.7327 s
agent0:                 episode reward: -0.2640,                 loss: nan
agent1:                 episode reward: 0.2640,                 loss: 0.1594
Episode: 41281/101000 (40.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3943s / 10569.1269 s
agent0:                 episode reward: -0.0998,                 loss: nan
agent1:                 episode reward: 0.0998,                 loss: 0.1598
Episode: 41301/101000 (40.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2670s / 10576.3939 s
agent0:                 episode reward: -0.3985,                 loss: nan
agent1:                 episode reward: 0.3985,                 loss: 0.1608
Episode: 41321/101000 (40.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2377s / 10585.6316 s
agent0:                 episode reward: 0.1347,                 loss: nan
agent1:                 episode reward: -0.1347,                 loss: 0.1622
Episode: 41341/101000 (40.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5641s / 10595.1957 s
agent0:                 episode reward: -0.4179,                 loss: 0.1756
agent1:                 episode reward: 0.4179,                 loss: 0.1631
Score delta: 1.5078223313123202, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/40905_1.
Episode: 41361/101000 (40.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4961s / 10600.6918 s
agent0:                 episode reward: 0.2085,                 loss: 0.1738
agent1:                 episode reward: -0.2085,                 loss: nan
Episode: 41381/101000 (40.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1666s / 10610.8584 s
agent0:                 episode reward: -0.1172,                 loss: 0.1770
agent1:                 episode reward: 0.1172,                 loss: nan
Episode: 41401/101000 (40.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3462s / 10619.2046 s
agent0:                 episode reward: -0.3215,                 loss: 0.1755
agent1:                 episode reward: 0.3215,                 loss: nan
Episode: 41421/101000 (41.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4047s / 10627.6093 s
agent0:                 episode reward: 0.6169,                 loss: 0.1766
agent1:                 episode reward: -0.6169,                 loss: nan
Score delta: 1.5344661165909312, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/40995_0.
Episode: 41441/101000 (41.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2839s / 10634.8932 s
agent0:                 episode reward: -0.2702,                 loss: nan
agent1:                 episode reward: 0.2702,                 loss: 0.1642
Episode: 41461/101000 (41.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5887s / 10641.4818 s
agent0:                 episode reward: -0.6351,                 loss: nan
agent1:                 episode reward: 0.6351,                 loss: 0.1634
Episode: 41481/101000 (41.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3893s / 10649.8712 s
agent0:                 episode reward: -0.1324,                 loss: nan
agent1:                 episode reward: 0.1324,                 loss: 0.1612
Episode: 41501/101000 (41.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 12.3220s / 10662.1932 s
agent0:                 episode reward: -0.4638,                 loss: nan
agent1:                 episode reward: 0.4638,                 loss: 0.1610
Episode: 41521/101000 (41.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7171s / 10669.9103 s
agent0:                 episode reward: 0.0932,                 loss: 0.1759
agent1:                 episode reward: -0.0932,                 loss: 0.1609
Score delta: 1.7509569014130637, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/41077_1.
Episode: 41541/101000 (41.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9093s / 10677.8196 s
agent0:                 episode reward: 0.0897,                 loss: 0.1745
agent1:                 episode reward: -0.0897,                 loss: nan
Episode: 41561/101000 (41.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6648s / 10685.4845 s
agent0:                 episode reward: 0.4937,                 loss: 0.1836
agent1:                 episode reward: -0.4937,                 loss: nan
Episode: 41581/101000 (41.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8837s / 10694.3682 s
agent0:                 episode reward: -0.2891,                 loss: 0.1911
agent1:                 episode reward: 0.2891,                 loss: nan
Episode: 41601/101000 (41.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1833s / 10704.5515 s
agent0:                 episode reward: -0.0150,                 loss: 0.1869
agent1:                 episode reward: 0.0150,                 loss: nan
Episode: 41621/101000 (41.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8941s / 10713.4456 s
agent0:                 episode reward: -0.2569,                 loss: 0.1867
agent1:                 episode reward: 0.2569,                 loss: nan
Episode: 41641/101000 (41.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9946s / 10724.4402 s
agent0:                 episode reward: -0.1950,                 loss: 0.1877
agent1:                 episode reward: 0.1950,                 loss: nan
Episode: 41661/101000 (41.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2364s / 10733.6766 s
agent0:                 episode reward: 0.1228,                 loss: 0.1884
agent1:                 episode reward: -0.1228,                 loss: nan
Episode: 41681/101000 (41.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0883s / 10741.7650 s
agent0:                 episode reward: 0.0196,                 loss: 0.1861
agent1:                 episode reward: -0.0196,                 loss: nan
Episode: 41701/101000 (41.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6863s / 10751.4513 s
agent0:                 episode reward: -0.1487,                 loss: 0.1893
agent1:                 episode reward: 0.1487,                 loss: nan
Episode: 41721/101000 (41.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9865s / 10760.4378 s
agent0:                 episode reward: 0.2447,                 loss: 0.1866
agent1:                 episode reward: -0.2447,                 loss: nan
Episode: 41741/101000 (41.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8666s / 10766.3044 s
agent0:                 episode reward: 0.0572,                 loss: 0.1882
agent1:                 episode reward: -0.0572,                 loss: nan
Episode: 41761/101000 (41.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7842s / 10776.0886 s
agent0:                 episode reward: 0.0731,                 loss: 0.1879
agent1:                 episode reward: -0.0731,                 loss: nan
Episode: 41781/101000 (41.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1822s / 10782.2709 s
agent0:                 episode reward: 0.7061,                 loss: 0.1858
agent1:                 episode reward: -0.7061,                 loss: 0.1666
Score delta: 1.6002942250214314, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/41342_0.
Episode: 41801/101000 (41.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4450s / 10790.7159 s
agent0:                 episode reward: 0.0352,                 loss: nan
agent1:                 episode reward: -0.0352,                 loss: 0.1643
Episode: 41821/101000 (41.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4529s / 10800.1688 s
agent0:                 episode reward: -0.3152,                 loss: nan
agent1:                 episode reward: 0.3152,                 loss: 0.1636
Episode: 41841/101000 (41.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1590s / 10806.3278 s
agent0:                 episode reward: -0.3656,                 loss: 0.1701
agent1:                 episode reward: 0.3656,                 loss: 0.1632
Score delta: 1.6052511706106938, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/41407_1.
Episode: 41861/101000 (41.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7678s / 10816.0956 s
agent0:                 episode reward: 0.1522,                 loss: 0.1679
agent1:                 episode reward: -0.1522,                 loss: nan
Episode: 41881/101000 (41.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0539s / 10825.1495 s
agent0:                 episode reward: -0.1372,                 loss: 0.1654
agent1:                 episode reward: 0.1372,                 loss: nan
Episode: 41901/101000 (41.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7306s / 10834.8801 s
agent0:                 episode reward: 0.1010,                 loss: 0.1636
agent1:                 episode reward: -0.1010,                 loss: nan
Episode: 41921/101000 (41.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 11.5305s / 10846.4106 s
agent0:                 episode reward: 0.1978,                 loss: 0.1652
agent1:                 episode reward: -0.1978,                 loss: nan
Episode: 41941/101000 (41.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1828s / 10855.5934 s
agent0:                 episode reward: -0.0559,                 loss: 0.1653
agent1:                 episode reward: 0.0559,                 loss: nan
Episode: 41961/101000 (41.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1594s / 10864.7528 s
agent0:                 episode reward: 0.2169,                 loss: 0.1881
agent1:                 episode reward: -0.2169,                 loss: nan
Episode: 41981/101000 (41.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2289s / 10871.9817 s
agent0:                 episode reward: 0.1930,                 loss: 0.1914
agent1:                 episode reward: -0.1930,                 loss: nan
Episode: 42001/101000 (41.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1150s / 10880.0967 s
agent0:                 episode reward: 0.4456,                 loss: 0.1901
agent1:                 episode reward: -0.4456,                 loss: 0.1589
Score delta: 1.736251940092745, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/41569_0.
Episode: 42021/101000 (41.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 11.4791s / 10891.5758 s
agent0:                 episode reward: -0.4114,                 loss: nan
agent1:                 episode reward: 0.4114,                 loss: 0.1564
Episode: 42041/101000 (41.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4971s / 10899.0729 s
agent0:                 episode reward: -0.3239,                 loss: nan
agent1:                 episode reward: 0.3239,                 loss: 0.1563
Episode: 42061/101000 (41.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0345s / 10907.1075 s
agent0:                 episode reward: -0.4483,                 loss: nan
agent1:                 episode reward: 0.4483,                 loss: 0.1560
Episode: 42081/101000 (41.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2874s / 10915.3949 s
agent0:                 episode reward: -0.2306,                 loss: 0.1724
agent1:                 episode reward: 0.2306,                 loss: 0.1564
Score delta: 1.5230727622597597, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/41652_1.
Episode: 42101/101000 (41.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3275s / 10923.7225 s
agent0:                 episode reward: -0.3400,                 loss: 0.1740
agent1:                 episode reward: 0.3400,                 loss: nan
Episode: 42121/101000 (41.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3239s / 10934.0464 s
agent0:                 episode reward: 0.2342,                 loss: 0.1730
agent1:                 episode reward: -0.2342,                 loss: nan
Episode: 42141/101000 (41.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7042s / 10942.7506 s
agent0:                 episode reward: 0.3651,                 loss: 0.1716
agent1:                 episode reward: -0.3651,                 loss: nan
Episode: 42161/101000 (41.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9424s / 10949.6930 s
agent0:                 episode reward: 0.0966,                 loss: 0.1711
agent1:                 episode reward: -0.0966,                 loss: nan
Episode: 42181/101000 (41.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5875s / 10958.2805 s
agent0:                 episode reward: 0.1972,                 loss: 0.1720
agent1:                 episode reward: -0.1972,                 loss: nan
Episode: 42201/101000 (41.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6864s / 10967.9669 s
agent0:                 episode reward: -0.0609,                 loss: 0.1732
agent1:                 episode reward: 0.0609,                 loss: 0.1643
Score delta: 1.5216104231073888, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/41766_0.
Episode: 42221/101000 (41.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1186s / 10976.0855 s
agent0:                 episode reward: 0.2159,                 loss: nan
agent1:                 episode reward: -0.2159,                 loss: 0.1637
Episode: 42241/101000 (41.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9611s / 10983.0466 s
agent0:                 episode reward: -0.1836,                 loss: nan
agent1:                 episode reward: 0.1836,                 loss: 0.1649
Episode: 42261/101000 (41.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4531s / 10990.4997 s
agent0:                 episode reward: -0.0799,                 loss: nan
agent1:                 episode reward: 0.0799,                 loss: 0.1630
Episode: 42281/101000 (41.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9545s / 10998.4542 s
agent0:                 episode reward: -0.6246,                 loss: 0.2670
agent1:                 episode reward: 0.6246,                 loss: 0.1598
Score delta: 1.5134575620248394, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/41842_1.
Episode: 42301/101000 (41.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4974s / 11008.9516 s
agent0:                 episode reward: -0.0655,                 loss: 0.2577
agent1:                 episode reward: 0.0655,                 loss: nan
Episode: 42321/101000 (41.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4099s / 11015.3615 s
agent0:                 episode reward: -0.1698,                 loss: 0.2542
agent1:                 episode reward: 0.1698,                 loss: nan
Episode: 42341/101000 (41.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9875s / 11024.3490 s
agent0:                 episode reward: -0.4856,                 loss: 0.2525
agent1:                 episode reward: 0.4856,                 loss: nan
Episode: 42361/101000 (41.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2791s / 11033.6281 s
agent0:                 episode reward: -0.0421,                 loss: 0.2514
agent1:                 episode reward: 0.0421,                 loss: nan
Episode: 42381/101000 (41.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5311s / 11041.1591 s
agent0:                 episode reward: 0.0691,                 loss: 0.2532
agent1:                 episode reward: -0.0691,                 loss: nan
Episode: 42401/101000 (41.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0515s / 11049.2107 s
agent0:                 episode reward: 0.0413,                 loss: 0.2520
agent1:                 episode reward: -0.0413,                 loss: nan
Episode: 42421/101000 (42.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3228s / 11058.5335 s
agent0:                 episode reward: 0.0542,                 loss: 0.2511
agent1:                 episode reward: -0.0542,                 loss: nan
Episode: 42441/101000 (42.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1450s / 11065.6785 s
agent0:                 episode reward: -0.1308,                 loss: 0.2389
agent1:                 episode reward: 0.1308,                 loss: nan
Episode: 42461/101000 (42.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0266s / 11075.7051 s
agent0:                 episode reward: 0.0870,                 loss: 0.1851
agent1:                 episode reward: -0.0870,                 loss: nan
Episode: 42481/101000 (42.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5566s / 11085.2617 s
agent0:                 episode reward: 0.3143,                 loss: 0.1839
agent1:                 episode reward: -0.3143,                 loss: 0.1685
Score delta: 1.6224848811206982, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/42047_0.
Episode: 42501/101000 (42.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8357s / 11092.0974 s
agent0:                 episode reward: -0.1350,                 loss: nan
agent1:                 episode reward: 0.1350,                 loss: 0.1649
Episode: 42521/101000 (42.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8794s / 11101.9768 s
agent0:                 episode reward: -0.1680,                 loss: nan
agent1:                 episode reward: 0.1680,                 loss: 0.1624
Episode: 42541/101000 (42.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4089s / 11109.3857 s
agent0:                 episode reward: -0.4221,                 loss: nan
agent1:                 episode reward: 0.4221,                 loss: 0.1636
Episode: 42561/101000 (42.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7668s / 11114.1525 s
agent0:                 episode reward: 0.0201,                 loss: nan
agent1:                 episode reward: -0.0201,                 loss: 0.1643
Episode: 42581/101000 (42.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2396s / 11120.3922 s
agent0:                 episode reward: -0.3266,                 loss: nan
agent1:                 episode reward: 0.3266,                 loss: 0.1644
Episode: 42601/101000 (42.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4022s / 11129.7944 s
agent0:                 episode reward: -0.2346,                 loss: nan
agent1:                 episode reward: 0.2346,                 loss: 0.1628
Episode: 42621/101000 (42.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6998s / 11137.4942 s
agent0:                 episode reward: -0.1865,                 loss: nan
agent1:                 episode reward: 0.1865,                 loss: 0.1630
Episode: 42641/101000 (42.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3494s / 11145.8436 s
agent0:                 episode reward: -0.1045,                 loss: nan
agent1:                 episode reward: 0.1045,                 loss: 0.1612
Episode: 42661/101000 (42.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9909s / 11154.8345 s
agent0:                 episode reward: -0.4252,                 loss: 0.2003
agent1:                 episode reward: 0.4252,                 loss: 0.1652
Score delta: 1.60275386524822, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/42218_1.
Episode: 42681/101000 (42.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7160s / 11163.5505 s
agent0:                 episode reward: 0.1876,                 loss: 0.1917
agent1:                 episode reward: -0.1876,                 loss: nan
Episode: 42701/101000 (42.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1995s / 11172.7500 s
agent0:                 episode reward: -0.1251,                 loss: 0.1881
agent1:                 episode reward: 0.1251,                 loss: nan
Episode: 42721/101000 (42.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5018s / 11180.2518 s
agent0:                 episode reward: -0.3488,                 loss: 0.1921
agent1:                 episode reward: 0.3488,                 loss: nan
Episode: 42741/101000 (42.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4833s / 11187.7351 s
agent0:                 episode reward: 0.3097,                 loss: 0.1929
agent1:                 episode reward: -0.3097,                 loss: 0.1617
Score delta: 1.5036816180395631, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/42305_0.
Episode: 42761/101000 (42.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0119s / 11195.7470 s
agent0:                 episode reward: -0.0436,                 loss: nan
agent1:                 episode reward: 0.0436,                 loss: 0.1615
Episode: 42781/101000 (42.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1845s / 11204.9315 s
agent0:                 episode reward: -0.3687,                 loss: nan
agent1:                 episode reward: 0.3687,                 loss: 0.1615
Episode: 42801/101000 (42.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7081s / 11214.6396 s
agent0:                 episode reward: -0.1507,                 loss: nan
agent1:                 episode reward: 0.1507,                 loss: 0.1617
Episode: 42821/101000 (42.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4021s / 11222.0417 s
agent0:                 episode reward: -0.1961,                 loss: nan
agent1:                 episode reward: 0.1961,                 loss: 0.1611
Episode: 42841/101000 (42.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5962s / 11230.6379 s
agent0:                 episode reward: -0.0038,                 loss: nan
agent1:                 episode reward: 0.0038,                 loss: 0.1612
Episode: 42861/101000 (42.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6948s / 11240.3327 s
agent0:                 episode reward: -0.2038,                 loss: nan
agent1:                 episode reward: 0.2038,                 loss: 0.1594
Score delta: 1.5971823281540096, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/42435_1.
Episode: 42881/101000 (42.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2583s / 11249.5910 s
agent0:                 episode reward: 0.1432,                 loss: 0.1748
agent1:                 episode reward: -0.1432,                 loss: nan
Episode: 42901/101000 (42.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3675s / 11258.9585 s
agent0:                 episode reward: 0.2559,                 loss: 0.1726
agent1:                 episode reward: -0.2559,                 loss: nan
Episode: 42921/101000 (42.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7377s / 11265.6963 s
agent0:                 episode reward: -0.0233,                 loss: 0.1744
agent1:                 episode reward: 0.0233,                 loss: nan
Episode: 42941/101000 (42.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2426s / 11273.9389 s
agent0:                 episode reward: 0.2399,                 loss: 0.1746
agent1:                 episode reward: -0.2399,                 loss: nan
Episode: 42961/101000 (42.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1355s / 11282.0744 s
agent0:                 episode reward: 0.2424,                 loss: 0.1733
agent1:                 episode reward: -0.2424,                 loss: nan
Episode: 42981/101000 (42.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3295s / 11290.4038 s
agent0:                 episode reward: -0.0939,                 loss: 0.1768
agent1:                 episode reward: 0.0939,                 loss: 0.1654
Score delta: 1.934312970103108, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/42538_0.
Episode: 43001/101000 (42.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0571s / 11298.4609 s
agent0:                 episode reward: 0.1402,                 loss: nan
agent1:                 episode reward: -0.1402,                 loss: 0.1627
Episode: 43021/101000 (42.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0055s / 11306.4665 s
agent0:                 episode reward: 0.0291,                 loss: nan
agent1:                 episode reward: -0.0291,                 loss: 0.1608
Episode: 43041/101000 (42.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4302s / 11314.8967 s
agent0:                 episode reward: -0.5999,                 loss: 0.1733
agent1:                 episode reward: 0.5999,                 loss: 0.1608
Score delta: 1.6844701783197977, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/42606_1.
Episode: 43061/101000 (42.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9583s / 11323.8550 s
agent0:                 episode reward: 0.5103,                 loss: 0.1672
agent1:                 episode reward: -0.5103,                 loss: nan
Episode: 43081/101000 (42.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8837s / 11332.7388 s
agent0:                 episode reward: -0.2592,                 loss: 0.1702
agent1:                 episode reward: 0.2592,                 loss: nan
Episode: 43101/101000 (42.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6085s / 11342.3473 s
agent0:                 episode reward: 0.0840,                 loss: 0.1683
agent1:                 episode reward: -0.0840,                 loss: nan
Episode: 43121/101000 (42.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6084s / 11349.9556 s
agent0:                 episode reward: 0.1579,                 loss: 0.1690
agent1:                 episode reward: -0.1579,                 loss: nan
Episode: 43141/101000 (42.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5692s / 11359.5249 s
agent0:                 episode reward: 0.4203,                 loss: 0.1627
agent1:                 episode reward: -0.4203,                 loss: 0.1636
Score delta: 1.5382938574979181, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/42701_0.
Episode: 43161/101000 (42.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4500s / 11366.9749 s
agent0:                 episode reward: -0.3340,                 loss: nan
agent1:                 episode reward: 0.3340,                 loss: 0.1614
Episode: 43181/101000 (42.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2969s / 11373.2718 s
agent0:                 episode reward: -0.3882,                 loss: nan
agent1:                 episode reward: 0.3882,                 loss: 0.1605
Episode: 43201/101000 (42.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7904s / 11378.0622 s
agent0:                 episode reward: 0.1841,                 loss: nan
agent1:                 episode reward: -0.1841,                 loss: 0.1597
Episode: 43221/101000 (42.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9643s / 11385.0265 s
agent0:                 episode reward: -0.2075,                 loss: nan
agent1:                 episode reward: 0.2075,                 loss: 0.1591
Episode: 43241/101000 (42.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8358s / 11392.8623 s
agent0:                 episode reward: -0.0915,                 loss: nan
agent1:                 episode reward: 0.0915,                 loss: 0.1590
Episode: 43261/101000 (42.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7540s / 11401.6163 s
agent0:                 episode reward: -0.1148,                 loss: nan
agent1:                 episode reward: 0.1148,                 loss: 0.1579
Episode: 43281/101000 (42.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0626s / 11409.6789 s
agent0:                 episode reward: -0.4947,                 loss: 0.1729
agent1:                 episode reward: 0.4947,                 loss: 0.1591
Score delta: 1.5313760998420247, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/42847_1.
Episode: 43301/101000 (42.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2930s / 11418.9719 s
agent0:                 episode reward: 0.1543,                 loss: 0.1732
agent1:                 episode reward: -0.1543,                 loss: nan
Episode: 43321/101000 (42.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4304s / 11428.4023 s
agent0:                 episode reward: 0.4885,                 loss: 0.1719
agent1:                 episode reward: -0.4885,                 loss: nan
Episode: 43341/101000 (42.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1439s / 11435.5461 s
agent0:                 episode reward: 0.1423,                 loss: 0.1713
agent1:                 episode reward: -0.1423,                 loss: nan
Episode: 43361/101000 (42.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3088s / 11445.8550 s
agent0:                 episode reward: -0.0227,                 loss: 0.1733
agent1:                 episode reward: 0.0227,                 loss: nan
Episode: 43381/101000 (42.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5695s / 11455.4245 s
agent0:                 episode reward: -0.0740,                 loss: 0.1733
agent1:                 episode reward: 0.0740,                 loss: nan
Episode: 43401/101000 (42.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8322s / 11464.2567 s
agent0:                 episode reward: -0.1704,                 loss: 0.1727
agent1:                 episode reward: 0.1704,                 loss: nan
Episode: 43421/101000 (42.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2734s / 11472.5301 s
agent0:                 episode reward: 0.0804,                 loss: 0.1743
agent1:                 episode reward: -0.0804,                 loss: nan
Episode: 43441/101000 (43.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4894s / 11482.0195 s
agent0:                 episode reward: -0.1496,                 loss: 0.1727
agent1:                 episode reward: 0.1496,                 loss: nan
Episode: 43461/101000 (43.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7462s / 11489.7657 s
agent0:                 episode reward: 0.2087,                 loss: 0.1710
agent1:                 episode reward: -0.2087,                 loss: nan
Episode: 43481/101000 (43.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9310s / 11498.6967 s
agent0:                 episode reward: 0.0032,                 loss: 0.1729
agent1:                 episode reward: -0.0032,                 loss: nan
Episode: 43501/101000 (43.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3714s / 11507.0681 s
agent0:                 episode reward: 0.4170,                 loss: 0.1726
agent1:                 episode reward: -0.4170,                 loss: nan
Episode: 43521/101000 (43.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6509s / 11514.7190 s
agent0:                 episode reward: 0.2928,                 loss: 0.1735
agent1:                 episode reward: -0.2928,                 loss: nan
Episode: 43541/101000 (43.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9888s / 11522.7078 s
agent0:                 episode reward: 0.1069,                 loss: 0.1720
agent1:                 episode reward: -0.1069,                 loss: nan
Episode: 43561/101000 (43.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6474s / 11532.3552 s
agent0:                 episode reward: -0.4276,                 loss: 0.1735
agent1:                 episode reward: 0.4276,                 loss: nan
Episode: 43581/101000 (43.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5936s / 11539.9488 s
agent0:                 episode reward: -0.0456,                 loss: 0.1732
agent1:                 episode reward: 0.0456,                 loss: nan
Episode: 43601/101000 (43.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5258s / 11550.4746 s
agent0:                 episode reward: 0.4050,                 loss: 0.1714
agent1:                 episode reward: -0.4050,                 loss: 0.1509
Score delta: 1.5117503802019097, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/43174_0.
Episode: 43621/101000 (43.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9563s / 11558.4308 s
agent0:                 episode reward: -0.9998,                 loss: nan
agent1:                 episode reward: 0.9998,                 loss: 0.1460
Episode: 43641/101000 (43.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5269s / 11564.9578 s
agent0:                 episode reward: -0.4456,                 loss: nan
agent1:                 episode reward: 0.4456,                 loss: 0.1438
Episode: 43661/101000 (43.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3928s / 11572.3506 s
agent0:                 episode reward: -0.1919,                 loss: nan
agent1:                 episode reward: 0.1919,                 loss: 0.1440
Episode: 43681/101000 (43.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0124s / 11580.3630 s
agent0:                 episode reward: -0.2967,                 loss: nan
agent1:                 episode reward: 0.2967,                 loss: 0.1429
Episode: 43701/101000 (43.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1131s / 11587.4762 s
agent0:                 episode reward: -0.4541,                 loss: 0.1757
agent1:                 episode reward: 0.4541,                 loss: 0.1430
Score delta: 1.5066978802397621, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/43269_1.
Episode: 43721/101000 (43.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3642s / 11596.8404 s
agent0:                 episode reward: -0.3369,                 loss: 0.1734
agent1:                 episode reward: 0.3369,                 loss: nan
Episode: 43741/101000 (43.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0728s / 11602.9132 s
agent0:                 episode reward: 0.1060,                 loss: 0.1840
agent1:                 episode reward: -0.1060,                 loss: nan
Episode: 43761/101000 (43.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0901s / 11613.0033 s
agent0:                 episode reward: 0.1935,                 loss: 0.1813
agent1:                 episode reward: -0.1935,                 loss: nan
Episode: 43781/101000 (43.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6305s / 11619.6338 s
agent0:                 episode reward: 0.1666,                 loss: 0.1830
agent1:                 episode reward: -0.1666,                 loss: 0.1650
Score delta: 1.8411728747278702, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/43341_0.
Episode: 43801/101000 (43.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4737s / 11627.1075 s
agent0:                 episode reward: -0.8860,                 loss: nan
agent1:                 episode reward: 0.8860,                 loss: 0.1634
Episode: 43821/101000 (43.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5816s / 11634.6891 s
agent0:                 episode reward: -0.4215,                 loss: nan
agent1:                 episode reward: 0.4215,                 loss: 0.1647
Episode: 43841/101000 (43.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0245s / 11642.7136 s
agent0:                 episode reward: -0.2122,                 loss: nan
agent1:                 episode reward: 0.2122,                 loss: 0.1612
Episode: 43861/101000 (43.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0259s / 11650.7395 s
agent0:                 episode reward: -0.2301,                 loss: nan
agent1:                 episode reward: 0.2301,                 loss: 0.1625
Episode: 43881/101000 (43.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0403s / 11656.7798 s
agent0:                 episode reward: -0.0146,                 loss: nan
agent1:                 episode reward: 0.0146,                 loss: 0.1616
Episode: 43901/101000 (43.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3035s / 11665.0833 s
agent0:                 episode reward: -0.5046,                 loss: 0.3065
agent1:                 episode reward: 0.5046,                 loss: 0.1621
Score delta: 1.5665859551342252, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/43472_1.
Episode: 43921/101000 (43.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3511s / 11675.4344 s
agent0:                 episode reward: -0.3434,                 loss: 0.2583
agent1:                 episode reward: 0.3434,                 loss: nan
Episode: 43941/101000 (43.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4204s / 11685.8548 s
agent0:                 episode reward: 0.3820,                 loss: 0.2520
agent1:                 episode reward: -0.3820,                 loss: nan
Episode: 43961/101000 (43.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7395s / 11694.5943 s
agent0:                 episode reward: -0.5396,                 loss: 0.2496
agent1:                 episode reward: 0.5396,                 loss: nan
Episode: 43981/101000 (43.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9241s / 11703.5185 s
agent0:                 episode reward: -0.4360,                 loss: 0.2502
agent1:                 episode reward: 0.4360,                 loss: nan
Episode: 44001/101000 (43.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5624s / 11711.0808 s
agent0:                 episode reward: 0.2399,                 loss: 0.2496
agent1:                 episode reward: -0.2399,                 loss: nan
Episode: 44021/101000 (43.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0373s / 11721.1181 s
agent0:                 episode reward: -0.3188,                 loss: 0.2504
agent1:                 episode reward: 0.3188,                 loss: nan
Episode: 44041/101000 (43.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9942s / 11732.1123 s
agent0:                 episode reward: 0.0037,                 loss: 0.2492
agent1:                 episode reward: -0.0037,                 loss: nan
Episode: 44061/101000 (43.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7677s / 11739.8800 s
agent0:                 episode reward: 0.3111,                 loss: 0.2522
agent1:                 episode reward: -0.3111,                 loss: nan
Episode: 44081/101000 (43.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0803s / 11747.9602 s
agent0:                 episode reward: -0.0920,                 loss: 0.2519
agent1:                 episode reward: 0.0920,                 loss: nan
Episode: 44101/101000 (43.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3774s / 11757.3376 s
agent0:                 episode reward: -0.1010,                 loss: 0.2492
agent1:                 episode reward: 0.1010,                 loss: nan
Episode: 44121/101000 (43.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4545s / 11766.7922 s
agent0:                 episode reward: 0.2818,                 loss: 0.2513
agent1:                 episode reward: -0.2818,                 loss: nan
Episode: 44141/101000 (43.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7015s / 11774.4936 s
agent0:                 episode reward: 0.0362,                 loss: 0.2548
agent1:                 episode reward: -0.0362,                 loss: 0.1610
Score delta: 1.9090488546764466, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/43699_0.
Episode: 44161/101000 (43.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1095s / 11781.6032 s
agent0:                 episode reward: -0.2307,                 loss: nan
agent1:                 episode reward: 0.2307,                 loss: 0.1584
Episode: 44181/101000 (43.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5541s / 11790.1572 s
agent0:                 episode reward: -0.3247,                 loss: nan
agent1:                 episode reward: 0.3247,                 loss: 0.1601
Episode: 44201/101000 (43.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7515s / 11799.9087 s
agent0:                 episode reward: -0.4445,                 loss: nan
agent1:                 episode reward: 0.4445,                 loss: 0.1575
Episode: 44221/101000 (43.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4782s / 11810.3869 s
agent0:                 episode reward: -0.0380,                 loss: nan
agent1:                 episode reward: 0.0380,                 loss: 0.1602
Episode: 44241/101000 (43.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8381s / 11819.2250 s
agent0:                 episode reward: -0.5051,                 loss: 0.2035
agent1:                 episode reward: 0.5051,                 loss: 0.1572
Score delta: 1.6032019016195733, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/43800_1.
Episode: 44261/101000 (43.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3374s / 11826.5624 s
agent0:                 episode reward: 0.1820,                 loss: 0.1930
agent1:                 episode reward: -0.1820,                 loss: nan
Episode: 44281/101000 (43.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8391s / 11835.4014 s
agent0:                 episode reward: -0.2255,                 loss: 0.1927
agent1:                 episode reward: 0.2255,                 loss: nan
Episode: 44301/101000 (43.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0797s / 11843.4811 s
agent0:                 episode reward: 0.1881,                 loss: 0.1836
agent1:                 episode reward: -0.1881,                 loss: nan
Episode: 44321/101000 (43.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2951s / 11851.7762 s
agent0:                 episode reward: 0.1725,                 loss: 0.1842
agent1:                 episode reward: -0.1725,                 loss: nan
Episode: 44341/101000 (43.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5442s / 11860.3204 s
agent0:                 episode reward: -0.0201,                 loss: 0.1842
agent1:                 episode reward: 0.0201,                 loss: nan
Episode: 44361/101000 (43.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9133s / 11868.2337 s
agent0:                 episode reward: -0.5037,                 loss: 0.1856
agent1:                 episode reward: 0.5037,                 loss: nan
Episode: 44381/101000 (43.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6132s / 11875.8469 s
agent0:                 episode reward: 0.3762,                 loss: 0.1814
agent1:                 episode reward: -0.3762,                 loss: nan
Episode: 44401/101000 (43.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0178s / 11885.8646 s
agent0:                 episode reward: 0.2460,                 loss: 0.1843
agent1:                 episode reward: -0.2460,                 loss: nan
Episode: 44421/101000 (43.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1317s / 11894.9963 s
agent0:                 episode reward: 0.2972,                 loss: 0.1830
agent1:                 episode reward: -0.2972,                 loss: nan
Episode: 44441/101000 (44.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3621s / 11903.3584 s
agent0:                 episode reward: 0.0214,                 loss: 0.1806
agent1:                 episode reward: -0.0214,                 loss: nan
Episode: 44461/101000 (44.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4121s / 11911.7706 s
agent0:                 episode reward: 0.1506,                 loss: 0.1841
agent1:                 episode reward: -0.1506,                 loss: nan
Episode: 44481/101000 (44.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0760s / 11918.8465 s
agent0:                 episode reward: 0.3671,                 loss: 0.1811
agent1:                 episode reward: -0.3671,                 loss: 0.1408
Score delta: 1.6418706128595368, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/44049_0.
Episode: 44501/101000 (44.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5637s / 11926.4102 s
agent0:                 episode reward: -0.2135,                 loss: nan
agent1:                 episode reward: 0.2135,                 loss: 0.1401
Episode: 44521/101000 (44.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4787s / 11936.8889 s
agent0:                 episode reward: -0.7354,                 loss: nan
agent1:                 episode reward: 0.7354,                 loss: 0.1420
Episode: 44541/101000 (44.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7635s / 11944.6524 s
agent0:                 episode reward: -0.3903,                 loss: nan
agent1:                 episode reward: 0.3903,                 loss: 0.1418
Episode: 44561/101000 (44.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2657s / 11951.9181 s
agent0:                 episode reward: 0.0701,                 loss: nan
agent1:                 episode reward: -0.0701,                 loss: 0.1413
Episode: 44581/101000 (44.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4363s / 11961.3544 s
agent0:                 episode reward: -0.5532,                 loss: 0.1724
agent1:                 episode reward: 0.5532,                 loss: 0.1420
Score delta: 1.503699113488776, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/44154_1.
Episode: 44601/101000 (44.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9953s / 11971.3497 s
agent0:                 episode reward: -0.4380,                 loss: 0.1715
agent1:                 episode reward: 0.4380,                 loss: nan
Episode: 44621/101000 (44.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5954s / 11980.9451 s
agent0:                 episode reward: 0.1065,                 loss: 0.1691
agent1:                 episode reward: -0.1065,                 loss: nan
Episode: 44641/101000 (44.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7507s / 11991.6958 s
agent0:                 episode reward: -0.0292,                 loss: 0.1692
agent1:                 episode reward: 0.0292,                 loss: nan
Episode: 44661/101000 (44.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7187s / 12000.4145 s
agent0:                 episode reward: -0.0660,                 loss: 0.1690
agent1:                 episode reward: 0.0660,                 loss: nan
Episode: 44681/101000 (44.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9521s / 12008.3666 s
agent0:                 episode reward: -0.2967,                 loss: 0.1689
agent1:                 episode reward: 0.2967,                 loss: nan
Episode: 44701/101000 (44.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3718s / 12017.7384 s
agent0:                 episode reward: 0.2501,                 loss: 0.1690
agent1:                 episode reward: -0.2501,                 loss: nan
Episode: 44721/101000 (44.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3320s / 12027.0704 s
agent0:                 episode reward: 0.3165,                 loss: 0.1729
agent1:                 episode reward: -0.3165,                 loss: nan
Episode: 44741/101000 (44.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9651s / 12037.0355 s
agent0:                 episode reward: 0.0926,                 loss: 0.1885
agent1:                 episode reward: -0.0926,                 loss: nan
Episode: 44761/101000 (44.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6370s / 12044.6726 s
agent0:                 episode reward: -0.1144,                 loss: 0.1890
agent1:                 episode reward: 0.1144,                 loss: nan
Episode: 44781/101000 (44.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4686s / 12054.1411 s
agent0:                 episode reward: 0.1636,                 loss: 0.1868
agent1:                 episode reward: -0.1636,                 loss: nan
Episode: 44801/101000 (44.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0379s / 12061.1790 s
agent0:                 episode reward: 0.3122,                 loss: 0.1886
agent1:                 episode reward: -0.3122,                 loss: 0.1612
Score delta: 1.6499807304405387, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/44362_0.
Episode: 44821/101000 (44.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5934s / 12067.7724 s
agent0:                 episode reward: -0.1208,                 loss: nan
agent1:                 episode reward: 0.1208,                 loss: 0.1613
Episode: 44841/101000 (44.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8418s / 12076.6142 s
agent0:                 episode reward: -0.3488,                 loss: nan
agent1:                 episode reward: 0.3488,                 loss: 0.1622
Episode: 44861/101000 (44.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9433s / 12085.5576 s
agent0:                 episode reward: -0.5077,                 loss: nan
agent1:                 episode reward: 0.5077,                 loss: 0.1611
Episode: 44881/101000 (44.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0574s / 12093.6150 s
agent0:                 episode reward: -0.2424,                 loss: nan
agent1:                 episode reward: 0.2424,                 loss: 0.1614
Episode: 44901/101000 (44.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9535s / 12101.5685 s
agent0:                 episode reward: -0.0471,                 loss: nan
agent1:                 episode reward: 0.0471,                 loss: 0.1609
Episode: 44921/101000 (44.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1859s / 12108.7543 s
agent0:                 episode reward: -0.1034,                 loss: nan
agent1:                 episode reward: 0.1034,                 loss: 0.1610
Episode: 44941/101000 (44.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1303s / 12116.8847 s
agent0:                 episode reward: -0.4522,                 loss: nan
agent1:                 episode reward: 0.4522,                 loss: 0.1589
Episode: 44961/101000 (44.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1455s / 12127.0302 s
agent0:                 episode reward: -0.3014,                 loss: nan
agent1:                 episode reward: 0.3014,                 loss: 0.1599
Episode: 44981/101000 (44.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3993s / 12136.4294 s
agent0:                 episode reward: -0.2703,                 loss: 0.1924
agent1:                 episode reward: 0.2703,                 loss: 0.1621
Score delta: 1.5942258959696303, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/44549_1.
Episode: 45001/101000 (44.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5248s / 12144.9543 s
agent0:                 episode reward: -0.1752,                 loss: 0.1810
agent1:                 episode reward: 0.1752,                 loss: nan
Episode: 45021/101000 (44.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8987s / 12153.8530 s
agent0:                 episode reward: 0.2902,                 loss: 0.1813
agent1:                 episode reward: -0.2902,                 loss: nan
Episode: 45041/101000 (44.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8118s / 12161.6648 s
agent0:                 episode reward: 0.2169,                 loss: 0.1803
agent1:                 episode reward: -0.2169,                 loss: nan
Episode: 45061/101000 (44.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2139s / 12168.8787 s
agent0:                 episode reward: 0.5037,                 loss: 0.1796
agent1:                 episode reward: -0.5037,                 loss: 0.1647
Score delta: 1.5727938239565238, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/44624_0.
Episode: 45081/101000 (44.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7556s / 12177.6343 s
agent0:                 episode reward: -0.3710,                 loss: nan
agent1:                 episode reward: 0.3710,                 loss: 0.1657
Episode: 45101/101000 (44.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1762s / 12185.8105 s
agent0:                 episode reward: -0.2270,                 loss: nan
agent1:                 episode reward: 0.2270,                 loss: 0.1634
Episode: 45121/101000 (44.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8500s / 12194.6605 s
agent0:                 episode reward: -0.2600,                 loss: nan
agent1:                 episode reward: 0.2600,                 loss: 0.1635
Episode: 45141/101000 (44.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5412s / 12203.2017 s
agent0:                 episode reward: -0.3261,                 loss: nan
agent1:                 episode reward: 0.3261,                 loss: 0.1640
Episode: 45161/101000 (44.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3035s / 12211.5052 s
agent0:                 episode reward: 0.0830,                 loss: nan
agent1:                 episode reward: -0.0830,                 loss: 0.1644
Episode: 45181/101000 (44.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2519s / 12220.7571 s
agent0:                 episode reward: -0.4551,                 loss: 0.1716
agent1:                 episode reward: 0.4551,                 loss: 0.1640
Score delta: 1.6273381881841764, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/44749_1.
Episode: 45201/101000 (44.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2934s / 12228.0505 s
agent0:                 episode reward: -0.2871,                 loss: 0.1700
agent1:                 episode reward: 0.2871,                 loss: nan
Episode: 45221/101000 (44.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1936s / 12237.2441 s
agent0:                 episode reward: 0.0924,                 loss: 0.1686
agent1:                 episode reward: -0.0924,                 loss: nan
Episode: 45241/101000 (44.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9904s / 12245.2345 s
agent0:                 episode reward: 0.1932,                 loss: 0.1691
agent1:                 episode reward: -0.1932,                 loss: nan
Episode: 45261/101000 (44.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2108s / 12255.4452 s
agent0:                 episode reward: 0.0160,                 loss: 0.1709
agent1:                 episode reward: -0.0160,                 loss: nan
Episode: 45281/101000 (44.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2433s / 12262.6885 s
agent0:                 episode reward: 0.1886,                 loss: 0.1693
agent1:                 episode reward: -0.1886,                 loss: nan
Episode: 45301/101000 (44.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5121s / 12273.2006 s
agent0:                 episode reward: 0.4438,                 loss: 0.1687
agent1:                 episode reward: -0.4438,                 loss: nan
Episode: 45321/101000 (44.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7077s / 12282.9082 s
agent0:                 episode reward: 0.2678,                 loss: 0.1675
agent1:                 episode reward: -0.2678,                 loss: nan
Episode: 45341/101000 (44.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9223s / 12293.8305 s
agent0:                 episode reward: 0.2221,                 loss: 0.1680
agent1:                 episode reward: -0.2221,                 loss: nan
Episode: 45361/101000 (44.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6361s / 12302.4666 s
agent0:                 episode reward: 0.1309,                 loss: 0.1689
agent1:                 episode reward: -0.1309,                 loss: nan
Episode: 45381/101000 (44.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9653s / 12310.4319 s
agent0:                 episode reward: 0.0712,                 loss: 0.1888
agent1:                 episode reward: -0.0712,                 loss: nan
Episode: 45401/101000 (44.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2093s / 12319.6412 s
agent0:                 episode reward: -0.0942,                 loss: 0.1902
agent1:                 episode reward: 0.0942,                 loss: nan
Episode: 45421/101000 (44.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6558s / 12325.2970 s
agent0:                 episode reward: 0.3649,                 loss: 0.1888
agent1:                 episode reward: -0.3649,                 loss: 0.1562
Score delta: 1.6815537081403371, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/44985_0.
Episode: 45441/101000 (44.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3976s / 12334.6945 s
agent0:                 episode reward: -0.1184,                 loss: nan
agent1:                 episode reward: 0.1184,                 loss: 0.1546
Episode: 45461/101000 (45.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5276s / 12342.2221 s
agent0:                 episode reward: -0.5194,                 loss: nan
agent1:                 episode reward: 0.5194,                 loss: 0.1578
Episode: 45481/101000 (45.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4205s / 12350.6425 s
agent0:                 episode reward: -0.1768,                 loss: nan
agent1:                 episode reward: 0.1768,                 loss: 0.1548
Episode: 45501/101000 (45.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9306s / 12359.5731 s
agent0:                 episode reward: -0.4488,                 loss: 0.2116
agent1:                 episode reward: 0.4488,                 loss: 0.1538
Score delta: 1.7693241326414473, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/45073_1.
Episode: 45521/101000 (45.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0373s / 12369.6104 s
agent0:                 episode reward: 0.0606,                 loss: 0.1916
agent1:                 episode reward: -0.0606,                 loss: nan
Episode: 45541/101000 (45.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1521s / 12377.7625 s
agent0:                 episode reward: -0.1304,                 loss: 0.1911
agent1:                 episode reward: 0.1304,                 loss: nan
Episode: 45561/101000 (45.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5351s / 12386.2976 s
agent0:                 episode reward: 0.0006,                 loss: 0.1914
agent1:                 episode reward: -0.0006,                 loss: nan
Episode: 45581/101000 (45.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6835s / 12394.9811 s
agent0:                 episode reward: -0.0944,                 loss: 0.1900
agent1:                 episode reward: 0.0944,                 loss: nan
Episode: 45601/101000 (45.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1573s / 12405.1384 s
agent0:                 episode reward: -0.1096,                 loss: 0.1891
agent1:                 episode reward: 0.1096,                 loss: nan
Episode: 45621/101000 (45.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 11.4295s / 12416.5680 s
agent0:                 episode reward: 0.1336,                 loss: 0.1894
agent1:                 episode reward: -0.1336,                 loss: 0.1661
Score delta: 1.617112859105966, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/45191_0.
Episode: 45641/101000 (45.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3649s / 12424.9329 s
agent0:                 episode reward: -0.3611,                 loss: nan
agent1:                 episode reward: 0.3611,                 loss: 0.1646
Episode: 45661/101000 (45.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4795s / 12432.4124 s
agent0:                 episode reward: -0.4317,                 loss: nan
agent1:                 episode reward: 0.4317,                 loss: 0.1640
Episode: 45681/101000 (45.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2162s / 12438.6286 s
agent0:                 episode reward: -0.5202,                 loss: nan
agent1:                 episode reward: 0.5202,                 loss: 0.1644
Episode: 45701/101000 (45.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3052s / 12445.9337 s
agent0:                 episode reward: -0.4820,                 loss: nan
agent1:                 episode reward: 0.4820,                 loss: 0.1639
Episode: 45721/101000 (45.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2163s / 12455.1500 s
agent0:                 episode reward: -0.4693,                 loss: nan
agent1:                 episode reward: 0.4693,                 loss: 0.1631
Score delta: 1.5165267432123117, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/45295_1.
Episode: 45741/101000 (45.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0088s / 12465.1588 s
agent0:                 episode reward: -0.2011,                 loss: 0.1799
agent1:                 episode reward: 0.2011,                 loss: nan
Episode: 45761/101000 (45.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8793s / 12476.0381 s
agent0:                 episode reward: 0.1792,                 loss: 0.1822
agent1:                 episode reward: -0.1792,                 loss: nan
Episode: 45781/101000 (45.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2525s / 12485.2907 s
agent0:                 episode reward: -0.0041,                 loss: 0.1826
agent1:                 episode reward: 0.0041,                 loss: nan
Episode: 45801/101000 (45.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2936s / 12493.5842 s
agent0:                 episode reward: -0.1130,                 loss: 0.1809
agent1:                 episode reward: 0.1130,                 loss: nan
Episode: 45821/101000 (45.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5910s / 12502.1752 s
agent0:                 episode reward: 0.1900,                 loss: 0.1801
agent1:                 episode reward: -0.1900,                 loss: nan
Episode: 45841/101000 (45.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1180s / 12511.2932 s
agent0:                 episode reward: 0.3258,                 loss: 0.1811
agent1:                 episode reward: -0.3258,                 loss: nan
Episode: 45861/101000 (45.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4859s / 12519.7791 s
agent0:                 episode reward: 0.2154,                 loss: 0.1787
agent1:                 episode reward: -0.2154,                 loss: nan
Episode: 45881/101000 (45.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6943s / 12529.4734 s
agent0:                 episode reward: 0.0365,                 loss: 0.1788
agent1:                 episode reward: -0.0365,                 loss: nan
Episode: 45901/101000 (45.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6933s / 12539.1667 s
agent0:                 episode reward: 0.0420,                 loss: 0.1891
agent1:                 episode reward: -0.0420,                 loss: nan
Episode: 45921/101000 (45.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3194s / 12548.4861 s
agent0:                 episode reward: -0.0225,                 loss: 0.1921
agent1:                 episode reward: 0.0225,                 loss: nan
Episode: 45941/101000 (45.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0843s / 12557.5705 s
agent0:                 episode reward: 0.3253,                 loss: 0.1882
agent1:                 episode reward: -0.3253,                 loss: nan
Episode: 45961/101000 (45.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3938s / 12565.9642 s
agent0:                 episode reward: -0.1566,                 loss: 0.1953
agent1:                 episode reward: 0.1566,                 loss: 0.1599
Score delta: 1.5053892526429824, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/45521_0.
Episode: 45981/101000 (45.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5090s / 12576.4733 s
agent0:                 episode reward: -0.6283,                 loss: nan
agent1:                 episode reward: 0.6283,                 loss: 0.1579
Episode: 46001/101000 (45.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2260s / 12584.6993 s
agent0:                 episode reward: -0.3680,                 loss: nan
agent1:                 episode reward: 0.3680,                 loss: 0.1577
Episode: 46021/101000 (45.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6954s / 12593.3947 s
agent0:                 episode reward: -0.7595,                 loss: nan
agent1:                 episode reward: 0.7595,                 loss: 0.1565
Episode: 46041/101000 (45.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8878s / 12601.2825 s
agent0:                 episode reward: -0.0256,                 loss: 0.1889
agent1:                 episode reward: 0.0256,                 loss: 0.1549
Score delta: 1.6678837572886542, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/45599_1.
Episode: 46061/101000 (45.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5339s / 12611.8164 s
agent0:                 episode reward: 0.1862,                 loss: 0.1867
agent1:                 episode reward: -0.1862,                 loss: nan
Episode: 46081/101000 (45.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5901s / 12619.4066 s
agent0:                 episode reward: 0.2364,                 loss: 0.1855
agent1:                 episode reward: -0.2364,                 loss: nan
Episode: 46101/101000 (45.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5902s / 12628.9968 s
agent0:                 episode reward: 0.0468,                 loss: 0.1854
agent1:                 episode reward: -0.0468,                 loss: nan
Episode: 46121/101000 (45.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6413s / 12637.6381 s
agent0:                 episode reward: 0.4462,                 loss: 0.1863
agent1:                 episode reward: -0.4462,                 loss: 0.1555
Score delta: 1.7831817054358345, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/45691_0.
Episode: 46141/101000 (45.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6006s / 12646.2387 s
agent0:                 episode reward: -0.0939,                 loss: nan
agent1:                 episode reward: 0.0939,                 loss: 0.1597
Episode: 46161/101000 (45.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0817s / 12653.3204 s
agent0:                 episode reward: -0.1904,                 loss: nan
agent1:                 episode reward: 0.1904,                 loss: 0.1603
Episode: 46181/101000 (45.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3053s / 12663.6257 s
agent0:                 episode reward: -0.8951,                 loss: nan
agent1:                 episode reward: 0.8951,                 loss: 0.1589
Episode: 46201/101000 (45.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4927s / 12672.1184 s
agent0:                 episode reward: -0.5177,                 loss: 0.1754
agent1:                 episode reward: 0.5177,                 loss: 0.1590
Score delta: 1.64035516873608, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/45770_1.
Episode: 46221/101000 (45.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9515s / 12679.0699 s
agent0:                 episode reward: 0.2451,                 loss: 0.1744
agent1:                 episode reward: -0.2451,                 loss: nan
Episode: 46241/101000 (45.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2994s / 12688.3694 s
agent0:                 episode reward: 0.1887,                 loss: 0.1743
agent1:                 episode reward: -0.1887,                 loss: nan
Episode: 46261/101000 (45.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4220s / 12697.7914 s
agent0:                 episode reward: 0.2852,                 loss: 0.1753
agent1:                 episode reward: -0.2852,                 loss: nan
Episode: 46281/101000 (45.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6587s / 12707.4501 s
agent0:                 episode reward: -0.0418,                 loss: 0.1735
agent1:                 episode reward: 0.0418,                 loss: nan
Episode: 46301/101000 (45.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4397s / 12711.8898 s
agent0:                 episode reward: 0.3259,                 loss: 0.1727
agent1:                 episode reward: -0.3259,                 loss: nan
Episode: 46321/101000 (45.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3819s / 12718.2717 s
agent0:                 episode reward: -0.4823,                 loss: 0.1724
agent1:                 episode reward: 0.4823,                 loss: nan
Episode: 46341/101000 (45.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6492s / 12728.9209 s
agent0:                 episode reward: 0.0663,                 loss: 0.1739
agent1:                 episode reward: -0.0663,                 loss: nan
Episode: 46361/101000 (45.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0661s / 12737.9870 s
agent0:                 episode reward: 0.2971,                 loss: 0.1726
agent1:                 episode reward: -0.2971,                 loss: nan
Episode: 46381/101000 (45.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9978s / 12747.9848 s
agent0:                 episode reward: 0.2761,                 loss: 0.1763
agent1:                 episode reward: -0.2761,                 loss: nan
Episode: 46401/101000 (45.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5646s / 12757.5493 s
agent0:                 episode reward: -0.0087,                 loss: 0.1824
agent1:                 episode reward: 0.0087,                 loss: nan
Episode: 46421/101000 (45.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6975s / 12767.2468 s
agent0:                 episode reward: -0.1344,                 loss: 0.1840
agent1:                 episode reward: 0.1344,                 loss: nan
Episode: 46441/101000 (45.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7218s / 12774.9686 s
agent0:                 episode reward: -0.3460,                 loss: 0.1806
agent1:                 episode reward: 0.3460,                 loss: nan
Episode: 46461/101000 (46.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4787s / 12784.4473 s
agent0:                 episode reward: 0.2330,                 loss: 0.1816
agent1:                 episode reward: -0.2330,                 loss: nan
Episode: 46481/101000 (46.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6590s / 12794.1064 s
agent0:                 episode reward: 0.3797,                 loss: 0.1826
agent1:                 episode reward: -0.3797,                 loss: nan
Episode: 46501/101000 (46.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2525s / 12803.3589 s
agent0:                 episode reward: 0.1018,                 loss: 0.1825
agent1:                 episode reward: -0.1018,                 loss: nan
Episode: 46521/101000 (46.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1894s / 12811.5482 s
agent0:                 episode reward: 0.1098,                 loss: 0.1794
agent1:                 episode reward: -0.1098,                 loss: 0.1595
Score delta: 1.6846108156831903, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/46078_0.
Episode: 46541/101000 (46.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0458s / 12817.5940 s
agent0:                 episode reward: -0.2829,                 loss: nan
agent1:                 episode reward: 0.2829,                 loss: 0.1600
Episode: 46561/101000 (46.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1415s / 12825.7356 s
agent0:                 episode reward: -0.7592,                 loss: nan
agent1:                 episode reward: 0.7592,                 loss: 0.1602
Episode: 46581/101000 (46.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3597s / 12835.0952 s
agent0:                 episode reward: -0.2306,                 loss: nan
agent1:                 episode reward: 0.2306,                 loss: 0.1603
Episode: 46601/101000 (46.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9884s / 12844.0836 s
agent0:                 episode reward: -0.1745,                 loss: nan
agent1:                 episode reward: 0.1745,                 loss: 0.1612
Episode: 46621/101000 (46.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9489s / 12851.0325 s
agent0:                 episode reward: -0.2413,                 loss: nan
agent1:                 episode reward: 0.2413,                 loss: 0.1600
Episode: 46641/101000 (46.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0034s / 12860.0359 s
agent0:                 episode reward: -0.4216,                 loss: 0.1895
agent1:                 episode reward: 0.4216,                 loss: 0.1619
Score delta: 1.6074989933568613, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/46199_1.
Episode: 46661/101000 (46.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9914s / 12869.0274 s
agent0:                 episode reward: 0.0015,                 loss: 0.1908
agent1:                 episode reward: -0.0015,                 loss: nan
Episode: 46681/101000 (46.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4362s / 12875.4635 s
agent0:                 episode reward: 0.0939,                 loss: 0.1891
agent1:                 episode reward: -0.0939,                 loss: nan
Episode: 46701/101000 (46.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0728s / 12884.5363 s
agent0:                 episode reward: 0.3406,                 loss: 0.1896
agent1:                 episode reward: -0.3406,                 loss: nan
Episode: 46721/101000 (46.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7343s / 12894.2706 s
agent0:                 episode reward: 0.2068,                 loss: 0.1896
agent1:                 episode reward: -0.2068,                 loss: nan
Episode: 46741/101000 (46.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2285s / 12902.4991 s
agent0:                 episode reward: -0.0327,                 loss: 0.1908
agent1:                 episode reward: 0.0327,                 loss: nan
Episode: 46761/101000 (46.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0542s / 12909.5533 s
agent0:                 episode reward: 0.1590,                 loss: 0.1894
agent1:                 episode reward: -0.1590,                 loss: nan
Episode: 46781/101000 (46.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3107s / 12918.8640 s
agent0:                 episode reward: 0.1336,                 loss: 0.1896
agent1:                 episode reward: -0.1336,                 loss: nan
Episode: 46801/101000 (46.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7835s / 12926.6475 s
agent0:                 episode reward: -0.4267,                 loss: 0.1907
agent1:                 episode reward: 0.4267,                 loss: nan
Episode: 46821/101000 (46.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8849s / 12933.5324 s
agent0:                 episode reward: 0.2178,                 loss: 0.1887
agent1:                 episode reward: -0.2178,                 loss: nan
Episode: 46841/101000 (46.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 11.8934s / 12945.4258 s
agent0:                 episode reward: 0.1991,                 loss: 0.1867
agent1:                 episode reward: -0.1991,                 loss: nan
Episode: 46861/101000 (46.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6842s / 12956.1101 s
agent0:                 episode reward: -0.0232,                 loss: 0.1813
agent1:                 episode reward: 0.0232,                 loss: nan
Episode: 46881/101000 (46.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7759s / 12964.8860 s
agent0:                 episode reward: 0.3596,                 loss: 0.1827
agent1:                 episode reward: -0.3596,                 loss: nan
Episode: 46901/101000 (46.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3549s / 12972.2409 s
agent0:                 episode reward: -0.5892,                 loss: 0.1833
agent1:                 episode reward: 0.5892,                 loss: nan
Episode: 46921/101000 (46.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3775s / 12979.6183 s
agent0:                 episode reward: -0.0689,                 loss: 0.1822
agent1:                 episode reward: 0.0689,                 loss: nan
Episode: 46941/101000 (46.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9812s / 12987.5996 s
agent0:                 episode reward: 0.1230,                 loss: 0.1827
agent1:                 episode reward: -0.1230,                 loss: nan
Episode: 46961/101000 (46.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4909s / 12996.0905 s
agent0:                 episode reward: 0.2022,                 loss: 0.1832
agent1:                 episode reward: -0.2022,                 loss: nan
Episode: 46981/101000 (46.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9651s / 13005.0556 s
agent0:                 episode reward: 0.0786,                 loss: 0.1818
agent1:                 episode reward: -0.0786,                 loss: nan
Episode: 47001/101000 (46.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2173s / 13014.2728 s
agent0:                 episode reward: -0.0378,                 loss: 0.1812
agent1:                 episode reward: 0.0378,                 loss: nan
Episode: 47021/101000 (46.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0713s / 13023.3441 s
agent0:                 episode reward: -0.4502,                 loss: 0.1820
agent1:                 episode reward: 0.4502,                 loss: nan
Episode: 47041/101000 (46.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6732s / 13031.0173 s
agent0:                 episode reward: 0.1800,                 loss: 0.1822
agent1:                 episode reward: -0.1800,                 loss: nan
Episode: 47061/101000 (46.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9379s / 13038.9552 s
agent0:                 episode reward: 0.0411,                 loss: 0.1825
agent1:                 episode reward: -0.0411,                 loss: nan
Episode: 47081/101000 (46.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5793s / 13047.5345 s
agent0:                 episode reward: -0.1366,                 loss: 0.1828
agent1:                 episode reward: 0.1366,                 loss: nan
Episode: 47101/101000 (46.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2273s / 13056.7617 s
agent0:                 episode reward: -0.1711,                 loss: 0.1824
agent1:                 episode reward: 0.1711,                 loss: nan
Episode: 47121/101000 (46.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 12.8002s / 13069.5620 s
agent0:                 episode reward: -0.4037,                 loss: 0.1832
agent1:                 episode reward: 0.4037,                 loss: nan
Episode: 47141/101000 (46.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0046s / 13077.5665 s
agent0:                 episode reward: 0.5137,                 loss: 0.1840
agent1:                 episode reward: -0.5137,                 loss: 0.1461
Score delta: 1.656703367133325, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/46713_0.
Episode: 47161/101000 (46.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6572s / 13085.2237 s
agent0:                 episode reward: -0.2917,                 loss: nan
agent1:                 episode reward: 0.2917,                 loss: 0.1417
Episode: 47181/101000 (46.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3121s / 13094.5359 s
agent0:                 episode reward: 0.1316,                 loss: nan
agent1:                 episode reward: -0.1316,                 loss: 0.1415
Episode: 47201/101000 (46.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9107s / 13101.4466 s
agent0:                 episode reward: -0.4795,                 loss: nan
agent1:                 episode reward: 0.4795,                 loss: 0.1426
Episode: 47221/101000 (46.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9854s / 13111.4320 s
agent0:                 episode reward: -0.2624,                 loss: nan
agent1:                 episode reward: 0.2624,                 loss: 0.1419
Episode: 47241/101000 (46.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0022s / 13120.4342 s
agent0:                 episode reward: 0.1175,                 loss: 0.1851
agent1:                 episode reward: -0.1175,                 loss: 0.1472
Score delta: 1.5387938799431982, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/46796_1.
Episode: 47261/101000 (46.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0753s / 13129.5095 s
agent0:                 episode reward: 0.1860,                 loss: 0.1792
agent1:                 episode reward: -0.1860,                 loss: nan
Episode: 47281/101000 (46.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6763s / 13136.1858 s
agent0:                 episode reward: 0.1314,                 loss: 0.1806
agent1:                 episode reward: -0.1314,                 loss: nan
Episode: 47301/101000 (46.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2326s / 13146.4184 s
agent0:                 episode reward: -0.1592,                 loss: 0.1761
agent1:                 episode reward: 0.1592,                 loss: nan
Episode: 47321/101000 (46.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6681s / 13154.0865 s
agent0:                 episode reward: 0.3920,                 loss: 0.1783
agent1:                 episode reward: -0.3920,                 loss: nan
Episode: 47341/101000 (46.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4086s / 13162.4951 s
agent0:                 episode reward: 0.0447,                 loss: 0.1785
agent1:                 episode reward: -0.0447,                 loss: nan
Episode: 47361/101000 (46.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0531s / 13169.5482 s
agent0:                 episode reward: -0.1092,                 loss: 0.1762
agent1:                 episode reward: 0.1092,                 loss: nan
Episode: 47381/101000 (46.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1933s / 13179.7415 s
agent0:                 episode reward: 0.1871,                 loss: 0.1756
agent1:                 episode reward: -0.1871,                 loss: nan
Episode: 47401/101000 (46.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1543s / 13188.8959 s
agent0:                 episode reward: 0.2861,                 loss: 0.1767
agent1:                 episode reward: -0.2861,                 loss: 0.1460
Score delta: 1.7725197406842532, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/46974_0.
Episode: 47421/101000 (46.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8808s / 13195.7767 s
agent0:                 episode reward: -0.2329,                 loss: nan
agent1:                 episode reward: 0.2329,                 loss: 0.1417
Episode: 47441/101000 (46.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8866s / 13204.6633 s
agent0:                 episode reward: -0.1247,                 loss: nan
agent1:                 episode reward: 0.1247,                 loss: 0.1415
Episode: 47461/101000 (46.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3104s / 13212.9737 s
agent0:                 episode reward: -0.4509,                 loss: nan
agent1:                 episode reward: 0.4509,                 loss: 0.1428
Episode: 47481/101000 (47.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8315s / 13218.8051 s
agent0:                 episode reward: -0.5008,                 loss: nan
agent1:                 episode reward: 0.5008,                 loss: 0.1404
Episode: 47501/101000 (47.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5599s / 13225.3651 s
agent0:                 episode reward: -0.2863,                 loss: nan
agent1:                 episode reward: 0.2863,                 loss: 0.1414
Episode: 47521/101000 (47.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4900s / 13233.8551 s
agent0:                 episode reward: -0.3374,                 loss: nan
agent1:                 episode reward: 0.3374,                 loss: 0.1411
Episode: 47541/101000 (47.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8597s / 13241.7148 s
agent0:                 episode reward: -0.2589,                 loss: nan
agent1:                 episode reward: 0.2589,                 loss: 0.1395
Episode: 47561/101000 (47.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8999s / 13250.6147 s
agent0:                 episode reward: -0.1524,                 loss: nan
agent1:                 episode reward: 0.1524,                 loss: 0.1410
Episode: 47581/101000 (47.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6498s / 13259.2646 s
agent0:                 episode reward: -0.2857,                 loss: nan
agent1:                 episode reward: 0.2857,                 loss: 0.1443
Episode: 47601/101000 (47.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0335s / 13268.2981 s
agent0:                 episode reward: 0.0077,                 loss: 0.1814
agent1:                 episode reward: -0.0077,                 loss: 0.1612
Score delta: 1.6355424201052202, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/47171_1.
Episode: 47621/101000 (47.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5961s / 13277.8942 s
agent0:                 episode reward: -0.0412,                 loss: 0.1795
agent1:                 episode reward: 0.0412,                 loss: nan
Episode: 47641/101000 (47.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 12.0962s / 13289.9904 s
agent0:                 episode reward: 0.2465,                 loss: 0.1788
agent1:                 episode reward: -0.2465,                 loss: nan
Episode: 47661/101000 (47.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8916s / 13298.8819 s
agent0:                 episode reward: 0.2405,                 loss: 0.1789
agent1:                 episode reward: -0.2405,                 loss: nan
Episode: 47681/101000 (47.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6968s / 13308.5787 s
agent0:                 episode reward: 0.1690,                 loss: 0.1784
agent1:                 episode reward: -0.1690,                 loss: nan
Episode: 47701/101000 (47.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9886s / 13318.5673 s
agent0:                 episode reward: 0.1937,                 loss: 0.1736
agent1:                 episode reward: -0.1937,                 loss: 0.1458
Score delta: 1.6274505880830852, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/47256_0.
Episode: 47721/101000 (47.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4247s / 13325.9920 s
agent0:                 episode reward: -0.0420,                 loss: nan
agent1:                 episode reward: 0.0420,                 loss: 0.1415
Episode: 47741/101000 (47.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0965s / 13335.0885 s
agent0:                 episode reward: -0.2196,                 loss: nan
agent1:                 episode reward: 0.2196,                 loss: 0.1406
Episode: 47761/101000 (47.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8941s / 13341.9826 s
agent0:                 episode reward: -0.8789,                 loss: nan
agent1:                 episode reward: 0.8789,                 loss: 0.1427
Episode: 47781/101000 (47.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 11.1141s / 13353.0967 s
agent0:                 episode reward: -0.1970,                 loss: 0.1755
agent1:                 episode reward: 0.1970,                 loss: 0.1418
Score delta: 1.507463212105493, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/47349_1.
Episode: 47801/101000 (47.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2234s / 13362.3201 s
agent0:                 episode reward: 0.0083,                 loss: 0.1759
agent1:                 episode reward: -0.0083,                 loss: nan
Episode: 47821/101000 (47.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4460s / 13370.7661 s
agent0:                 episode reward: -0.0302,                 loss: 0.1730
agent1:                 episode reward: 0.0302,                 loss: nan
Episode: 47841/101000 (47.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3499s / 13380.1160 s
agent0:                 episode reward: 0.0241,                 loss: 0.1716
agent1:                 episode reward: -0.0241,                 loss: nan
Episode: 47861/101000 (47.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4970s / 13390.6130 s
agent0:                 episode reward: 0.1651,                 loss: 0.1734
agent1:                 episode reward: -0.1651,                 loss: nan
Episode: 47881/101000 (47.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7588s / 13399.3719 s
agent0:                 episode reward: 0.6180,                 loss: 0.1725
agent1:                 episode reward: -0.6180,                 loss: 0.1632
Score delta: 1.642244803679887, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/47446_0.
Episode: 47901/101000 (47.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9461s / 13407.3179 s
agent0:                 episode reward: -0.4379,                 loss: nan
agent1:                 episode reward: 0.4379,                 loss: 0.1625
Episode: 47921/101000 (47.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4781s / 13413.7960 s
agent0:                 episode reward: -0.4928,                 loss: nan
agent1:                 episode reward: 0.4928,                 loss: 0.1611
Episode: 47941/101000 (47.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3352s / 13422.1312 s
agent0:                 episode reward: -0.5169,                 loss: nan
agent1:                 episode reward: 0.5169,                 loss: 0.1600
Episode: 47961/101000 (47.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1573s / 13429.2885 s
agent0:                 episode reward: -0.2961,                 loss: nan
agent1:                 episode reward: 0.2961,                 loss: 0.1606
Episode: 47981/101000 (47.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9255s / 13436.2139 s
agent0:                 episode reward: -0.4951,                 loss: 0.3284
agent1:                 episode reward: 0.4951,                 loss: 0.1595
Score delta: 1.571432088767648, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/47554_1.
Episode: 48001/101000 (47.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9126s / 13447.1265 s
agent0:                 episode reward: -0.4177,                 loss: 0.2584
agent1:                 episode reward: 0.4177,                 loss: nan
Episode: 48021/101000 (47.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8586s / 13455.9851 s
agent0:                 episode reward: -0.0088,                 loss: 0.2469
agent1:                 episode reward: 0.0088,                 loss: nan
Episode: 48041/101000 (47.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5480s / 13464.5331 s
agent0:                 episode reward: 0.0899,                 loss: 0.2482
agent1:                 episode reward: -0.0899,                 loss: nan
Episode: 48061/101000 (47.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4811s / 13474.0143 s
agent0:                 episode reward: -0.6596,                 loss: 0.2469
agent1:                 episode reward: 0.6596,                 loss: nan
Episode: 48081/101000 (47.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8629s / 13482.8772 s
agent0:                 episode reward: -0.1426,                 loss: 0.2443
agent1:                 episode reward: 0.1426,                 loss: nan
Episode: 48101/101000 (47.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6796s / 13490.5568 s
agent0:                 episode reward: 0.1944,                 loss: 0.2463
agent1:                 episode reward: -0.1944,                 loss: 0.1434
Score delta: 1.686776326763577, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/47662_0.
Episode: 48121/101000 (47.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6516s / 13497.2084 s
agent0:                 episode reward: -0.1303,                 loss: nan
agent1:                 episode reward: 0.1303,                 loss: 0.1413
Episode: 48141/101000 (47.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1038s / 13507.3121 s
agent0:                 episode reward: -0.1403,                 loss: nan
agent1:                 episode reward: 0.1403,                 loss: 0.1419
Episode: 48161/101000 (47.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1720s / 13512.4841 s
agent0:                 episode reward: -0.6744,                 loss: nan
agent1:                 episode reward: 0.6744,                 loss: 0.1417
Episode: 48181/101000 (47.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5250s / 13521.0092 s
agent0:                 episode reward: 0.1072,                 loss: nan
agent1:                 episode reward: -0.1072,                 loss: 0.1408
Episode: 48201/101000 (47.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0165s / 13530.0257 s
agent0:                 episode reward: -0.3818,                 loss: nan
agent1:                 episode reward: 0.3818,                 loss: 0.1415
Episode: 48221/101000 (47.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7109s / 13537.7366 s
agent0:                 episode reward: -0.1704,                 loss: nan
agent1:                 episode reward: 0.1704,                 loss: 0.1582
Episode: 48241/101000 (47.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1262s / 13546.8628 s
agent0:                 episode reward: -0.3449,                 loss: 0.1707
agent1:                 episode reward: 0.3449,                 loss: 0.1583
Score delta: 1.7091887901144633, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/47806_1.
Episode: 48261/101000 (47.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6328s / 13556.4956 s
agent0:                 episode reward: 0.1586,                 loss: 0.1739
agent1:                 episode reward: -0.1586,                 loss: nan
Episode: 48281/101000 (47.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4280s / 13563.9236 s
agent0:                 episode reward: -0.0906,                 loss: 0.1715
agent1:                 episode reward: 0.0906,                 loss: nan
Episode: 48301/101000 (47.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3438s / 13572.2674 s
agent0:                 episode reward: -0.0807,                 loss: 0.1720
agent1:                 episode reward: 0.0807,                 loss: nan
Episode: 48321/101000 (47.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3760s / 13580.6435 s
agent0:                 episode reward: 0.1595,                 loss: 0.1698
agent1:                 episode reward: -0.1595,                 loss: nan
Episode: 48341/101000 (47.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3619s / 13590.0054 s
agent0:                 episode reward: 0.2629,                 loss: 0.1725
agent1:                 episode reward: -0.2629,                 loss: nan
Episode: 48361/101000 (47.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7007s / 13598.7061 s
agent0:                 episode reward: 0.2110,                 loss: 0.1707
agent1:                 episode reward: -0.2110,                 loss: nan
Episode: 48381/101000 (47.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0433s / 13605.7494 s
agent0:                 episode reward: 0.5035,                 loss: 0.1704
agent1:                 episode reward: -0.5035,                 loss: 0.1686
Score delta: 1.583612493345735, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/47949_0.
Episode: 48401/101000 (47.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2172s / 13613.9666 s
agent0:                 episode reward: -0.0718,                 loss: nan
agent1:                 episode reward: 0.0718,                 loss: 0.1654
Episode: 48421/101000 (47.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 11.2982s / 13625.2648 s
agent0:                 episode reward: -0.1725,                 loss: nan
agent1:                 episode reward: 0.1725,                 loss: 0.1675
Episode: 48441/101000 (47.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6488s / 13632.9136 s
agent0:                 episode reward: -0.8766,                 loss: nan
agent1:                 episode reward: 0.8766,                 loss: 0.1644
Episode: 48461/101000 (47.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9023s / 13640.8159 s
agent0:                 episode reward: -0.4647,                 loss: nan
agent1:                 episode reward: 0.4647,                 loss: 0.1654
Episode: 48481/101000 (48.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1525s / 13648.9684 s
agent0:                 episode reward: -0.5532,                 loss: 0.1733
agent1:                 episode reward: 0.5532,                 loss: 0.1614
Score delta: 1.5121584604644418, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/48043_1.
Episode: 48501/101000 (48.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6382s / 13659.6066 s
agent0:                 episode reward: 0.0351,                 loss: 0.1733
agent1:                 episode reward: -0.0351,                 loss: nan
Episode: 48521/101000 (48.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5718s / 13670.1784 s
agent0:                 episode reward: -0.3751,                 loss: 0.1708
agent1:                 episode reward: 0.3751,                 loss: nan
Episode: 48541/101000 (48.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1727s / 13679.3511 s
agent0:                 episode reward: 0.0964,                 loss: 0.1749
agent1:                 episode reward: -0.0964,                 loss: nan
Episode: 48561/101000 (48.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3901s / 13689.7412 s
agent0:                 episode reward: -0.0220,                 loss: 0.1781
agent1:                 episode reward: 0.0220,                 loss: nan
Episode: 48581/101000 (48.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9760s / 13698.7172 s
agent0:                 episode reward: 0.2178,                 loss: 0.1791
agent1:                 episode reward: -0.2178,                 loss: nan
Episode: 48601/101000 (48.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9927s / 13709.7099 s
agent0:                 episode reward: 0.0416,                 loss: 0.1797
agent1:                 episode reward: -0.0416,                 loss: nan
Episode: 48621/101000 (48.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0416s / 13717.7515 s
agent0:                 episode reward: 0.0660,                 loss: 0.1780
agent1:                 episode reward: -0.0660,                 loss: nan
Score delta: 1.6655005642820373, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/48195_0.
Episode: 48641/101000 (48.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4831s / 13726.2345 s
agent0:                 episode reward: 0.2203,                 loss: nan
agent1:                 episode reward: -0.2203,                 loss: 0.1455
Episode: 48661/101000 (48.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3539s / 13734.5885 s
agent0:                 episode reward: -0.5045,                 loss: nan
agent1:                 episode reward: 0.5045,                 loss: 0.1438
Episode: 48681/101000 (48.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8976s / 13740.4861 s
agent0:                 episode reward: -0.2436,                 loss: nan
agent1:                 episode reward: 0.2436,                 loss: 0.1428
Episode: 48701/101000 (48.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3416s / 13748.8277 s
agent0:                 episode reward: -0.1499,                 loss: nan
agent1:                 episode reward: 0.1499,                 loss: 0.1444
Episode: 48721/101000 (48.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7676s / 13758.5953 s
agent0:                 episode reward: -0.1169,                 loss: nan
agent1:                 episode reward: 0.1169,                 loss: 0.1441
Episode: 48741/101000 (48.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9715s / 13769.5668 s
agent0:                 episode reward: -0.4552,                 loss: nan
agent1:                 episode reward: 0.4552,                 loss: 0.1437
Episode: 48761/101000 (48.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8952s / 13777.4620 s
agent0:                 episode reward: 0.2664,                 loss: 0.1884
agent1:                 episode reward: -0.2664,                 loss: 0.1406
Score delta: 1.620631683091959, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/48316_1.
Episode: 48781/101000 (48.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3724s / 13787.8344 s
agent0:                 episode reward: 0.1291,                 loss: 0.1874
agent1:                 episode reward: -0.1291,                 loss: nan
Episode: 48801/101000 (48.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3475s / 13795.1819 s
agent0:                 episode reward: -0.0170,                 loss: 0.1859
agent1:                 episode reward: 0.0170,                 loss: nan
Episode: 48821/101000 (48.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6815s / 13802.8634 s
agent0:                 episode reward: -0.0020,                 loss: 0.1853
agent1:                 episode reward: 0.0020,                 loss: nan
Episode: 48841/101000 (48.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3546s / 13811.2180 s
agent0:                 episode reward: 0.0347,                 loss: 0.1843
agent1:                 episode reward: -0.0347,                 loss: nan
Episode: 48861/101000 (48.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0232s / 13820.2412 s
agent0:                 episode reward: 0.4767,                 loss: 0.1858
agent1:                 episode reward: -0.4767,                 loss: 0.1574
Score delta: 1.5291967511562372, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/48431_0.
Episode: 48881/101000 (48.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8645s / 13828.1057 s
agent0:                 episode reward: -0.4252,                 loss: nan
agent1:                 episode reward: 0.4252,                 loss: 0.1551
Episode: 48901/101000 (48.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8831s / 13835.9888 s
agent0:                 episode reward: -0.5349,                 loss: nan
agent1:                 episode reward: 0.5349,                 loss: 0.1566
Episode: 48921/101000 (48.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6428s / 13843.6316 s
agent0:                 episode reward: -0.3611,                 loss: nan
agent1:                 episode reward: 0.3611,                 loss: 0.1564
Episode: 48941/101000 (48.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9064s / 13851.5380 s
agent0:                 episode reward: 0.0925,                 loss: nan
agent1:                 episode reward: -0.0925,                 loss: 0.1545
Episode: 48961/101000 (48.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5978s / 13859.1359 s
agent0:                 episode reward: -0.4474,                 loss: 0.1729
agent1:                 episode reward: 0.4474,                 loss: 0.1603
Score delta: 1.6590386594424409, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/48534_1.
Episode: 48981/101000 (48.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4368s / 13869.5726 s
agent0:                 episode reward: -0.1977,                 loss: 0.1706
agent1:                 episode reward: 0.1977,                 loss: nan
Episode: 49001/101000 (48.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9408s / 13878.5134 s
agent0:                 episode reward: 0.0969,                 loss: 0.1693
agent1:                 episode reward: -0.0969,                 loss: nan
Episode: 49021/101000 (48.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7967s / 13886.3101 s
agent0:                 episode reward: 0.0993,                 loss: 0.1678
agent1:                 episode reward: -0.0993,                 loss: nan
Episode: 49041/101000 (48.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4931s / 13896.8032 s
agent0:                 episode reward: 0.4615,                 loss: 0.1686
agent1:                 episode reward: -0.4615,                 loss: nan
Episode: 49061/101000 (48.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9175s / 13901.7207 s
agent0:                 episode reward: -0.1311,                 loss: 0.1688
agent1:                 episode reward: 0.1311,                 loss: nan
Episode: 49081/101000 (48.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4590s / 13912.1796 s
agent0:                 episode reward: -0.0016,                 loss: 0.1672
agent1:                 episode reward: 0.0016,                 loss: nan
Episode: 49101/101000 (48.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0245s / 13921.2042 s
agent0:                 episode reward: 0.1414,                 loss: 0.1690
agent1:                 episode reward: -0.1414,                 loss: nan
Episode: 49121/101000 (48.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0360s / 13930.2401 s
agent0:                 episode reward: -0.0328,                 loss: 0.1799
agent1:                 episode reward: 0.0328,                 loss: nan
Episode: 49141/101000 (48.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1231s / 13937.3632 s
agent0:                 episode reward: -0.1284,                 loss: 0.1844
agent1:                 episode reward: 0.1284,                 loss: nan
Episode: 49161/101000 (48.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4749s / 13945.8381 s
agent0:                 episode reward: -0.0061,                 loss: 0.1864
agent1:                 episode reward: 0.0061,                 loss: nan
Episode: 49181/101000 (48.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 11.1527s / 13956.9908 s
agent0:                 episode reward: 0.0183,                 loss: 0.1850
agent1:                 episode reward: -0.0183,                 loss: nan
Episode: 49201/101000 (48.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3590s / 13964.3498 s
agent0:                 episode reward: 0.2567,                 loss: 0.1838
agent1:                 episode reward: -0.2567,                 loss: nan
Episode: 49221/101000 (48.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1710s / 13972.5208 s
agent0:                 episode reward: 0.3134,                 loss: 0.1833
agent1:                 episode reward: -0.3134,                 loss: nan
Episode: 49241/101000 (48.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1479s / 13981.6687 s
agent0:                 episode reward: 0.0464,                 loss: 0.1844
agent1:                 episode reward: -0.0464,                 loss: nan
Episode: 49261/101000 (48.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 11.0519s / 13992.7206 s
agent0:                 episode reward: 0.0906,                 loss: 0.1845
agent1:                 episode reward: -0.0906,                 loss: nan
Episode: 49281/101000 (48.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 11.6899s / 14004.4105 s
agent0:                 episode reward: 0.0229,                 loss: 0.1829
agent1:                 episode reward: -0.0229,                 loss: nan
Episode: 49301/101000 (48.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2904s / 14012.7010 s
agent0:                 episode reward: 0.1209,                 loss: 0.1822
agent1:                 episode reward: -0.1209,                 loss: nan
Episode: 49321/101000 (48.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4566s / 14021.1576 s
agent0:                 episode reward: -0.2531,                 loss: 0.1840
agent1:                 episode reward: 0.2531,                 loss: nan
Episode: 49341/101000 (48.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8255s / 14030.9831 s
agent0:                 episode reward: 0.0414,                 loss: 0.1814
agent1:                 episode reward: -0.0414,                 loss: nan
Episode: 49361/101000 (48.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9412s / 14038.9244 s
agent0:                 episode reward: -0.2631,                 loss: 0.1838
agent1:                 episode reward: 0.2631,                 loss: nan
Episode: 49381/101000 (48.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8957s / 14049.8201 s
agent0:                 episode reward: 0.1804,                 loss: 0.1832
agent1:                 episode reward: -0.1804,                 loss: nan
Episode: 49401/101000 (48.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0885s / 14058.9086 s
agent0:                 episode reward: 0.2146,                 loss: 0.1823
agent1:                 episode reward: -0.2146,                 loss: nan
Episode: 49421/101000 (48.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1842s / 14069.0928 s
agent0:                 episode reward: -0.2207,                 loss: 0.1830
agent1:                 episode reward: 0.2207,                 loss: nan
Episode: 49441/101000 (48.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1993s / 14079.2921 s
agent0:                 episode reward: 0.3160,                 loss: 0.1842
agent1:                 episode reward: -0.3160,                 loss: nan
Episode: 49461/101000 (48.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4011s / 14089.6932 s
agent0:                 episode reward: 0.4719,                 loss: 0.1839
agent1:                 episode reward: -0.4719,                 loss: nan
Episode: 49481/101000 (48.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4486s / 14099.1418 s
agent0:                 episode reward: -0.1060,                 loss: 0.1866
agent1:                 episode reward: 0.1060,                 loss: 0.1543
Score delta: 1.5006019143453058, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/49041_0.
Episode: 49501/101000 (49.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0038s / 14107.1456 s
agent0:                 episode reward: -0.1373,                 loss: nan
agent1:                 episode reward: 0.1373,                 loss: 0.1532
Episode: 49521/101000 (49.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5840s / 14114.7296 s
agent0:                 episode reward: -0.5460,                 loss: nan
agent1:                 episode reward: 0.5460,                 loss: 0.1526
Episode: 49541/101000 (49.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2664s / 14123.9960 s
agent0:                 episode reward: -0.1936,                 loss: nan
agent1:                 episode reward: 0.1936,                 loss: 0.1537
Episode: 49561/101000 (49.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5105s / 14133.5065 s
agent0:                 episode reward: -0.2301,                 loss: nan
agent1:                 episode reward: 0.2301,                 loss: 0.1529
Episode: 49581/101000 (49.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7040s / 14141.2105 s
agent0:                 episode reward: -0.7708,                 loss: 0.1707
agent1:                 episode reward: 0.7708,                 loss: 0.1519
Score delta: 1.5182921627155412, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/49148_1.
Episode: 49601/101000 (49.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3851s / 14150.5956 s
agent0:                 episode reward: -0.0359,                 loss: 0.1675
agent1:                 episode reward: 0.0359,                 loss: nan
Episode: 49621/101000 (49.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6234s / 14159.2191 s
agent0:                 episode reward: -0.1068,                 loss: 0.1646
agent1:                 episode reward: 0.1068,                 loss: nan
Episode: 49641/101000 (49.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4814s / 14168.7004 s
agent0:                 episode reward: 0.2292,                 loss: 0.1638
agent1:                 episode reward: -0.2292,                 loss: nan
Episode: 49661/101000 (49.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9760s / 14178.6764 s
agent0:                 episode reward: -0.0163,                 loss: 0.1622
agent1:                 episode reward: 0.0163,                 loss: nan
Episode: 49681/101000 (49.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1118s / 14188.7882 s
agent0:                 episode reward: -0.3527,                 loss: 0.1640
agent1:                 episode reward: 0.3527,                 loss: nan
Episode: 49701/101000 (49.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4503s / 14199.2385 s
agent0:                 episode reward: 0.2161,                 loss: 0.1646
agent1:                 episode reward: -0.2161,                 loss: nan
Episode: 49721/101000 (49.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2479s / 14207.4864 s
agent0:                 episode reward: -0.0527,                 loss: 0.1639
agent1:                 episode reward: 0.0527,                 loss: nan
Episode: 49741/101000 (49.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1465s / 14216.6329 s
agent0:                 episode reward: 0.2164,                 loss: 0.1627
agent1:                 episode reward: -0.2164,                 loss: nan
Episode: 49761/101000 (49.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8102s / 14227.4431 s
agent0:                 episode reward: 0.3731,                 loss: 0.1608
agent1:                 episode reward: -0.3731,                 loss: nan
Episode: 49781/101000 (49.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8480s / 14237.2911 s
agent0:                 episode reward: 0.0593,                 loss: 0.1643
agent1:                 episode reward: -0.0593,                 loss: nan
Episode: 49801/101000 (49.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2183s / 14245.5094 s
agent0:                 episode reward: -0.2785,                 loss: 0.1652
agent1:                 episode reward: 0.2785,                 loss: nan
Episode: 49821/101000 (49.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4951s / 14254.0045 s
agent0:                 episode reward: 0.3862,                 loss: 0.1616
agent1:                 episode reward: -0.3862,                 loss: 0.1710
Score delta: 1.6496047488591667, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/49385_0.
Episode: 49841/101000 (49.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7387s / 14262.7432 s
agent0:                 episode reward: -0.1345,                 loss: nan
agent1:                 episode reward: 0.1345,                 loss: 0.1677
Episode: 49861/101000 (49.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8733s / 14271.6166 s
agent0:                 episode reward: -0.3595,                 loss: nan
agent1:                 episode reward: 0.3595,                 loss: 0.1654
Episode: 49881/101000 (49.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4003s / 14280.0169 s
agent0:                 episode reward: -0.1884,                 loss: nan
agent1:                 episode reward: 0.1884,                 loss: 0.1667
Episode: 49901/101000 (49.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6553s / 14286.6722 s
agent0:                 episode reward: -0.7675,                 loss: nan
agent1:                 episode reward: 0.7675,                 loss: 0.1669
Score delta: 1.543549760123637, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/49475_1.
Episode: 49921/101000 (49.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9379s / 14295.6101 s
agent0:                 episode reward: 0.5671,                 loss: 0.1858
agent1:                 episode reward: -0.5671,                 loss: nan
Episode: 49941/101000 (49.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2644s / 14305.8745 s
agent0:                 episode reward: 0.1432,                 loss: 0.1842
agent1:                 episode reward: -0.1432,                 loss: nan
Episode: 49961/101000 (49.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7089s / 14313.5834 s
agent0:                 episode reward: -0.1212,                 loss: 0.1861
agent1:                 episode reward: 0.1212,                 loss: nan
Episode: 49981/101000 (49.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 11.0280s / 14324.6114 s
agent0:                 episode reward: 0.1866,                 loss: 0.1877
agent1:                 episode reward: -0.1866,                 loss: nan
Episode: 50001/101000 (49.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7978s / 14335.4093 s
agent0:                 episode reward: 0.2102,                 loss: 0.1908
agent1:                 episode reward: -0.2102,                 loss: nan
Episode: 50021/101000 (49.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9044s / 14343.3136 s
agent0:                 episode reward: -0.1377,                 loss: 0.1906
agent1:                 episode reward: 0.1377,                 loss: nan
Episode: 50041/101000 (49.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 12.0032s / 14355.3169 s
agent0:                 episode reward: 0.2733,                 loss: 0.1896
agent1:                 episode reward: -0.2733,                 loss: nan
Episode: 50061/101000 (49.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1766s / 14365.4935 s
agent0:                 episode reward: 0.4051,                 loss: 0.1912
agent1:                 episode reward: -0.4051,                 loss: nan
Episode: 50081/101000 (49.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 11.0435s / 14376.5370 s
agent0:                 episode reward: 0.3602,                 loss: 0.1937
agent1:                 episode reward: -0.3602,                 loss: nan
Episode: 50101/101000 (49.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5256s / 14387.0626 s
agent0:                 episode reward: 0.1849,                 loss: 0.1906
agent1:                 episode reward: -0.1849,                 loss: nan
Episode: 50121/101000 (49.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9085s / 14394.9711 s
agent0:                 episode reward: -0.1174,                 loss: 0.1880
agent1:                 episode reward: 0.1174,                 loss: 0.1622
Score delta: 1.6612165419556333, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/49683_0.
Episode: 50141/101000 (49.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7674s / 14402.7385 s
agent0:                 episode reward: -0.8520,                 loss: nan
agent1:                 episode reward: 0.8520,                 loss: 0.1640
Episode: 50161/101000 (49.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2805s / 14410.0190 s
agent0:                 episode reward: -0.2670,                 loss: nan
agent1:                 episode reward: 0.2670,                 loss: 0.1635
Episode: 50181/101000 (49.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9875s / 14420.0065 s
agent0:                 episode reward: -0.0806,                 loss: nan
agent1:                 episode reward: 0.0806,                 loss: 0.1626
Episode: 50201/101000 (49.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2831s / 14428.2896 s
agent0:                 episode reward: -0.4332,                 loss: nan
agent1:                 episode reward: 0.4332,                 loss: 0.1634
Episode: 50221/101000 (49.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5573s / 14438.8469 s
agent0:                 episode reward: -0.2491,                 loss: nan
agent1:                 episode reward: 0.2491,                 loss: 0.1629
Episode: 50241/101000 (49.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8329s / 14447.6798 s
agent0:                 episode reward: -0.2979,                 loss: nan
agent1:                 episode reward: 0.2979,                 loss: 0.1635
Episode: 50261/101000 (49.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7428s / 14456.4226 s
agent0:                 episode reward: -0.2470,                 loss: nan
agent1:                 episode reward: 0.2470,                 loss: 0.1593
Episode: 50281/101000 (49.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6017s / 14466.0242 s
agent0:                 episode reward: -0.5184,                 loss: nan
agent1:                 episode reward: 0.5184,                 loss: 0.1583
Episode: 50301/101000 (49.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1578s / 14475.1821 s
agent0:                 episode reward: -0.5267,                 loss: 0.1719
agent1:                 episode reward: 0.5267,                 loss: 0.1595
Score delta: 1.6442689060677371, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/49858_1.
Episode: 50321/101000 (49.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7887s / 14484.9708 s
agent0:                 episode reward: 0.0361,                 loss: 0.1662
agent1:                 episode reward: -0.0361,                 loss: nan
Episode: 50341/101000 (49.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1830s / 14493.1538 s
agent0:                 episode reward: -0.4159,                 loss: 0.1660
agent1:                 episode reward: 0.4159,                 loss: nan
Episode: 50361/101000 (49.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3658s / 14501.5196 s
agent0:                 episode reward: 0.2964,                 loss: 0.1665
agent1:                 episode reward: -0.2964,                 loss: nan
Episode: 50381/101000 (49.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9311s / 14510.4507 s
agent0:                 episode reward: 0.1931,                 loss: 0.1657
agent1:                 episode reward: -0.1931,                 loss: nan
Episode: 50401/101000 (49.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7380s / 14521.1887 s
agent0:                 episode reward: 0.2282,                 loss: 0.1639
agent1:                 episode reward: -0.2282,                 loss: nan
Episode: 50421/101000 (49.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0559s / 14530.2446 s
agent0:                 episode reward: 0.0158,                 loss: 0.1659
agent1:                 episode reward: -0.0158,                 loss: nan
Episode: 50441/101000 (49.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9532s / 14540.1978 s
agent0:                 episode reward: -0.0124,                 loss: 0.1651
agent1:                 episode reward: 0.0124,                 loss: nan
Episode: 50461/101000 (49.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2177s / 14547.4155 s
agent0:                 episode reward: -0.4427,                 loss: 0.1659
agent1:                 episode reward: 0.4427,                 loss: nan
Episode: 50481/101000 (49.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4178s / 14555.8334 s
agent0:                 episode reward: -0.0486,                 loss: 0.1642
agent1:                 episode reward: 0.0486,                 loss: nan
Episode: 50501/101000 (50.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5676s / 14564.4009 s
agent0:                 episode reward: -0.0827,                 loss: 0.1964
agent1:                 episode reward: 0.0827,                 loss: nan
Episode: 50521/101000 (50.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5226s / 14574.9235 s
agent0:                 episode reward: -0.0126,                 loss: 0.1965
agent1:                 episode reward: 0.0126,                 loss: nan
Episode: 50541/101000 (50.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 11.7587s / 14586.6822 s
agent0:                 episode reward: 0.0438,                 loss: 0.1974
agent1:                 episode reward: -0.0438,                 loss: nan
Episode: 50561/101000 (50.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8097s / 14594.4920 s
agent0:                 episode reward: 0.0315,                 loss: 0.1947
agent1:                 episode reward: -0.0315,                 loss: nan
Episode: 50581/101000 (50.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5323s / 14604.0243 s
agent0:                 episode reward: 0.0191,                 loss: 0.1953
agent1:                 episode reward: -0.0191,                 loss: nan
Episode: 50601/101000 (50.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5849s / 14611.6091 s
agent0:                 episode reward: 0.2145,                 loss: 0.1934
agent1:                 episode reward: -0.2145,                 loss: nan
Episode: 50621/101000 (50.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7205s / 14620.3297 s
agent0:                 episode reward: 0.1579,                 loss: 0.1944
agent1:                 episode reward: -0.1579,                 loss: nan
Episode: 50641/101000 (50.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4866s / 14628.8163 s
agent0:                 episode reward: -0.1630,                 loss: 0.1947
agent1:                 episode reward: 0.1630,                 loss: nan
Episode: 50661/101000 (50.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2149s / 14639.0312 s
agent0:                 episode reward: -0.0573,                 loss: 0.1943
agent1:                 episode reward: 0.0573,                 loss: nan
Episode: 50681/101000 (50.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 11.8210s / 14650.8522 s
agent0:                 episode reward: -0.0421,                 loss: 0.1929
agent1:                 episode reward: 0.0421,                 loss: nan
Episode: 50701/101000 (50.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6063s / 14661.4586 s
agent0:                 episode reward: 0.1906,                 loss: 0.1950
agent1:                 episode reward: -0.1906,                 loss: nan
Episode: 50721/101000 (50.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3063s / 14670.7649 s
agent0:                 episode reward: 0.0321,                 loss: 0.1942
agent1:                 episode reward: -0.0321,                 loss: nan
Episode: 50741/101000 (50.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7710s / 14681.5359 s
agent0:                 episode reward: 0.0037,                 loss: 0.1957
agent1:                 episode reward: -0.0037,                 loss: nan
Episode: 50761/101000 (50.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4100s / 14690.9459 s
agent0:                 episode reward: 0.1260,                 loss: 0.1936
agent1:                 episode reward: -0.1260,                 loss: nan
Episode: 50781/101000 (50.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5847s / 14698.5306 s
agent0:                 episode reward: 0.0956,                 loss: 0.1941
agent1:                 episode reward: -0.0956,                 loss: nan
Episode: 50801/101000 (50.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1096s / 14708.6402 s
agent0:                 episode reward: -0.2881,                 loss: 0.1958
agent1:                 episode reward: 0.2881,                 loss: nan
Episode: 50821/101000 (50.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1155s / 14715.7557 s
agent0:                 episode reward: 0.4406,                 loss: 0.1934
agent1:                 episode reward: -0.4406,                 loss: nan
Episode: 50841/101000 (50.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3525s / 14724.1082 s
agent0:                 episode reward: 0.1688,                 loss: 0.1909
agent1:                 episode reward: -0.1688,                 loss: nan
Episode: 50861/101000 (50.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3330s / 14733.4412 s
agent0:                 episode reward: -0.1225,                 loss: 0.1917
agent1:                 episode reward: 0.1225,                 loss: nan
Episode: 50881/101000 (50.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5828s / 14742.0240 s
agent0:                 episode reward: 0.3858,                 loss: 0.1918
agent1:                 episode reward: -0.3858,                 loss: 0.1411
Score delta: 1.5086097033328019, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/50449_0.
Episode: 50901/101000 (50.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0763s / 14751.1004 s
agent0:                 episode reward: -0.2717,                 loss: nan
agent1:                 episode reward: 0.2717,                 loss: 0.1431
Episode: 50921/101000 (50.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3150s / 14759.4154 s
agent0:                 episode reward: -0.5521,                 loss: nan
agent1:                 episode reward: 0.5521,                 loss: 0.1410
Episode: 50941/101000 (50.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5494s / 14766.9647 s
agent0:                 episode reward: -0.5488,                 loss: nan
agent1:                 episode reward: 0.5488,                 loss: 0.1430
Episode: 50961/101000 (50.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5331s / 14775.4978 s
agent0:                 episode reward: -0.7015,                 loss: nan
agent1:                 episode reward: 0.7015,                 loss: 0.1406
Episode: 50981/101000 (50.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4204s / 14782.9182 s
agent0:                 episode reward: 0.0657,                 loss: nan
agent1:                 episode reward: -0.0657,                 loss: 0.1430
Episode: 51001/101000 (50.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 11.2084s / 14794.1267 s
agent0:                 episode reward: -0.4928,                 loss: nan
agent1:                 episode reward: 0.4928,                 loss: 0.1414
Episode: 51021/101000 (50.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7605s / 14801.8872 s
agent0:                 episode reward: -0.4540,                 loss: nan
agent1:                 episode reward: 0.4540,                 loss: 0.1431
Episode: 51041/101000 (50.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8113s / 14810.6985 s
agent0:                 episode reward: -0.0660,                 loss: 0.1817
agent1:                 episode reward: 0.0660,                 loss: 0.1385
Score delta: 1.5367985629840906, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/50597_1.
Episode: 51061/101000 (50.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3503s / 14820.0488 s
agent0:                 episode reward: -0.2027,                 loss: 0.1797
agent1:                 episode reward: 0.2027,                 loss: nan
Episode: 51081/101000 (50.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7229s / 14828.7717 s
agent0:                 episode reward: -0.2962,                 loss: 0.1789
agent1:                 episode reward: 0.2962,                 loss: nan
Episode: 51101/101000 (50.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3028s / 14838.0746 s
agent0:                 episode reward: 0.0975,                 loss: 0.1780
agent1:                 episode reward: -0.0975,                 loss: nan
Episode: 51121/101000 (50.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3660s / 14847.4406 s
agent0:                 episode reward: -0.6197,                 loss: 0.1782
agent1:                 episode reward: 0.6197,                 loss: nan
Episode: 51141/101000 (50.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3242s / 14856.7648 s
agent0:                 episode reward: 0.1032,                 loss: 0.1790
agent1:                 episode reward: -0.1032,                 loss: nan
Episode: 51161/101000 (50.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7426s / 14864.5074 s
agent0:                 episode reward: -0.6598,                 loss: 0.1777
agent1:                 episode reward: 0.6598,                 loss: nan
Episode: 51181/101000 (50.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9305s / 14873.4379 s
agent0:                 episode reward: -0.3589,                 loss: 0.1780
agent1:                 episode reward: 0.3589,                 loss: nan
Episode: 51201/101000 (50.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8131s / 14884.2510 s
agent0:                 episode reward: -0.5375,                 loss: 0.1774
agent1:                 episode reward: 0.5375,                 loss: nan
Episode: 51221/101000 (50.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4202s / 14891.6713 s
agent0:                 episode reward: 0.0273,                 loss: 0.1772
agent1:                 episode reward: -0.0273,                 loss: nan
Episode: 51241/101000 (50.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 11.5707s / 14903.2419 s
agent0:                 episode reward: 0.1734,                 loss: 0.1779
agent1:                 episode reward: -0.1734,                 loss: nan
Episode: 51261/101000 (50.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1753s / 14913.4172 s
agent0:                 episode reward: 0.0015,                 loss: 0.1785
agent1:                 episode reward: -0.0015,                 loss: nan
Episode: 51281/101000 (50.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0230s / 14923.4402 s
agent0:                 episode reward: 0.2554,                 loss: 0.1769
agent1:                 episode reward: -0.2554,                 loss: nan
Episode: 51301/101000 (50.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5929s / 14933.0331 s
agent0:                 episode reward: 0.2725,                 loss: 0.1799
agent1:                 episode reward: -0.2725,                 loss: nan
Episode: 51321/101000 (50.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3145s / 14941.3476 s
agent0:                 episode reward: 0.1858,                 loss: 0.1853
agent1:                 episode reward: -0.1858,                 loss: nan
Episode: 51341/101000 (50.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6449s / 14947.9925 s
agent0:                 episode reward: 0.0778,                 loss: 0.1851
agent1:                 episode reward: -0.0778,                 loss: nan
Episode: 51361/101000 (50.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 11.4867s / 14959.4792 s
agent0:                 episode reward: -0.0332,                 loss: 0.1839
agent1:                 episode reward: 0.0332,                 loss: nan
Episode: 51381/101000 (50.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0120s / 14967.4912 s
agent0:                 episode reward: 0.2441,                 loss: 0.1850
agent1:                 episode reward: -0.2441,                 loss: nan
Episode: 51401/101000 (50.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3920s / 14976.8831 s
agent0:                 episode reward: 0.2266,                 loss: 0.1861
agent1:                 episode reward: -0.2266,                 loss: nan
Episode: 51421/101000 (50.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2194s / 14986.1025 s
agent0:                 episode reward: -0.1451,                 loss: 0.1840
agent1:                 episode reward: 0.1451,                 loss: nan
Episode: 51441/101000 (50.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8551s / 14995.9576 s
agent0:                 episode reward: -0.0608,                 loss: 0.1847
agent1:                 episode reward: 0.0608,                 loss: nan
Episode: 51461/101000 (50.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9105s / 15004.8681 s
agent0:                 episode reward: -0.4345,                 loss: 0.1845
agent1:                 episode reward: 0.4345,                 loss: nan
Episode: 51481/101000 (50.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4397s / 15013.3078 s
agent0:                 episode reward: 0.1353,                 loss: 0.1846
agent1:                 episode reward: -0.1353,                 loss: nan
Episode: 51501/101000 (50.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5060s / 15022.8138 s
agent0:                 episode reward: -0.4031,                 loss: 0.1817
agent1:                 episode reward: 0.4031,                 loss: nan
Episode: 51521/101000 (51.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7067s / 15032.5205 s
agent0:                 episode reward: 0.5628,                 loss: 0.1847
agent1:                 episode reward: -0.5628,                 loss: 0.1472
Score delta: 1.833772118410063, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/51093_0.
Episode: 51541/101000 (51.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3779s / 15040.8983 s
agent0:                 episode reward: -0.3167,                 loss: nan
agent1:                 episode reward: 0.3167,                 loss: 0.1437
Episode: 51561/101000 (51.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3628s / 15051.2611 s
agent0:                 episode reward: -0.1156,                 loss: nan
agent1:                 episode reward: 0.1156,                 loss: 0.1428
Episode: 51581/101000 (51.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6760s / 15060.9371 s
agent0:                 episode reward: -0.2864,                 loss: nan
agent1:                 episode reward: 0.2864,                 loss: 0.1410
Episode: 51601/101000 (51.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0351s / 15069.9722 s
agent0:                 episode reward: -0.2765,                 loss: nan
agent1:                 episode reward: 0.2765,                 loss: 0.1416
Episode: 51621/101000 (51.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0085s / 15078.9807 s
agent0:                 episode reward: -0.4431,                 loss: nan
agent1:                 episode reward: 0.4431,                 loss: 0.1411
Episode: 51641/101000 (51.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4539s / 15089.4346 s
agent0:                 episode reward: -0.1972,                 loss: 0.2542
agent1:                 episode reward: 0.1972,                 loss: 0.1427
Score delta: 1.569071074475733, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/51198_1.
Episode: 51661/101000 (51.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0420s / 15096.4766 s
agent0:                 episode reward: -0.3425,                 loss: 0.2483
agent1:                 episode reward: 0.3425,                 loss: nan
Episode: 51681/101000 (51.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8300s / 15107.3065 s
agent0:                 episode reward: -0.5752,                 loss: 0.2492
agent1:                 episode reward: 0.5752,                 loss: nan
Episode: 51701/101000 (51.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7921s / 15114.0986 s
agent0:                 episode reward: -0.2998,                 loss: 0.2480
agent1:                 episode reward: 0.2998,                 loss: nan
Episode: 51721/101000 (51.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5819s / 15121.6806 s
agent0:                 episode reward: -0.6385,                 loss: 0.2459
agent1:                 episode reward: 0.6385,                 loss: nan
Episode: 51741/101000 (51.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6091s / 15131.2897 s
agent0:                 episode reward: -0.1608,                 loss: 0.2309
agent1:                 episode reward: 0.1608,                 loss: nan
Episode: 51761/101000 (51.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2682s / 15141.5579 s
agent0:                 episode reward: 0.2266,                 loss: 0.1891
agent1:                 episode reward: -0.2266,                 loss: nan
Episode: 51781/101000 (51.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1754s / 15149.7333 s
agent0:                 episode reward: -0.1229,                 loss: 0.1894
agent1:                 episode reward: 0.1229,                 loss: nan
Episode: 51801/101000 (51.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7160s / 15159.4493 s
agent0:                 episode reward: -0.4524,                 loss: 0.1870
agent1:                 episode reward: 0.4524,                 loss: nan
Episode: 51821/101000 (51.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 12.4855s / 15171.9348 s
agent0:                 episode reward: 0.1924,                 loss: 0.1873
agent1:                 episode reward: -0.1924,                 loss: nan
Episode: 51841/101000 (51.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7209s / 15182.6557 s
agent0:                 episode reward: 0.4871,                 loss: 0.1891
agent1:                 episode reward: -0.4871,                 loss: nan
Score delta: 1.6135279197341994, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/51415_0.
Episode: 51861/101000 (51.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6272s / 15189.2829 s
agent0:                 episode reward: 0.0389,                 loss: nan
agent1:                 episode reward: -0.0389,                 loss: 0.1616
Episode: 51881/101000 (51.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6799s / 15199.9628 s
agent0:                 episode reward: -0.5750,                 loss: nan
agent1:                 episode reward: 0.5750,                 loss: 0.1598
Episode: 51901/101000 (51.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7870s / 15208.7498 s
agent0:                 episode reward: -0.3160,                 loss: nan
agent1:                 episode reward: 0.3160,                 loss: 0.1570
Episode: 51921/101000 (51.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8402s / 15218.5900 s
agent0:                 episode reward: -0.1953,                 loss: nan
agent1:                 episode reward: 0.1953,                 loss: 0.1582
Episode: 51941/101000 (51.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0528s / 15228.6428 s
agent0:                 episode reward: -0.5413,                 loss: nan
agent1:                 episode reward: 0.5413,                 loss: 0.1577
Episode: 51961/101000 (51.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7800s / 15237.4227 s
agent0:                 episode reward: -0.1700,                 loss: nan
agent1:                 episode reward: 0.1700,                 loss: 0.1583
Episode: 51981/101000 (51.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6341s / 15247.0568 s
agent0:                 episode reward: -0.4093,                 loss: 0.1798
agent1:                 episode reward: 0.4093,                 loss: 0.1608
Score delta: 1.5923268782443978, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/51537_1.
Episode: 52001/101000 (51.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6204s / 15257.6772 s
agent0:                 episode reward: 0.3452,                 loss: 0.1718
agent1:                 episode reward: -0.3452,                 loss: nan
Episode: 52021/101000 (51.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3636s / 15267.0408 s
agent0:                 episode reward: -0.3223,                 loss: 0.1710
agent1:                 episode reward: 0.3223,                 loss: nan
Episode: 52041/101000 (51.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3763s / 15277.4171 s
agent0:                 episode reward: 0.0682,                 loss: 0.1725
agent1:                 episode reward: -0.0682,                 loss: nan
Episode: 52061/101000 (51.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8642s / 15285.2813 s
agent0:                 episode reward: 0.0076,                 loss: 0.1707
agent1:                 episode reward: -0.0076,                 loss: nan
Episode: 52081/101000 (51.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8998s / 15294.1810 s
agent0:                 episode reward: 0.2909,                 loss: 0.1724
agent1:                 episode reward: -0.2909,                 loss: 0.1605
Score delta: 1.551146149156154, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/51652_0.
Episode: 52101/101000 (51.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3868s / 15304.5678 s
agent0:                 episode reward: -0.4758,                 loss: nan
agent1:                 episode reward: 0.4758,                 loss: 0.1567
Episode: 52121/101000 (51.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 11.0529s / 15315.6207 s
agent0:                 episode reward: -0.4245,                 loss: nan
agent1:                 episode reward: 0.4245,                 loss: 0.1581
Episode: 52141/101000 (51.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9088s / 15323.5296 s
agent0:                 episode reward: -0.0721,                 loss: nan
agent1:                 episode reward: 0.0721,                 loss: 0.1577
Episode: 52161/101000 (51.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2667s / 15331.7962 s
agent0:                 episode reward: -0.5214,                 loss: nan
agent1:                 episode reward: 0.5214,                 loss: 0.1587
Episode: 52181/101000 (51.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8654s / 15340.6616 s
agent0:                 episode reward: -0.5144,                 loss: nan
agent1:                 episode reward: 0.5144,                 loss: 0.1568
Episode: 52201/101000 (51.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3849s / 15351.0465 s
agent0:                 episode reward: -0.4511,                 loss: 0.1745
agent1:                 episode reward: 0.4511,                 loss: 0.1566
Score delta: 1.8110211781649586, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/51772_1.
Episode: 52221/101000 (51.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4151s / 15358.4616 s
agent0:                 episode reward: 0.0913,                 loss: 0.1734
agent1:                 episode reward: -0.0913,                 loss: nan
Episode: 52241/101000 (51.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9096s / 15366.3712 s
agent0:                 episode reward: 0.0729,                 loss: 0.1670
agent1:                 episode reward: -0.0729,                 loss: nan
Episode: 52261/101000 (51.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1473s / 15376.5185 s
agent0:                 episode reward: -0.0309,                 loss: 0.1683
agent1:                 episode reward: 0.0309,                 loss: nan
Episode: 52281/101000 (51.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7194s / 15384.2379 s
agent0:                 episode reward: 0.0490,                 loss: 0.1678
agent1:                 episode reward: -0.0490,                 loss: nan
Episode: 52301/101000 (51.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6485s / 15394.8864 s
agent0:                 episode reward: -0.0595,                 loss: 0.1686
agent1:                 episode reward: 0.0595,                 loss: nan
Episode: 52321/101000 (51.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4200s / 15405.3064 s
agent0:                 episode reward: -0.1135,                 loss: 0.1844
agent1:                 episode reward: 0.1135,                 loss: nan
Episode: 52341/101000 (51.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4380s / 15415.7443 s
agent0:                 episode reward: -0.3550,                 loss: 0.1907
agent1:                 episode reward: 0.3550,                 loss: nan
Episode: 52361/101000 (51.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8686s / 15425.6129 s
agent0:                 episode reward: 0.1321,                 loss: 0.1902
agent1:                 episode reward: -0.1321,                 loss: nan
Episode: 52381/101000 (51.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2402s / 15433.8531 s
agent0:                 episode reward: -0.1539,                 loss: 0.1889
agent1:                 episode reward: 0.1539,                 loss: nan
Episode: 52401/101000 (51.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9320s / 15443.7851 s
agent0:                 episode reward: -0.1278,                 loss: 0.1899
agent1:                 episode reward: 0.1278,                 loss: nan
Episode: 52421/101000 (51.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4236s / 15454.2088 s
agent0:                 episode reward: 0.1800,                 loss: 0.1897
agent1:                 episode reward: -0.1800,                 loss: nan
Episode: 52441/101000 (51.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0497s / 15464.2585 s
agent0:                 episode reward: 0.0344,                 loss: 0.1910
agent1:                 episode reward: -0.0344,                 loss: nan
Episode: 52461/101000 (51.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7151s / 15474.9736 s
agent0:                 episode reward: -0.0327,                 loss: 0.1884
agent1:                 episode reward: 0.0327,                 loss: nan
Episode: 52481/101000 (51.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7373s / 15484.7109 s
agent0:                 episode reward: -0.1974,                 loss: 0.1879
agent1:                 episode reward: 0.1974,                 loss: nan
Episode: 52501/101000 (51.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3799s / 15494.0909 s
agent0:                 episode reward: 0.3008,                 loss: 0.1878
agent1:                 episode reward: -0.3008,                 loss: 0.1731
Score delta: 1.8590984064178215, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/52069_0.
Episode: 52521/101000 (52.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2944s / 15501.3852 s
agent0:                 episode reward: -0.2163,                 loss: nan
agent1:                 episode reward: 0.2163,                 loss: 0.1674
Episode: 52541/101000 (52.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1266s / 15509.5118 s
agent0:                 episode reward: -0.5844,                 loss: nan
agent1:                 episode reward: 0.5844,                 loss: 0.1676
Episode: 52561/101000 (52.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3567s / 15518.8685 s
agent0:                 episode reward: -0.4865,                 loss: nan
agent1:                 episode reward: 0.4865,                 loss: 0.1672
Episode: 52581/101000 (52.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2884s / 15528.1568 s
agent0:                 episode reward: -0.3491,                 loss: nan
agent1:                 episode reward: 0.3491,                 loss: 0.1664
Episode: 52601/101000 (52.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3900s / 15538.5469 s
agent0:                 episode reward: -0.3605,                 loss: nan
agent1:                 episode reward: 0.3605,                 loss: 0.1682
Episode: 52621/101000 (52.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9300s / 15549.4769 s
agent0:                 episode reward: -0.2136,                 loss: nan
agent1:                 episode reward: 0.2136,                 loss: 0.1611
Episode: 52641/101000 (52.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4655s / 15558.9423 s
agent0:                 episode reward: 0.0200,                 loss: 0.1690
agent1:                 episode reward: -0.0200,                 loss: 0.1566
Score delta: 1.5297650404089629, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/52199_1.
Episode: 52661/101000 (52.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5126s / 15566.4549 s
agent0:                 episode reward: 0.1858,                 loss: 0.1659
agent1:                 episode reward: -0.1858,                 loss: nan
Episode: 52681/101000 (52.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4696s / 15572.9245 s
agent0:                 episode reward: -0.1865,                 loss: 0.1676
agent1:                 episode reward: 0.1865,                 loss: nan
Episode: 52701/101000 (52.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6458s / 15583.5703 s
agent0:                 episode reward: -0.2467,                 loss: 0.1674
agent1:                 episode reward: 0.2467,                 loss: nan
Episode: 52721/101000 (52.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4954s / 15590.0658 s
agent0:                 episode reward: -0.0482,                 loss: 0.1661
agent1:                 episode reward: 0.0482,                 loss: nan
Episode: 52741/101000 (52.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0960s / 15599.1618 s
agent0:                 episode reward: 0.3645,                 loss: 0.1663
agent1:                 episode reward: -0.3645,                 loss: nan
Episode: 52761/101000 (52.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8927s / 15609.0545 s
agent0:                 episode reward: -0.1700,                 loss: 0.1663
agent1:                 episode reward: 0.1700,                 loss: nan
Episode: 52781/101000 (52.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4413s / 15619.4958 s
agent0:                 episode reward: 0.2417,                 loss: 0.1765
agent1:                 episode reward: -0.2417,                 loss: nan
Episode: 52801/101000 (52.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0443s / 15627.5401 s
agent0:                 episode reward: -0.1139,                 loss: 0.1887
agent1:                 episode reward: 0.1139,                 loss: nan
Episode: 52821/101000 (52.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3036s / 15635.8436 s
agent0:                 episode reward: -0.1439,                 loss: 0.1904
agent1:                 episode reward: 0.1439,                 loss: 0.1698
Score delta: 1.678773638384578, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/52383_0.
Episode: 52841/101000 (52.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5067s / 15644.3503 s
agent0:                 episode reward: -0.1838,                 loss: nan
agent1:                 episode reward: 0.1838,                 loss: 0.1676
Episode: 52861/101000 (52.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7841s / 15652.1344 s
agent0:                 episode reward: -0.2305,                 loss: nan
agent1:                 episode reward: 0.2305,                 loss: 0.1678
Episode: 52881/101000 (52.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5714s / 15660.7058 s
agent0:                 episode reward: -0.1664,                 loss: nan
agent1:                 episode reward: 0.1664,                 loss: 0.1669
Episode: 52901/101000 (52.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0086s / 15670.7143 s
agent0:                 episode reward: -0.3142,                 loss: nan
agent1:                 episode reward: 0.3142,                 loss: 0.1686
Episode: 52921/101000 (52.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4089s / 15680.1232 s
agent0:                 episode reward: -0.3059,                 loss: 0.1771
agent1:                 episode reward: 0.3059,                 loss: 0.1643
Score delta: 1.6927427233368557, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/52493_1.
Episode: 52941/101000 (52.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4588s / 15689.5820 s
agent0:                 episode reward: -0.2998,                 loss: 0.1676
agent1:                 episode reward: 0.2998,                 loss: nan
Episode: 52961/101000 (52.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5315s / 15698.1135 s
agent0:                 episode reward: -0.2819,                 loss: 0.1651
agent1:                 episode reward: 0.2819,                 loss: nan
Episode: 52981/101000 (52.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1134s / 15707.2269 s
agent0:                 episode reward: -0.1136,                 loss: 0.1626
agent1:                 episode reward: 0.1136,                 loss: nan
Episode: 53001/101000 (52.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8477s / 15716.0746 s
agent0:                 episode reward: 0.3911,                 loss: 0.1651
agent1:                 episode reward: -0.3911,                 loss: nan
Episode: 53021/101000 (52.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5435s / 15723.6180 s
agent0:                 episode reward: 0.2026,                 loss: 0.1624
agent1:                 episode reward: -0.2026,                 loss: nan
Episode: 53041/101000 (52.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0900s / 15732.7080 s
agent0:                 episode reward: -0.0968,                 loss: 0.1623
agent1:                 episode reward: 0.0968,                 loss: nan
Episode: 53061/101000 (52.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3507s / 15743.0587 s
agent0:                 episode reward: 0.4480,                 loss: 0.1623
agent1:                 episode reward: -0.4480,                 loss: 0.1716
Score delta: 1.5240761380104018, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/52633_0.
Episode: 53081/101000 (52.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 11.1879s / 15754.2467 s
agent0:                 episode reward: -0.1953,                 loss: nan
agent1:                 episode reward: 0.1953,                 loss: 0.1692
Episode: 53101/101000 (52.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4127s / 15762.6594 s
agent0:                 episode reward: -0.3578,                 loss: nan
agent1:                 episode reward: 0.3578,                 loss: 0.1672
Episode: 53121/101000 (52.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6768s / 15773.3362 s
agent0:                 episode reward: -0.6818,                 loss: nan
agent1:                 episode reward: 0.6818,                 loss: 0.1665
Episode: 53141/101000 (52.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8448s / 15784.1810 s
agent0:                 episode reward: -0.6227,                 loss: nan
agent1:                 episode reward: 0.6227,                 loss: 0.1694
Episode: 53161/101000 (52.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5982s / 15794.7792 s
agent0:                 episode reward: -0.1579,                 loss: nan
agent1:                 episode reward: 0.1579,                 loss: 0.1677
Episode: 53181/101000 (52.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4640s / 15802.2432 s
agent0:                 episode reward: -0.0092,                 loss: nan
agent1:                 episode reward: 0.0092,                 loss: 0.1651
Episode: 53201/101000 (52.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5260s / 15808.7693 s
agent0:                 episode reward: -0.3728,                 loss: 0.1897
agent1:                 episode reward: 0.3728,                 loss: 0.1668
Score delta: 1.908947316733776, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/52762_1.
Episode: 53221/101000 (52.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6650s / 15817.4342 s
agent0:                 episode reward: 0.0955,                 loss: 0.1877
agent1:                 episode reward: -0.0955,                 loss: nan
Episode: 53241/101000 (52.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5413s / 15824.9756 s
agent0:                 episode reward: -0.2641,                 loss: 0.1900
agent1:                 episode reward: 0.2641,                 loss: nan
Episode: 53261/101000 (52.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5242s / 15835.4998 s
agent0:                 episode reward: 0.0860,                 loss: 0.1876
agent1:                 episode reward: -0.0860,                 loss: nan
Episode: 53281/101000 (52.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 11.2828s / 15846.7826 s
agent0:                 episode reward: -0.1208,                 loss: 0.1891
agent1:                 episode reward: 0.1208,                 loss: nan
Episode: 53301/101000 (52.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3821s / 15854.1647 s
agent0:                 episode reward: -0.2684,                 loss: 0.1877
agent1:                 episode reward: 0.2684,                 loss: nan
Episode: 53321/101000 (52.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6440s / 15862.8087 s
agent0:                 episode reward: -0.0128,                 loss: 0.1867
agent1:                 episode reward: 0.0128,                 loss: nan
Episode: 53341/101000 (52.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3010s / 15872.1097 s
agent0:                 episode reward: -0.1192,                 loss: 0.1896
agent1:                 episode reward: 0.1192,                 loss: nan
Episode: 53361/101000 (52.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9456s / 15881.0553 s
agent0:                 episode reward: 0.3971,                 loss: 0.1847
agent1:                 episode reward: -0.3971,                 loss: 0.1655
Score delta: 1.6583140756268453, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/52926_0.
Episode: 53381/101000 (52.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3722s / 15890.4275 s
agent0:                 episode reward: -0.2458,                 loss: nan
agent1:                 episode reward: 0.2458,                 loss: 0.1675
Episode: 53401/101000 (52.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7727s / 15897.2001 s
agent0:                 episode reward: -0.5114,                 loss: nan
agent1:                 episode reward: 0.5114,                 loss: 0.1682
Episode: 53421/101000 (52.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2751s / 15907.4752 s
agent0:                 episode reward: -0.3042,                 loss: nan
agent1:                 episode reward: 0.3042,                 loss: 0.1645
Episode: 53441/101000 (52.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5851s / 15916.0603 s
agent0:                 episode reward: -0.2789,                 loss: nan
agent1:                 episode reward: 0.2789,                 loss: 0.1633
Episode: 53461/101000 (52.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1532s / 15925.2135 s
agent0:                 episode reward: -0.0166,                 loss: nan
agent1:                 episode reward: 0.0166,                 loss: 0.1560
Episode: 53481/101000 (52.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7006s / 15932.9141 s
agent0:                 episode reward: -0.6948,                 loss: nan
agent1:                 episode reward: 0.6948,                 loss: 0.1577
Episode: 53501/101000 (52.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5305s / 15939.4446 s
agent0:                 episode reward: -0.2412,                 loss: 0.1805
agent1:                 episode reward: 0.2412,                 loss: 0.1535
Score delta: 1.691879978515581, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/53056_1.
Episode: 53521/101000 (52.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6121s / 15950.0567 s
agent0:                 episode reward: -0.4459,                 loss: 0.1772
agent1:                 episode reward: 0.4459,                 loss: nan
Episode: 53541/101000 (53.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 12.3620s / 15962.4188 s
agent0:                 episode reward: -0.1835,                 loss: 0.1779
agent1:                 episode reward: 0.1835,                 loss: nan
Episode: 53561/101000 (53.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3710s / 15971.7897 s
agent0:                 episode reward: -0.0753,                 loss: 0.1762
agent1:                 episode reward: 0.0753,                 loss: nan
Episode: 53581/101000 (53.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2191s / 15980.0088 s
agent0:                 episode reward: -0.1345,                 loss: 0.1780
agent1:                 episode reward: 0.1345,                 loss: nan
Episode: 53601/101000 (53.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5771s / 15989.5859 s
agent0:                 episode reward: -0.3454,                 loss: 0.1766
agent1:                 episode reward: 0.3454,                 loss: nan
Episode: 53621/101000 (53.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1224s / 15998.7084 s
agent0:                 episode reward: -0.4271,                 loss: 0.1779
agent1:                 episode reward: 0.4271,                 loss: nan
Episode: 53641/101000 (53.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3860s / 16008.0943 s
agent0:                 episode reward: -0.0472,                 loss: 0.1770
agent1:                 episode reward: 0.0472,                 loss: nan
Episode: 53661/101000 (53.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4094s / 16018.5037 s
agent0:                 episode reward: -0.1169,                 loss: 0.1772
agent1:                 episode reward: 0.1169,                 loss: nan
Episode: 53681/101000 (53.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 12.2494s / 16030.7532 s
agent0:                 episode reward: -0.3669,                 loss: 0.1768
agent1:                 episode reward: 0.3669,                 loss: nan
Episode: 53701/101000 (53.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3139s / 16040.0671 s
agent0:                 episode reward: 0.1619,                 loss: 0.1745
agent1:                 episode reward: -0.1619,                 loss: nan
Episode: 53721/101000 (53.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5085s / 16050.5756 s
agent0:                 episode reward: -0.0165,                 loss: 0.1747
agent1:                 episode reward: 0.0165,                 loss: nan
Episode: 53741/101000 (53.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8345s / 16060.4101 s
agent0:                 episode reward: 0.1476,                 loss: 0.1752
agent1:                 episode reward: -0.1476,                 loss: 0.1677
Score delta: 1.7287086572393613, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/53301_0.
Episode: 53761/101000 (53.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6576s / 16070.0676 s
agent0:                 episode reward: -0.2230,                 loss: nan
agent1:                 episode reward: 0.2230,                 loss: 0.1658
Episode: 53781/101000 (53.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 11.2313s / 16081.2989 s
agent0:                 episode reward: -0.7014,                 loss: nan
agent1:                 episode reward: 0.7014,                 loss: 0.1672
Episode: 53801/101000 (53.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3293s / 16089.6282 s
agent0:                 episode reward: -0.6192,                 loss: nan
agent1:                 episode reward: 0.6192,                 loss: 0.1657
Episode: 53821/101000 (53.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6995s / 16098.3277 s
agent0:                 episode reward: -0.6339,                 loss: nan
agent1:                 episode reward: 0.6339,                 loss: 0.1630
Episode: 53841/101000 (53.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7555s / 16109.0832 s
agent0:                 episode reward: -0.4037,                 loss: 0.2557
agent1:                 episode reward: 0.4037,                 loss: 0.1641
Score delta: 2.077043670229776, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/53401_1.
Episode: 53861/101000 (53.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7018s / 16118.7850 s
agent0:                 episode reward: 0.3017,                 loss: 0.2500
agent1:                 episode reward: -0.3017,                 loss: nan
Episode: 53881/101000 (53.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7017s / 16127.4867 s
agent0:                 episode reward: 0.0714,                 loss: 0.2483
agent1:                 episode reward: -0.0714,                 loss: nan
Episode: 53901/101000 (53.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8100s / 16135.2966 s
agent0:                 episode reward: 0.0652,                 loss: 0.2483
agent1:                 episode reward: -0.0652,                 loss: nan
Episode: 53921/101000 (53.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3563s / 16144.6529 s
agent0:                 episode reward: -0.1168,                 loss: 0.2108
agent1:                 episode reward: 0.1168,                 loss: nan
Episode: 53941/101000 (53.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6279s / 16155.2808 s
agent0:                 episode reward: -0.2709,                 loss: 0.1842
agent1:                 episode reward: 0.2709,                 loss: nan
Episode: 53961/101000 (53.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3156s / 16165.5964 s
agent0:                 episode reward: -0.3114,                 loss: 0.1833
agent1:                 episode reward: 0.3114,                 loss: nan
Episode: 53981/101000 (53.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0149s / 16175.6113 s
agent0:                 episode reward: -0.0113,                 loss: 0.1834
agent1:                 episode reward: 0.0113,                 loss: nan
Episode: 54001/101000 (53.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5543s / 16184.1657 s
agent0:                 episode reward: 0.0239,                 loss: 0.1831
agent1:                 episode reward: -0.0239,                 loss: nan
Episode: 54021/101000 (53.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7490s / 16193.9147 s
agent0:                 episode reward: -0.2036,                 loss: 0.1827
agent1:                 episode reward: 0.2036,                 loss: nan
Episode: 54041/101000 (53.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 12.6047s / 16206.5194 s
agent0:                 episode reward: 0.2006,                 loss: 0.1836
agent1:                 episode reward: -0.2006,                 loss: nan
Episode: 54061/101000 (53.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4904s / 16215.0098 s
agent0:                 episode reward: -0.1052,                 loss: 0.1805
agent1:                 episode reward: 0.1052,                 loss: 0.1570
Score delta: 1.550362704105855, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/53620_0.
Episode: 54081/101000 (53.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9174s / 16223.9272 s
agent0:                 episode reward: -0.4322,                 loss: nan
agent1:                 episode reward: 0.4322,                 loss: 0.1480
Episode: 54101/101000 (53.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0456s / 16232.9728 s
agent0:                 episode reward: -0.2403,                 loss: nan
agent1:                 episode reward: 0.2403,                 loss: 0.1449
Episode: 54121/101000 (53.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6740s / 16241.6468 s
agent0:                 episode reward: -0.2107,                 loss: nan
agent1:                 episode reward: 0.2107,                 loss: 0.1440
Episode: 54141/101000 (53.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7285s / 16249.3753 s
agent0:                 episode reward: -0.1898,                 loss: nan
agent1:                 episode reward: 0.1898,                 loss: 0.1431
Episode: 54161/101000 (53.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8000s / 16258.1753 s
agent0:                 episode reward: -0.2194,                 loss: nan
agent1:                 episode reward: 0.2194,                 loss: 0.1430
Episode: 54181/101000 (53.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0480s / 16266.2234 s
agent0:                 episode reward: -0.3742,                 loss: nan
agent1:                 episode reward: 0.3742,                 loss: 0.1444
Episode: 54201/101000 (53.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2312s / 16275.4546 s
agent0:                 episode reward: -0.4824,                 loss: nan
agent1:                 episode reward: 0.4824,                 loss: 0.1426
Episode: 54221/101000 (53.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3965s / 16285.8511 s
agent0:                 episode reward: -0.4364,                 loss: 0.1751
agent1:                 episode reward: 0.4364,                 loss: 0.1470
Score delta: 1.5336099800545544, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/53776_1.
Episode: 54241/101000 (53.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1760s / 16295.0271 s
agent0:                 episode reward: 0.0415,                 loss: 0.1724
agent1:                 episode reward: -0.0415,                 loss: nan
Episode: 54261/101000 (53.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 11.8831s / 16306.9102 s
agent0:                 episode reward: -0.1151,                 loss: 0.1726
agent1:                 episode reward: 0.1151,                 loss: nan
Episode: 54281/101000 (53.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8785s / 16314.7887 s
agent0:                 episode reward: -0.2042,                 loss: 0.1712
agent1:                 episode reward: 0.2042,                 loss: nan
Episode: 54301/101000 (53.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9008s / 16323.6894 s
agent0:                 episode reward: -0.2030,                 loss: 0.1731
agent1:                 episode reward: 0.2030,                 loss: nan
Episode: 54321/101000 (53.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8288s / 16334.5182 s
agent0:                 episode reward: 0.0612,                 loss: 0.1720
agent1:                 episode reward: -0.0612,                 loss: nan
Episode: 54341/101000 (53.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5210s / 16344.0392 s
agent0:                 episode reward: -0.1133,                 loss: 0.1724
agent1:                 episode reward: 0.1133,                 loss: nan
Episode: 54361/101000 (53.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 12.3547s / 16356.3940 s
agent0:                 episode reward: 0.1540,                 loss: 0.1717
agent1:                 episode reward: -0.1540,                 loss: nan
Episode: 54381/101000 (53.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6297s / 16365.0236 s
agent0:                 episode reward: -0.1111,                 loss: 0.1724
agent1:                 episode reward: 0.1111,                 loss: nan
Episode: 54401/101000 (53.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9287s / 16374.9523 s
agent0:                 episode reward: 0.4275,                 loss: 0.1742
agent1:                 episode reward: -0.4275,                 loss: nan
Episode: 54421/101000 (53.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8762s / 16384.8285 s
agent0:                 episode reward: -0.2840,                 loss: 0.1824
agent1:                 episode reward: 0.2840,                 loss: nan
Episode: 54441/101000 (53.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 11.3939s / 16396.2224 s
agent0:                 episode reward: -0.1436,                 loss: 0.1812
agent1:                 episode reward: 0.1436,                 loss: nan
Episode: 54461/101000 (53.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 11.2752s / 16407.4976 s
agent0:                 episode reward: 0.2914,                 loss: 0.1803
agent1:                 episode reward: -0.2914,                 loss: nan
Episode: 54481/101000 (53.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2028s / 16417.7004 s
agent0:                 episode reward: -0.1277,                 loss: 0.1805
agent1:                 episode reward: 0.1277,                 loss: nan
Episode: 54501/101000 (53.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7558s / 16426.4562 s
agent0:                 episode reward: -0.1283,                 loss: 0.1801
agent1:                 episode reward: 0.1283,                 loss: nan
Episode: 54521/101000 (53.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8720s / 16436.3282 s
agent0:                 episode reward: 0.1373,                 loss: 0.1809
agent1:                 episode reward: -0.1373,                 loss: nan
Episode: 54541/101000 (54.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7850s / 16447.1132 s
agent0:                 episode reward: -0.3511,                 loss: 0.1814
agent1:                 episode reward: 0.3511,                 loss: nan
Episode: 54561/101000 (54.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9776s / 16458.0908 s
agent0:                 episode reward: -0.0006,                 loss: 0.1839
agent1:                 episode reward: 0.0006,                 loss: nan
Episode: 54581/101000 (54.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 12.0277s / 16470.1185 s
agent0:                 episode reward: 0.1492,                 loss: 0.1821
agent1:                 episode reward: -0.1492,                 loss: nan
Episode: 54601/101000 (54.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 12.1585s / 16482.2770 s
agent0:                 episode reward: -0.0539,                 loss: 0.1811
agent1:                 episode reward: 0.0539,                 loss: nan
Episode: 54621/101000 (54.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2526s / 16491.5297 s
agent0:                 episode reward: -0.0631,                 loss: 0.1817
agent1:                 episode reward: 0.0631,                 loss: nan
Episode: 54641/101000 (54.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1849s / 16499.7146 s
agent0:                 episode reward: 0.2421,                 loss: 0.1831
agent1:                 episode reward: -0.2421,                 loss: 0.1443
Score delta: 1.660076822055224, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/54202_0.
Episode: 54661/101000 (54.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4373s / 16507.1518 s
agent0:                 episode reward: -0.0666,                 loss: nan
agent1:                 episode reward: 0.0666,                 loss: 0.1469
Episode: 54681/101000 (54.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5535s / 16516.7053 s
agent0:                 episode reward: -0.5599,                 loss: nan
agent1:                 episode reward: 0.5599,                 loss: 0.1568
Episode: 54701/101000 (54.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5145s / 16525.2198 s
agent0:                 episode reward: -0.2167,                 loss: nan
agent1:                 episode reward: 0.2167,                 loss: 0.1551
Episode: 54721/101000 (54.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8757s / 16533.0955 s
agent0:                 episode reward: -0.5209,                 loss: nan
agent1:                 episode reward: 0.5209,                 loss: 0.1558
Episode: 54741/101000 (54.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1872s / 16542.2827 s
agent0:                 episode reward: -0.4901,                 loss: 0.1881
agent1:                 episode reward: 0.4901,                 loss: 0.1563
Score delta: 1.7862649233343972, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/54308_1.
Episode: 54761/101000 (54.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7652s / 16552.0479 s
agent0:                 episode reward: -0.2007,                 loss: 0.1918
agent1:                 episode reward: 0.2007,                 loss: nan
Episode: 54781/101000 (54.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7161s / 16561.7639 s
agent0:                 episode reward: -0.0930,                 loss: 0.1899
agent1:                 episode reward: 0.0930,                 loss: nan
Episode: 54801/101000 (54.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6702s / 16572.4342 s
agent0:                 episode reward: 0.1896,                 loss: 0.1905
agent1:                 episode reward: -0.1896,                 loss: nan
Episode: 54821/101000 (54.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3135s / 16579.7476 s
agent0:                 episode reward: -0.0522,                 loss: 0.1899
agent1:                 episode reward: 0.0522,                 loss: nan
Episode: 54841/101000 (54.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2893s / 16587.0369 s
agent0:                 episode reward: 0.1710,                 loss: 0.1892
agent1:                 episode reward: -0.1710,                 loss: nan
Episode: 54861/101000 (54.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5924s / 16596.6293 s
agent0:                 episode reward: -0.2669,                 loss: 0.1874
agent1:                 episode reward: 0.2669,                 loss: nan
Episode: 54881/101000 (54.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2500s / 16606.8793 s
agent0:                 episode reward: -0.2347,                 loss: 0.1884
agent1:                 episode reward: 0.2347,                 loss: nan
Episode: 54901/101000 (54.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 11.1593s / 16618.0386 s
agent0:                 episode reward: -0.0466,                 loss: 0.1885
agent1:                 episode reward: 0.0466,                 loss: nan
Episode: 54921/101000 (54.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9998s / 16629.0384 s
agent0:                 episode reward: 0.0248,                 loss: 0.1884
agent1:                 episode reward: -0.0248,                 loss: nan
Episode: 54941/101000 (54.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1291s / 16637.1675 s
agent0:                 episode reward: -0.2678,                 loss: 0.1858
agent1:                 episode reward: 0.2678,                 loss: nan
Episode: 54961/101000 (54.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7583s / 16645.9258 s
agent0:                 episode reward: 0.0263,                 loss: 0.1876
agent1:                 episode reward: -0.0263,                 loss: nan
Episode: 54981/101000 (54.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3487s / 16655.2745 s
agent0:                 episode reward: -0.1706,                 loss: 0.1887
agent1:                 episode reward: 0.1706,                 loss: nan
Episode: 55001/101000 (54.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6062s / 16665.8807 s
agent0:                 episode reward: 0.4020,                 loss: 0.1886
agent1:                 episode reward: -0.4020,                 loss: nan
Episode: 55021/101000 (54.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0750s / 16675.9558 s
agent0:                 episode reward: -0.3045,                 loss: 0.1895
agent1:                 episode reward: 0.3045,                 loss: nan
Episode: 55041/101000 (54.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 11.1128s / 16687.0686 s
agent0:                 episode reward: -0.2536,                 loss: 0.1862
agent1:                 episode reward: 0.2536,                 loss: nan
Episode: 55061/101000 (54.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0994s / 16695.1680 s
agent0:                 episode reward: 0.0415,                 loss: 0.1869
agent1:                 episode reward: -0.0415,                 loss: nan
Episode: 55081/101000 (54.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9345s / 16704.1025 s
agent0:                 episode reward: 0.0623,                 loss: 0.1869
agent1:                 episode reward: -0.0623,                 loss: nan
Episode: 55101/101000 (54.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 11.6315s / 16715.7340 s
agent0:                 episode reward: 0.5171,                 loss: 0.1870
agent1:                 episode reward: -0.5171,                 loss: 0.1742
Score delta: 1.8191824559585978, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/54671_0.
Episode: 55121/101000 (54.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0103s / 16725.7443 s
agent0:                 episode reward: -0.0691,                 loss: nan
agent1:                 episode reward: 0.0691,                 loss: 0.1687
Episode: 55141/101000 (54.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9109s / 16736.6552 s
agent0:                 episode reward: -0.6125,                 loss: nan
agent1:                 episode reward: 0.6125,                 loss: 0.1701
Episode: 55161/101000 (54.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8675s / 16744.5227 s
agent0:                 episode reward: -0.8491,                 loss: nan
agent1:                 episode reward: 0.8491,                 loss: 0.1689
Episode: 55181/101000 (54.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4775s / 16755.0002 s
agent0:                 episode reward: -0.5220,                 loss: nan
agent1:                 episode reward: 0.5220,                 loss: 0.1694
Episode: 55201/101000 (54.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3457s / 16764.3459 s
agent0:                 episode reward: -0.3484,                 loss: nan
agent1:                 episode reward: 0.3484,                 loss: 0.1667
Episode: 55221/101000 (54.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8905s / 16773.2364 s
agent0:                 episode reward: -0.3223,                 loss: 0.1749
agent1:                 episode reward: 0.3223,                 loss: 0.1654
Score delta: 1.5569054809639513, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/54780_1.
Episode: 55241/101000 (54.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5724s / 16778.8088 s
agent0:                 episode reward: -0.1871,                 loss: 0.1717
agent1:                 episode reward: 0.1871,                 loss: nan
Episode: 55261/101000 (54.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6514s / 16789.4602 s
agent0:                 episode reward: 0.1364,                 loss: 0.1733
agent1:                 episode reward: -0.1364,                 loss: nan
Episode: 55281/101000 (54.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8613s / 16798.3215 s
agent0:                 episode reward: -0.2443,                 loss: 0.1732
agent1:                 episode reward: 0.2443,                 loss: nan
Episode: 55301/101000 (54.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0714s / 16807.3930 s
agent0:                 episode reward: -0.4212,                 loss: 0.1847
agent1:                 episode reward: 0.4212,                 loss: nan
Episode: 55321/101000 (54.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 11.9791s / 16819.3720 s
agent0:                 episode reward: -0.0710,                 loss: 0.1842
agent1:                 episode reward: 0.0710,                 loss: nan
Episode: 55341/101000 (54.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2673s / 16827.6394 s
agent0:                 episode reward: -0.2082,                 loss: 0.1829
agent1:                 episode reward: 0.2082,                 loss: nan
Episode: 55361/101000 (54.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9152s / 16835.5545 s
agent0:                 episode reward: 0.4310,                 loss: 0.1847
agent1:                 episode reward: -0.4310,                 loss: nan
Episode: 55381/101000 (54.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 11.3556s / 16846.9101 s
agent0:                 episode reward: -0.1043,                 loss: 0.1832
agent1:                 episode reward: 0.1043,                 loss: nan
Episode: 55401/101000 (54.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4045s / 16857.3146 s
agent0:                 episode reward: 0.2527,                 loss: 0.1836
agent1:                 episode reward: -0.2527,                 loss: nan
Episode: 55421/101000 (54.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8303s / 16867.1449 s
agent0:                 episode reward: 0.3695,                 loss: 0.1829
agent1:                 episode reward: -0.3695,                 loss: 0.1536
Score delta: 1.6129720472425189, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/54994_0.
Episode: 55441/101000 (54.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1089s / 16876.2538 s
agent0:                 episode reward: -0.3000,                 loss: nan
agent1:                 episode reward: 0.3000,                 loss: 0.1475
Episode: 55461/101000 (54.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9789s / 16885.2327 s
agent0:                 episode reward: -0.2651,                 loss: nan
agent1:                 episode reward: 0.2651,                 loss: 0.1450
Episode: 55481/101000 (54.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2960s / 16894.5286 s
agent0:                 episode reward: -0.3438,                 loss: nan
agent1:                 episode reward: 0.3438,                 loss: 0.1461
Episode: 55501/101000 (54.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5567s / 16904.0853 s
agent0:                 episode reward: -0.7254,                 loss: nan
agent1:                 episode reward: 0.7254,                 loss: 0.1436
Episode: 55521/101000 (54.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9488s / 16911.0341 s
agent0:                 episode reward: -0.1571,                 loss: nan
agent1:                 episode reward: 0.1571,                 loss: 0.1454
Episode: 55541/101000 (54.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 11.6682s / 16922.7023 s
agent0:                 episode reward: -0.4808,                 loss: 0.1829
agent1:                 episode reward: 0.4808,                 loss: 0.1454
Score delta: 1.5391867406125388, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/55112_1.
Episode: 55561/101000 (55.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7421s / 16931.4444 s
agent0:                 episode reward: 0.4939,                 loss: 0.1820
agent1:                 episode reward: -0.4939,                 loss: nan
Episode: 55581/101000 (55.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4169s / 16941.8614 s
agent0:                 episode reward: 0.1576,                 loss: 0.1843
agent1:                 episode reward: -0.1576,                 loss: nan
Episode: 55601/101000 (55.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8929s / 16951.7543 s
agent0:                 episode reward: -0.1451,                 loss: 0.1846
agent1:                 episode reward: 0.1451,                 loss: nan
Episode: 55621/101000 (55.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1512s / 16961.9055 s
agent0:                 episode reward: 0.2412,                 loss: 0.1823
agent1:                 episode reward: -0.2412,                 loss: nan
Episode: 55641/101000 (55.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3086s / 16970.2141 s
agent0:                 episode reward: -0.1151,                 loss: 0.1820
agent1:                 episode reward: 0.1151,                 loss: nan
Episode: 55661/101000 (55.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4019s / 16979.6160 s
agent0:                 episode reward: 0.0142,                 loss: 0.1832
agent1:                 episode reward: -0.0142,                 loss: nan
Episode: 55681/101000 (55.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0581s / 16989.6741 s
agent0:                 episode reward: 0.0311,                 loss: 0.1831
agent1:                 episode reward: -0.0311,                 loss: nan
Episode: 55701/101000 (55.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 11.1164s / 17000.7906 s
agent0:                 episode reward: -0.2886,                 loss: 0.1853
agent1:                 episode reward: 0.2886,                 loss: nan
Episode: 55721/101000 (55.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4858s / 17010.2763 s
agent0:                 episode reward: 0.0930,                 loss: 0.1826
agent1:                 episode reward: -0.0930,                 loss: nan
Episode: 55741/101000 (55.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6492s / 17019.9255 s
agent0:                 episode reward: -0.0606,                 loss: 0.1817
agent1:                 episode reward: 0.0606,                 loss: nan
Episode: 55761/101000 (55.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8269s / 17029.7524 s
agent0:                 episode reward: 0.0310,                 loss: 0.1795
agent1:                 episode reward: -0.0310,                 loss: nan
Episode: 55781/101000 (55.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2483s / 17040.0007 s
agent0:                 episode reward: 0.0201,                 loss: 0.1778
agent1:                 episode reward: -0.0201,                 loss: nan
Episode: 55801/101000 (55.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9754s / 17049.9761 s
agent0:                 episode reward: 0.1105,                 loss: 0.1783
agent1:                 episode reward: -0.1105,                 loss: nan
Episode: 55821/101000 (55.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8499s / 17059.8260 s
agent0:                 episode reward: 0.0291,                 loss: 0.1797
agent1:                 episode reward: -0.0291,                 loss: nan
Episode: 55841/101000 (55.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8395s / 17065.6655 s
agent0:                 episode reward: -0.0773,                 loss: 0.1801
agent1:                 episode reward: 0.0773,                 loss: nan
Episode: 55861/101000 (55.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4034s / 17076.0688 s
agent0:                 episode reward: 0.0180,                 loss: 0.1795
agent1:                 episode reward: -0.0180,                 loss: nan
Episode: 55881/101000 (55.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1497s / 17086.2185 s
agent0:                 episode reward: -0.2219,                 loss: 0.1808
agent1:                 episode reward: 0.2219,                 loss: nan
Episode: 55901/101000 (55.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0277s / 17094.2463 s
agent0:                 episode reward: 0.2548,                 loss: 0.1795
agent1:                 episode reward: -0.2548,                 loss: nan
Episode: 55921/101000 (55.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 12.6970s / 17106.9432 s
agent0:                 episode reward: -0.2791,                 loss: 0.1807
agent1:                 episode reward: 0.2791,                 loss: nan
Episode: 55941/101000 (55.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1430s / 17117.0863 s
agent0:                 episode reward: 0.1984,                 loss: 0.1787
agent1:                 episode reward: -0.1984,                 loss: nan
Episode: 55961/101000 (55.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6492s / 17126.7355 s
agent0:                 episode reward: 0.2723,                 loss: 0.1794
agent1:                 episode reward: -0.2723,                 loss: nan
Episode: 55981/101000 (55.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9556s / 17137.6911 s
agent0:                 episode reward: -0.2808,                 loss: 0.1788
agent1:                 episode reward: 0.2808,                 loss: nan
Episode: 56001/101000 (55.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0771s / 17146.7682 s
agent0:                 episode reward: -0.0987,                 loss: 0.1803
agent1:                 episode reward: 0.0987,                 loss: nan
Episode: 56021/101000 (55.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7380s / 17156.5063 s
agent0:                 episode reward: 0.2546,                 loss: 0.1786
agent1:                 episode reward: -0.2546,                 loss: nan
Episode: 56041/101000 (55.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8320s / 17166.3383 s
agent0:                 episode reward: -0.0799,                 loss: 0.1799
agent1:                 episode reward: 0.0799,                 loss: nan
Episode: 56061/101000 (55.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1416s / 17175.4799 s
agent0:                 episode reward: -0.1732,                 loss: 0.1796
agent1:                 episode reward: 0.1732,                 loss: nan
Episode: 56081/101000 (55.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 12.4704s / 17187.9504 s
agent0:                 episode reward: -0.3048,                 loss: 0.1854
agent1:                 episode reward: 0.3048,                 loss: nan
Episode: 56101/101000 (55.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4095s / 17196.3599 s
agent0:                 episode reward: -0.0595,                 loss: 0.1866
agent1:                 episode reward: 0.0595,                 loss: nan
Episode: 56121/101000 (55.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0516s / 17205.4115 s
agent0:                 episode reward: -0.1437,                 loss: 0.1873
agent1:                 episode reward: 0.1437,                 loss: nan
Episode: 56141/101000 (55.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3485s / 17213.7599 s
agent0:                 episode reward: -0.0801,                 loss: 0.1864
agent1:                 episode reward: 0.0801,                 loss: nan
Episode: 56161/101000 (55.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3423s / 17223.1022 s
agent0:                 episode reward: 0.1245,                 loss: 0.1849
agent1:                 episode reward: -0.1245,                 loss: nan
Episode: 56181/101000 (55.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3772s / 17232.4794 s
agent0:                 episode reward: -0.0661,                 loss: 0.1854
agent1:                 episode reward: 0.0661,                 loss: nan
Episode: 56201/101000 (55.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 11.0248s / 17243.5042 s
agent0:                 episode reward: -0.1133,                 loss: 0.1853
agent1:                 episode reward: 0.1133,                 loss: nan
Episode: 56221/101000 (55.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6307s / 17252.1349 s
agent0:                 episode reward: -0.0749,                 loss: 0.1852
agent1:                 episode reward: 0.0749,                 loss: nan
Episode: 56241/101000 (55.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 11.0035s / 17263.1384 s
agent0:                 episode reward: -0.1138,                 loss: 0.1870
agent1:                 episode reward: 0.1138,                 loss: nan
Episode: 56261/101000 (55.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7486s / 17273.8871 s
agent0:                 episode reward: 0.0259,                 loss: 0.1861
agent1:                 episode reward: -0.0259,                 loss: nan
Episode: 56281/101000 (55.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0027s / 17283.8897 s
agent0:                 episode reward: 0.2814,                 loss: 0.1867
agent1:                 episode reward: -0.2814,                 loss: nan
Episode: 56301/101000 (55.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 13.2845s / 17297.1742 s
agent0:                 episode reward: -0.3556,                 loss: 0.1838
agent1:                 episode reward: 0.3556,                 loss: nan
Episode: 56321/101000 (55.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3635s / 17305.5377 s
agent0:                 episode reward: -0.1703,                 loss: 0.1883
agent1:                 episode reward: 0.1703,                 loss: nan
Episode: 56341/101000 (55.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7248s / 17314.2625 s
agent0:                 episode reward: -0.0628,                 loss: 0.1867
agent1:                 episode reward: 0.0628,                 loss: nan
Episode: 56361/101000 (55.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 11.0560s / 17325.3185 s
agent0:                 episode reward: 0.2112,                 loss: 0.1874
agent1:                 episode reward: -0.2112,                 loss: nan
Episode: 56381/101000 (55.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3573s / 17333.6757 s
agent0:                 episode reward: -0.0429,                 loss: 0.1856
agent1:                 episode reward: 0.0429,                 loss: nan
Episode: 56401/101000 (55.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7835s / 17342.4592 s
agent0:                 episode reward: 0.3875,                 loss: 0.1850
agent1:                 episode reward: -0.3875,                 loss: nan
Episode: 56421/101000 (55.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8130s / 17351.2723 s
agent0:                 episode reward: -0.1088,                 loss: 0.1812
agent1:                 episode reward: 0.1088,                 loss: nan
Episode: 56441/101000 (55.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9984s / 17360.2707 s
agent0:                 episode reward: 0.3688,                 loss: 0.1801
agent1:                 episode reward: -0.3688,                 loss: 0.1593
Score delta: 1.706091065042159, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/56004_0.
Episode: 56461/101000 (55.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7340s / 17369.0047 s
agent0:                 episode reward: -0.4520,                 loss: nan
agent1:                 episode reward: 0.4520,                 loss: 0.1599
Episode: 56481/101000 (55.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4860s / 17377.4907 s
agent0:                 episode reward: -0.1484,                 loss: nan
agent1:                 episode reward: 0.1484,                 loss: 0.1607
Episode: 56501/101000 (55.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1151s / 17385.6057 s
agent0:                 episode reward: -0.2116,                 loss: nan
agent1:                 episode reward: 0.2116,                 loss: 0.1571
Episode: 56521/101000 (55.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3285s / 17389.9343 s
agent0:                 episode reward: -0.5750,                 loss: nan
agent1:                 episode reward: 0.5750,                 loss: 0.1581
Episode: 56541/101000 (55.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5691s / 17398.5034 s
agent0:                 episode reward: -0.6766,                 loss: 0.1881
agent1:                 episode reward: 0.6766,                 loss: 0.1589
Score delta: 1.8595018765540892, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/56109_1.
Episode: 56561/101000 (56.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 11.2491s / 17409.7525 s
agent0:                 episode reward: -0.2204,                 loss: 0.1797
agent1:                 episode reward: 0.2204,                 loss: nan
Episode: 56581/101000 (56.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9288s / 17418.6813 s
agent0:                 episode reward: -0.1605,                 loss: 0.1785
agent1:                 episode reward: 0.1605,                 loss: nan
Episode: 56601/101000 (56.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6960s / 17427.3773 s
agent0:                 episode reward: -0.2304,                 loss: 0.1783
agent1:                 episode reward: 0.2304,                 loss: nan
Episode: 56621/101000 (56.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1434s / 17436.5207 s
agent0:                 episode reward: -0.1894,                 loss: 0.1765
agent1:                 episode reward: 0.1894,                 loss: nan
Episode: 56641/101000 (56.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 11.6439s / 17448.1646 s
agent0:                 episode reward: -0.2363,                 loss: 0.1772
agent1:                 episode reward: 0.2363,                 loss: nan
Episode: 56661/101000 (56.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8362s / 17457.0008 s
agent0:                 episode reward: -0.3321,                 loss: 0.1782
agent1:                 episode reward: 0.3321,                 loss: nan
Episode: 56681/101000 (56.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5494s / 17466.5502 s
agent0:                 episode reward: -0.0638,                 loss: 0.1776
agent1:                 episode reward: 0.0638,                 loss: nan
Episode: 56701/101000 (56.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9009s / 17477.4512 s
agent0:                 episode reward: -0.3141,                 loss: 0.1781
agent1:                 episode reward: 0.3141,                 loss: nan
Episode: 56721/101000 (56.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 11.2938s / 17488.7450 s
agent0:                 episode reward: -0.0982,                 loss: 0.1763
agent1:                 episode reward: 0.0982,                 loss: nan
Episode: 56741/101000 (56.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4296s / 17499.1746 s
agent0:                 episode reward: -0.0405,                 loss: 0.1754
agent1:                 episode reward: 0.0405,                 loss: nan
Episode: 56761/101000 (56.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7604s / 17509.9350 s
agent0:                 episode reward: -0.4759,                 loss: 0.1788
agent1:                 episode reward: 0.4759,                 loss: nan
Episode: 56781/101000 (56.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4126s / 17520.3476 s
agent0:                 episode reward: -0.2460,                 loss: 0.1770
agent1:                 episode reward: 0.2460,                 loss: nan
Episode: 56801/101000 (56.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7331s / 17530.0807 s
agent0:                 episode reward: -0.5101,                 loss: 0.1785
agent1:                 episode reward: 0.5101,                 loss: nan
Episode: 56821/101000 (56.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1376s / 17540.2183 s
agent0:                 episode reward: -0.1098,                 loss: 0.1771
agent1:                 episode reward: 0.1098,                 loss: nan
Episode: 56841/101000 (56.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5746s / 17549.7930 s
agent0:                 episode reward: -0.2421,                 loss: 0.1820
agent1:                 episode reward: 0.2421,                 loss: nan
Episode: 56861/101000 (56.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7276s / 17559.5206 s
agent0:                 episode reward: 0.2776,                 loss: 0.1914
agent1:                 episode reward: -0.2776,                 loss: nan
Episode: 56881/101000 (56.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 11.1826s / 17570.7032 s
agent0:                 episode reward: 0.2792,                 loss: 0.1930
agent1:                 episode reward: -0.2792,                 loss: nan
Episode: 56901/101000 (56.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5514s / 17578.2546 s
agent0:                 episode reward: 0.0381,                 loss: 0.1904
agent1:                 episode reward: -0.0381,                 loss: nan
Episode: 56921/101000 (56.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0727s / 17587.3273 s
agent0:                 episode reward: 0.0812,                 loss: 0.1908
agent1:                 episode reward: -0.0812,                 loss: nan
Episode: 56941/101000 (56.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4161s / 17597.7434 s
agent0:                 episode reward: 0.0284,                 loss: 0.1918
agent1:                 episode reward: -0.0284,                 loss: nan
Episode: 56961/101000 (56.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 11.1199s / 17608.8634 s
agent0:                 episode reward: 0.0483,                 loss: 0.1906
agent1:                 episode reward: -0.0483,                 loss: nan
Episode: 56981/101000 (56.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6051s / 17619.4684 s
agent0:                 episode reward: -0.3449,                 loss: 0.1907
agent1:                 episode reward: 0.3449,                 loss: nan
Episode: 57001/101000 (56.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4811s / 17626.9495 s
agent0:                 episode reward: -0.3790,                 loss: 0.1904
agent1:                 episode reward: 0.3790,                 loss: nan
Episode: 57021/101000 (56.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7292s / 17637.6788 s
agent0:                 episode reward: 0.0643,                 loss: 0.1898
agent1:                 episode reward: -0.0643,                 loss: nan
Episode: 57041/101000 (56.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0127s / 17646.6915 s
agent0:                 episode reward: 0.2419,                 loss: 0.1906
agent1:                 episode reward: -0.2419,                 loss: nan
Episode: 57061/101000 (56.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9081s / 17653.5996 s
agent0:                 episode reward: -0.1330,                 loss: 0.1888
agent1:                 episode reward: 0.1330,                 loss: nan
Episode: 57081/101000 (56.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 12.6200s / 17666.2196 s
agent0:                 episode reward: 0.1470,                 loss: 0.1924
agent1:                 episode reward: -0.1470,                 loss: nan
Episode: 57101/101000 (56.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5028s / 17676.7224 s
agent0:                 episode reward: -0.4310,                 loss: 0.1902
agent1:                 episode reward: 0.4310,                 loss: nan
Episode: 57121/101000 (56.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4204s / 17687.1428 s
agent0:                 episode reward: 0.0282,                 loss: 0.1902
agent1:                 episode reward: -0.0282,                 loss: nan
Episode: 57141/101000 (56.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 11.5010s / 17698.6437 s
agent0:                 episode reward: -0.1396,                 loss: 0.1924
agent1:                 episode reward: 0.1396,                 loss: nan
Episode: 57161/101000 (56.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8400s / 17707.4837 s
agent0:                 episode reward: 0.2428,                 loss: 0.1891
agent1:                 episode reward: -0.2428,                 loss: nan
Episode: 57181/101000 (56.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0496s / 17715.5333 s
agent0:                 episode reward: 0.3926,                 loss: 0.1912
agent1:                 episode reward: -0.3926,                 loss: nan
Episode: 57201/101000 (56.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7737s / 17723.3070 s
agent0:                 episode reward: -0.4672,                 loss: 0.1847
agent1:                 episode reward: 0.4672,                 loss: 0.1440
Score delta: 1.5168745776570423, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/56757_0.
Episode: 57221/101000 (56.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 12.6660s / 17735.9730 s
agent0:                 episode reward: -0.3714,                 loss: nan
agent1:                 episode reward: 0.3714,                 loss: 0.1427
Episode: 57241/101000 (56.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4136s / 17744.3865 s
agent0:                 episode reward: -0.5272,                 loss: nan
agent1:                 episode reward: 0.5272,                 loss: 0.1435
Episode: 57261/101000 (56.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7134s / 17754.0999 s
agent0:                 episode reward: -0.2540,                 loss: nan
agent1:                 episode reward: 0.2540,                 loss: 0.1431
Episode: 57281/101000 (56.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7156s / 17763.8155 s
agent0:                 episode reward: -0.3652,                 loss: nan
agent1:                 episode reward: 0.3652,                 loss: 0.1419
Episode: 57301/101000 (56.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1333s / 17771.9488 s
agent0:                 episode reward: -0.3476,                 loss: nan
agent1:                 episode reward: 0.3476,                 loss: 0.1416
Episode: 57321/101000 (56.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4804s / 17779.4292 s
agent0:                 episode reward: -0.3731,                 loss: 0.1722
agent1:                 episode reward: 0.3731,                 loss: 0.1428
Score delta: 1.5660392585961591, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/56883_1.
Episode: 57341/101000 (56.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8064s / 17788.2356 s
agent0:                 episode reward: 0.0455,                 loss: 0.1707
agent1:                 episode reward: -0.0455,                 loss: nan
Episode: 57361/101000 (56.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5021s / 17797.7377 s
agent0:                 episode reward: -0.1217,                 loss: 0.1728
agent1:                 episode reward: 0.1217,                 loss: nan
Episode: 57381/101000 (56.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0968s / 17806.8345 s
agent0:                 episode reward: 0.2360,                 loss: 0.1713
agent1:                 episode reward: -0.2360,                 loss: nan
Episode: 57401/101000 (56.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3087s / 17816.1432 s
agent0:                 episode reward: 0.1689,                 loss: 0.1734
agent1:                 episode reward: -0.1689,                 loss: nan
Episode: 57421/101000 (56.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 11.4124s / 17827.5556 s
agent0:                 episode reward: 0.0657,                 loss: 0.1717
agent1:                 episode reward: -0.0657,                 loss: nan
Episode: 57441/101000 (56.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1766s / 17835.7323 s
agent0:                 episode reward: -0.0718,                 loss: 0.1736
agent1:                 episode reward: 0.0718,                 loss: nan
Episode: 57461/101000 (56.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0896s / 17845.8219 s
agent0:                 episode reward: 0.0385,                 loss: 0.1751
agent1:                 episode reward: -0.0385,                 loss: nan
Episode: 57481/101000 (56.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5818s / 17855.4037 s
agent0:                 episode reward: 0.1827,                 loss: 0.1716
agent1:                 episode reward: -0.1827,                 loss: nan
Episode: 57501/101000 (56.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5195s / 17865.9232 s
agent0:                 episode reward: 0.3859,                 loss: 0.1724
agent1:                 episode reward: -0.3859,                 loss: 0.1452
Score delta: 1.7007343592240414, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/57073_0.
Episode: 57521/101000 (56.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8189s / 17874.7420 s
agent0:                 episode reward: -0.1711,                 loss: nan
agent1:                 episode reward: 0.1711,                 loss: 0.1432
Episode: 57541/101000 (56.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2143s / 17883.9563 s
agent0:                 episode reward: -0.7403,                 loss: nan
agent1:                 episode reward: 0.7403,                 loss: 0.1428
Episode: 57561/101000 (56.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 11.1411s / 17895.0974 s
agent0:                 episode reward: 0.0862,                 loss: nan
agent1:                 episode reward: -0.0862,                 loss: 0.1431
Episode: 57581/101000 (57.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4459s / 17902.5433 s
agent0:                 episode reward: -0.0896,                 loss: nan
agent1:                 episode reward: 0.0896,                 loss: 0.1430
Episode: 57601/101000 (57.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4902s / 17910.0335 s
agent0:                 episode reward: -0.2683,                 loss: nan
agent1:                 episode reward: 0.2683,                 loss: 0.1439
Episode: 57621/101000 (57.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7236s / 17917.7571 s
agent0:                 episode reward: -0.1975,                 loss: nan
agent1:                 episode reward: 0.1975,                 loss: 0.1436
Episode: 57641/101000 (57.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1619s / 17926.9190 s
agent0:                 episode reward: -0.1103,                 loss: nan
agent1:                 episode reward: 0.1103,                 loss: 0.1502
Episode: 57661/101000 (57.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7220s / 17937.6410 s
agent0:                 episode reward: -0.4446,                 loss: nan
agent1:                 episode reward: 0.4446,                 loss: 0.1554
Episode: 57681/101000 (57.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1120s / 17945.7530 s
agent0:                 episode reward: -0.1446,                 loss: 0.1611
agent1:                 episode reward: 0.1446,                 loss: 0.1534
Score delta: 1.5221673026247196, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/57236_1.
Episode: 57701/101000 (57.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 12.0302s / 17957.7832 s
agent0:                 episode reward: 0.0131,                 loss: 0.1649
agent1:                 episode reward: -0.0131,                 loss: nan
Episode: 57721/101000 (57.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3026s / 17966.0858 s
agent0:                 episode reward: 0.1265,                 loss: 0.1624
agent1:                 episode reward: -0.1265,                 loss: nan
Episode: 57741/101000 (57.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 10.1082s / 17976.1941 s
agent0:                 episode reward: 0.5916,                 loss: 0.1611
agent1:                 episode reward: -0.5916,                 loss: nan
Episode: 57761/101000 (57.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1547s / 17985.3487 s
agent0:                 episode reward: -0.1284,                 loss: 0.1607
agent1:                 episode reward: 0.1284,                 loss: nan
Episode: 57781/101000 (57.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3503s / 17993.6990 s
agent0:                 episode reward: -0.2906,                 loss: 0.1608
agent1:                 episode reward: 0.2906,                 loss: nan
Episode: 57801/101000 (57.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3040s / 18003.0031 s
agent0:                 episode reward: -0.2114,                 loss: 0.1724
agent1:                 episode reward: 0.2114,                 loss: nan
Episode: 57821/101000 (57.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7875s / 18013.7906 s
agent0:                 episode reward: -0.0534,                 loss: 0.1837
agent1:                 episode reward: 0.0534,                 loss: nan
Episode: 57841/101000 (57.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4155s / 18024.2061 s
agent0:                 episode reward: -0.5919,                 loss: 0.1843
agent1:                 episode reward: 0.5919,                 loss: nan
Episode: 57861/101000 (57.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2731s / 18033.4792 s
agent0:                 episode reward: -0.2537,                 loss: 0.1828
agent1:                 episode reward: 0.2537,                 loss: nan
Episode: 57881/101000 (57.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0917s / 18042.5710 s
agent0:                 episode reward: -0.1905,                 loss: 0.1834
agent1:                 episode reward: 0.1905,                 loss: nan
Episode: 57901/101000 (57.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4526s / 18053.0235 s
agent0:                 episode reward: 0.3473,                 loss: 0.1843
agent1:                 episode reward: -0.3473,                 loss: 0.1486
Score delta: 1.6159578981732348, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/57470_0.
Episode: 57921/101000 (57.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5892s / 18063.6128 s
agent0:                 episode reward: -0.4619,                 loss: nan
agent1:                 episode reward: 0.4619,                 loss: 0.1461
Episode: 57941/101000 (57.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9389s / 18073.5517 s
agent0:                 episode reward: -0.4186,                 loss: nan
agent1:                 episode reward: 0.4186,                 loss: 0.1432
Episode: 57961/101000 (57.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7907s / 18082.3423 s
agent0:                 episode reward: -0.3235,                 loss: nan
agent1:                 episode reward: 0.3235,                 loss: 0.1443
Episode: 57981/101000 (57.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3646s / 18091.7070 s
agent0:                 episode reward: -0.4617,                 loss: nan
agent1:                 episode reward: 0.4617,                 loss: 0.1456
Episode: 58001/101000 (57.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5346s / 18101.2416 s
agent0:                 episode reward: 0.0641,                 loss: nan
agent1:                 episode reward: -0.0641,                 loss: 0.1426
Episode: 58021/101000 (57.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 11.2447s / 18112.4863 s
agent0:                 episode reward: -0.4415,                 loss: nan
agent1:                 episode reward: 0.4415,                 loss: 0.1431
Episode: 58041/101000 (57.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8311s / 18120.3173 s
agent0:                 episode reward: -0.5847,                 loss: nan
agent1:                 episode reward: 0.5847,                 loss: 0.1436
Score delta: 1.5965080623385581, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/57615_1.
Episode: 58061/101000 (57.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3035s / 18130.6208 s
agent0:                 episode reward: -0.2348,                 loss: 0.1765
agent1:                 episode reward: 0.2348,                 loss: nan
Episode: 58081/101000 (57.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7353s / 18139.3561 s
agent0:                 episode reward: 0.0642,                 loss: 0.1759
agent1:                 episode reward: -0.0642,                 loss: nan
Episode: 58101/101000 (57.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4648s / 18149.8210 s
agent0:                 episode reward: -0.1007,                 loss: 0.1764
agent1:                 episode reward: 0.1007,                 loss: nan
Episode: 58121/101000 (57.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0573s / 18158.8782 s
agent0:                 episode reward: 0.1600,                 loss: 0.1762
agent1:                 episode reward: -0.1600,                 loss: nan
Episode: 58141/101000 (57.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0679s / 18167.9461 s
agent0:                 episode reward: 0.0753,                 loss: 0.1745
agent1:                 episode reward: -0.0753,                 loss: nan
Episode: 58161/101000 (57.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0406s / 18174.9867 s
agent0:                 episode reward: 0.0528,                 loss: 0.1737
agent1:                 episode reward: -0.0528,                 loss: nan
Episode: 58181/101000 (57.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3107s / 18185.2974 s
agent0:                 episode reward: -0.0120,                 loss: 0.1781
agent1:                 episode reward: 0.0120,                 loss: nan
Episode: 58201/101000 (57.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8098s / 18194.1073 s
agent0:                 episode reward: -0.2027,                 loss: 0.1759
agent1:                 episode reward: 0.2027,                 loss: nan
Episode: 58221/101000 (57.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2630s / 18202.3703 s
agent0:                 episode reward: 0.1750,                 loss: 0.1767
agent1:                 episode reward: -0.1750,                 loss: nan
Episode: 58241/101000 (57.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9427s / 18211.3130 s
agent0:                 episode reward: -0.3149,                 loss: 0.1734
agent1:                 episode reward: 0.3149,                 loss: 0.1684
Score delta: 1.5199210836552501, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/57800_0.
Episode: 58261/101000 (57.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7342s / 18220.0471 s
agent0:                 episode reward: -0.4925,                 loss: nan
agent1:                 episode reward: 0.4925,                 loss: 0.1669
Episode: 58281/101000 (57.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2065s / 18230.2536 s
agent0:                 episode reward: -0.6345,                 loss: nan
agent1:                 episode reward: 0.6345,                 loss: 0.1668
Episode: 58301/101000 (57.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6641s / 18238.9177 s
agent0:                 episode reward: -0.6013,                 loss: nan
agent1:                 episode reward: 0.6013,                 loss: 0.1662
Episode: 58321/101000 (57.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 11.8597s / 18250.7774 s
agent0:                 episode reward: -0.3095,                 loss: nan
agent1:                 episode reward: 0.3095,                 loss: 0.1671
Episode: 58341/101000 (57.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9971s / 18259.7745 s
agent0:                 episode reward: -0.6886,                 loss: 0.1797
agent1:                 episode reward: 0.6886,                 loss: 0.1669
Score delta: 2.130862182786586, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/57909_1.
Episode: 58361/101000 (57.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0379s / 18268.8124 s
agent0:                 episode reward: 0.1863,                 loss: 0.1787
agent1:                 episode reward: -0.1863,                 loss: nan
Episode: 58381/101000 (57.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 12.6572s / 18281.4696 s
agent0:                 episode reward: -0.1677,                 loss: 0.1786
agent1:                 episode reward: 0.1677,                 loss: nan
Episode: 58401/101000 (57.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0668s / 18291.5364 s
agent0:                 episode reward: 0.1151,                 loss: 0.1787
agent1:                 episode reward: -0.1151,                 loss: nan
Episode: 58421/101000 (57.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8400s / 18301.3765 s
agent0:                 episode reward: -0.2288,                 loss: 0.1780
agent1:                 episode reward: 0.2288,                 loss: nan
Episode: 58441/101000 (57.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8081s / 18308.1846 s
agent0:                 episode reward: 0.3157,                 loss: 0.1788
agent1:                 episode reward: -0.3157,                 loss: nan
Episode: 58461/101000 (57.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0571s / 18318.2417 s
agent0:                 episode reward: 0.0442,                 loss: 0.1803
agent1:                 episode reward: -0.0442,                 loss: nan
Episode: 58481/101000 (57.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2258s / 18325.4675 s
agent0:                 episode reward: -0.1041,                 loss: 0.1792
agent1:                 episode reward: 0.1041,                 loss: nan
Episode: 58501/101000 (57.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6195s / 18335.0870 s
agent0:                 episode reward: -0.3063,                 loss: 0.1781
agent1:                 episode reward: 0.3063,                 loss: nan
Episode: 58521/101000 (57.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 11.3155s / 18346.4025 s
agent0:                 episode reward: -0.2038,                 loss: 0.1775
agent1:                 episode reward: 0.2038,                 loss: nan
Episode: 58541/101000 (57.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 11.2343s / 18357.6368 s
agent0:                 episode reward: 0.2986,                 loss: 0.1772
agent1:                 episode reward: -0.2986,                 loss: nan
Episode: 58561/101000 (57.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8478s / 18365.4846 s
agent0:                 episode reward: 0.0164,                 loss: 0.1794
agent1:                 episode reward: -0.0164,                 loss: nan
Episode: 58581/101000 (58.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 11.1194s / 18376.6039 s
agent0:                 episode reward: 0.0013,                 loss: 0.1771
agent1:                 episode reward: -0.0013,                 loss: nan
Episode: 58601/101000 (58.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5881s / 18385.1921 s
agent0:                 episode reward: 0.1564,                 loss: 0.1780
agent1:                 episode reward: -0.1564,                 loss: nan
Episode: 58621/101000 (58.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6424s / 18394.8344 s
agent0:                 episode reward: -0.0616,                 loss: 0.1791
agent1:                 episode reward: 0.0616,                 loss: nan
Episode: 58641/101000 (58.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6512s / 18402.4857 s
agent0:                 episode reward: -0.0379,                 loss: 0.1795
agent1:                 episode reward: 0.0379,                 loss: nan
Episode: 58661/101000 (58.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 11.3604s / 18413.8461 s
agent0:                 episode reward: 0.1490,                 loss: 0.1798
agent1:                 episode reward: -0.1490,                 loss: nan
Episode: 58681/101000 (58.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0777s / 18422.9238 s
agent0:                 episode reward: -0.1689,                 loss: 0.1788
agent1:                 episode reward: 0.1689,                 loss: nan
Episode: 58701/101000 (58.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 10.9082s / 18433.8320 s
agent0:                 episode reward: -0.3357,                 loss: 0.1794
agent1:                 episode reward: 0.3357,                 loss: nan
Episode: 58721/101000 (58.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4407s / 18444.2727 s
agent0:                 episode reward: 0.2195,                 loss: 0.1785
agent1:                 episode reward: -0.2195,                 loss: nan
Episode: 58741/101000 (58.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3044s / 18453.5771 s
agent0:                 episode reward: 0.2687,                 loss: 0.1823
agent1:                 episode reward: -0.2687,                 loss: nan
Episode: 58761/101000 (58.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 11.8662s / 18465.4433 s
agent0:                 episode reward: -0.1394,                 loss: 0.1808
agent1:                 episode reward: 0.1394,                 loss: nan
Episode: 58781/101000 (58.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6435s / 18473.0868 s
agent0:                 episode reward: 0.2161,                 loss: 0.1806
agent1:                 episode reward: -0.2161,                 loss: nan
Episode: 58801/101000 (58.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8020s / 18481.8888 s
agent0:                 episode reward: -0.0850,                 loss: 0.1805
agent1:                 episode reward: 0.0850,                 loss: nan
Episode: 58821/101000 (58.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8590s / 18491.7478 s
agent0:                 episode reward: -0.1080,                 loss: 0.1803
agent1:                 episode reward: 0.1080,                 loss: nan
Episode: 58841/101000 (58.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4699s / 18502.2177 s
agent0:                 episode reward: -0.0852,                 loss: 0.1799
agent1:                 episode reward: 0.0852,                 loss: nan
Episode: 58861/101000 (58.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 11.2158s / 18513.4334 s
agent0:                 episode reward: -0.0850,                 loss: 0.1816
agent1:                 episode reward: 0.0850,                 loss: nan
Episode: 58881/101000 (58.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7728s / 18523.2063 s
agent0:                 episode reward: 0.2163,                 loss: 0.1795
agent1:                 episode reward: -0.2163,                 loss: nan
Episode: 58901/101000 (58.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 12.0355s / 18535.2417 s
agent0:                 episode reward: 0.0273,                 loss: 0.1786
agent1:                 episode reward: -0.0273,                 loss: nan
Episode: 58921/101000 (58.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 13.4131s / 18548.6548 s
agent0:                 episode reward: -0.0465,                 loss: 0.1808
agent1:                 episode reward: 0.0465,                 loss: nan
Episode: 58941/101000 (58.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3539s / 18557.0087 s
agent0:                 episode reward: -0.0462,                 loss: 0.1826
agent1:                 episode reward: 0.0462,                 loss: 0.1710
Score delta: 1.618075240906111, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/58504_0.
Episode: 58961/101000 (58.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 11.4979s / 18568.5066 s
agent0:                 episode reward: -0.4079,                 loss: nan
agent1:                 episode reward: 0.4079,                 loss: 0.1679
Episode: 58981/101000 (58.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8378s / 18578.3444 s
agent0:                 episode reward: -0.4344,                 loss: nan
agent1:                 episode reward: 0.4344,                 loss: 0.1646
Episode: 59001/101000 (58.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1065s / 18587.4509 s
agent0:                 episode reward: -0.3934,                 loss: nan
agent1:                 episode reward: 0.3934,                 loss: 0.1524
Episode: 59021/101000 (58.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6182s / 18594.0692 s
agent0:                 episode reward: -0.4428,                 loss: nan
agent1:                 episode reward: 0.4428,                 loss: 0.1536
Episode: 59041/101000 (58.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2627s / 18602.3318 s
agent0:                 episode reward: -0.3139,                 loss: nan
agent1:                 episode reward: 0.3139,                 loss: 0.1538
Episode: 59061/101000 (58.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8198s / 18611.1517 s
agent0:                 episode reward: -0.1346,                 loss: nan
agent1:                 episode reward: 0.1346,                 loss: 0.1536
Episode: 59081/101000 (58.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9419s / 18617.0935 s
agent0:                 episode reward: -0.6621,                 loss: nan
agent1:                 episode reward: 0.6621,                 loss: 0.1529
Score delta: 1.6284553107053976, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/58655_1.
Episode: 59101/101000 (58.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 11.1663s / 18628.2599 s
agent0:                 episode reward: 0.0818,                 loss: 0.2436
agent1:                 episode reward: -0.0818,                 loss: nan
Episode: 59121/101000 (58.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4057s / 18638.6655 s
agent0:                 episode reward: 0.1757,                 loss: 0.2407
agent1:                 episode reward: -0.1757,                 loss: nan
Episode: 59141/101000 (58.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 11.7704s / 18650.4360 s
agent0:                 episode reward: -0.0574,                 loss: 0.2429
agent1:                 episode reward: 0.0574,                 loss: nan
Episode: 59161/101000 (58.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2690s / 18658.7049 s
agent0:                 episode reward: -0.3521,                 loss: 0.2454
agent1:                 episode reward: 0.3521,                 loss: nan
Episode: 59181/101000 (58.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 11.4553s / 18670.1602 s
agent0:                 episode reward: 0.1094,                 loss: 0.2434
agent1:                 episode reward: -0.1094,                 loss: nan
Episode: 59201/101000 (58.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5849s / 18677.7451 s
agent0:                 episode reward: -0.4191,                 loss: 0.2282
agent1:                 episode reward: 0.4191,                 loss: nan
Episode: 59221/101000 (58.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6668s / 18688.4119 s
agent0:                 episode reward: -0.4268,                 loss: 0.1814
agent1:                 episode reward: 0.4268,                 loss: nan
Episode: 59241/101000 (58.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 11.2427s / 18699.6545 s
agent0:                 episode reward: -0.1997,                 loss: 0.1805
agent1:                 episode reward: 0.1997,                 loss: nan
Episode: 59261/101000 (58.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2142s / 18708.8688 s
agent0:                 episode reward: 0.0652,                 loss: 0.1809
agent1:                 episode reward: -0.0652,                 loss: nan
Episode: 59281/101000 (58.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2333s / 18717.1021 s
agent0:                 episode reward: 0.1367,                 loss: 0.1803
agent1:                 episode reward: -0.1367,                 loss: nan
Episode: 59301/101000 (58.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3518s / 18727.4539 s
agent0:                 episode reward: -0.1798,                 loss: 0.1809
agent1:                 episode reward: 0.1798,                 loss: nan
Episode: 59321/101000 (58.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2193s / 18737.6732 s
agent0:                 episode reward: 0.2317,                 loss: 0.1824
agent1:                 episode reward: -0.2317,                 loss: nan
Episode: 59341/101000 (58.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 11.5398s / 18749.2131 s
agent0:                 episode reward: -0.5708,                 loss: 0.1791
agent1:                 episode reward: 0.5708,                 loss: nan
Episode: 59361/101000 (58.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8832s / 18760.0963 s
agent0:                 episode reward: 0.1880,                 loss: 0.1803
agent1:                 episode reward: -0.1880,                 loss: nan
Episode: 59381/101000 (58.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6544s / 18770.7507 s
agent0:                 episode reward: -0.0732,                 loss: 0.1821
agent1:                 episode reward: 0.0732,                 loss: nan
Episode: 59401/101000 (58.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8995s / 18780.6502 s
agent0:                 episode reward: -0.0175,                 loss: 0.1803
agent1:                 episode reward: 0.0175,                 loss: nan
Episode: 59421/101000 (58.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4470s / 18788.0973 s
agent0:                 episode reward: -0.0351,                 loss: 0.1812
agent1:                 episode reward: 0.0351,                 loss: nan
Episode: 59441/101000 (58.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 11.5305s / 18799.6277 s
agent0:                 episode reward: -0.2793,                 loss: 0.1807
agent1:                 episode reward: 0.2793,                 loss: nan
Episode: 59461/101000 (58.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0513s / 18808.6790 s
agent0:                 episode reward: 0.2621,                 loss: 0.1797
agent1:                 episode reward: -0.2621,                 loss: nan
Episode: 59481/101000 (58.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7955s / 18817.4746 s
agent0:                 episode reward: 0.1246,                 loss: 0.1812
agent1:                 episode reward: -0.1246,                 loss: nan
Episode: 59501/101000 (58.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 10.0294s / 18827.5040 s
agent0:                 episode reward: -0.1065,                 loss: 0.1792
agent1:                 episode reward: 0.1065,                 loss: nan
Episode: 59521/101000 (58.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 11.6452s / 18839.1492 s
agent0:                 episode reward: -0.0627,                 loss: 0.1811
agent1:                 episode reward: 0.0627,                 loss: nan
Episode: 59541/101000 (58.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 11.4227s / 18850.5719 s
agent0:                 episode reward: 0.3185,                 loss: 0.1860
agent1:                 episode reward: -0.3185,                 loss: nan
Episode: 59561/101000 (58.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 9.0768s / 18859.6486 s
agent0:                 episode reward: 0.0326,                 loss: 0.1866
agent1:                 episode reward: -0.0326,                 loss: nan
Episode: 59581/101000 (58.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6678s / 18869.3164 s
agent0:                 episode reward: -0.3336,                 loss: 0.1879
agent1:                 episode reward: 0.3336,                 loss: nan
Episode: 59601/101000 (59.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8115s / 18879.1279 s
agent0:                 episode reward: -0.2210,                 loss: 0.1881
agent1:                 episode reward: 0.2210,                 loss: nan
Episode: 59621/101000 (59.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 11.1292s / 18890.2571 s
agent0:                 episode reward: -0.2139,                 loss: 0.1855
agent1:                 episode reward: 0.2139,                 loss: nan
Episode: 59641/101000 (59.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8219s / 18901.0791 s
agent0:                 episode reward: 0.2894,                 loss: 0.1878
agent1:                 episode reward: -0.2894,                 loss: nan
Episode: 59661/101000 (59.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4153s / 18911.4944 s
agent0:                 episode reward: -0.3263,                 loss: 0.1877
agent1:                 episode reward: 0.3263,                 loss: nan
Episode: 59681/101000 (59.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 12.3024s / 18923.7968 s
agent0:                 episode reward: -0.0446,                 loss: 0.1889
agent1:                 episode reward: 0.0446,                 loss: nan
Episode: 59701/101000 (59.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4413s / 18932.2381 s
agent0:                 episode reward: -0.2004,                 loss: 0.1865
agent1:                 episode reward: 0.2004,                 loss: nan
Episode: 59721/101000 (59.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 10.8312s / 18943.0693 s
agent0:                 episode reward: 0.2885,                 loss: 0.1889
agent1:                 episode reward: -0.2885,                 loss: nan
Episode: 59741/101000 (59.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7085s / 18953.7779 s
agent0:                 episode reward: -0.3627,                 loss: 0.1870
agent1:                 episode reward: 0.3627,                 loss: nan
Episode: 59761/101000 (59.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3108s / 18963.0887 s
agent0:                 episode reward: -0.0779,                 loss: 0.1867
agent1:                 episode reward: 0.0779,                 loss: nan
Episode: 59781/101000 (59.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5748s / 18971.6635 s
agent0:                 episode reward: -0.1780,                 loss: 0.1866
agent1:                 episode reward: 0.1780,                 loss: nan
Episode: 59801/101000 (59.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5271s / 18981.1907 s
agent0:                 episode reward: -0.1555,                 loss: 0.1874
agent1:                 episode reward: 0.1555,                 loss: nan
Episode: 59821/101000 (59.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9807s / 18990.1714 s
agent0:                 episode reward: -0.0021,                 loss: 0.1875
agent1:                 episode reward: 0.0021,                 loss: 0.1458
Score delta: 1.8427639702435599, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/59381_0.
Episode: 59841/101000 (59.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6297s / 18998.8012 s
agent0:                 episode reward: -0.4433,                 loss: nan
agent1:                 episode reward: 0.4433,                 loss: 0.1458
Episode: 59861/101000 (59.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6676s / 19007.4688 s
agent0:                 episode reward: -0.0506,                 loss: nan
agent1:                 episode reward: 0.0506,                 loss: 0.1431
Episode: 59881/101000 (59.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5576s / 19016.0264 s
agent0:                 episode reward: -0.3193,                 loss: nan
agent1:                 episode reward: 0.3193,                 loss: 0.1432
Episode: 59901/101000 (59.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3519s / 19024.3783 s
agent0:                 episode reward: -0.3972,                 loss: nan
agent1:                 episode reward: 0.3972,                 loss: 0.1418
Episode: 59921/101000 (59.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 11.9706s / 19036.3488 s
agent0:                 episode reward: -0.1925,                 loss: nan
agent1:                 episode reward: 0.1925,                 loss: 0.1423
Episode: 59941/101000 (59.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3467s / 19044.6955 s
agent0:                 episode reward: -0.3440,                 loss: 0.1884
agent1:                 episode reward: 0.3440,                 loss: 0.1417
Score delta: 1.6396334805960702, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/59508_1.
Episode: 59961/101000 (59.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2258s / 19053.9213 s
agent0:                 episode reward: 0.3246,                 loss: 0.1903
agent1:                 episode reward: -0.3246,                 loss: nan
Episode: 59981/101000 (59.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7889s / 19063.7102 s
agent0:                 episode reward: -0.1329,                 loss: 0.1902
agent1:                 episode reward: 0.1329,                 loss: nan
Episode: 60001/101000 (59.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3610s / 19074.0712 s
agent0:                 episode reward: 0.1885,                 loss: 0.1845
agent1:                 episode reward: -0.1885,                 loss: nan
Episode: 60021/101000 (59.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6401s / 19081.7112 s
agent0:                 episode reward: -0.0900,                 loss: 0.1814
agent1:                 episode reward: 0.0900,                 loss: nan
Episode: 60041/101000 (59.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6733s / 19091.3845 s
agent0:                 episode reward: -0.1366,                 loss: 0.1799
agent1:                 episode reward: 0.1366,                 loss: nan
Episode: 60061/101000 (59.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5059s / 19100.8904 s
agent0:                 episode reward: -0.4324,                 loss: 0.1814
agent1:                 episode reward: 0.4324,                 loss: nan
Episode: 60081/101000 (59.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2725s / 19111.1629 s
agent0:                 episode reward: 0.0429,                 loss: 0.1782
agent1:                 episode reward: -0.0429,                 loss: nan
Episode: 60101/101000 (59.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8489s / 19121.0118 s
agent0:                 episode reward: -0.5232,                 loss: 0.1800
agent1:                 episode reward: 0.5232,                 loss: nan
Episode: 60121/101000 (59.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 9.3793s / 19130.3912 s
agent0:                 episode reward: 0.1766,                 loss: 0.1805
agent1:                 episode reward: -0.1766,                 loss: nan
Episode: 60141/101000 (59.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2720s / 19136.6632 s
agent0:                 episode reward: 0.0171,                 loss: 0.1817
agent1:                 episode reward: -0.0171,                 loss: 0.1708
Score delta: 1.593155442108621, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/59711_0.
Episode: 60161/101000 (59.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 9.9357s / 19146.5989 s
agent0:                 episode reward: -0.4122,                 loss: nan
agent1:                 episode reward: 0.4122,                 loss: 0.1671
Episode: 60181/101000 (59.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7956s / 19155.3944 s
agent0:                 episode reward: -0.2378,                 loss: nan
agent1:                 episode reward: 0.2378,                 loss: 0.1683
Episode: 60201/101000 (59.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1195s / 19164.5139 s
agent0:                 episode reward: -0.0437,                 loss: nan
agent1:                 episode reward: 0.0437,                 loss: 0.1668
Episode: 60221/101000 (59.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7512s / 19174.2652 s
agent0:                 episode reward: -0.4452,                 loss: nan
agent1:                 episode reward: 0.4452,                 loss: 0.1662
Episode: 60241/101000 (59.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9590s / 19181.2242 s
agent0:                 episode reward: -0.5293,                 loss: nan
agent1:                 episode reward: 0.5293,                 loss: 0.1656
Episode: 60261/101000 (59.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 10.4758s / 19191.7000 s
agent0:                 episode reward: -0.0158,                 loss: nan
agent1:                 episode reward: 0.0158,                 loss: 0.1629
Episode: 60281/101000 (59.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4131s / 19199.1131 s
agent0:                 episode reward: -0.4018,                 loss: nan
agent1:                 episode reward: 0.4018,                 loss: 0.1601
Episode: 60301/101000 (59.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6226s / 19209.7357 s
agent0:                 episode reward: -0.2709,                 loss: nan
agent1:                 episode reward: 0.2709,                 loss: 0.1611
Episode: 60321/101000 (59.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 9.7875s / 19219.5232 s
agent0:                 episode reward: -0.4018,                 loss: nan
agent1:                 episode reward: 0.4018,                 loss: 0.1604
Episode: 60341/101000 (59.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4227s / 19226.9459 s
agent0:                 episode reward: -0.4573,                 loss: 0.1776
agent1:                 episode reward: 0.4573,                 loss: 0.1588
Score delta: 1.856970837780396, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/59911_1.
Episode: 60361/101000 (59.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 12.3462s / 19239.2922 s
agent0:                 episode reward: 0.0603,                 loss: 0.1706
agent1:                 episode reward: -0.0603,                 loss: nan
Episode: 60381/101000 (59.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 10.7160s / 19250.0082 s
agent0:                 episode reward: -0.4451,                 loss: 0.1730
agent1:                 episode reward: 0.4451,                 loss: nan
Episode: 60401/101000 (59.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7828s / 19258.7910 s
agent0:                 episode reward: -0.5938,                 loss: 0.1726
agent1:                 episode reward: 0.5938,                 loss: nan
Episode: 60421/101000 (59.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2192s / 19269.0101 s
agent0:                 episode reward: -0.4768,                 loss: 0.1716
agent1:                 episode reward: 0.4768,                 loss: nan
Episode: 60441/101000 (59.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6466s / 19279.6567 s
agent0:                 episode reward: 0.1245,                 loss: 0.1707
agent1:                 episode reward: -0.1245,                 loss: nan
Episode: 60461/101000 (59.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6739s / 19288.3306 s
agent0:                 episode reward: 0.1400,                 loss: 0.1710
agent1:                 episode reward: -0.1400,                 loss: nan
Episode: 60481/101000 (59.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6194s / 19297.9500 s
agent0:                 episode reward: -0.0528,                 loss: 0.1718
agent1:                 episode reward: 0.0528,                 loss: nan
Episode: 60501/101000 (59.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 12.1256s / 19310.0756 s
agent0:                 episode reward: 0.2049,                 loss: 0.1711
agent1:                 episode reward: -0.2049,                 loss: nan
Episode: 60521/101000 (59.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7821s / 19318.8577 s
agent0:                 episode reward: -0.1174,                 loss: 0.1721
agent1:                 episode reward: 0.1174,                 loss: nan
Episode: 60541/101000 (59.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 9.8908s / 19328.7485 s
agent0:                 episode reward: -0.3796,                 loss: 0.1860
agent1:                 episode reward: 0.3796,                 loss: nan
Episode: 60561/101000 (59.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 12.6343s / 19341.3829 s
agent0:                 episode reward: 0.1639,                 loss: 0.1859
agent1:                 episode reward: -0.1639,                 loss: 0.1440
Score delta: 1.7254674149837743, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/60127_0.
Episode: 60581/101000 (59.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5993s / 19350.9822 s
agent0:                 episode reward: -0.5923,                 loss: nan
agent1:                 episode reward: 0.5923,                 loss: 0.1464
Episode: 60601/101000 (60.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1900s / 19360.1722 s
agent0:                 episode reward: -0.3174,                 loss: nan
agent1:                 episode reward: 0.3174,                 loss: 0.1430
Episode: 60621/101000 (60.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 11.3790s / 19371.5512 s
agent0:                 episode reward: -0.7156,                 loss: nan
agent1:                 episode reward: 0.7156,                 loss: 0.1456
Episode: 60641/101000 (60.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1001s / 19379.6514 s
agent0:                 episode reward: -0.4400,                 loss: nan
agent1:                 episode reward: 0.4400,                 loss: 0.1445
Episode: 60661/101000 (60.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2502s / 19389.9016 s
agent0:                 episode reward: -0.8212,                 loss: nan
agent1:                 episode reward: 0.8212,                 loss: 0.1445
Episode: 60681/101000 (60.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9727s / 19397.8743 s
agent0:                 episode reward: -0.0754,                 loss: 0.1825
agent1:                 episode reward: 0.0754,                 loss: 0.1443
Score delta: 1.8294927331826982, save the model to .//data/model/20220117155134/mdp_arbitrary_mdp_fictitious_selfplay2/60240_1.
Episode: 60701/101000 (60.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 10.2566s / 19408.1309 s
agent0:                 episode reward: -0.0168,                 loss: 0.1807
agent1:                 episode reward: 0.0168,                 loss: nan
Episode: 60721/101000 (60.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5011s / 19418.6320 s
agent0:                 episode reward: 0.0336,                 loss: 0.1821
agent1:                 episode reward: -0.0336,                 loss: nan
Episode: 60741/101000 (60.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 10.3680s / 19429.0000 s
agent0:                 episode reward: 0.1359,                 loss: 0.1821
agent1:                 episode reward: -0.1359,                 loss: nan
Episode: 60761/101000 (60.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 10.5268s / 19439.5268 s
agent0:                 episode reward: 0.3382,                 loss: 0.1815
agent1:                 episode reward: -0.3382,                 loss: nan
Episode: 60781/101000 (60.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 10.6950s / 19450.2218 s
agent0:                 episode reward: 0.0640,                 loss: 0.1816
agent1:                 episode reward: -0.0640,                 loss: nan
Episode: 60801/101000 (60.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3641s / 19458.5859 s
agent0:                 episode reward: -0.0455,                 loss: 0.1802
agent1:                 episode reward: 0.0455,                 loss: nan
Episode: 60821/101000 (60.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 9.6845s / 19468.2704 s
agent0:                 episode reward: -0.2357,                 loss: 0.1785
agent1:                 episode reward: 0.2357,                 loss: nan
Episode: 60841/101000 (60.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4439s / 19477.7143 s
agent0:                 episode reward: 0.1368,                 loss: 0.1813
agent1:                 episode reward: -0.1368,                 loss: nan
Episode: 60861/101000 (60.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 9.5324s / 19487.2467 s