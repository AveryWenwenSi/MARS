pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7fc301a02c50>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 640, 'max_episodes': 101000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2.
Episode: 1/101000 (0.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1233s / 4.1233 s
agent0:                 episode reward: -1.4915,                 loss: nan
agent1:                 episode reward: 1.4915,                 loss: nan
Episode: 21/101000 (0.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1733s / 4.2966 s
agent0:                 episode reward: -0.0568,                 loss: nan
agent1:                 episode reward: 0.0568,                 loss: nan
Episode: 41/101000 (0.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1622s / 4.4588 s
agent0:                 episode reward: -0.0105,                 loss: nan
agent1:                 episode reward: 0.0105,                 loss: nan
Episode: 61/101000 (0.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1634s / 4.6222 s
agent0:                 episode reward: 0.1556,                 loss: nan
agent1:                 episode reward: -0.1556,                 loss: nan
Episode: 81/101000 (0.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2062s / 4.8284 s
agent0:                 episode reward: 0.4400,                 loss: nan
agent1:                 episode reward: -0.4400,                 loss: nan
Episode: 101/101000 (0.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1678s / 4.9962 s
agent0:                 episode reward: -0.1269,                 loss: nan
agent1:                 episode reward: 0.1269,                 loss: nan
Episode: 121/101000 (0.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1181s / 5.1144 s
agent0:                 episode reward: 0.2821,                 loss: nan
agent1:                 episode reward: -0.2821,                 loss: nan
Episode: 141/101000 (0.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1952s / 5.3095 s
agent0:                 episode reward: 0.0748,                 loss: nan
agent1:                 episode reward: -0.0748,                 loss: nan
Episode: 161/101000 (0.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2788s / 5.5883 s
agent0:                 episode reward: -0.2012,                 loss: nan
agent1:                 episode reward: 0.2012,                 loss: nan
Episode: 181/101000 (0.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1455s / 5.7338 s
agent0:                 episode reward: -0.2092,                 loss: nan
agent1:                 episode reward: 0.2092,                 loss: nan
Episode: 201/101000 (0.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1814s / 5.9151 s
agent0:                 episode reward: 0.1663,                 loss: nan
agent1:                 episode reward: -0.1663,                 loss: nan
Episode: 221/101000 (0.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 0.5954s / 6.5106 s
agent0:                 episode reward: 0.2364,                 loss: 0.1716
agent1:                 episode reward: -0.2364,                 loss: nan
Episode: 241/101000 (0.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 0.9857s / 7.4962 s
agent0:                 episode reward: 0.1508,                 loss: 0.1537
agent1:                 episode reward: -0.1508,                 loss: nan
Episode: 261/101000 (0.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1998s / 8.6961 s
agent0:                 episode reward: -0.0637,                 loss: 0.1495
agent1:                 episode reward: 0.0637,                 loss: nan
Episode: 281/101000 (0.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5325s / 10.2286 s
agent0:                 episode reward: -0.0624,                 loss: 0.1426
agent1:                 episode reward: 0.0624,                 loss: nan
Episode: 301/101000 (0.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4310s / 11.6596 s
agent0:                 episode reward: -0.0744,                 loss: 0.1354
agent1:                 episode reward: 0.0744,                 loss: nan
Episode: 321/101000 (0.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0268s / 12.6864 s
agent0:                 episode reward: -0.0961,                 loss: 0.1273
agent1:                 episode reward: 0.0961,                 loss: nan
Episode: 341/101000 (0.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4729s / 14.1594 s
agent0:                 episode reward: 0.1259,                 loss: 0.1204
agent1:                 episode reward: -0.1259,                 loss: nan
Episode: 361/101000 (0.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0281s / 15.1875 s
agent0:                 episode reward: 0.3883,                 loss: 0.1147
agent1:                 episode reward: -0.3883,                 loss: nan
Episode: 381/101000 (0.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0437s / 16.2312 s
agent0:                 episode reward: 0.1163,                 loss: 0.1111
agent1:                 episode reward: -0.1163,                 loss: nan
Episode: 401/101000 (0.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3142s / 17.5454 s
agent0:                 episode reward: -0.3025,                 loss: 0.1098
agent1:                 episode reward: 0.3025,                 loss: nan
Episode: 421/101000 (0.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4940s / 19.0394 s
agent0:                 episode reward: 0.4501,                 loss: 0.1086
agent1:                 episode reward: -0.4501,                 loss: nan
Episode: 441/101000 (0.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2164s / 20.2558 s
agent0:                 episode reward: -0.3092,                 loss: 0.1068
agent1:                 episode reward: 0.3092,                 loss: nan
Episode: 461/101000 (0.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 0.9728s / 21.2285 s
agent0:                 episode reward: 0.1355,                 loss: 0.1054
agent1:                 episode reward: -0.1355,                 loss: nan
Episode: 481/101000 (0.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4271s / 22.6556 s
agent0:                 episode reward: 0.2141,                 loss: 0.1050
agent1:                 episode reward: -0.2141,                 loss: nan
Episode: 501/101000 (0.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5371s / 24.1926 s
agent0:                 episode reward: -0.0066,                 loss: 0.1038
agent1:                 episode reward: 0.0066,                 loss: nan
Episode: 521/101000 (0.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5688s / 25.7614 s
agent0:                 episode reward: 0.1200,                 loss: 0.1034
agent1:                 episode reward: -0.1200,                 loss: nan
Episode: 541/101000 (0.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1815s / 26.9429 s
agent0:                 episode reward: 0.3863,                 loss: 0.1033
agent1:                 episode reward: -0.3863,                 loss: nan
Score delta: 1.589273134581242, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/325_0.
Episode: 561/101000 (0.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1400s / 27.0829 s
agent0:                 episode reward: 0.1482,                 loss: nan
agent1:                 episode reward: -0.1482,                 loss: nan
Episode: 581/101000 (0.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3294s / 27.4123 s
agent0:                 episode reward: 0.0128,                 loss: nan
agent1:                 episode reward: -0.0128,                 loss: nan
Episode: 601/101000 (0.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 0.3127s / 27.7250 s
agent0:                 episode reward: -0.0424,                 loss: nan
agent1:                 episode reward: 0.0424,                 loss: nan
Episode: 621/101000 (0.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2729s / 27.9979 s
agent0:                 episode reward: 0.1629,                 loss: nan
agent1:                 episode reward: -0.1629,                 loss: nan
Episode: 641/101000 (0.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2536s / 28.2515 s
agent0:                 episode reward: 0.3353,                 loss: nan
agent1:                 episode reward: -0.3353,                 loss: nan
Episode: 661/101000 (0.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2602s / 28.5117 s
agent0:                 episode reward: 0.2761,                 loss: nan
agent1:                 episode reward: -0.2761,                 loss: nan
Episode: 681/101000 (0.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2549s / 28.7666 s
agent0:                 episode reward: -0.1219,                 loss: nan
agent1:                 episode reward: 0.1219,                 loss: nan
Episode: 701/101000 (0.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1503s / 28.9170 s
agent0:                 episode reward: 0.0463,                 loss: nan
agent1:                 episode reward: -0.0463,                 loss: nan
Episode: 721/101000 (0.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1442s / 29.0611 s
agent0:                 episode reward: 0.2847,                 loss: nan
agent1:                 episode reward: -0.2847,                 loss: nan
Episode: 741/101000 (0.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1477s / 29.2088 s
agent0:                 episode reward: 0.1221,                 loss: nan
agent1:                 episode reward: -0.1221,                 loss: nan
Episode: 761/101000 (0.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0999s / 30.3087 s
agent0:                 episode reward: 0.2249,                 loss: nan
agent1:                 episode reward: -0.2249,                 loss: 0.2106
Episode: 781/101000 (0.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1856s / 31.4943 s
agent0:                 episode reward: 0.0766,                 loss: nan
agent1:                 episode reward: -0.0766,                 loss: 0.1671
Episode: 801/101000 (0.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7592s / 33.2536 s
agent0:                 episode reward: -0.0888,                 loss: nan
agent1:                 episode reward: 0.0888,                 loss: 0.1360
Episode: 821/101000 (0.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8696s / 35.1232 s
agent0:                 episode reward: -0.1415,                 loss: nan
agent1:                 episode reward: 0.1415,                 loss: 0.1197
Episode: 841/101000 (0.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0994s / 36.2226 s
agent0:                 episode reward: -0.8400,                 loss: 0.1183
agent1:                 episode reward: 0.8400,                 loss: 0.1155
Score delta: 1.590399182408613, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/396_1.
Episode: 861/101000 (0.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0488s / 38.2714 s
agent0:                 episode reward: -0.7082,                 loss: 0.1221
agent1:                 episode reward: 0.7082,                 loss: nan
Episode: 881/101000 (0.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8963s / 40.1677 s
agent0:                 episode reward: -1.0323,                 loss: 0.1255
agent1:                 episode reward: 1.0323,                 loss: nan
Episode: 901/101000 (0.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5879s / 41.7556 s
agent0:                 episode reward: -1.1299,                 loss: 0.1292
agent1:                 episode reward: 1.1299,                 loss: nan
Episode: 921/101000 (0.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9698s / 43.7254 s
agent0:                 episode reward: -0.7084,                 loss: 0.1326
agent1:                 episode reward: 0.7084,                 loss: nan
Episode: 941/101000 (0.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4907s / 45.2161 s
agent0:                 episode reward: -0.5330,                 loss: 0.1360
agent1:                 episode reward: 0.5330,                 loss: nan
Episode: 961/101000 (0.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3404s / 46.5566 s
agent0:                 episode reward: -0.8308,                 loss: 0.1380
agent1:                 episode reward: 0.8308,                 loss: nan
Episode: 981/101000 (0.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4808s / 48.0373 s
agent0:                 episode reward: -0.4170,                 loss: 0.1392
agent1:                 episode reward: 0.4170,                 loss: nan
Episode: 1001/101000 (0.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9995s / 50.0368 s
agent0:                 episode reward: -1.0426,                 loss: 0.1400
agent1:                 episode reward: 1.0426,                 loss: nan
Episode: 1021/101000 (1.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7183s / 51.7552 s
agent0:                 episode reward: -1.2264,                 loss: 0.1429
agent1:                 episode reward: 1.2264,                 loss: nan
Episode: 1041/101000 (1.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1706s / 53.9257 s
agent0:                 episode reward: -0.8179,                 loss: 0.1450
agent1:                 episode reward: 0.8179,                 loss: nan
Episode: 1061/101000 (1.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9505s / 55.8762 s
agent0:                 episode reward: -0.3500,                 loss: 0.1439
agent1:                 episode reward: 0.3500,                 loss: nan
Episode: 1081/101000 (1.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2489s / 57.1251 s
agent0:                 episode reward: -0.7579,                 loss: 0.1441
agent1:                 episode reward: 0.7579,                 loss: nan
Episode: 1101/101000 (1.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9431s / 59.0682 s
agent0:                 episode reward: -0.6165,                 loss: 0.1470
agent1:                 episode reward: 0.6165,                 loss: nan
Episode: 1121/101000 (1.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4479s / 60.5161 s
agent0:                 episode reward: -0.3150,                 loss: 0.1468
agent1:                 episode reward: 0.3150,                 loss: nan
Episode: 1141/101000 (1.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6767s / 62.1928 s
agent0:                 episode reward: -0.6604,                 loss: 0.1470
agent1:                 episode reward: 0.6604,                 loss: nan
Episode: 1161/101000 (1.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4973s / 63.6901 s
agent0:                 episode reward: -0.9437,                 loss: 0.1477
agent1:                 episode reward: 0.9437,                 loss: nan
Episode: 1181/101000 (1.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3492s / 65.0393 s
agent0:                 episode reward: -0.6456,                 loss: 0.1547
agent1:                 episode reward: 0.6456,                 loss: nan
Episode: 1201/101000 (1.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4764s / 66.5157 s
agent0:                 episode reward: -1.1364,                 loss: 0.1525
agent1:                 episode reward: 1.1364,                 loss: nan
Episode: 1221/101000 (1.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5794s / 68.0951 s
agent0:                 episode reward: -0.4452,                 loss: 0.1533
agent1:                 episode reward: 0.4452,                 loss: nan
Episode: 1241/101000 (1.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5472s / 69.6423 s
agent0:                 episode reward: -0.9996,                 loss: 0.1527
agent1:                 episode reward: 0.9996,                 loss: nan
Episode: 1261/101000 (1.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4168s / 71.0591 s
agent0:                 episode reward: -0.4261,                 loss: 0.1515
agent1:                 episode reward: 0.4261,                 loss: nan
Episode: 1281/101000 (1.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4150s / 72.4741 s
agent0:                 episode reward: -0.8164,                 loss: 0.1523
agent1:                 episode reward: 0.8164,                 loss: nan
Episode: 1301/101000 (1.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4030s / 73.8771 s
agent0:                 episode reward: -0.5989,                 loss: 0.1526
agent1:                 episode reward: 0.5989,                 loss: nan
Episode: 1321/101000 (1.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4845s / 75.3616 s
agent0:                 episode reward: -0.7421,                 loss: 0.1528
agent1:                 episode reward: 0.7421,                 loss: nan
Episode: 1341/101000 (1.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5744s / 76.9360 s
agent0:                 episode reward: -0.8438,                 loss: 0.1526
agent1:                 episode reward: 0.8438,                 loss: nan
Episode: 1361/101000 (1.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5602s / 78.4962 s
agent0:                 episode reward: -0.5951,                 loss: 0.1529
agent1:                 episode reward: 0.5951,                 loss: nan
Episode: 1381/101000 (1.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4239s / 79.9201 s
agent0:                 episode reward: -0.8422,                 loss: 0.1535
agent1:                 episode reward: 0.8422,                 loss: nan
Episode: 1401/101000 (1.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2871s / 81.2073 s
agent0:                 episode reward: -0.5289,                 loss: 0.1538
agent1:                 episode reward: 0.5289,                 loss: nan
Episode: 1421/101000 (1.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6278s / 82.8350 s
agent0:                 episode reward: -0.5092,                 loss: 0.1529
agent1:                 episode reward: 0.5092,                 loss: nan
Episode: 1441/101000 (1.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7521s / 84.5871 s
agent0:                 episode reward: -0.8415,                 loss: 0.1544
agent1:                 episode reward: 0.8415,                 loss: nan
Episode: 1461/101000 (1.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3888s / 85.9759 s
agent0:                 episode reward: -0.7213,                 loss: 0.1514
agent1:                 episode reward: 0.7213,                 loss: nan
Episode: 1481/101000 (1.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8833s / 87.8592 s
agent0:                 episode reward: -0.7329,                 loss: 0.1521
agent1:                 episode reward: 0.7329,                 loss: nan
Episode: 1501/101000 (1.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3013s / 89.1605 s
agent0:                 episode reward: -0.5513,                 loss: 0.1555
agent1:                 episode reward: 0.5513,                 loss: nan
Episode: 1521/101000 (1.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4150s / 90.5754 s
agent0:                 episode reward: -0.6329,                 loss: 0.1511
agent1:                 episode reward: 0.6329,                 loss: nan
Episode: 1541/101000 (1.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7951s / 92.3705 s
agent0:                 episode reward: -0.3713,                 loss: 0.1505
agent1:                 episode reward: 0.3713,                 loss: nan
Episode: 1561/101000 (1.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0616s / 93.4321 s
agent0:                 episode reward: -0.8282,                 loss: 0.1507
agent1:                 episode reward: 0.8282,                 loss: nan
Episode: 1581/101000 (1.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4073s / 94.8394 s
agent0:                 episode reward: -0.6243,                 loss: 0.1498
agent1:                 episode reward: 0.6243,                 loss: nan
Episode: 1601/101000 (1.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6727s / 96.5122 s
agent0:                 episode reward: -0.6238,                 loss: 0.1511
agent1:                 episode reward: 0.6238,                 loss: nan
Episode: 1621/101000 (1.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7147s / 98.2268 s
agent0:                 episode reward: -0.8741,                 loss: 0.1496
agent1:                 episode reward: 0.8741,                 loss: nan
Episode: 1641/101000 (1.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6423s / 99.8691 s
agent0:                 episode reward: -0.5487,                 loss: 0.1500
agent1:                 episode reward: 0.5487,                 loss: nan
Episode: 1661/101000 (1.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5797s / 101.4488 s
agent0:                 episode reward: -0.7270,                 loss: 0.1506
agent1:                 episode reward: 0.7270,                 loss: nan
Episode: 1681/101000 (1.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6631s / 103.1120 s
agent0:                 episode reward: -0.5807,                 loss: 0.1477
agent1:                 episode reward: 0.5807,                 loss: nan
Episode: 1701/101000 (1.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4884s / 104.6004 s
agent0:                 episode reward: -0.0484,                 loss: 0.1484
agent1:                 episode reward: 0.0484,                 loss: nan
Episode: 1721/101000 (1.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8861s / 106.4865 s
agent0:                 episode reward: -0.6267,                 loss: 0.1501
agent1:                 episode reward: 0.6267,                 loss: nan
Episode: 1741/101000 (1.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3721s / 107.8585 s
agent0:                 episode reward: -0.9883,                 loss: 0.1486
agent1:                 episode reward: 0.9883,                 loss: nan
Episode: 1761/101000 (1.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0776s / 108.9362 s
agent0:                 episode reward: -0.2072,                 loss: 0.1487
agent1:                 episode reward: 0.2072,                 loss: nan
Episode: 1781/101000 (1.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1257s / 110.0619 s
agent0:                 episode reward: -0.4667,                 loss: 0.1476
agent1:                 episode reward: 0.4667,                 loss: nan
Episode: 1801/101000 (1.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2977s / 111.3596 s
agent0:                 episode reward: -0.4061,                 loss: 0.1477
agent1:                 episode reward: 0.4061,                 loss: nan
Episode: 1821/101000 (1.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6924s / 113.0520 s
agent0:                 episode reward: -0.6237,                 loss: 0.1472
agent1:                 episode reward: 0.6237,                 loss: nan
Episode: 1841/101000 (1.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4973s / 114.5493 s
agent0:                 episode reward: -0.6689,                 loss: 0.1485
agent1:                 episode reward: 0.6689,                 loss: nan
Episode: 1861/101000 (1.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6999s / 116.2493 s
agent0:                 episode reward: -0.3916,                 loss: 0.1480
agent1:                 episode reward: 0.3916,                 loss: nan
Episode: 1881/101000 (1.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5849s / 117.8342 s
agent0:                 episode reward: -0.7492,                 loss: 0.1479
agent1:                 episode reward: 0.7492,                 loss: nan
Episode: 1901/101000 (1.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2244s / 119.0586 s
agent0:                 episode reward: -0.7921,                 loss: 0.1478
agent1:                 episode reward: 0.7921,                 loss: nan
Episode: 1921/101000 (1.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6917s / 120.7503 s
agent0:                 episode reward: -0.4168,                 loss: 0.1478
agent1:                 episode reward: 0.4168,                 loss: nan
Episode: 1941/101000 (1.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5754s / 122.3257 s
agent0:                 episode reward: -0.7111,                 loss: 0.1485
agent1:                 episode reward: 0.7111,                 loss: nan
Episode: 1961/101000 (1.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5419s / 123.8676 s
agent0:                 episode reward: -0.5434,                 loss: 0.1477
agent1:                 episode reward: 0.5434,                 loss: nan
Episode: 1981/101000 (1.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7580s / 125.6257 s
agent0:                 episode reward: -0.4374,                 loss: 0.1471
agent1:                 episode reward: 0.4374,                 loss: nan
Episode: 2001/101000 (1.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5345s / 127.1602 s
agent0:                 episode reward: -0.3626,                 loss: 0.1473
agent1:                 episode reward: 0.3626,                 loss: nan
Episode: 2021/101000 (2.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3823s / 128.5425 s
agent0:                 episode reward: -0.6310,                 loss: 0.1456
agent1:                 episode reward: 0.6310,                 loss: nan
Episode: 2041/101000 (2.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3007s / 129.8432 s
agent0:                 episode reward: -0.6491,                 loss: 0.1468
agent1:                 episode reward: 0.6491,                 loss: nan
Episode: 2061/101000 (2.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1731s / 131.0163 s
agent0:                 episode reward: -0.8266,                 loss: 0.1474
agent1:                 episode reward: 0.8266,                 loss: nan
Episode: 2081/101000 (2.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5540s / 132.5703 s
agent0:                 episode reward: -0.2926,                 loss: 0.1469
agent1:                 episode reward: 0.2926,                 loss: nan
Episode: 2101/101000 (2.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3696s / 133.9399 s
agent0:                 episode reward: -0.3708,                 loss: 0.1447
agent1:                 episode reward: 0.3708,                 loss: nan
Episode: 2121/101000 (2.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3856s / 135.3255 s
agent0:                 episode reward: -0.4339,                 loss: 0.1459
agent1:                 episode reward: 0.4339,                 loss: nan
Episode: 2141/101000 (2.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7794s / 137.1049 s
agent0:                 episode reward: -0.1834,                 loss: 0.1454
agent1:                 episode reward: 0.1834,                 loss: nan
Episode: 2161/101000 (2.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7943s / 138.8992 s
agent0:                 episode reward: 0.0505,                 loss: 0.1444
agent1:                 episode reward: -0.0505,                 loss: nan
Episode: 2181/101000 (2.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2847s / 140.1839 s
agent0:                 episode reward: -0.8755,                 loss: 0.1502
agent1:                 episode reward: 0.8755,                 loss: nan
Episode: 2201/101000 (2.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5978s / 141.7817 s
agent0:                 episode reward: -0.4816,                 loss: 0.1494
agent1:                 episode reward: 0.4816,                 loss: nan
Episode: 2221/101000 (2.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5485s / 143.3302 s
agent0:                 episode reward: -0.6486,                 loss: 0.1504
agent1:                 episode reward: 0.6486,                 loss: nan
Episode: 2241/101000 (2.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7793s / 145.1095 s
agent0:                 episode reward: -0.0547,                 loss: 0.1489
agent1:                 episode reward: 0.0547,                 loss: nan
Episode: 2261/101000 (2.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5319s / 146.6414 s
agent0:                 episode reward: -0.3391,                 loss: 0.1491
agent1:                 episode reward: 0.3391,                 loss: nan
Episode: 2281/101000 (2.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0176s / 148.6590 s
agent0:                 episode reward: -0.6571,                 loss: 0.1503
agent1:                 episode reward: 0.6571,                 loss: nan
Episode: 2301/101000 (2.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6416s / 150.3006 s
agent0:                 episode reward: -0.6539,                 loss: 0.1469
agent1:                 episode reward: 0.6539,                 loss: nan
Episode: 2321/101000 (2.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6508s / 151.9514 s
agent0:                 episode reward: -0.6952,                 loss: 0.1480
agent1:                 episode reward: 0.6952,                 loss: nan
Episode: 2341/101000 (2.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2854s / 153.2368 s
agent0:                 episode reward: -0.5097,                 loss: 0.1499
agent1:                 episode reward: 0.5097,                 loss: nan
Episode: 2361/101000 (2.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5861s / 154.8228 s
agent0:                 episode reward: -0.8711,                 loss: 0.1481
agent1:                 episode reward: 0.8711,                 loss: nan
Episode: 2381/101000 (2.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7139s / 156.5367 s
agent0:                 episode reward: -0.5301,                 loss: 0.1467
agent1:                 episode reward: 0.5301,                 loss: nan
Episode: 2401/101000 (2.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4999s / 158.0367 s
agent0:                 episode reward: -0.6777,                 loss: 0.1483
agent1:                 episode reward: 0.6777,                 loss: nan
Episode: 2421/101000 (2.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7180s / 159.7546 s
agent0:                 episode reward: -0.4708,                 loss: 0.1480
agent1:                 episode reward: 0.4708,                 loss: nan
Episode: 2441/101000 (2.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0245s / 161.7792 s
agent0:                 episode reward: -0.3375,                 loss: 0.1480
agent1:                 episode reward: 0.3375,                 loss: nan
Episode: 2461/101000 (2.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7239s / 163.5031 s
agent0:                 episode reward: -0.5051,                 loss: 0.1464
agent1:                 episode reward: 0.5051,                 loss: nan
Episode: 2481/101000 (2.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7474s / 165.2505 s
agent0:                 episode reward: -0.8891,                 loss: 0.1467
agent1:                 episode reward: 0.8891,                 loss: nan
Episode: 2501/101000 (2.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8284s / 167.0790 s
agent0:                 episode reward: -0.6667,                 loss: 0.1490
agent1:                 episode reward: 0.6667,                 loss: nan
Episode: 2521/101000 (2.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7634s / 168.8423 s
agent0:                 episode reward: -0.5953,                 loss: 0.1511
agent1:                 episode reward: 0.5953,                 loss: nan
Episode: 2541/101000 (2.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5793s / 170.4216 s
agent0:                 episode reward: -0.8041,                 loss: 0.1523
agent1:                 episode reward: 0.8041,                 loss: nan
Episode: 2561/101000 (2.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7612s / 172.1828 s
agent0:                 episode reward: -0.4626,                 loss: 0.1506
agent1:                 episode reward: 0.4626,                 loss: nan
Episode: 2581/101000 (2.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2046s / 173.3873 s
agent0:                 episode reward: -0.7297,                 loss: 0.1514
agent1:                 episode reward: 0.7297,                 loss: nan
Episode: 2601/101000 (2.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4244s / 174.8118 s
agent0:                 episode reward: -0.6543,                 loss: 0.1509
agent1:                 episode reward: 0.6543,                 loss: nan
Episode: 2621/101000 (2.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2359s / 176.0476 s
agent0:                 episode reward: -0.3763,                 loss: 0.1490
agent1:                 episode reward: 0.3763,                 loss: nan
Episode: 2641/101000 (2.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5081s / 177.5557 s
agent0:                 episode reward: -0.4630,                 loss: 0.1503
agent1:                 episode reward: 0.4630,                 loss: nan
Episode: 2661/101000 (2.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7095s / 179.2652 s
agent0:                 episode reward: -0.7343,                 loss: 0.1492
agent1:                 episode reward: 0.7343,                 loss: nan
Episode: 2681/101000 (2.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3466s / 180.6118 s
agent0:                 episode reward: -0.7882,                 loss: 0.1498
agent1:                 episode reward: 0.7882,                 loss: nan
Episode: 2701/101000 (2.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7966s / 182.4084 s
agent0:                 episode reward: -0.3039,                 loss: 0.1501
agent1:                 episode reward: 0.3039,                 loss: nan
Episode: 2721/101000 (2.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5083s / 183.9167 s
agent0:                 episode reward: -0.2850,                 loss: 0.1510
agent1:                 episode reward: 0.2850,                 loss: nan
Episode: 2741/101000 (2.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3937s / 185.3105 s
agent0:                 episode reward: -0.8061,                 loss: 0.1506
agent1:                 episode reward: 0.8061,                 loss: nan
Episode: 2761/101000 (2.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2510s / 186.5615 s
agent0:                 episode reward: -0.8431,                 loss: 0.1483
agent1:                 episode reward: 0.8431,                 loss: nan
Episode: 2781/101000 (2.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7139s / 188.2754 s
agent0:                 episode reward: -0.6639,                 loss: 0.1488
agent1:                 episode reward: 0.6639,                 loss: nan
Episode: 2801/101000 (2.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5531s / 189.8285 s
agent0:                 episode reward: -1.0176,                 loss: 0.1489
agent1:                 episode reward: 1.0176,                 loss: nan
Episode: 2821/101000 (2.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5998s / 191.4284 s
agent0:                 episode reward: -0.6514,                 loss: 0.1496
agent1:                 episode reward: 0.6514,                 loss: nan
Episode: 2841/101000 (2.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6618s / 193.0901 s
agent0:                 episode reward: -0.7045,                 loss: 0.1511
agent1:                 episode reward: 0.7045,                 loss: nan
Episode: 2861/101000 (2.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3467s / 194.4368 s
agent0:                 episode reward: -0.1552,                 loss: 0.1508
agent1:                 episode reward: 0.1552,                 loss: nan
Episode: 2881/101000 (2.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6587s / 196.0955 s
agent0:                 episode reward: -0.3651,                 loss: 0.1527
agent1:                 episode reward: 0.3651,                 loss: nan
Episode: 2901/101000 (2.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3845s / 197.4800 s
agent0:                 episode reward: -0.4084,                 loss: 0.1521
agent1:                 episode reward: 0.4084,                 loss: nan
Episode: 2921/101000 (2.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4274s / 198.9074 s
agent0:                 episode reward: -0.4051,                 loss: 0.1528
agent1:                 episode reward: 0.4051,                 loss: nan
Episode: 2941/101000 (2.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4107s / 200.3182 s
agent0:                 episode reward: -0.4401,                 loss: 0.1511
agent1:                 episode reward: 0.4401,                 loss: nan
Episode: 2961/101000 (2.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6263s / 201.9445 s
agent0:                 episode reward: -0.5728,                 loss: 0.1520
agent1:                 episode reward: 0.5728,                 loss: nan
Episode: 2981/101000 (2.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2645s / 203.2090 s
agent0:                 episode reward: -0.3833,                 loss: 0.1516
agent1:                 episode reward: 0.3833,                 loss: nan
Episode: 3001/101000 (2.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4985s / 204.7075 s
agent0:                 episode reward: -0.3188,                 loss: 0.1492
agent1:                 episode reward: 0.3188,                 loss: nan
Episode: 3021/101000 (2.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4542s / 206.1617 s
agent0:                 episode reward: -0.7820,                 loss: 0.1502
agent1:                 episode reward: 0.7820,                 loss: nan
Episode: 3041/101000 (3.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1923s / 208.3540 s
agent0:                 episode reward: -0.4768,                 loss: 0.1518
agent1:                 episode reward: 0.4768,                 loss: nan
Episode: 3061/101000 (3.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3848s / 209.7388 s
agent0:                 episode reward: -0.0016,                 loss: 0.1520
agent1:                 episode reward: 0.0016,                 loss: nan
Episode: 3081/101000 (3.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6264s / 211.3653 s
agent0:                 episode reward: 0.1037,                 loss: 0.1535
agent1:                 episode reward: -0.1037,                 loss: nan
Episode: 3101/101000 (3.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6985s / 213.0638 s
agent0:                 episode reward: -0.3815,                 loss: 0.1516
agent1:                 episode reward: 0.3815,                 loss: nan
Episode: 3121/101000 (3.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5626s / 214.6263 s
agent0:                 episode reward: -0.2118,                 loss: 0.1524
agent1:                 episode reward: 0.2118,                 loss: nan
Episode: 3141/101000 (3.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0070s / 216.6333 s
agent0:                 episode reward: -0.5559,                 loss: 0.1522
agent1:                 episode reward: 0.5559,                 loss: nan
Episode: 3161/101000 (3.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7634s / 218.3966 s
agent0:                 episode reward: -0.3974,                 loss: 0.1515
agent1:                 episode reward: 0.3974,                 loss: nan
Episode: 3181/101000 (3.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8254s / 220.2220 s
agent0:                 episode reward: -0.5597,                 loss: 0.1530
agent1:                 episode reward: 0.5597,                 loss: nan
Episode: 3201/101000 (3.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6341s / 221.8561 s
agent0:                 episode reward: -0.2451,                 loss: 0.1563
agent1:                 episode reward: 0.2451,                 loss: nan
Episode: 3221/101000 (3.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1006s / 223.9567 s
agent0:                 episode reward: -0.3226,                 loss: 0.1549
agent1:                 episode reward: 0.3226,                 loss: nan
Episode: 3241/101000 (3.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5266s / 225.4833 s
agent0:                 episode reward: -0.0877,                 loss: 0.1537
agent1:                 episode reward: 0.0877,                 loss: nan
Episode: 3261/101000 (3.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3934s / 226.8766 s
agent0:                 episode reward: -0.5528,                 loss: 0.1558
agent1:                 episode reward: 0.5528,                 loss: nan
Episode: 3281/101000 (3.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6200s / 228.4966 s
agent0:                 episode reward: -0.5198,                 loss: 0.1551
agent1:                 episode reward: 0.5198,                 loss: nan
Episode: 3301/101000 (3.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9118s / 230.4084 s
agent0:                 episode reward: -0.0671,                 loss: 0.1545
agent1:                 episode reward: 0.0671,                 loss: nan
Episode: 3321/101000 (3.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8124s / 232.2208 s
agent0:                 episode reward: -0.4359,                 loss: 0.1546
agent1:                 episode reward: 0.4359,                 loss: nan
Episode: 3341/101000 (3.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8847s / 234.1055 s
agent0:                 episode reward: -0.5874,                 loss: 0.1537
agent1:                 episode reward: 0.5874,                 loss: nan
Episode: 3361/101000 (3.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7763s / 235.8818 s
agent0:                 episode reward: -0.0450,                 loss: 0.1557
agent1:                 episode reward: 0.0450,                 loss: nan
Episode: 3381/101000 (3.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8903s / 237.7722 s
agent0:                 episode reward: -0.4500,                 loss: 0.1541
agent1:                 episode reward: 0.4500,                 loss: nan
Episode: 3401/101000 (3.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5439s / 239.3161 s
agent0:                 episode reward: -0.4093,                 loss: 0.1526
agent1:                 episode reward: 0.4093,                 loss: nan
Episode: 3421/101000 (3.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7330s / 241.0491 s
agent0:                 episode reward: -0.4164,                 loss: 0.1539
agent1:                 episode reward: 0.4164,                 loss: nan
Episode: 3441/101000 (3.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9297s / 242.9789 s
agent0:                 episode reward: -0.3663,                 loss: 0.1538
agent1:                 episode reward: 0.3663,                 loss: nan
Episode: 3461/101000 (3.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9693s / 244.9482 s
agent0:                 episode reward: -0.7120,                 loss: 0.1537
agent1:                 episode reward: 0.7120,                 loss: nan
Episode: 3481/101000 (3.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7226s / 246.6708 s
agent0:                 episode reward: -0.5275,                 loss: 0.1530
agent1:                 episode reward: 0.5275,                 loss: nan
Episode: 3501/101000 (3.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8025s / 248.4733 s
agent0:                 episode reward: -0.7763,                 loss: 0.1531
agent1:                 episode reward: 0.7763,                 loss: nan
Episode: 3521/101000 (3.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2797s / 250.7530 s
agent0:                 episode reward: -0.4788,                 loss: 0.1559
agent1:                 episode reward: 0.4788,                 loss: nan
Episode: 3541/101000 (3.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4147s / 252.1678 s
agent0:                 episode reward: -0.7533,                 loss: 0.1565
agent1:                 episode reward: 0.7533,                 loss: nan
Episode: 3561/101000 (3.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4924s / 253.6602 s
agent0:                 episode reward: -0.6066,                 loss: 0.1566
agent1:                 episode reward: 0.6066,                 loss: nan
Episode: 3581/101000 (3.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6398s / 255.3000 s
agent0:                 episode reward: -0.1650,                 loss: 0.1556
agent1:                 episode reward: 0.1650,                 loss: nan
Episode: 3601/101000 (3.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5724s / 256.8724 s
agent0:                 episode reward: -0.1074,                 loss: 0.1558
agent1:                 episode reward: 0.1074,                 loss: nan
Episode: 3621/101000 (3.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8082s / 258.6806 s
agent0:                 episode reward: -0.6958,                 loss: 0.1550
agent1:                 episode reward: 0.6958,                 loss: nan
Episode: 3641/101000 (3.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7267s / 260.4074 s
agent0:                 episode reward: -0.3285,                 loss: 0.1545
agent1:                 episode reward: 0.3285,                 loss: nan
Episode: 3661/101000 (3.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7278s / 262.1352 s
agent0:                 episode reward: -0.6708,                 loss: 0.1561
agent1:                 episode reward: 0.6708,                 loss: nan
Episode: 3681/101000 (3.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0764s / 264.2116 s
agent0:                 episode reward: -0.5526,                 loss: 0.1568
agent1:                 episode reward: 0.5526,                 loss: nan
Episode: 3701/101000 (3.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4758s / 265.6874 s
agent0:                 episode reward: -0.1557,                 loss: 0.1561
agent1:                 episode reward: 0.1557,                 loss: nan
Episode: 3721/101000 (3.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2632s / 266.9506 s
agent0:                 episode reward: -0.2429,                 loss: 0.1563
agent1:                 episode reward: 0.2429,                 loss: nan
Episode: 3741/101000 (3.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6076s / 268.5582 s
agent0:                 episode reward: -0.2850,                 loss: 0.1532
agent1:                 episode reward: 0.2850,                 loss: nan
Episode: 3761/101000 (3.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5281s / 270.0863 s
agent0:                 episode reward: -0.4804,                 loss: 0.1536
agent1:                 episode reward: 0.4804,                 loss: nan
Episode: 3781/101000 (3.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3459s / 271.4322 s
agent0:                 episode reward: -0.3414,                 loss: 0.1563
agent1:                 episode reward: 0.3414,                 loss: nan
Episode: 3801/101000 (3.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 0.9749s / 272.4071 s
agent0:                 episode reward: -0.2165,                 loss: 0.1551
agent1:                 episode reward: 0.2165,                 loss: nan
Episode: 3821/101000 (3.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4495s / 273.8566 s
agent0:                 episode reward: -0.3582,                 loss: 0.1554
agent1:                 episode reward: 0.3582,                 loss: nan
Episode: 3841/101000 (3.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3398s / 275.1964 s
agent0:                 episode reward: -0.2862,                 loss: 0.1548
agent1:                 episode reward: 0.2862,                 loss: nan
Episode: 3861/101000 (3.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2013s / 276.3976 s
agent0:                 episode reward: -0.5820,                 loss: 0.1505
agent1:                 episode reward: 0.5820,                 loss: nan
Episode: 3881/101000 (3.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7414s / 278.1391 s
agent0:                 episode reward: -0.9211,                 loss: 0.1539
agent1:                 episode reward: 0.9211,                 loss: nan
Episode: 3901/101000 (3.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2083s / 280.3474 s
agent0:                 episode reward: -0.4189,                 loss: 0.1533
agent1:                 episode reward: 0.4189,                 loss: nan
Episode: 3921/101000 (3.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3955s / 281.7429 s
agent0:                 episode reward: -0.4315,                 loss: 0.1539
agent1:                 episode reward: 0.4315,                 loss: nan
Episode: 3941/101000 (3.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3176s / 283.0605 s
agent0:                 episode reward: -0.0517,                 loss: 0.1543
agent1:                 episode reward: 0.0517,                 loss: nan
Episode: 3961/101000 (3.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8309s / 284.8914 s
agent0:                 episode reward: -0.7435,                 loss: 0.1547
agent1:                 episode reward: 0.7435,                 loss: nan
Episode: 3981/101000 (3.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6163s / 286.5076 s
agent0:                 episode reward: -0.4521,                 loss: 0.1541
agent1:                 episode reward: 0.4521,                 loss: nan
Episode: 4001/101000 (3.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4472s / 287.9548 s
agent0:                 episode reward: -0.3908,                 loss: 0.1531
agent1:                 episode reward: 0.3908,                 loss: nan
Episode: 4021/101000 (3.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6063s / 289.5612 s
agent0:                 episode reward: 0.0140,                 loss: 0.1543
agent1:                 episode reward: -0.0140,                 loss: nan
Episode: 4041/101000 (4.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1570s / 291.7181 s
agent0:                 episode reward: -0.4897,                 loss: 0.1543
agent1:                 episode reward: 0.4897,                 loss: nan
Episode: 4061/101000 (4.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8605s / 293.5786 s
agent0:                 episode reward: -0.2930,                 loss: 0.1530
agent1:                 episode reward: 0.2930,                 loss: nan
Episode: 4081/101000 (4.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0727s / 295.6513 s
agent0:                 episode reward: -0.4705,                 loss: 0.1533
agent1:                 episode reward: 0.4705,                 loss: nan
Episode: 4101/101000 (4.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7879s / 297.4392 s
agent0:                 episode reward: -0.1921,                 loss: 0.1539
agent1:                 episode reward: 0.1921,                 loss: nan
Episode: 4121/101000 (4.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6198s / 299.0590 s
agent0:                 episode reward: 0.0648,                 loss: 0.1529
agent1:                 episode reward: -0.0648,                 loss: nan
Episode: 4141/101000 (4.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7837s / 300.8428 s
agent0:                 episode reward: -0.7239,                 loss: 0.1513
agent1:                 episode reward: 0.7239,                 loss: nan
Episode: 4161/101000 (4.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4566s / 302.2994 s
agent0:                 episode reward: -0.3511,                 loss: 0.1523
agent1:                 episode reward: 0.3511,                 loss: nan
Episode: 4181/101000 (4.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8211s / 304.1205 s
agent0:                 episode reward: -0.5517,                 loss: 0.1570
agent1:                 episode reward: 0.5517,                 loss: nan
Episode: 4201/101000 (4.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8113s / 305.9318 s
agent0:                 episode reward: -0.2310,                 loss: 0.1576
agent1:                 episode reward: 0.2310,                 loss: nan
Episode: 4221/101000 (4.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6985s / 307.6303 s
agent0:                 episode reward: -0.2141,                 loss: 0.1565
agent1:                 episode reward: 0.2141,                 loss: nan
Episode: 4241/101000 (4.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8002s / 309.4305 s
agent0:                 episode reward: -0.2173,                 loss: 0.1587
agent1:                 episode reward: 0.2173,                 loss: nan
Episode: 4261/101000 (4.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7255s / 311.1560 s
agent0:                 episode reward: 0.1133,                 loss: 0.1562
agent1:                 episode reward: -0.1133,                 loss: nan
Episode: 4281/101000 (4.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5927s / 312.7487 s
agent0:                 episode reward: 0.1776,                 loss: 0.1563
agent1:                 episode reward: -0.1776,                 loss: nan
Episode: 4301/101000 (4.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0220s / 314.7707 s
agent0:                 episode reward: -0.7371,                 loss: 0.1562
agent1:                 episode reward: 0.7371,                 loss: nan
Episode: 4321/101000 (4.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5354s / 316.3062 s
agent0:                 episode reward: -0.7932,                 loss: 0.1591
agent1:                 episode reward: 0.7932,                 loss: nan
Episode: 4341/101000 (4.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3840s / 317.6901 s
agent0:                 episode reward: -0.7458,                 loss: 0.1576
agent1:                 episode reward: 0.7458,                 loss: nan
Episode: 4361/101000 (4.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9075s / 319.5976 s
agent0:                 episode reward: -0.1861,                 loss: 0.1577
agent1:                 episode reward: 0.1861,                 loss: nan
Episode: 4381/101000 (4.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4704s / 321.0680 s
agent0:                 episode reward: -0.1371,                 loss: 0.1577
agent1:                 episode reward: 0.1371,                 loss: nan
Episode: 4401/101000 (4.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5708s / 322.6388 s
agent0:                 episode reward: -0.4150,                 loss: 0.1577
agent1:                 episode reward: 0.4150,                 loss: nan
Episode: 4421/101000 (4.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1320s / 323.7709 s
agent0:                 episode reward: -0.3748,                 loss: 0.1565
agent1:                 episode reward: 0.3748,                 loss: nan
Episode: 4441/101000 (4.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2411s / 325.0120 s
agent0:                 episode reward: -0.1193,                 loss: 0.1548
agent1:                 episode reward: 0.1193,                 loss: nan
Episode: 4461/101000 (4.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9131s / 326.9251 s
agent0:                 episode reward: 0.2860,                 loss: 0.1563
agent1:                 episode reward: -0.2860,                 loss: nan
Episode: 4481/101000 (4.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3960s / 328.3211 s
agent0:                 episode reward: -0.0336,                 loss: 0.1561
agent1:                 episode reward: 0.0336,                 loss: nan
Episode: 4501/101000 (4.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4771s / 329.7982 s
agent0:                 episode reward: -0.5546,                 loss: 0.1570
agent1:                 episode reward: 0.5546,                 loss: nan
Episode: 4521/101000 (4.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1824s / 330.9806 s
agent0:                 episode reward: -0.1931,                 loss: 0.1562
agent1:                 episode reward: 0.1931,                 loss: nan
Episode: 4541/101000 (4.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0761s / 332.0567 s
agent0:                 episode reward: -0.2601,                 loss: 0.1571
agent1:                 episode reward: 0.2601,                 loss: nan
Episode: 4561/101000 (4.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4813s / 333.5380 s
agent0:                 episode reward: -0.0412,                 loss: 0.1573
agent1:                 episode reward: 0.0412,                 loss: nan
Episode: 4581/101000 (4.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8766s / 335.4145 s
agent0:                 episode reward: -0.7040,                 loss: 0.1565
agent1:                 episode reward: 0.7040,                 loss: nan
Episode: 4601/101000 (4.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4635s / 336.8780 s
agent0:                 episode reward: -0.5543,                 loss: 0.1576
agent1:                 episode reward: 0.5543,                 loss: nan
Episode: 4621/101000 (4.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4117s / 338.2898 s
agent0:                 episode reward: -0.4515,                 loss: 0.1566
agent1:                 episode reward: 0.4515,                 loss: nan
Episode: 4641/101000 (4.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5970s / 339.8868 s
agent0:                 episode reward: -0.2388,                 loss: 0.1561
agent1:                 episode reward: 0.2388,                 loss: nan
Episode: 4661/101000 (4.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8525s / 341.7393 s
agent0:                 episode reward: -0.3816,                 loss: 0.1562
agent1:                 episode reward: 0.3816,                 loss: nan
Episode: 4681/101000 (4.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7691s / 343.5084 s
agent0:                 episode reward: -0.5834,                 loss: 0.1558
agent1:                 episode reward: 0.5834,                 loss: nan
Episode: 4701/101000 (4.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6646s / 345.1730 s
agent0:                 episode reward: -0.1513,                 loss: 0.1557
agent1:                 episode reward: 0.1513,                 loss: nan
Episode: 4721/101000 (4.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4463s / 346.6193 s
agent0:                 episode reward: -0.2068,                 loss: 0.1570
agent1:                 episode reward: 0.2068,                 loss: nan
Episode: 4741/101000 (4.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6633s / 348.2827 s
agent0:                 episode reward: -0.1424,                 loss: 0.1565
agent1:                 episode reward: 0.1424,                 loss: nan
Episode: 4761/101000 (4.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7399s / 350.0226 s
agent0:                 episode reward: -0.5410,                 loss: 0.1575
agent1:                 episode reward: 0.5410,                 loss: nan
Episode: 4781/101000 (4.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1637s / 352.1862 s
agent0:                 episode reward: -0.1394,                 loss: 0.1546
agent1:                 episode reward: 0.1394,                 loss: nan
Episode: 4801/101000 (4.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5401s / 353.7263 s
agent0:                 episode reward: -0.2900,                 loss: 0.1569
agent1:                 episode reward: 0.2900,                 loss: nan
Episode: 4821/101000 (4.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6160s / 355.3424 s
agent0:                 episode reward: -0.5754,                 loss: 0.1564
agent1:                 episode reward: 0.5754,                 loss: nan
Episode: 4841/101000 (4.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9057s / 357.2481 s
agent0:                 episode reward: -0.1562,                 loss: 0.1548
agent1:                 episode reward: 0.1562,                 loss: nan
Episode: 4861/101000 (4.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9064s / 359.1545 s
agent0:                 episode reward: -0.8302,                 loss: 0.1523
agent1:                 episode reward: 0.8302,                 loss: nan
Episode: 4881/101000 (4.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6279s / 360.7824 s
agent0:                 episode reward: -0.7931,                 loss: 0.1544
agent1:                 episode reward: 0.7931,                 loss: nan
Episode: 4901/101000 (4.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6458s / 362.4282 s
agent0:                 episode reward: -0.7561,                 loss: 0.1522
agent1:                 episode reward: 0.7561,                 loss: nan
Episode: 4921/101000 (4.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4523s / 363.8805 s
agent0:                 episode reward: -1.0618,                 loss: 0.1539
agent1:                 episode reward: 1.0618,                 loss: nan
Episode: 4941/101000 (4.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8192s / 365.6997 s
agent0:                 episode reward: -0.4517,                 loss: 0.1531
agent1:                 episode reward: 0.4517,                 loss: nan
Episode: 4961/101000 (4.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5198s / 367.2196 s
agent0:                 episode reward: -0.0308,                 loss: 0.1523
agent1:                 episode reward: 0.0308,                 loss: nan
Episode: 4981/101000 (4.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6999s / 368.9195 s
agent0:                 episode reward: -0.3088,                 loss: 0.1526
agent1:                 episode reward: 0.3088,                 loss: nan
Episode: 5001/101000 (4.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4508s / 370.3703 s
agent0:                 episode reward: -0.2172,                 loss: 0.1525
agent1:                 episode reward: 0.2172,                 loss: nan
Episode: 5021/101000 (4.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9741s / 372.3444 s
agent0:                 episode reward: -0.3560,                 loss: 0.1526
agent1:                 episode reward: 0.3560,                 loss: nan
Episode: 5041/101000 (4.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3807s / 373.7251 s
agent0:                 episode reward: -0.1454,                 loss: 0.1536
agent1:                 episode reward: 0.1454,                 loss: nan
Episode: 5061/101000 (5.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7018s / 375.4269 s
agent0:                 episode reward: -0.2046,                 loss: 0.1551
agent1:                 episode reward: 0.2046,                 loss: nan
Episode: 5081/101000 (5.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2647s / 376.6916 s
agent0:                 episode reward: -0.7495,                 loss: 0.1524
agent1:                 episode reward: 0.7495,                 loss: nan
Episode: 5101/101000 (5.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8660s / 378.5576 s
agent0:                 episode reward: -0.5396,                 loss: 0.1531
agent1:                 episode reward: 0.5396,                 loss: nan
Episode: 5121/101000 (5.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7652s / 380.3228 s
agent0:                 episode reward: -0.2725,                 loss: 0.1526
agent1:                 episode reward: 0.2725,                 loss: nan
Episode: 5141/101000 (5.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4574s / 381.7803 s
agent0:                 episode reward: -0.6027,                 loss: 0.1526
agent1:                 episode reward: 0.6027,                 loss: nan
Episode: 5161/101000 (5.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8833s / 383.6636 s
agent0:                 episode reward: -0.5199,                 loss: 0.1527
agent1:                 episode reward: 0.5199,                 loss: nan
Episode: 5181/101000 (5.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6875s / 385.3511 s
agent0:                 episode reward: -0.4527,                 loss: 0.1584
agent1:                 episode reward: 0.4527,                 loss: nan
Episode: 5201/101000 (5.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2975s / 386.6485 s
agent0:                 episode reward: -0.6546,                 loss: 0.1592
agent1:                 episode reward: 0.6546,                 loss: nan
Episode: 5221/101000 (5.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9785s / 388.6270 s
agent0:                 episode reward: -0.2224,                 loss: 0.1590
agent1:                 episode reward: 0.2224,                 loss: nan
Episode: 5241/101000 (5.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8256s / 390.4527 s
agent0:                 episode reward: -0.3179,                 loss: 0.1592
agent1:                 episode reward: 0.3179,                 loss: nan
Episode: 5261/101000 (5.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4292s / 391.8818 s
agent0:                 episode reward: -0.6468,                 loss: 0.1608
agent1:                 episode reward: 0.6468,                 loss: nan
Episode: 5281/101000 (5.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6175s / 393.4993 s
agent0:                 episode reward: -0.6782,                 loss: 0.1599
agent1:                 episode reward: 0.6782,                 loss: nan
Episode: 5301/101000 (5.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5339s / 395.0332 s
agent0:                 episode reward: -0.1156,                 loss: 0.1586
agent1:                 episode reward: 0.1156,                 loss: nan
Episode: 5321/101000 (5.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7307s / 396.7640 s
agent0:                 episode reward: -0.4030,                 loss: 0.1608
agent1:                 episode reward: 0.4030,                 loss: nan
Episode: 5341/101000 (5.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7463s / 398.5103 s
agent0:                 episode reward: 0.1850,                 loss: 0.1584
agent1:                 episode reward: -0.1850,                 loss: nan
Episode: 5361/101000 (5.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8334s / 400.3437 s
agent0:                 episode reward: -0.2393,                 loss: 0.1569
agent1:                 episode reward: 0.2393,                 loss: nan
Episode: 5381/101000 (5.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8178s / 402.1615 s
agent0:                 episode reward: -0.1474,                 loss: 0.1584
agent1:                 episode reward: 0.1474,                 loss: nan
Episode: 5401/101000 (5.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5236s / 403.6850 s
agent0:                 episode reward: -0.4734,                 loss: 0.1577
agent1:                 episode reward: 0.4734,                 loss: nan
Episode: 5421/101000 (5.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5986s / 405.2836 s
agent0:                 episode reward: -0.2824,                 loss: 0.1581
agent1:                 episode reward: 0.2824,                 loss: nan
Episode: 5441/101000 (5.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5616s / 406.8453 s
agent0:                 episode reward: 0.2167,                 loss: 0.1592
agent1:                 episode reward: -0.2167,                 loss: 0.1166
Score delta: 1.707492118402455, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/5008_0.
Episode: 5461/101000 (5.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9520s / 408.7973 s
agent0:                 episode reward: -0.0894,                 loss: nan
agent1:                 episode reward: 0.0894,                 loss: 0.1166
Episode: 5481/101000 (5.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7833s / 410.5805 s
agent0:                 episode reward: 0.2454,                 loss: nan
agent1:                 episode reward: -0.2454,                 loss: 0.1204
Episode: 5501/101000 (5.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6746s / 412.2551 s
agent0:                 episode reward: -0.4202,                 loss: 0.1566
agent1:                 episode reward: 0.4202,                 loss: 0.1208
Score delta: 1.5186055990622933, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/5068_1.
Episode: 5521/101000 (5.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5598s / 413.8149 s
agent0:                 episode reward: -0.1215,                 loss: 0.1574
agent1:                 episode reward: 0.1215,                 loss: nan
Episode: 5541/101000 (5.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3734s / 415.1883 s
agent0:                 episode reward: -0.4093,                 loss: 0.1585
agent1:                 episode reward: 0.4093,                 loss: nan
Episode: 5561/101000 (5.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3908s / 416.5790 s
agent0:                 episode reward: -0.0337,                 loss: 0.1591
agent1:                 episode reward: 0.0337,                 loss: nan
Episode: 5581/101000 (5.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4340s / 418.0131 s
agent0:                 episode reward: -0.5775,                 loss: 0.1543
agent1:                 episode reward: 0.5775,                 loss: nan
Episode: 5601/101000 (5.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6306s / 419.6436 s
agent0:                 episode reward: -0.4445,                 loss: 0.1560
agent1:                 episode reward: 0.4445,                 loss: nan
Episode: 5621/101000 (5.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7536s / 421.3972 s
agent0:                 episode reward: -0.5027,                 loss: 0.1554
agent1:                 episode reward: 0.5027,                 loss: nan
Episode: 5641/101000 (5.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5705s / 422.9678 s
agent0:                 episode reward: -0.1020,                 loss: 0.1545
agent1:                 episode reward: 0.1020,                 loss: nan
Episode: 5661/101000 (5.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4599s / 424.4277 s
agent0:                 episode reward: -0.3298,                 loss: 0.1570
agent1:                 episode reward: 0.3298,                 loss: nan
Episode: 5681/101000 (5.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8985s / 426.3262 s
agent0:                 episode reward: 0.0344,                 loss: 0.1549
agent1:                 episode reward: -0.0344,                 loss: nan
Episode: 5701/101000 (5.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6289s / 427.9551 s
agent0:                 episode reward: -0.4230,                 loss: 0.1562
agent1:                 episode reward: 0.4230,                 loss: nan
Episode: 5721/101000 (5.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5757s / 429.5308 s
agent0:                 episode reward: -0.6139,                 loss: 0.1552
agent1:                 episode reward: 0.6139,                 loss: nan
Episode: 5741/101000 (5.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2869s / 431.8176 s
agent0:                 episode reward: -0.7528,                 loss: 0.1573
agent1:                 episode reward: 0.7528,                 loss: nan
Episode: 5761/101000 (5.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5418s / 433.3594 s
agent0:                 episode reward: -0.0580,                 loss: 0.1551
agent1:                 episode reward: 0.0580,                 loss: nan
Episode: 5781/101000 (5.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8336s / 435.1931 s
agent0:                 episode reward: -0.3380,                 loss: 0.1555
agent1:                 episode reward: 0.3380,                 loss: nan
Episode: 5801/101000 (5.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5729s / 436.7660 s
agent0:                 episode reward: -0.1995,                 loss: 0.1554
agent1:                 episode reward: 0.1995,                 loss: nan
Episode: 5821/101000 (5.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6635s / 438.4295 s
agent0:                 episode reward: -0.3267,                 loss: 0.1558
agent1:                 episode reward: 0.3267,                 loss: nan
Episode: 5841/101000 (5.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5577s / 439.9872 s
agent0:                 episode reward: -0.5145,                 loss: 0.1557
agent1:                 episode reward: 0.5145,                 loss: nan
Episode: 5861/101000 (5.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1546s / 441.1418 s
agent0:                 episode reward: -0.3296,                 loss: 0.1549
agent1:                 episode reward: 0.3296,                 loss: nan
Episode: 5881/101000 (5.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7398s / 442.8816 s
agent0:                 episode reward: -0.1678,                 loss: 0.1552
agent1:                 episode reward: 0.1678,                 loss: nan
Episode: 5901/101000 (5.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5049s / 444.3865 s
agent0:                 episode reward: -0.2833,                 loss: 0.1569
agent1:                 episode reward: 0.2833,                 loss: nan
Episode: 5921/101000 (5.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0121s / 446.3986 s
agent0:                 episode reward: -0.3502,                 loss: 0.1558
agent1:                 episode reward: 0.3502,                 loss: nan
Episode: 5941/101000 (5.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6076s / 448.0062 s
agent0:                 episode reward: -0.3934,                 loss: 0.1557
agent1:                 episode reward: 0.3934,                 loss: nan
Episode: 5961/101000 (5.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0236s / 450.0298 s
agent0:                 episode reward: 0.0759,                 loss: 0.1567
agent1:                 episode reward: -0.0759,                 loss: nan
Episode: 5981/101000 (5.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1383s / 452.1682 s
agent0:                 episode reward: -0.0523,                 loss: 0.1546
agent1:                 episode reward: 0.0523,                 loss: nan
Episode: 6001/101000 (5.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2491s / 453.4172 s
agent0:                 episode reward: -0.2709,                 loss: 0.1553
agent1:                 episode reward: 0.2709,                 loss: nan
Episode: 6021/101000 (5.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5763s / 454.9936 s
agent0:                 episode reward: -0.0195,                 loss: 0.1568
agent1:                 episode reward: 0.0195,                 loss: nan
Episode: 6041/101000 (5.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7508s / 456.7444 s
agent0:                 episode reward: -0.5469,                 loss: 0.1537
agent1:                 episode reward: 0.5469,                 loss: nan
Episode: 6061/101000 (6.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8850s / 458.6294 s
agent0:                 episode reward: -0.0970,                 loss: 0.1575
agent1:                 episode reward: 0.0970,                 loss: nan
Episode: 6081/101000 (6.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5482s / 460.1775 s
agent0:                 episode reward: -0.3502,                 loss: 0.1549
agent1:                 episode reward: 0.3502,                 loss: nan
Episode: 6101/101000 (6.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9983s / 462.1758 s
agent0:                 episode reward: -0.3326,                 loss: 0.1547
agent1:                 episode reward: 0.3326,                 loss: nan
Episode: 6121/101000 (6.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7200s / 463.8958 s
agent0:                 episode reward: -0.3986,                 loss: 0.1556
agent1:                 episode reward: 0.3986,                 loss: nan
Episode: 6141/101000 (6.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7703s / 465.6661 s
agent0:                 episode reward: -0.2087,                 loss: 0.1562
agent1:                 episode reward: 0.2087,                 loss: nan
Episode: 6161/101000 (6.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2845s / 466.9506 s
agent0:                 episode reward: -0.7348,                 loss: 0.1531
agent1:                 episode reward: 0.7348,                 loss: nan
Episode: 6181/101000 (6.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7458s / 468.6964 s
agent0:                 episode reward: 0.1627,                 loss: 0.1567
agent1:                 episode reward: -0.1627,                 loss: nan
Episode: 6201/101000 (6.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5741s / 470.2705 s
agent0:                 episode reward: -0.1041,                 loss: 0.1543
agent1:                 episode reward: 0.1041,                 loss: nan
Episode: 6221/101000 (6.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0646s / 472.3351 s
agent0:                 episode reward: -0.4708,                 loss: 0.1557
agent1:                 episode reward: 0.4708,                 loss: nan
Episode: 6241/101000 (6.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0774s / 474.4125 s
agent0:                 episode reward: -0.5291,                 loss: 0.1523
agent1:                 episode reward: 0.5291,                 loss: nan
Episode: 6261/101000 (6.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6956s / 476.1082 s
agent0:                 episode reward: -0.3486,                 loss: 0.1543
agent1:                 episode reward: 0.3486,                 loss: nan
Episode: 6281/101000 (6.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6856s / 477.7938 s
agent0:                 episode reward: -0.2103,                 loss: 0.1548
agent1:                 episode reward: 0.2103,                 loss: nan
Episode: 6301/101000 (6.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9217s / 479.7155 s
agent0:                 episode reward: -0.3645,                 loss: 0.1528
agent1:                 episode reward: 0.3645,                 loss: nan
Episode: 6321/101000 (6.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6965s / 481.4120 s
agent0:                 episode reward: -0.6005,                 loss: 0.1534
agent1:                 episode reward: 0.6005,                 loss: nan
Episode: 6341/101000 (6.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4461s / 482.8580 s
agent0:                 episode reward: -0.1253,                 loss: 0.1546
agent1:                 episode reward: 0.1253,                 loss: nan
Episode: 6361/101000 (6.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5831s / 484.4412 s
agent0:                 episode reward: -0.3493,                 loss: 0.1545
agent1:                 episode reward: 0.3493,                 loss: nan
Episode: 6381/101000 (6.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4385s / 485.8796 s
agent0:                 episode reward: -0.2464,                 loss: 0.1563
agent1:                 episode reward: 0.2464,                 loss: nan
Episode: 6401/101000 (6.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6345s / 487.5141 s
agent0:                 episode reward: -0.2264,                 loss: 0.1546
agent1:                 episode reward: 0.2264,                 loss: nan
Episode: 6421/101000 (6.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7363s / 489.2504 s
agent0:                 episode reward: -0.5447,                 loss: 0.1558
agent1:                 episode reward: 0.5447,                 loss: nan
Episode: 6441/101000 (6.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7015s / 490.9519 s
agent0:                 episode reward: -0.2430,                 loss: 0.1542
agent1:                 episode reward: 0.2430,                 loss: nan
Episode: 6461/101000 (6.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7023s / 492.6542 s
agent0:                 episode reward: -0.0074,                 loss: 0.1548
agent1:                 episode reward: 0.0074,                 loss: nan
Episode: 6481/101000 (6.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6358s / 494.2900 s
agent0:                 episode reward: -0.7940,                 loss: 0.1545
agent1:                 episode reward: 0.7940,                 loss: nan
Episode: 6501/101000 (6.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0868s / 496.3768 s
agent0:                 episode reward: -0.7258,                 loss: 0.1544
agent1:                 episode reward: 0.7258,                 loss: nan
Episode: 6521/101000 (6.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8654s / 498.2422 s
agent0:                 episode reward: -0.3123,                 loss: 0.1549
agent1:                 episode reward: 0.3123,                 loss: nan
Episode: 6541/101000 (6.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6193s / 499.8615 s
agent0:                 episode reward: -0.0333,                 loss: 0.1538
agent1:                 episode reward: 0.0333,                 loss: nan
Episode: 6561/101000 (6.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1036s / 500.9651 s
agent0:                 episode reward: -0.2965,                 loss: 0.1553
agent1:                 episode reward: 0.2965,                 loss: nan
Episode: 6581/101000 (6.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9041s / 502.8692 s
agent0:                 episode reward: -0.4143,                 loss: 0.1587
agent1:                 episode reward: 0.4143,                 loss: nan
Episode: 6601/101000 (6.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1859s / 505.0551 s
agent0:                 episode reward: 0.1618,                 loss: 0.1557
agent1:                 episode reward: -0.1618,                 loss: nan
Episode: 6621/101000 (6.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7223s / 506.7774 s
agent0:                 episode reward: -0.1195,                 loss: 0.1565
agent1:                 episode reward: 0.1195,                 loss: nan
Episode: 6641/101000 (6.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4996s / 508.2770 s
agent0:                 episode reward: -0.2467,                 loss: 0.1573
agent1:                 episode reward: 0.2467,                 loss: nan
Episode: 6661/101000 (6.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7983s / 510.0753 s
agent0:                 episode reward: -0.2247,                 loss: 0.1579
agent1:                 episode reward: 0.2247,                 loss: nan
Episode: 6681/101000 (6.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6774s / 511.7527 s
agent0:                 episode reward: -0.2799,                 loss: 0.1581
agent1:                 episode reward: 0.2799,                 loss: nan
Episode: 6701/101000 (6.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9538s / 513.7064 s
agent0:                 episode reward: 0.0836,                 loss: 0.1575
agent1:                 episode reward: -0.0836,                 loss: nan
Episode: 6721/101000 (6.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6509s / 515.3574 s
agent0:                 episode reward: -0.5083,                 loss: 0.1559
agent1:                 episode reward: 0.5083,                 loss: nan
Episode: 6741/101000 (6.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0349s / 517.3923 s
agent0:                 episode reward: -0.0972,                 loss: 0.1576
agent1:                 episode reward: 0.0972,                 loss: nan
Episode: 6761/101000 (6.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2671s / 518.6594 s
agent0:                 episode reward: 0.0207,                 loss: 0.1565
agent1:                 episode reward: -0.0207,                 loss: nan
Episode: 6781/101000 (6.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7324s / 520.3918 s
agent0:                 episode reward: -0.2868,                 loss: 0.1579
agent1:                 episode reward: 0.2868,                 loss: nan
Episode: 6801/101000 (6.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8383s / 522.2301 s
agent0:                 episode reward: -0.1009,                 loss: 0.1585
agent1:                 episode reward: 0.1009,                 loss: nan
Episode: 6821/101000 (6.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7923s / 524.0224 s
agent0:                 episode reward: -0.1460,                 loss: 0.1578
agent1:                 episode reward: 0.1460,                 loss: nan
Episode: 6841/101000 (6.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6749s / 525.6973 s
agent0:                 episode reward: -0.4294,                 loss: 0.1573
agent1:                 episode reward: 0.4294,                 loss: nan
Episode: 6861/101000 (6.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2990s / 526.9963 s
agent0:                 episode reward: -0.4437,                 loss: 0.1542
agent1:                 episode reward: 0.4437,                 loss: nan
Episode: 6881/101000 (6.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6273s / 528.6236 s
agent0:                 episode reward: -0.6682,                 loss: 0.1563
agent1:                 episode reward: 0.6682,                 loss: nan
Episode: 6901/101000 (6.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0681s / 530.6917 s
agent0:                 episode reward: -0.0800,                 loss: 0.1572
agent1:                 episode reward: 0.0800,                 loss: nan
Episode: 6921/101000 (6.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0625s / 532.7542 s
agent0:                 episode reward: -0.2387,                 loss: 0.1567
agent1:                 episode reward: 0.2387,                 loss: nan
Episode: 6941/101000 (6.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8650s / 534.6192 s
agent0:                 episode reward: -0.1733,                 loss: 0.1555
agent1:                 episode reward: 0.1733,                 loss: nan
Episode: 6961/101000 (6.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8292s / 536.4485 s
agent0:                 episode reward: 0.3043,                 loss: 0.1554
agent1:                 episode reward: -0.3043,                 loss: nan
Episode: 6981/101000 (6.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4918s / 537.9403 s
agent0:                 episode reward: -0.3246,                 loss: 0.1570
agent1:                 episode reward: 0.3246,                 loss: nan
Episode: 7001/101000 (6.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8240s / 539.7643 s
agent0:                 episode reward: -0.3900,                 loss: 0.1563
agent1:                 episode reward: 0.3900,                 loss: nan
Episode: 7021/101000 (6.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2338s / 541.9981 s
agent0:                 episode reward: -0.3089,                 loss: 0.1563
agent1:                 episode reward: 0.3089,                 loss: nan
Episode: 7041/101000 (6.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1092s / 544.1073 s
agent0:                 episode reward: 0.2468,                 loss: 0.1573
agent1:                 episode reward: -0.2468,                 loss: nan
Episode: 7061/101000 (6.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7979s / 545.9052 s
agent0:                 episode reward: -0.2690,                 loss: 0.1584
agent1:                 episode reward: 0.2690,                 loss: nan
Episode: 7081/101000 (7.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8532s / 547.7584 s
agent0:                 episode reward: -0.4879,                 loss: 0.1571
agent1:                 episode reward: 0.4879,                 loss: nan
Episode: 7101/101000 (7.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5486s / 549.3070 s
agent0:                 episode reward: -0.4393,                 loss: 0.1554
agent1:                 episode reward: 0.4393,                 loss: nan
Episode: 7121/101000 (7.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1035s / 550.4105 s
agent0:                 episode reward: -0.2446,                 loss: 0.1560
agent1:                 episode reward: 0.2446,                 loss: nan
Episode: 7141/101000 (7.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6600s / 552.0705 s
agent0:                 episode reward: 0.2291,                 loss: 0.1554
agent1:                 episode reward: -0.2291,                 loss: nan
Episode: 7161/101000 (7.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9386s / 554.0091 s
agent0:                 episode reward: -0.0654,                 loss: 0.1552
agent1:                 episode reward: 0.0654,                 loss: nan
Episode: 7181/101000 (7.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6020s / 555.6111 s
agent0:                 episode reward: -0.1990,                 loss: 0.1554
agent1:                 episode reward: 0.1990,                 loss: nan
Episode: 7201/101000 (7.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3495s / 557.9607 s
agent0:                 episode reward: -0.4141,                 loss: 0.1571
agent1:                 episode reward: 0.4141,                 loss: nan
Episode: 7221/101000 (7.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2285s / 559.1892 s
agent0:                 episode reward: -0.8032,                 loss: 0.1584
agent1:                 episode reward: 0.8032,                 loss: nan
Episode: 7241/101000 (7.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7722s / 560.9614 s
agent0:                 episode reward: -0.6356,                 loss: 0.1631
agent1:                 episode reward: 0.6356,                 loss: nan
Episode: 7261/101000 (7.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7402s / 562.7015 s
agent0:                 episode reward: -0.0907,                 loss: 0.1625
agent1:                 episode reward: 0.0907,                 loss: nan
Episode: 7281/101000 (7.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7296s / 564.4311 s
agent0:                 episode reward: -0.3349,                 loss: 0.1624
agent1:                 episode reward: 0.3349,                 loss: nan
Episode: 7301/101000 (7.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0024s / 566.4336 s
agent0:                 episode reward: -0.3472,                 loss: 0.1621
agent1:                 episode reward: 0.3472,                 loss: nan
Episode: 7321/101000 (7.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6978s / 568.1314 s
agent0:                 episode reward: 0.0321,                 loss: 0.1637
agent1:                 episode reward: -0.0321,                 loss: nan
Episode: 7341/101000 (7.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2767s / 569.4081 s
agent0:                 episode reward: -0.2834,                 loss: 0.1626
agent1:                 episode reward: 0.2834,                 loss: nan
Episode: 7361/101000 (7.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4492s / 570.8572 s
agent0:                 episode reward: -0.2403,                 loss: 0.1626
agent1:                 episode reward: 0.2403,                 loss: nan
Episode: 7381/101000 (7.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7852s / 572.6425 s
agent0:                 episode reward: -0.0528,                 loss: 0.1641
agent1:                 episode reward: 0.0528,                 loss: nan
Episode: 7401/101000 (7.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6098s / 574.2523 s
agent0:                 episode reward: -0.1430,                 loss: 0.1644
agent1:                 episode reward: 0.1430,                 loss: nan
Episode: 7421/101000 (7.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8263s / 576.0786 s
agent0:                 episode reward: -0.3490,                 loss: 0.1663
agent1:                 episode reward: 0.3490,                 loss: nan
Episode: 7441/101000 (7.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3867s / 577.4653 s
agent0:                 episode reward: -0.1074,                 loss: 0.1644
agent1:                 episode reward: 0.1074,                 loss: nan
Episode: 7461/101000 (7.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6639s / 579.1292 s
agent0:                 episode reward: -0.2169,                 loss: 0.1657
agent1:                 episode reward: 0.2169,                 loss: nan
Episode: 7481/101000 (7.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4975s / 580.6267 s
agent0:                 episode reward: -0.2949,                 loss: 0.1635
agent1:                 episode reward: 0.2949,                 loss: nan
Episode: 7501/101000 (7.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0658s / 582.6925 s
agent0:                 episode reward: -0.3688,                 loss: 0.1614
agent1:                 episode reward: 0.3688,                 loss: nan
Episode: 7521/101000 (7.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3500s / 584.0424 s
agent0:                 episode reward: -0.3699,                 loss: 0.1641
agent1:                 episode reward: 0.3699,                 loss: nan
Episode: 7541/101000 (7.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3168s / 585.3593 s
agent0:                 episode reward: -0.3347,                 loss: 0.1639
agent1:                 episode reward: 0.3347,                 loss: nan
Episode: 7561/101000 (7.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8590s / 587.2183 s
agent0:                 episode reward: -0.3242,                 loss: 0.1625
agent1:                 episode reward: 0.3242,                 loss: nan
Episode: 7581/101000 (7.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8044s / 589.0227 s
agent0:                 episode reward: -0.4852,                 loss: 0.1625
agent1:                 episode reward: 0.4852,                 loss: nan
Episode: 7601/101000 (7.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5143s / 590.5371 s
agent0:                 episode reward: -0.2324,                 loss: 0.1647
agent1:                 episode reward: 0.2324,                 loss: nan
Episode: 7621/101000 (7.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.0893s / 591.6263 s
agent0:                 episode reward: -0.5706,                 loss: 0.1638
agent1:                 episode reward: 0.5706,                 loss: nan
Episode: 7641/101000 (7.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2110s / 593.8373 s
agent0:                 episode reward: -0.1245,                 loss: 0.1644
agent1:                 episode reward: 0.1245,                 loss: nan
Episode: 7661/101000 (7.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8557s / 595.6931 s
agent0:                 episode reward: -0.3532,                 loss: 0.1639
agent1:                 episode reward: 0.3532,                 loss: nan
Episode: 7681/101000 (7.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0540s / 597.7471 s
agent0:                 episode reward: 0.2735,                 loss: 0.1634
agent1:                 episode reward: -0.2735,                 loss: nan
Episode: 7701/101000 (7.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1273s / 599.8744 s
agent0:                 episode reward: -0.3169,                 loss: 0.1634
agent1:                 episode reward: 0.3169,                 loss: nan
Episode: 7721/101000 (7.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8410s / 601.7153 s
agent0:                 episode reward: -0.1583,                 loss: 0.1618
agent1:                 episode reward: 0.1583,                 loss: nan
Episode: 7741/101000 (7.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6477s / 603.3630 s
agent0:                 episode reward: -0.0042,                 loss: 0.1631
agent1:                 episode reward: 0.0042,                 loss: nan
Episode: 7761/101000 (7.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0468s / 605.4098 s
agent0:                 episode reward: -0.1284,                 loss: 0.1628
agent1:                 episode reward: 0.1284,                 loss: nan
Episode: 7781/101000 (7.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2678s / 607.6776 s
agent0:                 episode reward: 0.0744,                 loss: 0.1643
agent1:                 episode reward: -0.0744,                 loss: nan
Episode: 7801/101000 (7.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2910s / 609.9686 s
agent0:                 episode reward: 0.1258,                 loss: 0.1642
agent1:                 episode reward: -0.1258,                 loss: nan
Episode: 7821/101000 (7.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1486s / 612.1172 s
agent0:                 episode reward: -0.2857,                 loss: 0.1620
agent1:                 episode reward: 0.2857,                 loss: nan
Episode: 7841/101000 (7.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8380s / 613.9552 s
agent0:                 episode reward: 0.3014,                 loss: 0.1624
agent1:                 episode reward: -0.3014,                 loss: nan
Episode: 7861/101000 (7.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2910s / 616.2462 s
agent0:                 episode reward: -0.5612,                 loss: 0.1629
agent1:                 episode reward: 0.5612,                 loss: nan
Episode: 7881/101000 (7.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3797s / 617.6259 s
agent0:                 episode reward: -0.5455,                 loss: 0.1620
agent1:                 episode reward: 0.5455,                 loss: nan
Episode: 7901/101000 (7.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2669s / 618.8927 s
agent0:                 episode reward: -0.2435,                 loss: 0.1607
agent1:                 episode reward: 0.2435,                 loss: nan
Episode: 7921/101000 (7.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5560s / 620.4487 s
agent0:                 episode reward: -0.3866,                 loss: 0.1575
agent1:                 episode reward: 0.3866,                 loss: nan
Episode: 7941/101000 (7.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5296s / 621.9783 s
agent0:                 episode reward: 0.1240,                 loss: 0.1567
agent1:                 episode reward: -0.1240,                 loss: nan
Episode: 7961/101000 (7.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8258s / 623.8041 s
agent0:                 episode reward: 0.3335,                 loss: 0.1561
agent1:                 episode reward: -0.3335,                 loss: nan
Episode: 7981/101000 (7.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7465s / 625.5505 s
agent0:                 episode reward: -0.0436,                 loss: 0.1576
agent1:                 episode reward: 0.0436,                 loss: nan
Episode: 8001/101000 (7.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6586s / 627.2091 s
agent0:                 episode reward: 0.4149,                 loss: 0.1559
agent1:                 episode reward: -0.4149,                 loss: 0.1272
Score delta: 1.7507596717263112, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/7568_0.
Episode: 8021/101000 (7.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8012s / 629.0104 s
agent0:                 episode reward: -0.0555,                 loss: nan
agent1:                 episode reward: 0.0555,                 loss: 0.1307
Episode: 8041/101000 (7.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5377s / 630.5480 s
agent0:                 episode reward: -0.3032,                 loss: nan
agent1:                 episode reward: 0.3032,                 loss: 0.1322
Episode: 8061/101000 (7.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8770s / 632.4250 s
agent0:                 episode reward: 0.0587,                 loss: nan
agent1:                 episode reward: -0.0587,                 loss: 0.1385
Episode: 8081/101000 (8.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9026s / 634.3276 s
agent0:                 episode reward: -0.2190,                 loss: 0.1588
agent1:                 episode reward: 0.2190,                 loss: 0.1382
Score delta: 1.9284182638738188, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/7638_1.
Episode: 8101/101000 (8.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6128s / 635.9404 s
agent0:                 episode reward: -0.1519,                 loss: 0.1575
agent1:                 episode reward: 0.1519,                 loss: nan
Episode: 8121/101000 (8.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7830s / 637.7234 s
agent0:                 episode reward: -0.1635,                 loss: 0.1580
agent1:                 episode reward: 0.1635,                 loss: nan
Episode: 8141/101000 (8.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0000s / 639.7234 s
agent0:                 episode reward: 0.0147,                 loss: 0.1588
agent1:                 episode reward: -0.0147,                 loss: nan
Episode: 8161/101000 (8.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4152s / 642.1386 s
agent0:                 episode reward: 0.0650,                 loss: 0.1590
agent1:                 episode reward: -0.0650,                 loss: nan
Episode: 8181/101000 (8.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4062s / 644.5448 s
agent0:                 episode reward: -0.4238,                 loss: 0.1574
agent1:                 episode reward: 0.4238,                 loss: nan
Episode: 8201/101000 (8.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2908s / 646.8356 s
agent0:                 episode reward: -0.2496,                 loss: 0.1583
agent1:                 episode reward: 0.2496,                 loss: nan
Episode: 8221/101000 (8.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2710s / 649.1066 s
agent0:                 episode reward: -0.1053,                 loss: 0.1571
agent1:                 episode reward: 0.1053,                 loss: nan
Episode: 8241/101000 (8.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1724s / 651.2791 s
agent0:                 episode reward: 0.1283,                 loss: 0.1568
agent1:                 episode reward: -0.1283,                 loss: nan
Episode: 8261/101000 (8.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1991s / 653.4782 s
agent0:                 episode reward: -0.2031,                 loss: 0.1587
agent1:                 episode reward: 0.2031,                 loss: nan
Episode: 8281/101000 (8.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9220s / 655.4002 s
agent0:                 episode reward: -0.1058,                 loss: 0.1587
agent1:                 episode reward: 0.1058,                 loss: nan
Episode: 8301/101000 (8.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7570s / 657.1572 s
agent0:                 episode reward: -0.3160,                 loss: 0.1575
agent1:                 episode reward: 0.3160,                 loss: nan
Episode: 8321/101000 (8.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7102s / 658.8675 s
agent0:                 episode reward: -0.2468,                 loss: 0.1601
agent1:                 episode reward: 0.2468,                 loss: nan
Episode: 8341/101000 (8.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9576s / 660.8250 s
agent0:                 episode reward: 0.1892,                 loss: 0.1603
agent1:                 episode reward: -0.1892,                 loss: nan
Episode: 8361/101000 (8.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2321s / 663.0571 s
agent0:                 episode reward: -0.0885,                 loss: 0.1581
agent1:                 episode reward: 0.0885,                 loss: nan
Episode: 8381/101000 (8.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9340s / 664.9911 s
agent0:                 episode reward: -0.3026,                 loss: 0.1604
agent1:                 episode reward: 0.3026,                 loss: nan
Episode: 8401/101000 (8.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7787s / 666.7698 s
agent0:                 episode reward: -0.6742,                 loss: 0.1590
agent1:                 episode reward: 0.6742,                 loss: nan
Episode: 8421/101000 (8.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1105s / 668.8803 s
agent0:                 episode reward: -0.3131,                 loss: 0.1629
agent1:                 episode reward: 0.3131,                 loss: nan
Episode: 8441/101000 (8.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 1.2886s / 670.1689 s
agent0:                 episode reward: -0.1431,                 loss: 0.1599
agent1:                 episode reward: 0.1431,                 loss: nan
Episode: 8461/101000 (8.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6178s / 671.7867 s
agent0:                 episode reward: -0.3523,                 loss: 0.1596
agent1:                 episode reward: 0.3523,                 loss: nan
Episode: 8481/101000 (8.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6229s / 673.4096 s
agent0:                 episode reward: -0.0512,                 loss: 0.1619
agent1:                 episode reward: 0.0512,                 loss: nan
Episode: 8501/101000 (8.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8700s / 675.2796 s
agent0:                 episode reward: -0.1252,                 loss: 0.1611
agent1:                 episode reward: 0.1252,                 loss: nan
Episode: 8521/101000 (8.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8638s / 677.1434 s
agent0:                 episode reward: -0.1328,                 loss: 0.1585
agent1:                 episode reward: 0.1328,                 loss: nan
Episode: 8541/101000 (8.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6400s / 678.7834 s
agent0:                 episode reward: -0.2223,                 loss: 0.1621
agent1:                 episode reward: 0.2223,                 loss: nan
Episode: 8561/101000 (8.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7438s / 680.5272 s
agent0:                 episode reward: 0.0612,                 loss: 0.1614
agent1:                 episode reward: -0.0612,                 loss: nan
Episode: 8581/101000 (8.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9248s / 682.4520 s
agent0:                 episode reward: -0.3623,                 loss: 0.1587
agent1:                 episode reward: 0.3623,                 loss: nan
Episode: 8601/101000 (8.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7911s / 684.2431 s
agent0:                 episode reward: -0.2326,                 loss: 0.1624
agent1:                 episode reward: 0.2326,                 loss: nan
Episode: 8621/101000 (8.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7410s / 685.9842 s
agent0:                 episode reward: -0.5384,                 loss: 0.1586
agent1:                 episode reward: 0.5384,                 loss: nan
Episode: 8641/101000 (8.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2144s / 688.1986 s
agent0:                 episode reward: -0.2862,                 loss: 0.1608
agent1:                 episode reward: 0.2862,                 loss: nan
Episode: 8661/101000 (8.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3777s / 690.5762 s
agent0:                 episode reward: 0.0217,                 loss: 0.1589
agent1:                 episode reward: -0.0217,                 loss: nan
Episode: 8681/101000 (8.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5414s / 692.1176 s
agent0:                 episode reward: -0.2788,                 loss: 0.1601
agent1:                 episode reward: 0.2788,                 loss: nan
Episode: 8701/101000 (8.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4014s / 693.5190 s
agent0:                 episode reward: -0.0716,                 loss: 0.1579
agent1:                 episode reward: 0.0716,                 loss: nan
Episode: 8721/101000 (8.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8702s / 695.3892 s
agent0:                 episode reward: -0.3096,                 loss: 0.1594
agent1:                 episode reward: 0.3096,                 loss: nan
Episode: 8741/101000 (8.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6700s / 697.0592 s
agent0:                 episode reward: -0.1026,                 loss: 0.1588
agent1:                 episode reward: 0.1026,                 loss: nan
Episode: 8761/101000 (8.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7924s / 698.8516 s
agent0:                 episode reward: -0.5493,                 loss: 0.1585
agent1:                 episode reward: 0.5493,                 loss: nan
Episode: 8781/101000 (8.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1577s / 701.0093 s
agent0:                 episode reward: -0.0507,                 loss: 0.1615
agent1:                 episode reward: 0.0507,                 loss: nan
Episode: 8801/101000 (8.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0444s / 703.0537 s
agent0:                 episode reward: -0.2451,                 loss: 0.1581
agent1:                 episode reward: 0.2451,                 loss: nan
Episode: 8821/101000 (8.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5375s / 704.5912 s
agent0:                 episode reward: 0.0332,                 loss: 0.1588
agent1:                 episode reward: -0.0332,                 loss: nan
Episode: 8841/101000 (8.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0164s / 706.6076 s
agent0:                 episode reward: 0.2291,                 loss: 0.1570
agent1:                 episode reward: -0.2291,                 loss: nan
Episode: 8861/101000 (8.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9203s / 708.5279 s
agent0:                 episode reward: -0.0963,                 loss: 0.1589
agent1:                 episode reward: 0.0963,                 loss: nan
Episode: 8881/101000 (8.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8315s / 710.3594 s
agent0:                 episode reward: -0.0323,                 loss: 0.1586
agent1:                 episode reward: 0.0323,                 loss: nan
Episode: 8901/101000 (8.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6415s / 712.0008 s
agent0:                 episode reward: -0.2996,                 loss: 0.1594
agent1:                 episode reward: 0.2996,                 loss: nan
Episode: 8921/101000 (8.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8243s / 713.8251 s
agent0:                 episode reward: 0.0302,                 loss: 0.1590
agent1:                 episode reward: -0.0302,                 loss: nan
Episode: 8941/101000 (8.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4074s / 716.2325 s
agent0:                 episode reward: 0.4108,                 loss: 0.1595
agent1:                 episode reward: -0.4108,                 loss: 0.1380
Score delta: 1.5715830868003535, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/8514_0.
Episode: 8961/101000 (8.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8040s / 718.0365 s
agent0:                 episode reward: 0.1156,                 loss: nan
agent1:                 episode reward: -0.1156,                 loss: 0.1411
Episode: 8981/101000 (8.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6150s / 719.6514 s
agent0:                 episode reward: 0.1554,                 loss: nan
agent1:                 episode reward: -0.1554,                 loss: 0.1428
Episode: 9001/101000 (8.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6880s / 721.3394 s
agent0:                 episode reward: -0.2005,                 loss: nan
agent1:                 episode reward: 0.2005,                 loss: 0.1449
Episode: 9021/101000 (8.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9045s / 723.2439 s
agent0:                 episode reward: 0.1638,                 loss: nan
agent1:                 episode reward: -0.1638,                 loss: 0.1451
Episode: 9041/101000 (8.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6340s / 724.8779 s
agent0:                 episode reward: -0.0457,                 loss: nan
agent1:                 episode reward: 0.0457,                 loss: 0.1466
Episode: 9061/101000 (8.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8004s / 726.6783 s
agent0:                 episode reward: -0.2085,                 loss: nan
agent1:                 episode reward: 0.2085,                 loss: 0.1474
Episode: 9081/101000 (8.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7873s / 728.4656 s
agent0:                 episode reward: -0.2650,                 loss: nan
agent1:                 episode reward: 0.2650,                 loss: 0.1568
Episode: 9101/101000 (9.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8207s / 730.2863 s
agent0:                 episode reward: 0.0895,                 loss: nan
agent1:                 episode reward: -0.0895,                 loss: 0.1558
Episode: 9121/101000 (9.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1053s / 731.3916 s
agent0:                 episode reward: 0.3039,                 loss: nan
agent1:                 episode reward: -0.3039,                 loss: 0.1548
Episode: 9141/101000 (9.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0085s / 733.4001 s
agent0:                 episode reward: -0.1369,                 loss: nan
agent1:                 episode reward: 0.1369,                 loss: 0.1565
Episode: 9161/101000 (9.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0762s / 735.4763 s
agent0:                 episode reward: -0.4983,                 loss: nan
agent1:                 episode reward: 0.4983,                 loss: 0.1556
Episode: 9181/101000 (9.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0471s / 737.5235 s
agent0:                 episode reward: -1.1429,                 loss: 0.1888
agent1:                 episode reward: 1.1429,                 loss: 0.1568
Score delta: 1.8242576656284508, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/8737_1.
Episode: 9201/101000 (9.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7671s / 739.2906 s
agent0:                 episode reward: -1.0334,                 loss: 0.1615
agent1:                 episode reward: 1.0334,                 loss: nan
Episode: 9221/101000 (9.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9085s / 741.1991 s
agent0:                 episode reward: -0.4994,                 loss: 0.1543
agent1:                 episode reward: 0.4994,                 loss: nan
Episode: 9241/101000 (9.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7708s / 742.9699 s
agent0:                 episode reward: -0.3853,                 loss: 0.1513
agent1:                 episode reward: 0.3853,                 loss: nan
Episode: 9261/101000 (9.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1902s / 745.1601 s
agent0:                 episode reward: -0.5780,                 loss: 0.1468
agent1:                 episode reward: 0.5780,                 loss: nan
Episode: 9281/101000 (9.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0269s / 747.1870 s
agent0:                 episode reward: -0.8472,                 loss: 0.1437
agent1:                 episode reward: 0.8472,                 loss: nan
Episode: 9301/101000 (9.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2641s / 749.4511 s
agent0:                 episode reward: -0.0391,                 loss: 0.1419
agent1:                 episode reward: 0.0391,                 loss: nan
Episode: 9321/101000 (9.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7578s / 751.2089 s
agent0:                 episode reward: 0.0624,                 loss: 0.1404
agent1:                 episode reward: -0.0624,                 loss: nan
Episode: 9341/101000 (9.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9739s / 753.1828 s
agent0:                 episode reward: -0.3438,                 loss: 0.1392
agent1:                 episode reward: 0.3438,                 loss: nan
Episode: 9361/101000 (9.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0185s / 755.2013 s
agent0:                 episode reward: -0.2044,                 loss: 0.1390
agent1:                 episode reward: 0.2044,                 loss: nan
Episode: 9381/101000 (9.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1184s / 757.3197 s
agent0:                 episode reward: -0.3368,                 loss: 0.1390
agent1:                 episode reward: 0.3368,                 loss: nan
Episode: 9401/101000 (9.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6113s / 758.9311 s
agent0:                 episode reward: -0.1081,                 loss: 0.1378
agent1:                 episode reward: 0.1081,                 loss: nan
Episode: 9421/101000 (9.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0652s / 760.9963 s
agent0:                 episode reward: -0.6321,                 loss: 0.1371
agent1:                 episode reward: 0.6321,                 loss: nan
Episode: 9441/101000 (9.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9960s / 762.9923 s
agent0:                 episode reward: -0.5936,                 loss: 0.1364
agent1:                 episode reward: 0.5936,                 loss: nan
Episode: 9461/101000 (9.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1018s / 765.0941 s
agent0:                 episode reward: 0.0666,                 loss: 0.1373
agent1:                 episode reward: -0.0666,                 loss: nan
Episode: 9481/101000 (9.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3897s / 766.4838 s
agent0:                 episode reward: -0.6095,                 loss: 0.1359
agent1:                 episode reward: 0.6095,                 loss: nan
Episode: 9501/101000 (9.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7294s / 768.2132 s
agent0:                 episode reward: -0.1236,                 loss: 0.1369
agent1:                 episode reward: 0.1236,                 loss: nan
Episode: 9521/101000 (9.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0947s / 770.3079 s
agent0:                 episode reward: 0.1970,                 loss: 0.1402
agent1:                 episode reward: -0.1970,                 loss: nan
Episode: 9541/101000 (9.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0820s / 772.3899 s
agent0:                 episode reward: -0.2093,                 loss: 0.1577
agent1:                 episode reward: 0.2093,                 loss: nan
Episode: 9561/101000 (9.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2604s / 774.6503 s
agent0:                 episode reward: -0.1112,                 loss: 0.1588
agent1:                 episode reward: 0.1112,                 loss: nan
Episode: 9581/101000 (9.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8000s / 776.4503 s
agent0:                 episode reward: -0.3017,                 loss: 0.1581
agent1:                 episode reward: 0.3017,                 loss: nan
Episode: 9601/101000 (9.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4570s / 777.9073 s
agent0:                 episode reward: -0.1412,                 loss: 0.1584
agent1:                 episode reward: 0.1412,                 loss: nan
Episode: 9621/101000 (9.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9430s / 779.8503 s
agent0:                 episode reward: 0.0863,                 loss: 0.1573
agent1:                 episode reward: -0.0863,                 loss: nan
Episode: 9641/101000 (9.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6417s / 781.4920 s
agent0:                 episode reward: -0.4803,                 loss: 0.1554
agent1:                 episode reward: 0.4803,                 loss: nan
Episode: 9661/101000 (9.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1782s / 783.6703 s
agent0:                 episode reward: -0.4590,                 loss: 0.1554
agent1:                 episode reward: 0.4590,                 loss: nan
Episode: 9681/101000 (9.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1627s / 785.8330 s
agent0:                 episode reward: -0.7953,                 loss: 0.1552
agent1:                 episode reward: 0.7953,                 loss: nan
Episode: 9701/101000 (9.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2342s / 788.0672 s
agent0:                 episode reward: -0.4065,                 loss: 0.1572
agent1:                 episode reward: 0.4065,                 loss: nan
Episode: 9721/101000 (9.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0510s / 790.1182 s
agent0:                 episode reward: -0.1572,                 loss: 0.1582
agent1:                 episode reward: 0.1572,                 loss: nan
Episode: 9741/101000 (9.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9008s / 792.0190 s
agent0:                 episode reward: -0.4253,                 loss: 0.1574
agent1:                 episode reward: 0.4253,                 loss: nan
Episode: 9761/101000 (9.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0339s / 794.0528 s
agent0:                 episode reward: -0.1969,                 loss: 0.1543
agent1:                 episode reward: 0.1969,                 loss: nan
Episode: 9781/101000 (9.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7817s / 795.8346 s
agent0:                 episode reward: -0.8313,                 loss: 0.1560
agent1:                 episode reward: 0.8313,                 loss: nan
Episode: 9801/101000 (9.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1129s / 797.9475 s
agent0:                 episode reward: -0.1281,                 loss: 0.1573
agent1:                 episode reward: 0.1281,                 loss: nan
Episode: 9821/101000 (9.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0717s / 800.0192 s
agent0:                 episode reward: -0.1078,                 loss: 0.1568
agent1:                 episode reward: 0.1078,                 loss: nan
Episode: 9841/101000 (9.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9374s / 801.9566 s
agent0:                 episode reward: -0.6482,                 loss: 0.1566
agent1:                 episode reward: 0.6482,                 loss: nan
Episode: 9861/101000 (9.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3610s / 804.3176 s
agent0:                 episode reward: 0.0005,                 loss: 0.1613
agent1:                 episode reward: -0.0005,                 loss: nan
Episode: 9881/101000 (9.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5259s / 805.8434 s
agent0:                 episode reward: -0.4106,                 loss: 0.1665
agent1:                 episode reward: 0.4106,                 loss: nan
Episode: 9901/101000 (9.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8231s / 807.6666 s
agent0:                 episode reward: -0.2351,                 loss: 0.1660
agent1:                 episode reward: 0.2351,                 loss: nan
Episode: 9921/101000 (9.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5516s / 809.2182 s
agent0:                 episode reward: -0.8515,                 loss: 0.1668
agent1:                 episode reward: 0.8515,                 loss: nan
Episode: 9941/101000 (9.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0240s / 811.2421 s
agent0:                 episode reward: -0.4936,                 loss: 0.1651
agent1:                 episode reward: 0.4936,                 loss: nan
Episode: 9961/101000 (9.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0982s / 813.3403 s
agent0:                 episode reward: -0.4381,                 loss: 0.1659
agent1:                 episode reward: 0.4381,                 loss: nan
Episode: 9981/101000 (9.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4795s / 814.8198 s
agent0:                 episode reward: -0.3198,                 loss: 0.1649
agent1:                 episode reward: 0.3198,                 loss: nan
Episode: 10001/101000 (9.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6926s / 816.5124 s
agent0:                 episode reward: -0.5337,                 loss: 0.1675
agent1:                 episode reward: 0.5337,                 loss: nan
Episode: 10021/101000 (9.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7775s / 818.2899 s
agent0:                 episode reward: -0.1398,                 loss: 0.1661
agent1:                 episode reward: 0.1398,                 loss: nan
Episode: 10041/101000 (9.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1184s / 820.4084 s
agent0:                 episode reward: -0.4411,                 loss: 0.1654
agent1:                 episode reward: 0.4411,                 loss: nan
Episode: 10061/101000 (9.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7245s / 822.1329 s
agent0:                 episode reward: 0.1130,                 loss: 0.1670
agent1:                 episode reward: -0.1130,                 loss: nan
Episode: 10081/101000 (9.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1229s / 824.2558 s
agent0:                 episode reward: -0.3138,                 loss: 0.1651
agent1:                 episode reward: 0.3138,                 loss: nan
Episode: 10101/101000 (10.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0380s / 826.2937 s
agent0:                 episode reward: -0.4076,                 loss: 0.1662
agent1:                 episode reward: 0.4076,                 loss: nan
Episode: 10121/101000 (10.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7961s / 828.0898 s
agent0:                 episode reward: -0.2786,                 loss: 0.1638
agent1:                 episode reward: 0.2786,                 loss: nan
Episode: 10141/101000 (10.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0537s / 830.1435 s
agent0:                 episode reward: -0.8649,                 loss: 0.1647
agent1:                 episode reward: 0.8649,                 loss: nan
Episode: 10161/101000 (10.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8892s / 832.0327 s
agent0:                 episode reward: -0.7650,                 loss: 0.1636
agent1:                 episode reward: 0.7650,                 loss: nan
Episode: 10181/101000 (10.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8972s / 833.9299 s
agent0:                 episode reward: -0.1842,                 loss: 0.1639
agent1:                 episode reward: 0.1842,                 loss: nan
Episode: 10201/101000 (10.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8778s / 835.8077 s
agent0:                 episode reward: 0.1454,                 loss: 0.1605
agent1:                 episode reward: -0.1454,                 loss: nan
Episode: 10221/101000 (10.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6402s / 838.4479 s
agent0:                 episode reward: -0.2835,                 loss: 0.1596
agent1:                 episode reward: 0.2835,                 loss: nan
Episode: 10241/101000 (10.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9010s / 840.3488 s
agent0:                 episode reward: -0.3769,                 loss: 0.1569
agent1:                 episode reward: 0.3769,                 loss: nan
Episode: 10261/101000 (10.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0853s / 842.4341 s
agent0:                 episode reward: -0.2249,                 loss: 0.1586
agent1:                 episode reward: 0.2249,                 loss: nan
Episode: 10281/101000 (10.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7240s / 844.1581 s
agent0:                 episode reward: 0.0390,                 loss: 0.1598
agent1:                 episode reward: -0.0390,                 loss: nan
Episode: 10301/101000 (10.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4273s / 845.5855 s
agent0:                 episode reward: -0.3023,                 loss: 0.1610
agent1:                 episode reward: 0.3023,                 loss: nan
Episode: 10321/101000 (10.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0593s / 847.6448 s
agent0:                 episode reward: 0.0051,                 loss: 0.1588
agent1:                 episode reward: -0.0051,                 loss: nan
Episode: 10341/101000 (10.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7142s / 849.3590 s
agent0:                 episode reward: -0.1917,                 loss: 0.1582
agent1:                 episode reward: 0.1917,                 loss: nan
Episode: 10361/101000 (10.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0498s / 851.4088 s
agent0:                 episode reward: -0.4858,                 loss: 0.1613
agent1:                 episode reward: 0.4858,                 loss: nan
Episode: 10381/101000 (10.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6136s / 853.0224 s
agent0:                 episode reward: -0.2798,                 loss: 0.1606
agent1:                 episode reward: 0.2798,                 loss: nan
Episode: 10401/101000 (10.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9491s / 854.9715 s
agent0:                 episode reward: -0.1862,                 loss: 0.1601
agent1:                 episode reward: 0.1862,                 loss: nan
Episode: 10421/101000 (10.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0083s / 856.9798 s
agent0:                 episode reward: -0.2464,                 loss: 0.1589
agent1:                 episode reward: 0.2464,                 loss: nan
Episode: 10441/101000 (10.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7937s / 858.7735 s
agent0:                 episode reward: -0.0055,                 loss: 0.1588
agent1:                 episode reward: 0.0055,                 loss: nan
Episode: 10461/101000 (10.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7013s / 860.4748 s
agent0:                 episode reward: 0.0457,                 loss: 0.1566
agent1:                 episode reward: -0.0457,                 loss: nan
Episode: 10481/101000 (10.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3907s / 862.8655 s
agent0:                 episode reward: -0.2841,                 loss: 0.1591
agent1:                 episode reward: 0.2841,                 loss: nan
Episode: 10501/101000 (10.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0214s / 864.8870 s
agent0:                 episode reward: -0.4626,                 loss: 0.1590
agent1:                 episode reward: 0.4626,                 loss: nan
Episode: 10521/101000 (10.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7426s / 867.6296 s
agent0:                 episode reward: 0.0053,                 loss: 0.1585
agent1:                 episode reward: -0.0053,                 loss: nan
Episode: 10541/101000 (10.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8774s / 869.5069 s
agent0:                 episode reward: 0.2900,                 loss: 0.1614
agent1:                 episode reward: -0.2900,                 loss: nan
Episode: 10561/101000 (10.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7419s / 871.2488 s
agent0:                 episode reward: -0.1423,                 loss: 0.1610
agent1:                 episode reward: 0.1423,                 loss: nan
Episode: 10581/101000 (10.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8739s / 873.1227 s
agent0:                 episode reward: 0.3326,                 loss: 0.1621
agent1:                 episode reward: -0.3326,                 loss: nan
Episode: 10601/101000 (10.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7882s / 874.9110 s
agent0:                 episode reward: -0.4346,                 loss: 0.1618
agent1:                 episode reward: 0.4346,                 loss: nan
Episode: 10621/101000 (10.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8581s / 876.7691 s
agent0:                 episode reward: -0.1928,                 loss: 0.1595
agent1:                 episode reward: 0.1928,                 loss: nan
Episode: 10641/101000 (10.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7591s / 878.5282 s
agent0:                 episode reward: -0.0266,                 loss: 0.1615
agent1:                 episode reward: 0.0266,                 loss: nan
Episode: 10661/101000 (10.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9542s / 880.4823 s
agent0:                 episode reward: -0.2323,                 loss: 0.1623
agent1:                 episode reward: 0.2323,                 loss: nan
Episode: 10681/101000 (10.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6246s / 882.1069 s
agent0:                 episode reward: -0.0384,                 loss: 0.1611
agent1:                 episode reward: 0.0384,                 loss: nan
Episode: 10701/101000 (10.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3478s / 884.4548 s
agent0:                 episode reward: -0.3711,                 loss: 0.1601
agent1:                 episode reward: 0.3711,                 loss: nan
Episode: 10721/101000 (10.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5432s / 886.9979 s
agent0:                 episode reward: -0.2391,                 loss: 0.1593
agent1:                 episode reward: 0.2391,                 loss: nan
Episode: 10741/101000 (10.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7494s / 888.7473 s
agent0:                 episode reward: -0.2414,                 loss: 0.1616
agent1:                 episode reward: 0.2414,                 loss: nan
Episode: 10761/101000 (10.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9845s / 890.7318 s
agent0:                 episode reward: -0.0543,                 loss: 0.1599
agent1:                 episode reward: 0.0543,                 loss: nan
Episode: 10781/101000 (10.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0902s / 892.8220 s
agent0:                 episode reward: -0.2504,                 loss: 0.1622
agent1:                 episode reward: 0.2504,                 loss: nan
Episode: 10801/101000 (10.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0216s / 894.8436 s
agent0:                 episode reward: 0.0245,                 loss: 0.1635
agent1:                 episode reward: -0.0245,                 loss: nan
Episode: 10821/101000 (10.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4132s / 897.2568 s
agent0:                 episode reward: 0.1563,                 loss: 0.1601
agent1:                 episode reward: -0.1563,                 loss: 0.1814
Score delta: 1.6513499903260147, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/10394_0.
Episode: 10841/101000 (10.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0691s / 899.3259 s
agent0:                 episode reward: -0.2348,                 loss: nan
agent1:                 episode reward: 0.2348,                 loss: 0.1720
Episode: 10861/101000 (10.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6266s / 900.9526 s
agent0:                 episode reward: -0.2638,                 loss: nan
agent1:                 episode reward: 0.2638,                 loss: 0.1694
Episode: 10881/101000 (10.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7860s / 902.7385 s
agent0:                 episode reward: -0.2605,                 loss: nan
agent1:                 episode reward: 0.2605,                 loss: 0.1679
Episode: 10901/101000 (10.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9813s / 904.7199 s
agent0:                 episode reward: 0.1676,                 loss: nan
agent1:                 episode reward: -0.1676,                 loss: 0.1661
Episode: 10921/101000 (10.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9415s / 906.6614 s
agent0:                 episode reward: 0.0275,                 loss: nan
agent1:                 episode reward: -0.0275,                 loss: 0.1662
Episode: 10941/101000 (10.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5457s / 908.2071 s
agent0:                 episode reward: 0.0863,                 loss: nan
agent1:                 episode reward: -0.0863,                 loss: 0.1656
Episode: 10961/101000 (10.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0166s / 910.2237 s
agent0:                 episode reward: 0.2852,                 loss: nan
agent1:                 episode reward: -0.2852,                 loss: 0.1653
Episode: 10981/101000 (10.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7088s / 911.9325 s
agent0:                 episode reward: 0.0593,                 loss: nan
agent1:                 episode reward: -0.0593,                 loss: 0.1672
Episode: 11001/101000 (10.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8168s / 913.7493 s
agent0:                 episode reward: -0.2005,                 loss: nan
agent1:                 episode reward: 0.2005,                 loss: 0.1656
Episode: 11021/101000 (10.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6728s / 915.4221 s
agent0:                 episode reward: -0.2553,                 loss: nan
agent1:                 episode reward: 0.2553,                 loss: 0.1650
Episode: 11041/101000 (10.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6817s / 917.1038 s
agent0:                 episode reward: -0.0334,                 loss: 0.1589
agent1:                 episode reward: 0.0334,                 loss: 0.1640
Score delta: 1.5184278540768952, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/10599_1.
Episode: 11061/101000 (10.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1175s / 919.2213 s
agent0:                 episode reward: 0.0199,                 loss: 0.1599
agent1:                 episode reward: -0.0199,                 loss: 0.1879
Score delta: 1.5613043264704927, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/10623_0.
Episode: 11081/101000 (10.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1869s / 921.4083 s
agent0:                 episode reward: 0.0493,                 loss: nan
agent1:                 episode reward: -0.0493,                 loss: 0.1785
Episode: 11101/101000 (10.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5684s / 922.9767 s
agent0:                 episode reward: 0.0844,                 loss: nan
agent1:                 episode reward: -0.0844,                 loss: 0.1780
Episode: 11121/101000 (11.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7101s / 924.6868 s
agent0:                 episode reward: -0.2006,                 loss: nan
agent1:                 episode reward: 0.2006,                 loss: 0.1719
Episode: 11141/101000 (11.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6676s / 926.3544 s
agent0:                 episode reward: -0.1701,                 loss: nan
agent1:                 episode reward: 0.1701,                 loss: 0.1714
Episode: 11161/101000 (11.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2284s / 928.5829 s
agent0:                 episode reward: -0.0148,                 loss: nan
agent1:                 episode reward: 0.0148,                 loss: 0.1689
Episode: 11181/101000 (11.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8443s / 930.4272 s
agent0:                 episode reward: 0.0363,                 loss: nan
agent1:                 episode reward: -0.0363,                 loss: 0.1687
Episode: 11201/101000 (11.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0261s / 932.4533 s
agent0:                 episode reward: -0.1057,                 loss: nan
agent1:                 episode reward: 0.1057,                 loss: 0.1650
Episode: 11221/101000 (11.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7847s / 934.2381 s
agent0:                 episode reward: -0.1863,                 loss: 0.1605
agent1:                 episode reward: 0.1863,                 loss: 0.1682
Score delta: 1.5294806211972767, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/10785_1.
Episode: 11241/101000 (11.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9406s / 936.1786 s
agent0:                 episode reward: 0.0309,                 loss: 0.1590
agent1:                 episode reward: -0.0309,                 loss: nan
Episode: 11261/101000 (11.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3542s / 938.5329 s
agent0:                 episode reward: -0.2161,                 loss: 0.1568
agent1:                 episode reward: 0.2161,                 loss: nan
Episode: 11281/101000 (11.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1412s / 940.6741 s
agent0:                 episode reward: -0.0328,                 loss: 0.1584
agent1:                 episode reward: 0.0328,                 loss: nan
Episode: 11301/101000 (11.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8107s / 942.4848 s
agent0:                 episode reward: -0.4752,                 loss: 0.1590
agent1:                 episode reward: 0.4752,                 loss: nan
Episode: 11321/101000 (11.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4053s / 944.8902 s
agent0:                 episode reward: -0.6791,                 loss: 0.1583
agent1:                 episode reward: 0.6791,                 loss: nan
Episode: 11341/101000 (11.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5049s / 946.3950 s
agent0:                 episode reward: 0.2799,                 loss: 0.1588
agent1:                 episode reward: -0.2799,                 loss: nan
Episode: 11361/101000 (11.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0975s / 948.4925 s
agent0:                 episode reward: -0.5560,                 loss: 0.1584
agent1:                 episode reward: 0.5560,                 loss: nan
Episode: 11381/101000 (11.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1143s / 950.6068 s
agent0:                 episode reward: -0.0815,                 loss: 0.1596
agent1:                 episode reward: 0.0815,                 loss: nan
Episode: 11401/101000 (11.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4119s / 953.0186 s
agent0:                 episode reward: -0.2942,                 loss: 0.1569
agent1:                 episode reward: 0.2942,                 loss: nan
Episode: 11421/101000 (11.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9730s / 954.9916 s
agent0:                 episode reward: -0.0740,                 loss: 0.1590
agent1:                 episode reward: 0.0740,                 loss: nan
Episode: 11441/101000 (11.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7516s / 956.7432 s
agent0:                 episode reward: -0.1765,                 loss: 0.1589
agent1:                 episode reward: 0.1765,                 loss: nan
Episode: 11461/101000 (11.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0098s / 958.7530 s
agent0:                 episode reward: -0.2361,                 loss: 0.1578
agent1:                 episode reward: 0.2361,                 loss: nan
Episode: 11481/101000 (11.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9138s / 960.6669 s
agent0:                 episode reward: -0.4709,                 loss: 0.1590
agent1:                 episode reward: 0.4709,                 loss: nan
Episode: 11501/101000 (11.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8978s / 962.5647 s
agent0:                 episode reward: -0.3238,                 loss: 0.1581
agent1:                 episode reward: 0.3238,                 loss: nan
Episode: 11521/101000 (11.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9963s / 964.5609 s
agent0:                 episode reward: -0.0217,                 loss: 0.1587
agent1:                 episode reward: 0.0217,                 loss: nan
Episode: 11541/101000 (11.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9882s / 966.5491 s
agent0:                 episode reward: -0.1181,                 loss: 0.1599
agent1:                 episode reward: 0.1181,                 loss: nan
Episode: 11561/101000 (11.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5790s / 968.1281 s
agent0:                 episode reward: -0.4944,                 loss: 0.1601
agent1:                 episode reward: 0.4944,                 loss: nan
Episode: 11581/101000 (11.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2826s / 970.4107 s
agent0:                 episode reward: 0.0396,                 loss: 0.1633
agent1:                 episode reward: -0.0396,                 loss: nan
Episode: 11601/101000 (11.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3572s / 972.7679 s
agent0:                 episode reward: -0.5172,                 loss: 0.1603
agent1:                 episode reward: 0.5172,                 loss: nan
Episode: 11621/101000 (11.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8263s / 974.5942 s
agent0:                 episode reward: -0.3098,                 loss: 0.1627
agent1:                 episode reward: 0.3098,                 loss: nan
Episode: 11641/101000 (11.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5739s / 976.1681 s
agent0:                 episode reward: 0.0257,                 loss: 0.1629
agent1:                 episode reward: -0.0257,                 loss: nan
Episode: 11661/101000 (11.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5004s / 977.6685 s
agent0:                 episode reward: -0.3697,                 loss: 0.1598
agent1:                 episode reward: 0.3697,                 loss: nan
Episode: 11681/101000 (11.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9019s / 979.5705 s
agent0:                 episode reward: 0.0093,                 loss: 0.1616
agent1:                 episode reward: -0.0093,                 loss: nan
Episode: 11701/101000 (11.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0852s / 981.6557 s
agent0:                 episode reward: 0.1301,                 loss: 0.1638
agent1:                 episode reward: -0.1301,                 loss: nan
Episode: 11721/101000 (11.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4087s / 983.0643 s
agent0:                 episode reward: -0.0197,                 loss: 0.1632
agent1:                 episode reward: 0.0197,                 loss: nan
Episode: 11741/101000 (11.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9121s / 984.9764 s
agent0:                 episode reward: -0.2235,                 loss: 0.1621
agent1:                 episode reward: 0.2235,                 loss: nan
Episode: 11761/101000 (11.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0396s / 987.0160 s
agent0:                 episode reward: -0.1175,                 loss: 0.1620
agent1:                 episode reward: 0.1175,                 loss: nan
Episode: 11781/101000 (11.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0219s / 989.0379 s
agent0:                 episode reward: -0.1776,                 loss: 0.1621
agent1:                 episode reward: 0.1776,                 loss: nan
Episode: 11801/101000 (11.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0756s / 991.1135 s
agent0:                 episode reward: -0.1580,                 loss: 0.1614
agent1:                 episode reward: 0.1580,                 loss: nan
Episode: 11821/101000 (11.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6572s / 992.7707 s
agent0:                 episode reward: -0.0867,                 loss: 0.1618
agent1:                 episode reward: 0.0867,                 loss: nan
Episode: 11841/101000 (11.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5409s / 994.3116 s
agent0:                 episode reward: -0.3086,                 loss: 0.1622
agent1:                 episode reward: 0.3086,                 loss: nan
Episode: 11861/101000 (11.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1274s / 996.4390 s
agent0:                 episode reward: -0.3418,                 loss: 0.1625
agent1:                 episode reward: 0.3418,                 loss: nan
Episode: 11881/101000 (11.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9326s / 998.3716 s
agent0:                 episode reward: -0.1293,                 loss: 0.1598
agent1:                 episode reward: 0.1293,                 loss: nan
Episode: 11901/101000 (11.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2236s / 1000.5952 s
agent0:                 episode reward: -0.0641,                 loss: 0.1605
agent1:                 episode reward: 0.0641,                 loss: nan
Episode: 11921/101000 (11.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2436s / 1002.8388 s
agent0:                 episode reward: -0.2956,                 loss: 0.1580
agent1:                 episode reward: 0.2956,                 loss: nan
Episode: 11941/101000 (11.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1242s / 1004.9630 s
agent0:                 episode reward: -0.0648,                 loss: 0.1600
agent1:                 episode reward: 0.0648,                 loss: nan
Episode: 11961/101000 (11.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9375s / 1006.9005 s
agent0:                 episode reward: -0.3228,                 loss: 0.1606
agent1:                 episode reward: 0.3228,                 loss: nan
Episode: 11981/101000 (11.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8071s / 1008.7075 s
agent0:                 episode reward: 0.1694,                 loss: 0.1580
agent1:                 episode reward: -0.1694,                 loss: nan
Episode: 12001/101000 (11.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9353s / 1010.6428 s
agent0:                 episode reward: -0.0515,                 loss: 0.1598
agent1:                 episode reward: 0.0515,                 loss: nan
Episode: 12021/101000 (11.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 1.3271s / 1011.9699 s
agent0:                 episode reward: -0.4323,                 loss: 0.1611
agent1:                 episode reward: 0.4323,                 loss: nan
Episode: 12041/101000 (11.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7726s / 1013.7425 s
agent0:                 episode reward: -0.2585,                 loss: 0.1610
agent1:                 episode reward: 0.2585,                 loss: nan
Episode: 12061/101000 (11.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8719s / 1015.6145 s
agent0:                 episode reward: 0.1140,                 loss: 0.1597
agent1:                 episode reward: -0.1140,                 loss: nan
Episode: 12081/101000 (11.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4207s / 1017.0351 s
agent0:                 episode reward: -0.3506,                 loss: 0.1587
agent1:                 episode reward: 0.3506,                 loss: nan
Episode: 12101/101000 (11.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2396s / 1019.2747 s
agent0:                 episode reward: -0.3023,                 loss: 0.1600
agent1:                 episode reward: 0.3023,                 loss: nan
Episode: 12121/101000 (12.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9203s / 1021.1950 s
agent0:                 episode reward: -0.0475,                 loss: 0.1606
agent1:                 episode reward: 0.0475,                 loss: nan
Episode: 12141/101000 (12.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6615s / 1022.8565 s
agent0:                 episode reward: -0.2688,                 loss: 0.1602
agent1:                 episode reward: 0.2688,                 loss: nan
Episode: 12161/101000 (12.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8315s / 1024.6881 s
agent0:                 episode reward: -0.0164,                 loss: 0.1603
agent1:                 episode reward: 0.0164,                 loss: nan
Episode: 12181/101000 (12.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.1154s / 1025.8035 s
agent0:                 episode reward: -0.2495,                 loss: 0.1611
agent1:                 episode reward: 0.2495,                 loss: nan
Episode: 12201/101000 (12.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0740s / 1027.8775 s
agent0:                 episode reward: -0.0958,                 loss: 0.1602
agent1:                 episode reward: 0.0958,                 loss: nan
Episode: 12221/101000 (12.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0338s / 1029.9113 s
agent0:                 episode reward: -0.0742,                 loss: 0.1615
agent1:                 episode reward: 0.0742,                 loss: nan
Episode: 12241/101000 (12.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8247s / 1031.7360 s
agent0:                 episode reward: 0.2116,                 loss: 0.1570
agent1:                 episode reward: -0.2116,                 loss: nan
Episode: 12261/101000 (12.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8811s / 1033.6171 s
agent0:                 episode reward: -0.2654,                 loss: 0.1559
agent1:                 episode reward: 0.2654,                 loss: nan
Episode: 12281/101000 (12.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0947s / 1035.7118 s
agent0:                 episode reward: -0.5876,                 loss: 0.1592
agent1:                 episode reward: 0.5876,                 loss: nan
Episode: 12301/101000 (12.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8569s / 1037.5688 s
agent0:                 episode reward: -0.6781,                 loss: 0.1579
agent1:                 episode reward: 0.6781,                 loss: nan
Episode: 12321/101000 (12.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8756s / 1039.4444 s
agent0:                 episode reward: -0.4080,                 loss: 0.1548
agent1:                 episode reward: 0.4080,                 loss: nan
Episode: 12341/101000 (12.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8888s / 1041.3332 s
agent0:                 episode reward: 0.2406,                 loss: 0.1588
agent1:                 episode reward: -0.2406,                 loss: nan
Episode: 12361/101000 (12.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8398s / 1043.1730 s
agent0:                 episode reward: 0.1974,                 loss: 0.1590
agent1:                 episode reward: -0.1974,                 loss: 0.1649
Score delta: 1.5470141459519327, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/11919_0.
Episode: 12381/101000 (12.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3852s / 1045.5582 s
agent0:                 episode reward: -0.8186,                 loss: 0.2021
agent1:                 episode reward: 0.8186,                 loss: 0.1634
Score delta: 1.5901977693015616, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/11942_1.
Episode: 12401/101000 (12.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9127s / 1047.4709 s
agent0:                 episode reward: -0.7474,                 loss: 0.1733
agent1:                 episode reward: 0.7474,                 loss: nan
Episode: 12421/101000 (12.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1169s / 1049.5878 s
agent0:                 episode reward: -0.6254,                 loss: 0.1605
agent1:                 episode reward: 0.6254,                 loss: nan
Episode: 12441/101000 (12.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0675s / 1051.6554 s
agent0:                 episode reward: -0.2854,                 loss: 0.1573
agent1:                 episode reward: 0.2854,                 loss: nan
Episode: 12461/101000 (12.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0363s / 1053.6917 s
agent0:                 episode reward: -0.5770,                 loss: 0.1529
agent1:                 episode reward: 0.5770,                 loss: nan
Episode: 12481/101000 (12.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1555s / 1055.8472 s
agent0:                 episode reward: -0.2792,                 loss: 0.1504
agent1:                 episode reward: 0.2792,                 loss: nan
Episode: 12501/101000 (12.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2195s / 1058.0667 s
agent0:                 episode reward: -0.7120,                 loss: 0.1488
agent1:                 episode reward: 0.7120,                 loss: nan
Episode: 12521/101000 (12.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8203s / 1059.8870 s
agent0:                 episode reward: -0.6137,                 loss: 0.1455
agent1:                 episode reward: 0.6137,                 loss: nan
Episode: 12541/101000 (12.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5880s / 1061.4751 s
agent0:                 episode reward: -0.4732,                 loss: 0.1440
agent1:                 episode reward: 0.4732,                 loss: nan
Episode: 12561/101000 (12.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0265s / 1063.5015 s
agent0:                 episode reward: -0.0300,                 loss: 0.1440
agent1:                 episode reward: 0.0300,                 loss: nan
Episode: 12581/101000 (12.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1180s / 1065.6195 s
agent0:                 episode reward: -0.2219,                 loss: 0.1516
agent1:                 episode reward: 0.2219,                 loss: nan
Episode: 12601/101000 (12.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6520s / 1067.2715 s
agent0:                 episode reward: -0.3358,                 loss: 0.1630
agent1:                 episode reward: 0.3358,                 loss: nan
Episode: 12621/101000 (12.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9420s / 1069.2135 s
agent0:                 episode reward: -0.3626,                 loss: 0.1603
agent1:                 episode reward: 0.3626,                 loss: nan
Episode: 12641/101000 (12.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9544s / 1071.1680 s
agent0:                 episode reward: -0.5496,                 loss: 0.1589
agent1:                 episode reward: 0.5496,                 loss: nan
Episode: 12661/101000 (12.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0725s / 1073.2404 s
agent0:                 episode reward: -0.3482,                 loss: 0.1597
agent1:                 episode reward: 0.3482,                 loss: nan
Episode: 12681/101000 (12.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0088s / 1075.2492 s
agent0:                 episode reward: -0.4537,                 loss: 0.1580
agent1:                 episode reward: 0.4537,                 loss: nan
Episode: 12701/101000 (12.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2797s / 1077.5289 s
agent0:                 episode reward: -0.4467,                 loss: 0.1593
agent1:                 episode reward: 0.4467,                 loss: nan
Episode: 12721/101000 (12.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8959s / 1079.4248 s
agent0:                 episode reward: -0.6090,                 loss: 0.1586
agent1:                 episode reward: 0.6090,                 loss: nan
Episode: 12741/101000 (12.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3291s / 1081.7539 s
agent0:                 episode reward: -0.8214,                 loss: 0.1609
agent1:                 episode reward: 0.8214,                 loss: nan
Episode: 12761/101000 (12.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5880s / 1083.3419 s
agent0:                 episode reward: -0.2036,                 loss: 0.1600
agent1:                 episode reward: 0.2036,                 loss: nan
Episode: 12781/101000 (12.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3083s / 1085.6502 s
agent0:                 episode reward: -0.3740,                 loss: 0.1605
agent1:                 episode reward: 0.3740,                 loss: nan
Episode: 12801/101000 (12.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4002s / 1087.0503 s
agent0:                 episode reward: -0.5177,                 loss: 0.1577
agent1:                 episode reward: 0.5177,                 loss: nan
Episode: 12821/101000 (12.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4900s / 1088.5403 s
agent0:                 episode reward: -0.5351,                 loss: 0.1569
agent1:                 episode reward: 0.5351,                 loss: nan
Episode: 12841/101000 (12.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1318s / 1090.6721 s
agent0:                 episode reward: -0.6695,                 loss: 0.1595
agent1:                 episode reward: 0.6695,                 loss: nan
Episode: 12861/101000 (12.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9287s / 1092.6008 s
agent0:                 episode reward: -0.1034,                 loss: 0.1568
agent1:                 episode reward: 0.1034,                 loss: nan
Episode: 12881/101000 (12.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2223s / 1094.8231 s
agent0:                 episode reward: -0.3902,                 loss: 0.1574
agent1:                 episode reward: 0.3902,                 loss: nan
Episode: 12901/101000 (12.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7376s / 1096.5608 s
agent0:                 episode reward: -0.4530,                 loss: 0.1580
agent1:                 episode reward: 0.4530,                 loss: nan
Episode: 12921/101000 (12.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6808s / 1098.2416 s
agent0:                 episode reward: -0.9427,                 loss: 0.1637
agent1:                 episode reward: 0.9427,                 loss: nan
Episode: 12941/101000 (12.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4626s / 1100.7042 s
agent0:                 episode reward: -0.4463,                 loss: 0.1605
agent1:                 episode reward: 0.4463,                 loss: nan
Episode: 12961/101000 (12.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5502s / 1102.2544 s
agent0:                 episode reward: -0.4579,                 loss: 0.1618
agent1:                 episode reward: 0.4579,                 loss: nan
Episode: 12981/101000 (12.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0238s / 1104.2782 s
agent0:                 episode reward: -0.6444,                 loss: 0.1620
agent1:                 episode reward: 0.6444,                 loss: nan
Episode: 13001/101000 (12.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7779s / 1106.0561 s
agent0:                 episode reward: -0.5922,                 loss: 0.1620
agent1:                 episode reward: 0.5922,                 loss: nan
Episode: 13021/101000 (12.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6075s / 1107.6636 s
agent0:                 episode reward: 0.2495,                 loss: 0.1618
agent1:                 episode reward: -0.2495,                 loss: nan
Episode: 13041/101000 (12.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0266s / 1109.6902 s
agent0:                 episode reward: -0.2226,                 loss: 0.1602
agent1:                 episode reward: 0.2226,                 loss: nan
Episode: 13061/101000 (12.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2727s / 1111.9629 s
agent0:                 episode reward: 0.0124,                 loss: 0.1606
agent1:                 episode reward: -0.0124,                 loss: nan
Episode: 13081/101000 (12.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8309s / 1113.7938 s
agent0:                 episode reward: -0.4058,                 loss: 0.1592
agent1:                 episode reward: 0.4058,                 loss: nan
Episode: 13101/101000 (12.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9447s / 1115.7385 s
agent0:                 episode reward: -0.7657,                 loss: 0.1607
agent1:                 episode reward: 0.7657,                 loss: nan
Episode: 13121/101000 (12.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9193s / 1117.6577 s
agent0:                 episode reward: -0.8867,                 loss: 0.1591
agent1:                 episode reward: 0.8867,                 loss: nan
Episode: 13141/101000 (13.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1239s / 1119.7816 s
agent0:                 episode reward: -0.4308,                 loss: 0.1615
agent1:                 episode reward: 0.4308,                 loss: nan
Episode: 13161/101000 (13.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8702s / 1121.6518 s
agent0:                 episode reward: -0.3295,                 loss: 0.1598
agent1:                 episode reward: 0.3295,                 loss: nan
Episode: 13181/101000 (13.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0416s / 1123.6935 s
agent0:                 episode reward: -0.3061,                 loss: 0.1635
agent1:                 episode reward: 0.3061,                 loss: nan
Episode: 13201/101000 (13.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8701s / 1125.5636 s
agent0:                 episode reward: -0.3108,                 loss: 0.1623
agent1:                 episode reward: 0.3108,                 loss: nan
Episode: 13221/101000 (13.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3374s / 1127.9010 s
agent0:                 episode reward: -0.0880,                 loss: 0.1596
agent1:                 episode reward: 0.0880,                 loss: nan
Episode: 13241/101000 (13.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8714s / 1129.7724 s
agent0:                 episode reward: -0.3183,                 loss: 0.1593
agent1:                 episode reward: 0.3183,                 loss: nan
Episode: 13261/101000 (13.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2515s / 1132.0239 s
agent0:                 episode reward: -0.1097,                 loss: 0.1598
agent1:                 episode reward: 0.1097,                 loss: nan
Episode: 13281/101000 (13.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8250s / 1133.8489 s
agent0:                 episode reward: -0.1518,                 loss: 0.1601
agent1:                 episode reward: 0.1518,                 loss: nan
Episode: 13301/101000 (13.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1315s / 1135.9803 s
agent0:                 episode reward: -0.5078,                 loss: 0.1598
agent1:                 episode reward: 0.5078,                 loss: nan
Episode: 13321/101000 (13.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6355s / 1138.6158 s
agent0:                 episode reward: -0.5328,                 loss: 0.1600
agent1:                 episode reward: 0.5328,                 loss: nan
Episode: 13341/101000 (13.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9855s / 1140.6013 s
agent0:                 episode reward: 0.0313,                 loss: 0.1609
agent1:                 episode reward: -0.0313,                 loss: nan
Episode: 13361/101000 (13.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9374s / 1142.5388 s
agent0:                 episode reward: -0.1322,                 loss: 0.1608
agent1:                 episode reward: 0.1322,                 loss: nan
Episode: 13381/101000 (13.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9575s / 1144.4963 s
agent0:                 episode reward: -0.1763,                 loss: 0.1592
agent1:                 episode reward: 0.1763,                 loss: nan
Episode: 13401/101000 (13.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1506s / 1146.6468 s
agent0:                 episode reward: -0.2157,                 loss: 0.1594
agent1:                 episode reward: 0.2157,                 loss: nan
Episode: 13421/101000 (13.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9142s / 1148.5611 s
agent0:                 episode reward: -0.1452,                 loss: 0.1581
agent1:                 episode reward: 0.1452,                 loss: nan
Episode: 13441/101000 (13.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3777s / 1150.9387 s
agent0:                 episode reward: -0.3400,                 loss: 0.1594
agent1:                 episode reward: 0.3400,                 loss: nan
Episode: 13461/101000 (13.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0726s / 1153.0113 s
agent0:                 episode reward: -0.4514,                 loss: 0.1606
agent1:                 episode reward: 0.4514,                 loss: nan
Episode: 13481/101000 (13.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9162s / 1154.9276 s
agent0:                 episode reward: -0.5479,                 loss: 0.1602
agent1:                 episode reward: 0.5479,                 loss: nan
Episode: 13501/101000 (13.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8764s / 1156.8040 s
agent0:                 episode reward: -0.4895,                 loss: 0.1594
agent1:                 episode reward: 0.4895,                 loss: nan
Episode: 13521/101000 (13.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8150s / 1158.6190 s
agent0:                 episode reward: -0.1483,                 loss: 0.1598
agent1:                 episode reward: 0.1483,                 loss: nan
Episode: 13541/101000 (13.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9093s / 1160.5282 s
agent0:                 episode reward: -0.1871,                 loss: 0.1592
agent1:                 episode reward: 0.1871,                 loss: nan
Episode: 13561/101000 (13.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0455s / 1162.5738 s
agent0:                 episode reward: -0.2697,                 loss: 0.1596
agent1:                 episode reward: 0.2697,                 loss: nan
Episode: 13581/101000 (13.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1379s / 1164.7116 s
agent0:                 episode reward: 0.0803,                 loss: 0.1606
agent1:                 episode reward: -0.0803,                 loss: nan
Episode: 13601/101000 (13.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2189s / 1166.9305 s
agent0:                 episode reward: -0.3755,                 loss: 0.1641
agent1:                 episode reward: 0.3755,                 loss: nan
Episode: 13621/101000 (13.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1182s / 1169.0487 s
agent0:                 episode reward: 0.4265,                 loss: 0.1645
agent1:                 episode reward: -0.4265,                 loss: 0.1701
Score delta: 1.5586108123679883, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/13190_0.
Episode: 13641/101000 (13.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7940s / 1170.8427 s
agent0:                 episode reward: -0.1600,                 loss: nan
agent1:                 episode reward: 0.1600,                 loss: 0.1708
Episode: 13661/101000 (13.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1480s / 1172.9907 s
agent0:                 episode reward: -0.0371,                 loss: nan
agent1:                 episode reward: 0.0371,                 loss: 0.1705
Episode: 13681/101000 (13.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8679s / 1174.8586 s
agent0:                 episode reward: -0.3264,                 loss: 0.1600
agent1:                 episode reward: 0.3264,                 loss: 0.1696
Score delta: 1.5498522478724557, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/13248_1.
Episode: 13701/101000 (13.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9424s / 1176.8010 s
agent0:                 episode reward: -0.7447,                 loss: 0.1633
agent1:                 episode reward: 0.7447,                 loss: nan
Episode: 13721/101000 (13.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8248s / 1178.6258 s
agent0:                 episode reward: -0.1102,                 loss: 0.1643
agent1:                 episode reward: 0.1102,                 loss: nan
Episode: 13741/101000 (13.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1640s / 1180.7898 s
agent0:                 episode reward: -0.1070,                 loss: 0.1648
agent1:                 episode reward: 0.1070,                 loss: nan
Episode: 13761/101000 (13.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0459s / 1182.8357 s
agent0:                 episode reward: 0.0733,                 loss: 0.1636
agent1:                 episode reward: -0.0733,                 loss: nan
Episode: 13781/101000 (13.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8700s / 1184.7057 s
agent0:                 episode reward: -0.3504,                 loss: 0.1627
agent1:                 episode reward: 0.3504,                 loss: nan
Episode: 13801/101000 (13.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9519s / 1186.6577 s
agent0:                 episode reward: -0.4339,                 loss: 0.1667
agent1:                 episode reward: 0.4339,                 loss: nan
Episode: 13821/101000 (13.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1339s / 1188.7916 s
agent0:                 episode reward: -0.1466,                 loss: 0.1641
agent1:                 episode reward: 0.1466,                 loss: nan
Episode: 13841/101000 (13.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6746s / 1190.4662 s
agent0:                 episode reward: -0.0166,                 loss: 0.1634
agent1:                 episode reward: 0.0166,                 loss: nan
Episode: 13861/101000 (13.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2909s / 1192.7571 s
agent0:                 episode reward: 0.2265,                 loss: 0.1630
agent1:                 episode reward: -0.2265,                 loss: nan
Episode: 13881/101000 (13.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6718s / 1195.4289 s
agent0:                 episode reward: 0.0888,                 loss: 0.1649
agent1:                 episode reward: -0.0888,                 loss: nan
Episode: 13901/101000 (13.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6489s / 1197.0777 s
agent0:                 episode reward: -0.9843,                 loss: 0.1636
agent1:                 episode reward: 0.9843,                 loss: nan
Episode: 13921/101000 (13.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9159s / 1198.9936 s
agent0:                 episode reward: -0.4556,                 loss: 0.1645
agent1:                 episode reward: 0.4556,                 loss: nan
Episode: 13941/101000 (13.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8672s / 1200.8609 s
agent0:                 episode reward: -0.2004,                 loss: 0.1652
agent1:                 episode reward: 0.2004,                 loss: nan
Episode: 13961/101000 (13.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0330s / 1202.8939 s
agent0:                 episode reward: -0.5208,                 loss: 0.1646
agent1:                 episode reward: 0.5208,                 loss: nan
Episode: 13981/101000 (13.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3265s / 1205.2203 s
agent0:                 episode reward: 0.2201,                 loss: 0.1587
agent1:                 episode reward: -0.2201,                 loss: nan
Episode: 14001/101000 (13.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9348s / 1207.1552 s
agent0:                 episode reward: -0.3664,                 loss: 0.1535
agent1:                 episode reward: 0.3664,                 loss: 0.1657
Score delta: 1.5764007743419721, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/13556_0.
Episode: 14021/101000 (13.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2563s / 1209.4115 s
agent0:                 episode reward: -0.3435,                 loss: nan
agent1:                 episode reward: 0.3435,                 loss: 0.1646
Episode: 14041/101000 (13.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4757s / 1211.8872 s
agent0:                 episode reward: 0.1505,                 loss: 0.1649
agent1:                 episode reward: -0.1505,                 loss: 0.1625
Score delta: 1.5057661618402143, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/13596_1.
Episode: 14061/101000 (13.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3447s / 1214.2318 s
agent0:                 episode reward: 0.1053,                 loss: 0.1628
agent1:                 episode reward: -0.1053,                 loss: nan
Episode: 14081/101000 (13.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1003s / 1216.3321 s
agent0:                 episode reward: -0.5420,                 loss: 0.1630
agent1:                 episode reward: 0.5420,                 loss: nan
Episode: 14101/101000 (13.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9363s / 1218.2684 s
agent0:                 episode reward: -0.6003,                 loss: 0.1644
agent1:                 episode reward: 0.6003,                 loss: nan
Episode: 14121/101000 (13.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9835s / 1220.2519 s
agent0:                 episode reward: -0.0655,                 loss: 0.1643
agent1:                 episode reward: 0.0655,                 loss: nan
Episode: 14141/101000 (14.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1555s / 1222.4074 s
agent0:                 episode reward: -0.3968,                 loss: 0.1622
agent1:                 episode reward: 0.3968,                 loss: nan
Episode: 14161/101000 (14.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5012s / 1224.9086 s
agent0:                 episode reward: -0.0508,                 loss: 0.1639
agent1:                 episode reward: 0.0508,                 loss: nan
Episode: 14181/101000 (14.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9899s / 1226.8985 s
agent0:                 episode reward: -0.3120,                 loss: 0.1620
agent1:                 episode reward: 0.3120,                 loss: nan
Episode: 14201/101000 (14.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9955s / 1228.8940 s
agent0:                 episode reward: -0.8919,                 loss: 0.1640
agent1:                 episode reward: 0.8919,                 loss: nan
Episode: 14221/101000 (14.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0552s / 1230.9492 s
agent0:                 episode reward: -0.3503,                 loss: 0.1641
agent1:                 episode reward: 0.3503,                 loss: nan
Episode: 14241/101000 (14.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6351s / 1232.5843 s
agent0:                 episode reward: -0.2381,                 loss: 0.1633
agent1:                 episode reward: 0.2381,                 loss: nan
Episode: 14261/101000 (14.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2514s / 1234.8357 s
agent0:                 episode reward: -0.0785,                 loss: 0.1641
agent1:                 episode reward: 0.0785,                 loss: nan
Episode: 14281/101000 (14.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4990s / 1237.3347 s
agent0:                 episode reward: -0.1612,                 loss: 0.1624
agent1:                 episode reward: 0.1612,                 loss: nan
Episode: 14301/101000 (14.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2334s / 1239.5681 s
agent0:                 episode reward: 0.0110,                 loss: 0.1601
agent1:                 episode reward: -0.0110,                 loss: nan
Episode: 14321/101000 (14.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4931s / 1242.0612 s
agent0:                 episode reward: 0.2670,                 loss: 0.1615
agent1:                 episode reward: -0.2670,                 loss: nan
Episode: 14341/101000 (14.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5800s / 1244.6411 s
agent0:                 episode reward: -0.1965,                 loss: 0.1625
agent1:                 episode reward: 0.1965,                 loss: nan
Episode: 14361/101000 (14.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2152s / 1246.8563 s
agent0:                 episode reward: -0.1593,                 loss: 0.1599
agent1:                 episode reward: 0.1593,                 loss: nan
Episode: 14381/101000 (14.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9342s / 1248.7905 s
agent0:                 episode reward: -0.2117,                 loss: 0.1619
agent1:                 episode reward: 0.2117,                 loss: nan
Episode: 14401/101000 (14.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2156s / 1251.0061 s
agent0:                 episode reward: 0.0107,                 loss: 0.1611
agent1:                 episode reward: -0.0107,                 loss: nan
Episode: 14421/101000 (14.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8912s / 1252.8973 s
agent0:                 episode reward: -0.0922,                 loss: 0.1612
agent1:                 episode reward: 0.0922,                 loss: nan
Episode: 14441/101000 (14.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0707s / 1254.9680 s
agent0:                 episode reward: -0.1504,                 loss: 0.1612
agent1:                 episode reward: 0.1504,                 loss: nan
Episode: 14461/101000 (14.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1007s / 1257.0687 s
agent0:                 episode reward: -0.5374,                 loss: 0.1615
agent1:                 episode reward: 0.5374,                 loss: nan
Episode: 14481/101000 (14.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7011s / 1258.7697 s
agent0:                 episode reward: -0.4887,                 loss: 0.1608
agent1:                 episode reward: 0.4887,                 loss: nan
Episode: 14501/101000 (14.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0279s / 1260.7977 s
agent0:                 episode reward: -0.1424,                 loss: 0.1637
agent1:                 episode reward: 0.1424,                 loss: nan
Episode: 14521/101000 (14.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9661s / 1262.7638 s
agent0:                 episode reward: 0.1286,                 loss: 0.1616
agent1:                 episode reward: -0.1286,                 loss: nan
Episode: 14541/101000 (14.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3184s / 1265.0821 s
agent0:                 episode reward: -0.1423,                 loss: 0.1603
agent1:                 episode reward: 0.1423,                 loss: nan
Episode: 14561/101000 (14.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5871s / 1266.6693 s
agent0:                 episode reward: -0.3718,                 loss: 0.1600
agent1:                 episode reward: 0.3718,                 loss: nan
Episode: 14581/101000 (14.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1966s / 1268.8658 s
agent0:                 episode reward: -0.0809,                 loss: 0.1614
agent1:                 episode reward: 0.0809,                 loss: nan
Episode: 14601/101000 (14.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1854s / 1271.0513 s
agent0:                 episode reward: -0.3841,                 loss: 0.1617
agent1:                 episode reward: 0.3841,                 loss: nan
Episode: 14621/101000 (14.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0580s / 1273.1093 s
agent0:                 episode reward: -0.0927,                 loss: 0.1592
agent1:                 episode reward: 0.0927,                 loss: nan
Episode: 14641/101000 (14.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4166s / 1275.5259 s
agent0:                 episode reward: 0.3322,                 loss: 0.1610
agent1:                 episode reward: -0.3322,                 loss: 0.2009
Score delta: 1.5838362549305445, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/14205_0.
Episode: 14661/101000 (14.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9709s / 1277.4968 s
agent0:                 episode reward: -0.2026,                 loss: nan
agent1:                 episode reward: 0.2026,                 loss: 0.1888
Episode: 14681/101000 (14.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3489s / 1279.8457 s
agent0:                 episode reward: -0.1661,                 loss: nan
agent1:                 episode reward: 0.1661,                 loss: 0.1840
Episode: 14701/101000 (14.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6671s / 1281.5128 s
agent0:                 episode reward: 0.3720,                 loss: nan
agent1:                 episode reward: -0.3720,                 loss: 0.1826
Episode: 14721/101000 (14.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 1.5672s / 1283.0800 s
agent0:                 episode reward: -0.3004,                 loss: nan
agent1:                 episode reward: 0.3004,                 loss: 0.1834
Episode: 14741/101000 (14.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7825s / 1284.8625 s
agent0:                 episode reward: -0.0892,                 loss: nan
agent1:                 episode reward: 0.0892,                 loss: 0.1863
Episode: 14761/101000 (14.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6207s / 1286.4831 s
agent0:                 episode reward: -0.7459,                 loss: 0.1637
agent1:                 episode reward: 0.7459,                 loss: 0.1774
Score delta: 1.6083397852512806, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/14326_1.
Episode: 14781/101000 (14.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0315s / 1288.5146 s
agent0:                 episode reward: -0.1078,                 loss: 0.1630
agent1:                 episode reward: 0.1078,                 loss: nan
Episode: 14801/101000 (14.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6190s / 1290.1336 s
agent0:                 episode reward: -0.2513,                 loss: 0.1608
agent1:                 episode reward: 0.2513,                 loss: nan
Episode: 14821/101000 (14.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5549s / 1292.6886 s
agent0:                 episode reward: 0.1102,                 loss: 0.1528
agent1:                 episode reward: -0.1102,                 loss: nan
Episode: 14841/101000 (14.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9296s / 1294.6181 s
agent0:                 episode reward: -0.7389,                 loss: 0.1523
agent1:                 episode reward: 0.7389,                 loss: nan
Episode: 14861/101000 (14.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7088s / 1296.3269 s
agent0:                 episode reward: -0.1536,                 loss: 0.1534
agent1:                 episode reward: 0.1536,                 loss: nan
Episode: 14881/101000 (14.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9941s / 1298.3210 s
agent0:                 episode reward: -0.3976,                 loss: 0.1538
agent1:                 episode reward: 0.3976,                 loss: nan
Episode: 14901/101000 (14.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1023s / 1300.4233 s
agent0:                 episode reward: -0.4049,                 loss: 0.1544
agent1:                 episode reward: 0.4049,                 loss: nan
Episode: 14921/101000 (14.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8540s / 1302.2774 s
agent0:                 episode reward: -0.3838,                 loss: 0.1542
agent1:                 episode reward: 0.3838,                 loss: nan
Episode: 14941/101000 (14.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1045s / 1304.3819 s
agent0:                 episode reward: -0.2075,                 loss: 0.1548
agent1:                 episode reward: 0.2075,                 loss: nan
Episode: 14961/101000 (14.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2331s / 1306.6150 s
agent0:                 episode reward: 0.1126,                 loss: 0.1539
agent1:                 episode reward: -0.1126,                 loss: nan
Episode: 14981/101000 (14.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9855s / 1308.6004 s
agent0:                 episode reward: -0.4962,                 loss: 0.1551
agent1:                 episode reward: 0.4962,                 loss: nan
Episode: 15001/101000 (14.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1262s / 1310.7267 s
agent0:                 episode reward: -0.2995,                 loss: 0.1548
agent1:                 episode reward: 0.2995,                 loss: nan
Episode: 15021/101000 (14.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 1.4522s / 1312.1788 s
agent0:                 episode reward: -0.1161,                 loss: 0.1529
agent1:                 episode reward: 0.1161,                 loss: nan
Episode: 15041/101000 (14.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6721s / 1313.8509 s
agent0:                 episode reward: 0.0044,                 loss: 0.1540
agent1:                 episode reward: -0.0044,                 loss: nan
Episode: 15061/101000 (14.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0814s / 1315.9323 s
agent0:                 episode reward: -0.2428,                 loss: 0.1561
agent1:                 episode reward: 0.2428,                 loss: nan
Episode: 15081/101000 (14.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6665s / 1318.5988 s
agent0:                 episode reward: -0.0788,                 loss: 0.1543
agent1:                 episode reward: 0.0788,                 loss: nan
Episode: 15101/101000 (14.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1622s / 1320.7610 s
agent0:                 episode reward: -0.3032,                 loss: 0.1545
agent1:                 episode reward: 0.3032,                 loss: nan
Episode: 15121/101000 (14.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1811s / 1322.9422 s
agent0:                 episode reward: -0.5411,                 loss: 0.1523
agent1:                 episode reward: 0.5411,                 loss: nan
Episode: 15141/101000 (14.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0647s / 1325.0069 s
agent0:                 episode reward: 0.1296,                 loss: 0.1654
agent1:                 episode reward: -0.1296,                 loss: nan
Episode: 15161/101000 (15.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6772s / 1326.6841 s
agent0:                 episode reward: -0.0896,                 loss: 0.1663
agent1:                 episode reward: 0.0896,                 loss: nan
Episode: 15181/101000 (15.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0968s / 1328.7809 s
agent0:                 episode reward: -0.3796,                 loss: 0.1667
agent1:                 episode reward: 0.3796,                 loss: nan
Episode: 15201/101000 (15.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2068s / 1330.9877 s
agent0:                 episode reward: -0.4438,                 loss: 0.1663
agent1:                 episode reward: 0.4438,                 loss: nan
Episode: 15221/101000 (15.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9005s / 1332.8882 s
agent0:                 episode reward: 0.0705,                 loss: 0.1658
agent1:                 episode reward: -0.0705,                 loss: nan
Episode: 15241/101000 (15.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1557s / 1335.0440 s
agent0:                 episode reward: -0.2572,                 loss: 0.1654
agent1:                 episode reward: 0.2572,                 loss: nan
Episode: 15261/101000 (15.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7906s / 1336.8345 s
agent0:                 episode reward: 0.1260,                 loss: 0.1654
agent1:                 episode reward: -0.1260,                 loss: nan
Episode: 15281/101000 (15.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8255s / 1338.6601 s
agent0:                 episode reward: -0.0436,                 loss: 0.1678
agent1:                 episode reward: 0.0436,                 loss: nan
Episode: 15301/101000 (15.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2750s / 1340.9351 s
agent0:                 episode reward: -0.7591,                 loss: 0.1633
agent1:                 episode reward: 0.7591,                 loss: nan
Episode: 15321/101000 (15.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1138s / 1343.0489 s
agent0:                 episode reward: -0.1130,                 loss: 0.1662
agent1:                 episode reward: 0.1130,                 loss: nan
Episode: 15341/101000 (15.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9159s / 1344.9648 s
agent0:                 episode reward: -0.1956,                 loss: 0.1642
agent1:                 episode reward: 0.1956,                 loss: nan
Episode: 15361/101000 (15.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0839s / 1347.0487 s
agent0:                 episode reward: -0.4756,                 loss: 0.1658
agent1:                 episode reward: 0.4756,                 loss: nan
Episode: 15381/101000 (15.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4448s / 1349.4935 s
agent0:                 episode reward: -0.3923,                 loss: 0.1663
agent1:                 episode reward: 0.3923,                 loss: nan
Episode: 15401/101000 (15.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7936s / 1352.2871 s
agent0:                 episode reward: -0.3177,                 loss: 0.1692
agent1:                 episode reward: 0.3177,                 loss: nan
Episode: 15421/101000 (15.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0287s / 1354.3158 s
agent0:                 episode reward: -0.2813,                 loss: 0.1680
agent1:                 episode reward: 0.2813,                 loss: nan
Episode: 15441/101000 (15.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9458s / 1356.2616 s
agent0:                 episode reward: 0.1498,                 loss: 0.1655
agent1:                 episode reward: -0.1498,                 loss: nan
Episode: 15461/101000 (15.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0679s / 1358.3295 s
agent0:                 episode reward: 0.0929,                 loss: 0.1669
agent1:                 episode reward: -0.0929,                 loss: nan
Episode: 15481/101000 (15.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8319s / 1360.1613 s
agent0:                 episode reward: 0.4466,                 loss: 0.1600
agent1:                 episode reward: -0.4466,                 loss: 0.1749
Score delta: 1.5727835389302136, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/15040_0.
Episode: 15501/101000 (15.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8248s / 1361.9861 s
agent0:                 episode reward: -0.4278,                 loss: 0.1702
agent1:                 episode reward: 0.4278,                 loss: 0.1756
Score delta: 1.5863665786121384, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/15069_1.
Episode: 15521/101000 (15.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1195s / 1364.1057 s
agent0:                 episode reward: -0.4822,                 loss: 0.1682
agent1:                 episode reward: 0.4822,                 loss: nan
Episode: 15541/101000 (15.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9001s / 1366.0058 s
agent0:                 episode reward: 0.1175,                 loss: 0.1678
agent1:                 episode reward: -0.1175,                 loss: nan
Episode: 15561/101000 (15.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7866s / 1367.7924 s
agent0:                 episode reward: -0.1734,                 loss: 0.1670
agent1:                 episode reward: 0.1734,                 loss: nan
Episode: 15581/101000 (15.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8570s / 1369.6494 s
agent0:                 episode reward: -0.7032,                 loss: 0.1666
agent1:                 episode reward: 0.7032,                 loss: nan
Episode: 15601/101000 (15.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0217s / 1371.6711 s
agent0:                 episode reward: 0.1434,                 loss: 0.1681
agent1:                 episode reward: -0.1434,                 loss: nan
Episode: 15621/101000 (15.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3936s / 1374.0647 s
agent0:                 episode reward: -0.4426,                 loss: 0.1701
agent1:                 episode reward: 0.4426,                 loss: nan
Episode: 15641/101000 (15.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9601s / 1376.0248 s
agent0:                 episode reward: 0.0232,                 loss: 0.1675
agent1:                 episode reward: -0.0232,                 loss: nan
Episode: 15661/101000 (15.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1467s / 1378.1715 s
agent0:                 episode reward: -0.0187,                 loss: 0.1692
agent1:                 episode reward: 0.0187,                 loss: nan
Episode: 15681/101000 (15.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0459s / 1380.2175 s
agent0:                 episode reward: 0.0079,                 loss: 0.1678
agent1:                 episode reward: -0.0079,                 loss: nan
Episode: 15701/101000 (15.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3064s / 1382.5239 s
agent0:                 episode reward: -0.4873,                 loss: 0.1657
agent1:                 episode reward: 0.4873,                 loss: nan
Episode: 15721/101000 (15.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8786s / 1384.4025 s
agent0:                 episode reward: -0.5391,                 loss: 0.1704
agent1:                 episode reward: 0.5391,                 loss: nan
Episode: 15741/101000 (15.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1761s / 1386.5785 s
agent0:                 episode reward: -0.8395,                 loss: 0.1677
agent1:                 episode reward: 0.8395,                 loss: nan
Episode: 15761/101000 (15.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1156s / 1388.6942 s
agent0:                 episode reward: -0.1502,                 loss: 0.1678
agent1:                 episode reward: 0.1502,                 loss: nan
Episode: 15781/101000 (15.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1778s / 1390.8720 s
agent0:                 episode reward: -0.6071,                 loss: 0.1695
agent1:                 episode reward: 0.6071,                 loss: nan
Episode: 15801/101000 (15.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4676s / 1393.3396 s
agent0:                 episode reward: -0.5075,                 loss: 0.1690
agent1:                 episode reward: 0.5075,                 loss: nan
Episode: 15821/101000 (15.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7677s / 1396.1073 s
agent0:                 episode reward: -0.7061,                 loss: 0.1691
agent1:                 episode reward: 0.7061,                 loss: nan
Episode: 15841/101000 (15.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0727s / 1398.1800 s
agent0:                 episode reward: -0.4814,                 loss: 0.1575
agent1:                 episode reward: 0.4814,                 loss: nan
Episode: 15861/101000 (15.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2756s / 1400.4555 s
agent0:                 episode reward: 0.0206,                 loss: 0.1576
agent1:                 episode reward: -0.0206,                 loss: nan
Episode: 15881/101000 (15.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0781s / 1402.5336 s
agent0:                 episode reward: -0.4884,                 loss: 0.1580
agent1:                 episode reward: 0.4884,                 loss: nan
Episode: 15901/101000 (15.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5415s / 1405.0752 s
agent0:                 episode reward: -0.2663,                 loss: 0.1569
agent1:                 episode reward: 0.2663,                 loss: nan
Episode: 15921/101000 (15.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3922s / 1407.4674 s
agent0:                 episode reward: -0.5226,                 loss: 0.1562
agent1:                 episode reward: 0.5226,                 loss: nan
Episode: 15941/101000 (15.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4789s / 1409.9463 s
agent0:                 episode reward: -0.0170,                 loss: 0.1577
agent1:                 episode reward: 0.0170,                 loss: nan
Episode: 15961/101000 (15.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8208s / 1411.7671 s
agent0:                 episode reward: -0.5502,                 loss: 0.1577
agent1:                 episode reward: 0.5502,                 loss: nan
Episode: 15981/101000 (15.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0384s / 1413.8055 s
agent0:                 episode reward: -0.1408,                 loss: 0.1558
agent1:                 episode reward: 0.1408,                 loss: nan
Episode: 16001/101000 (15.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6544s / 1415.4599 s
agent0:                 episode reward: -0.0091,                 loss: 0.1586
agent1:                 episode reward: 0.0091,                 loss: nan
Episode: 16021/101000 (15.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1085s / 1417.5685 s
agent0:                 episode reward: -0.1604,                 loss: 0.1589
agent1:                 episode reward: 0.1604,                 loss: nan
Episode: 16041/101000 (15.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5406s / 1420.1090 s
agent0:                 episode reward: -0.4855,                 loss: 0.1562
agent1:                 episode reward: 0.4855,                 loss: nan
Episode: 16061/101000 (15.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0607s / 1422.1697 s
agent0:                 episode reward: -0.7113,                 loss: 0.1585
agent1:                 episode reward: 0.7113,                 loss: nan
Episode: 16081/101000 (15.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1198s / 1424.2895 s
agent0:                 episode reward: -0.2363,                 loss: 0.1573
agent1:                 episode reward: 0.2363,                 loss: nan
Episode: 16101/101000 (15.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7535s / 1426.0429 s
agent0:                 episode reward: -0.2238,                 loss: 0.1573
agent1:                 episode reward: 0.2238,                 loss: nan
Episode: 16121/101000 (15.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1300s / 1428.1729 s
agent0:                 episode reward: -0.3525,                 loss: 0.1582
agent1:                 episode reward: 0.3525,                 loss: nan
Episode: 16141/101000 (15.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2196s / 1430.3926 s
agent0:                 episode reward: -0.1306,                 loss: 0.1580
agent1:                 episode reward: 0.1306,                 loss: nan
Episode: 16161/101000 (16.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 1.8946s / 1432.2871 s
agent0:                 episode reward: -0.4856,                 loss: 0.1614
agent1:                 episode reward: 0.4856,                 loss: nan
Episode: 16181/101000 (16.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7456s / 1435.0327 s
agent0:                 episode reward: 0.1480,                 loss: 0.1676
agent1:                 episode reward: -0.1480,                 loss: nan
Episode: 16201/101000 (16.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1803s / 1437.2130 s
agent0:                 episode reward: -0.3995,                 loss: 0.1662
agent1:                 episode reward: 0.3995,                 loss: nan
Episode: 16221/101000 (16.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3048s / 1439.5179 s
agent0:                 episode reward: -0.0292,                 loss: 0.1648
agent1:                 episode reward: 0.0292,                 loss: nan
Episode: 16241/101000 (16.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1685s / 1441.6864 s
agent0:                 episode reward: -0.1484,                 loss: 0.1635
agent1:                 episode reward: 0.1484,                 loss: nan
Episode: 16261/101000 (16.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6558s / 1444.3422 s
agent0:                 episode reward: -0.0845,                 loss: 0.1658
agent1:                 episode reward: 0.0845,                 loss: nan
Episode: 16281/101000 (16.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3992s / 1446.7413 s
agent0:                 episode reward: 0.2410,                 loss: 0.1646
agent1:                 episode reward: -0.2410,                 loss: nan
Score delta: 1.6320019216058128, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/15855_0.
Episode: 16301/101000 (16.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2772s / 1449.0185 s
agent0:                 episode reward: 0.0739,                 loss: nan
agent1:                 episode reward: -0.0739,                 loss: 0.1681
Episode: 16321/101000 (16.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 1.7350s / 1450.7536 s
agent0:                 episode reward: -0.4484,                 loss: nan
agent1:                 episode reward: 0.4484,                 loss: 0.1675
Episode: 16341/101000 (16.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 1.6817s / 1452.4353 s
agent0:                 episode reward: -0.1780,                 loss: nan
agent1:                 episode reward: 0.1780,                 loss: 0.1656
Episode: 16361/101000 (16.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3443s / 1454.7796 s
agent0:                 episode reward: -0.8422,                 loss: 0.1752
agent1:                 episode reward: 0.8422,                 loss: 0.1657
Score delta: 1.5796811076053385, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/15922_1.
Episode: 16381/101000 (16.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0915s / 1457.8710 s
agent0:                 episode reward: -0.2555,                 loss: 0.1734
agent1:                 episode reward: 0.2555,                 loss: nan
Episode: 16401/101000 (16.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7695s / 1460.6405 s
agent0:                 episode reward: -0.5539,                 loss: 0.1710
agent1:                 episode reward: 0.5539,                 loss: nan
Episode: 16421/101000 (16.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2620s / 1462.9025 s
agent0:                 episode reward: 0.0999,                 loss: 0.1726
agent1:                 episode reward: -0.0999,                 loss: nan
Episode: 16441/101000 (16.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3805s / 1465.2829 s
agent0:                 episode reward: -0.2239,                 loss: 0.1728
agent1:                 episode reward: 0.2239,                 loss: nan
Episode: 16461/101000 (16.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7131s / 1467.9960 s
agent0:                 episode reward: -0.2892,                 loss: 0.1713
agent1:                 episode reward: 0.2892,                 loss: nan
Episode: 16481/101000 (16.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7681s / 1470.7641 s
agent0:                 episode reward: -0.5723,                 loss: 0.1709
agent1:                 episode reward: 0.5723,                 loss: nan
Episode: 16501/101000 (16.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9471s / 1473.7112 s
agent0:                 episode reward: -0.5899,                 loss: 0.1709
agent1:                 episode reward: 0.5899,                 loss: nan
Episode: 16521/101000 (16.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7854s / 1476.4967 s
agent0:                 episode reward: -0.4695,                 loss: 0.1721
agent1:                 episode reward: 0.4695,                 loss: nan
Episode: 16541/101000 (16.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7140s / 1479.2107 s
agent0:                 episode reward: -0.0840,                 loss: 0.1717
agent1:                 episode reward: 0.0840,                 loss: nan
Episode: 16561/101000 (16.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0546s / 1482.2653 s
agent0:                 episode reward: -0.5765,                 loss: 0.1719
agent1:                 episode reward: 0.5765,                 loss: nan
Episode: 16581/101000 (16.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2878s / 1484.5531 s
agent0:                 episode reward: -0.2200,                 loss: 0.1638
agent1:                 episode reward: 0.2200,                 loss: nan
Episode: 16601/101000 (16.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7460s / 1487.2991 s
agent0:                 episode reward: -0.3767,                 loss: 0.1653
agent1:                 episode reward: 0.3767,                 loss: nan
Episode: 16621/101000 (16.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4164s / 1489.7155 s
agent0:                 episode reward: 0.2378,                 loss: 0.1638
agent1:                 episode reward: -0.2378,                 loss: nan
Episode: 16641/101000 (16.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6717s / 1492.3872 s
agent0:                 episode reward: -0.5918,                 loss: 0.1625
agent1:                 episode reward: 0.5918,                 loss: nan
Episode: 16661/101000 (16.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0990s / 1495.4863 s
agent0:                 episode reward: -0.3348,                 loss: 0.1629
agent1:                 episode reward: 0.3348,                 loss: nan
Episode: 16681/101000 (16.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1637s / 1498.6500 s
agent0:                 episode reward: -0.4418,                 loss: 0.1642
agent1:                 episode reward: 0.4418,                 loss: nan
Episode: 16701/101000 (16.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0786s / 1502.7286 s
agent0:                 episode reward: -0.1266,                 loss: 0.1627
agent1:                 episode reward: 0.1266,                 loss: nan
Episode: 16721/101000 (16.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1547s / 1506.8833 s
agent0:                 episode reward: -0.3472,                 loss: 0.1623
agent1:                 episode reward: 0.3472,                 loss: nan
Episode: 16741/101000 (16.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3743s / 1510.2577 s
agent0:                 episode reward: -0.4376,                 loss: 0.1653
agent1:                 episode reward: 0.4376,                 loss: nan
Episode: 16761/101000 (16.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6119s / 1512.8696 s
agent0:                 episode reward: -0.7381,                 loss: 0.1649
agent1:                 episode reward: 0.7381,                 loss: nan
Episode: 16781/101000 (16.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1823s / 1516.0518 s
agent0:                 episode reward: -0.3274,                 loss: 0.1646
agent1:                 episode reward: 0.3274,                 loss: nan
Episode: 16801/101000 (16.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4495s / 1519.5014 s
agent0:                 episode reward: -0.1971,                 loss: 0.1642
agent1:                 episode reward: 0.1971,                 loss: nan
Episode: 16821/101000 (16.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1292s / 1523.6306 s
agent0:                 episode reward: 0.3306,                 loss: 0.1637
agent1:                 episode reward: -0.3306,                 loss: nan
Episode: 16841/101000 (16.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1218s / 1527.7523 s
agent0:                 episode reward: 0.2562,                 loss: 0.1648
agent1:                 episode reward: -0.2562,                 loss: nan
Episode: 16861/101000 (16.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6694s / 1531.4218 s
agent0:                 episode reward: -0.1091,                 loss: 0.1650
agent1:                 episode reward: 0.1091,                 loss: nan
Episode: 16881/101000 (16.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2200s / 1535.6417 s
agent0:                 episode reward: -0.1046,                 loss: 0.1640
agent1:                 episode reward: 0.1046,                 loss: nan
Episode: 16901/101000 (16.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5920s / 1539.2337 s
agent0:                 episode reward: -0.1890,                 loss: 0.1665
agent1:                 episode reward: 0.1890,                 loss: nan
Episode: 16921/101000 (16.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8761s / 1544.1098 s
agent0:                 episode reward: -0.0850,                 loss: 0.1648
agent1:                 episode reward: 0.0850,                 loss: nan
Episode: 16941/101000 (16.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0589s / 1548.1687 s
agent0:                 episode reward: -0.2610,                 loss: 0.1636
agent1:                 episode reward: 0.2610,                 loss: nan
Episode: 16961/101000 (16.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6061s / 1552.7748 s
agent0:                 episode reward: -0.0157,                 loss: 0.1650
agent1:                 episode reward: 0.0157,                 loss: nan
Episode: 16981/101000 (16.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0936s / 1556.8684 s
agent0:                 episode reward: 0.0921,                 loss: 0.1660
agent1:                 episode reward: -0.0921,                 loss: nan
Episode: 17001/101000 (16.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1457s / 1561.0142 s
agent0:                 episode reward: -0.1821,                 loss: 0.1629
agent1:                 episode reward: 0.1821,                 loss: nan
Episode: 17021/101000 (16.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9237s / 1563.9379 s
agent0:                 episode reward: -0.1830,                 loss: 0.1639
agent1:                 episode reward: 0.1830,                 loss: nan
Episode: 17041/101000 (16.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6676s / 1567.6055 s
agent0:                 episode reward: 0.2166,                 loss: 0.1633
agent1:                 episode reward: -0.2166,                 loss: nan
Episode: 17061/101000 (16.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2669s / 1571.8725 s
agent0:                 episode reward: 0.1854,                 loss: 0.1654
agent1:                 episode reward: -0.1854,                 loss: nan
Episode: 17081/101000 (16.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4565s / 1578.3290 s
agent0:                 episode reward: -0.1903,                 loss: 0.1661
agent1:                 episode reward: 0.1903,                 loss: nan
Episode: 17101/101000 (16.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1513s / 1584.4803 s
agent0:                 episode reward: -0.1213,                 loss: 0.1658
agent1:                 episode reward: 0.1213,                 loss: nan
Episode: 17121/101000 (16.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8929s / 1590.3732 s
agent0:                 episode reward: -0.1785,                 loss: 0.1664
agent1:                 episode reward: 0.1785,                 loss: nan
Episode: 17141/101000 (16.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4148s / 1597.7879 s
agent0:                 episode reward: -0.0885,                 loss: 0.1668
agent1:                 episode reward: 0.0885,                 loss: nan
Episode: 17161/101000 (16.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7551s / 1604.5430 s
agent0:                 episode reward: -0.3836,                 loss: 0.1662
agent1:                 episode reward: 0.3836,                 loss: nan
Episode: 17181/101000 (17.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5382s / 1610.0813 s
agent0:                 episode reward: -0.0315,                 loss: 0.1648
agent1:                 episode reward: 0.0315,                 loss: nan
Episode: 17201/101000 (17.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6877s / 1618.7689 s
agent0:                 episode reward: -0.3913,                 loss: 0.1625
agent1:                 episode reward: 0.3913,                 loss: nan
Episode: 17221/101000 (17.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9876s / 1624.7565 s
agent0:                 episode reward: -0.0111,                 loss: 0.1662
agent1:                 episode reward: 0.0111,                 loss: nan
Episode: 17241/101000 (17.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2470s / 1631.0036 s
agent0:                 episode reward: 0.0216,                 loss: 0.1637
agent1:                 episode reward: -0.0216,                 loss: nan
Episode: 17261/101000 (17.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2020s / 1638.2056 s
agent0:                 episode reward: -0.0686,                 loss: 0.1656
agent1:                 episode reward: 0.0686,                 loss: nan
Episode: 17281/101000 (17.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1512s / 1644.3568 s
agent0:                 episode reward: 0.2718,                 loss: 0.1630
agent1:                 episode reward: -0.2718,                 loss: nan
Episode: 17301/101000 (17.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0844s / 1650.4412 s
agent0:                 episode reward: -0.1070,                 loss: 0.1638
agent1:                 episode reward: 0.1070,                 loss: nan
Episode: 17321/101000 (17.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3879s / 1656.8290 s
agent0:                 episode reward: -0.1869,                 loss: 0.1633
agent1:                 episode reward: 0.1869,                 loss: nan
Episode: 17341/101000 (17.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5578s / 1665.3868 s
agent0:                 episode reward: -0.2489,                 loss: 0.1631
agent1:                 episode reward: 0.2489,                 loss: nan
Episode: 17361/101000 (17.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5628s / 1671.9496 s
agent0:                 episode reward: -0.2038,                 loss: 0.1639
agent1:                 episode reward: 0.2038,                 loss: nan
Episode: 17381/101000 (17.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4109s / 1678.3605 s
agent0:                 episode reward: -0.0289,                 loss: 0.1650
agent1:                 episode reward: 0.0289,                 loss: nan
Episode: 17401/101000 (17.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2385s / 1682.5990 s
agent0:                 episode reward: 0.0436,                 loss: 0.1649
agent1:                 episode reward: -0.0436,                 loss: nan
Episode: 17421/101000 (17.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0135s / 1689.6125 s
agent0:                 episode reward: 0.0733,                 loss: 0.1624
agent1:                 episode reward: -0.0733,                 loss: nan
Episode: 17441/101000 (17.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3588s / 1696.9713 s
agent0:                 episode reward: -0.1861,                 loss: 0.1645
agent1:                 episode reward: 0.1861,                 loss: nan
Episode: 17461/101000 (17.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0819s / 1704.0532 s
agent0:                 episode reward: 0.2053,                 loss: 0.1627
agent1:                 episode reward: -0.2053,                 loss: nan
Episode: 17481/101000 (17.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6006s / 1708.6538 s
agent0:                 episode reward: -0.0108,                 loss: 0.1637
agent1:                 episode reward: 0.0108,                 loss: nan
Episode: 17501/101000 (17.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6907s / 1714.3445 s
agent0:                 episode reward: -0.0469,                 loss: 0.1642
agent1:                 episode reward: 0.0469,                 loss: nan
Episode: 17521/101000 (17.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6087s / 1719.9532 s
agent0:                 episode reward: 0.0586,                 loss: 0.1660
agent1:                 episode reward: -0.0586,                 loss: nan
Episode: 17541/101000 (17.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1981s / 1727.1513 s
agent0:                 episode reward: -0.1574,                 loss: 0.1638
agent1:                 episode reward: 0.1574,                 loss: nan
Episode: 17561/101000 (17.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5428s / 1732.6941 s
agent0:                 episode reward: -0.1861,                 loss: 0.1626
agent1:                 episode reward: 0.1861,                 loss: nan
Episode: 17581/101000 (17.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3488s / 1739.0429 s
agent0:                 episode reward: -0.3141,                 loss: 0.1627
agent1:                 episode reward: 0.3141,                 loss: nan
Episode: 17601/101000 (17.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6253s / 1744.6682 s
agent0:                 episode reward: -0.5922,                 loss: 0.1627
agent1:                 episode reward: 0.5922,                 loss: nan
Episode: 17621/101000 (17.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2595s / 1752.9277 s
agent0:                 episode reward: 0.1321,                 loss: 0.1619
agent1:                 episode reward: -0.1321,                 loss: nan
Episode: 17641/101000 (17.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2033s / 1760.1311 s
agent0:                 episode reward: -0.1426,                 loss: 0.1625
agent1:                 episode reward: 0.1426,                 loss: nan
Episode: 17661/101000 (17.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3734s / 1766.5045 s
agent0:                 episode reward: -0.0429,                 loss: 0.1633
agent1:                 episode reward: 0.0429,                 loss: nan
Episode: 17681/101000 (17.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0389s / 1772.5433 s
agent0:                 episode reward: -0.1621,                 loss: 0.1618
agent1:                 episode reward: 0.1621,                 loss: nan
Episode: 17701/101000 (17.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6404s / 1778.1837 s
agent0:                 episode reward: -0.1885,                 loss: 0.1596
agent1:                 episode reward: 0.1885,                 loss: nan
Episode: 17721/101000 (17.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6827s / 1783.8665 s
agent0:                 episode reward: 0.0480,                 loss: 0.1615
agent1:                 episode reward: -0.0480,                 loss: nan
Episode: 17741/101000 (17.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9832s / 1789.8497 s
agent0:                 episode reward: -0.0406,                 loss: 0.1616
agent1:                 episode reward: 0.0406,                 loss: nan
Episode: 17761/101000 (17.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7861s / 1794.6357 s
agent0:                 episode reward: -0.2030,                 loss: 0.1634
agent1:                 episode reward: 0.2030,                 loss: nan
Episode: 17781/101000 (17.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0031s / 1801.6388 s
agent0:                 episode reward: -0.0552,                 loss: 0.1611
agent1:                 episode reward: 0.0552,                 loss: nan
Episode: 17801/101000 (17.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7219s / 1807.3607 s
agent0:                 episode reward: 0.0811,                 loss: 0.1627
agent1:                 episode reward: -0.0811,                 loss: nan
Episode: 17821/101000 (17.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4997s / 1813.8604 s
agent0:                 episode reward: 0.2693,                 loss: 0.1625
agent1:                 episode reward: -0.2693,                 loss: nan
Episode: 17841/101000 (17.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9122s / 1817.7726 s
agent0:                 episode reward: -0.3407,                 loss: 0.1629
agent1:                 episode reward: 0.3407,                 loss: 0.1739
Score delta: 1.5052532968688532, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/17400_0.
Episode: 17861/101000 (17.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6877s / 1824.4603 s
agent0:                 episode reward: -0.8879,                 loss: 0.2099
agent1:                 episode reward: 0.8879,                 loss: 0.1717
Score delta: 1.6634724091053208, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/17422_1.
Episode: 17881/101000 (17.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2494s / 1829.7097 s
agent0:                 episode reward: -0.7069,                 loss: 0.1787
agent1:                 episode reward: 0.7069,                 loss: nan
Episode: 17901/101000 (17.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4457s / 1838.1554 s
agent0:                 episode reward: -0.6380,                 loss: 0.1667
agent1:                 episode reward: 0.6380,                 loss: nan
Episode: 17921/101000 (17.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5672s / 1843.7226 s
agent0:                 episode reward: -0.5819,                 loss: 0.1676
agent1:                 episode reward: 0.5819,                 loss: nan
Episode: 17941/101000 (17.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8580s / 1848.5805 s
agent0:                 episode reward: -0.9351,                 loss: 0.1695
agent1:                 episode reward: 0.9351,                 loss: nan
Episode: 17961/101000 (17.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9686s / 1852.5491 s
agent0:                 episode reward: -0.5153,                 loss: 0.1673
agent1:                 episode reward: 0.5153,                 loss: nan
Episode: 17981/101000 (17.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8142s / 1859.3633 s
agent0:                 episode reward: -0.7209,                 loss: 0.1673
agent1:                 episode reward: 0.7209,                 loss: nan
Episode: 18001/101000 (17.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0587s / 1866.4220 s
agent0:                 episode reward: -0.8020,                 loss: 0.1643
agent1:                 episode reward: 0.8020,                 loss: nan
Episode: 18021/101000 (17.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3511s / 1873.7731 s
agent0:                 episode reward: -0.3718,                 loss: 0.1629
agent1:                 episode reward: 0.3718,                 loss: nan
Episode: 18041/101000 (17.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9192s / 1880.6923 s
agent0:                 episode reward: -0.9941,                 loss: 0.1624
agent1:                 episode reward: 0.9941,                 loss: nan
Episode: 18061/101000 (17.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2320s / 1885.9243 s
agent0:                 episode reward: -0.5384,                 loss: 0.1614
agent1:                 episode reward: 0.5384,                 loss: nan
Episode: 18081/101000 (17.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4573s / 1892.3817 s
agent0:                 episode reward: -0.4943,                 loss: 0.1631
agent1:                 episode reward: 0.4943,                 loss: nan
Episode: 18101/101000 (17.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0975s / 1897.4791 s
agent0:                 episode reward: -0.1660,                 loss: 0.1635
agent1:                 episode reward: 0.1660,                 loss: nan
Episode: 18121/101000 (17.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7182s / 1903.1973 s
agent0:                 episode reward: -0.6148,                 loss: 0.1596
agent1:                 episode reward: 0.6148,                 loss: nan
Episode: 18141/101000 (17.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9438s / 1908.1411 s
agent0:                 episode reward: -0.2007,                 loss: 0.1618
agent1:                 episode reward: 0.2007,                 loss: nan
Episode: 18161/101000 (17.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3377s / 1914.4788 s
agent0:                 episode reward: -0.2616,                 loss: 0.1610
agent1:                 episode reward: 0.2616,                 loss: nan
Episode: 18181/101000 (18.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1903s / 1921.6691 s
agent0:                 episode reward: -0.2244,                 loss: 0.1595
agent1:                 episode reward: 0.2244,                 loss: nan
Episode: 18201/101000 (18.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2273s / 1928.8964 s
agent0:                 episode reward: -0.6042,                 loss: 0.1598
agent1:                 episode reward: 0.6042,                 loss: nan
Episode: 18221/101000 (18.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4302s / 1933.3266 s
agent0:                 episode reward: -0.5740,                 loss: 0.1593
agent1:                 episode reward: 0.5740,                 loss: nan
Episode: 18241/101000 (18.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1592s / 1939.4858 s
agent0:                 episode reward: -0.2782,                 loss: 0.1601
agent1:                 episode reward: 0.2782,                 loss: nan
Episode: 18261/101000 (18.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2448s / 1945.7306 s
agent0:                 episode reward: -0.7472,                 loss: 0.1674
agent1:                 episode reward: 0.7472,                 loss: nan
Episode: 18281/101000 (18.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8063s / 1953.5369 s
agent0:                 episode reward: -0.4225,                 loss: 0.1668
agent1:                 episode reward: 0.4225,                 loss: nan
Episode: 18301/101000 (18.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9416s / 1959.4784 s
agent0:                 episode reward: -0.3913,                 loss: 0.1666
agent1:                 episode reward: 0.3913,                 loss: nan
Episode: 18321/101000 (18.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8035s / 1965.2819 s
agent0:                 episode reward: -0.5605,                 loss: 0.1661
agent1:                 episode reward: 0.5605,                 loss: nan
Episode: 18341/101000 (18.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3609s / 1971.6428 s
agent0:                 episode reward: -0.6270,                 loss: 0.1665
agent1:                 episode reward: 0.6270,                 loss: nan
Episode: 18361/101000 (18.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7930s / 1977.4357 s
agent0:                 episode reward: -0.0583,                 loss: 0.1670
agent1:                 episode reward: 0.0583,                 loss: nan
Episode: 18381/101000 (18.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0219s / 1985.4576 s
agent0:                 episode reward: -0.3430,                 loss: 0.1673
agent1:                 episode reward: 0.3430,                 loss: nan
Episode: 18401/101000 (18.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8841s / 1991.3418 s
agent0:                 episode reward: -0.2080,                 loss: 0.1654
agent1:                 episode reward: 0.2080,                 loss: nan
Episode: 18421/101000 (18.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6031s / 1998.9449 s
agent0:                 episode reward: -0.6218,                 loss: 0.1665
agent1:                 episode reward: 0.6218,                 loss: nan
Episode: 18441/101000 (18.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7303s / 2005.6751 s
agent0:                 episode reward: -0.4242,                 loss: 0.1652
agent1:                 episode reward: 0.4242,                 loss: nan
Episode: 18461/101000 (18.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0390s / 2009.7142 s
agent0:                 episode reward: -0.2909,                 loss: 0.1653
agent1:                 episode reward: 0.2909,                 loss: nan
Episode: 18481/101000 (18.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6232s / 2017.3374 s
agent0:                 episode reward: -0.0665,                 loss: 0.1635
agent1:                 episode reward: 0.0665,                 loss: nan
Episode: 18501/101000 (18.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4333s / 2025.7706 s
agent0:                 episode reward: 0.0202,                 loss: 0.1660
agent1:                 episode reward: -0.0202,                 loss: nan
Episode: 18521/101000 (18.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4329s / 2032.2035 s
agent0:                 episode reward: -0.2907,                 loss: 0.1650
agent1:                 episode reward: 0.2907,                 loss: nan
Episode: 18541/101000 (18.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8068s / 2039.0104 s
agent0:                 episode reward: -0.0323,                 loss: 0.1675
agent1:                 episode reward: 0.0323,                 loss: nan
Episode: 18561/101000 (18.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1769s / 2047.1872 s
agent0:                 episode reward: -0.6351,                 loss: 0.1669
agent1:                 episode reward: 0.6351,                 loss: nan
Episode: 18581/101000 (18.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3100s / 2053.4972 s
agent0:                 episode reward: -0.3512,                 loss: 0.1676
agent1:                 episode reward: 0.3512,                 loss: nan
Episode: 18601/101000 (18.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8003s / 2057.2975 s
agent0:                 episode reward: -0.0345,                 loss: 0.1729
agent1:                 episode reward: 0.0345,                 loss: nan
Episode: 18621/101000 (18.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9301s / 2062.2276 s
agent0:                 episode reward: -0.3856,                 loss: 0.1737
agent1:                 episode reward: 0.3856,                 loss: nan
Episode: 18641/101000 (18.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6831s / 2069.9107 s
agent0:                 episode reward: 0.0352,                 loss: 0.1706
agent1:                 episode reward: -0.0352,                 loss: nan
Episode: 18661/101000 (18.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6908s / 2076.6015 s
agent0:                 episode reward: -0.4459,                 loss: 0.1700
agent1:                 episode reward: 0.4459,                 loss: nan
Episode: 18681/101000 (18.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4346s / 2084.0361 s
agent0:                 episode reward: -0.4174,                 loss: 0.1707
agent1:                 episode reward: 0.4174,                 loss: nan
Episode: 18701/101000 (18.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3485s / 2090.3846 s
agent0:                 episode reward: -0.1193,                 loss: 0.1700
agent1:                 episode reward: 0.1193,                 loss: nan
Episode: 18721/101000 (18.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5974s / 2095.9821 s
agent0:                 episode reward: -0.2158,                 loss: 0.1705
agent1:                 episode reward: 0.2158,                 loss: nan
Episode: 18741/101000 (18.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6167s / 2101.5988 s
agent0:                 episode reward: -0.2019,                 loss: 0.1704
agent1:                 episode reward: 0.2019,                 loss: nan
Episode: 18761/101000 (18.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9089s / 2106.5077 s
agent0:                 episode reward: -0.5666,                 loss: 0.1691
agent1:                 episode reward: 0.5666,                 loss: nan
Episode: 18781/101000 (18.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2021s / 2112.7098 s
agent0:                 episode reward: -0.4878,                 loss: 0.1701
agent1:                 episode reward: 0.4878,                 loss: nan
Episode: 18801/101000 (18.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7970s / 2118.5068 s
agent0:                 episode reward: -0.2295,                 loss: 0.1685
agent1:                 episode reward: 0.2295,                 loss: nan
Episode: 18821/101000 (18.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8905s / 2126.3973 s
agent0:                 episode reward: 0.0775,                 loss: 0.1720
agent1:                 episode reward: -0.0775,                 loss: nan
Episode: 18841/101000 (18.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3690s / 2131.7664 s
agent0:                 episode reward: 0.0770,                 loss: 0.1731
agent1:                 episode reward: -0.0770,                 loss: 0.1778
Score delta: 1.5536555067651778, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/18400_0.
Episode: 18861/101000 (18.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8940s / 2136.6603 s
agent0:                 episode reward: -0.0577,                 loss: nan
agent1:                 episode reward: 0.0577,                 loss: 0.1744
Episode: 18881/101000 (18.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0988s / 2143.7591 s
agent0:                 episode reward: -1.0417,                 loss: 0.1749
agent1:                 episode reward: 1.0417,                 loss: 0.1738
Score delta: 2.0681308963852074, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/18445_1.
Episode: 18901/101000 (18.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5519s / 2149.3110 s
agent0:                 episode reward: -0.3378,                 loss: 0.1744
agent1:                 episode reward: 0.3378,                 loss: nan
Episode: 18921/101000 (18.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8273s / 2155.1383 s
agent0:                 episode reward: -0.0461,                 loss: 0.1742
agent1:                 episode reward: 0.0461,                 loss: nan
Episode: 18941/101000 (18.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4958s / 2161.6341 s
agent0:                 episode reward: -0.4273,                 loss: 0.1759
agent1:                 episode reward: 0.4273,                 loss: 0.1634
Score delta: 1.6298387660635527, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/18498_0.
Episode: 18961/101000 (18.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7754s / 2168.4095 s
agent0:                 episode reward: -0.2739,                 loss: 0.1792
agent1:                 episode reward: 0.2739,                 loss: 0.1633
Score delta: 1.7739357403271405, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/18531_1.
Episode: 18981/101000 (18.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7502s / 2176.1597 s
agent0:                 episode reward: -0.5855,                 loss: 0.1757
agent1:                 episode reward: 0.5855,                 loss: nan
Episode: 19001/101000 (18.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0395s / 2181.1992 s
agent0:                 episode reward: 0.0084,                 loss: 0.1703
agent1:                 episode reward: -0.0084,                 loss: nan
Episode: 19021/101000 (18.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6165s / 2188.8157 s
agent0:                 episode reward: -0.1466,                 loss: 0.1685
agent1:                 episode reward: 0.1466,                 loss: nan
Episode: 19041/101000 (18.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 8.6133s / 2197.4290 s
agent0:                 episode reward: -0.3443,                 loss: 0.1699
agent1:                 episode reward: 0.3443,                 loss: nan
Episode: 19061/101000 (18.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5955s / 2203.0245 s
agent0:                 episode reward: 0.1188,                 loss: 0.1680
agent1:                 episode reward: -0.1188,                 loss: nan
Episode: 19081/101000 (18.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9718s / 2210.9963 s
agent0:                 episode reward: -0.4999,                 loss: 0.1691
agent1:                 episode reward: 0.4999,                 loss: nan
Episode: 19101/101000 (18.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3066s / 2217.3029 s
agent0:                 episode reward: 0.1825,                 loss: 0.1697
agent1:                 episode reward: -0.1825,                 loss: nan
Episode: 19121/101000 (18.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9544s / 2225.2573 s
agent0:                 episode reward: -0.2527,                 loss: 0.1675
agent1:                 episode reward: 0.2527,                 loss: nan
Episode: 19141/101000 (18.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7553s / 2233.0126 s
agent0:                 episode reward: 0.0717,                 loss: 0.1683
agent1:                 episode reward: -0.0717,                 loss: nan
Episode: 19161/101000 (18.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1953s / 2239.2079 s
agent0:                 episode reward: -0.0417,                 loss: 0.1694
agent1:                 episode reward: 0.0417,                 loss: nan
Episode: 19181/101000 (18.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7246s / 2245.9326 s
agent0:                 episode reward: 0.2598,                 loss: 0.1679
agent1:                 episode reward: -0.2598,                 loss: nan
Episode: 19201/101000 (19.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1933s / 2252.1258 s
agent0:                 episode reward: 0.2744,                 loss: 0.1645
agent1:                 episode reward: -0.2744,                 loss: 0.1944
Score delta: 1.5099328956656737, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/18756_0.
Episode: 19221/101000 (19.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9292s / 2258.0550 s
agent0:                 episode reward: 0.1418,                 loss: nan
agent1:                 episode reward: -0.1418,                 loss: 0.1837
Episode: 19241/101000 (19.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3876s / 2263.4426 s
agent0:                 episode reward: -0.1884,                 loss: nan
agent1:                 episode reward: 0.1884,                 loss: 0.1799
Episode: 19261/101000 (19.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9250s / 2271.3676 s
agent0:                 episode reward: -0.2047,                 loss: nan
agent1:                 episode reward: 0.2047,                 loss: 0.1772
Episode: 19281/101000 (19.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9922s / 2278.3598 s
agent0:                 episode reward: -0.1686,                 loss: nan
agent1:                 episode reward: 0.1686,                 loss: 0.1775
Episode: 19301/101000 (19.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9187s / 2284.2785 s
agent0:                 episode reward: -0.5482,                 loss: nan
agent1:                 episode reward: 0.5482,                 loss: 0.1767
Episode: 19321/101000 (19.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1473s / 2292.4258 s
agent0:                 episode reward: -0.5758,                 loss: 0.1761
agent1:                 episode reward: 0.5758,                 loss: 0.1720
Score delta: 1.5233359971018943, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/18877_1.
Episode: 19341/101000 (19.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2627s / 2298.6885 s
agent0:                 episode reward: -0.0104,                 loss: 0.1735
agent1:                 episode reward: 0.0104,                 loss: nan
Episode: 19361/101000 (19.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6890s / 2305.3776 s
agent0:                 episode reward: 0.1951,                 loss: 0.1740
agent1:                 episode reward: -0.1951,                 loss: nan
Episode: 19381/101000 (19.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9839s / 2313.3615 s
agent0:                 episode reward: -0.2777,                 loss: 0.1741
agent1:                 episode reward: 0.2777,                 loss: nan
Episode: 19401/101000 (19.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9052s / 2321.2667 s
agent0:                 episode reward: -0.3125,                 loss: 0.1743
agent1:                 episode reward: 0.3125,                 loss: nan
Episode: 19421/101000 (19.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7522s / 2329.0189 s
agent0:                 episode reward: 0.0260,                 loss: 0.1750
agent1:                 episode reward: -0.0260,                 loss: nan
Episode: 19441/101000 (19.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4012s / 2335.4202 s
agent0:                 episode reward: 0.2096,                 loss: 0.1761
agent1:                 episode reward: -0.2096,                 loss: nan
Episode: 19461/101000 (19.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9362s / 2340.3564 s
agent0:                 episode reward: -0.1979,                 loss: 0.1699
agent1:                 episode reward: 0.1979,                 loss: nan
Episode: 19481/101000 (19.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9700s / 2347.3264 s
agent0:                 episode reward: -0.0112,                 loss: 0.1704
agent1:                 episode reward: 0.0112,                 loss: nan
Episode: 19501/101000 (19.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8083s / 2354.1347 s
agent0:                 episode reward: 0.4470,                 loss: 0.1669
agent1:                 episode reward: -0.4470,                 loss: 0.1720
Score delta: 1.7057775348353261, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/19071_0.
Episode: 19521/101000 (19.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4690s / 2361.6037 s
agent0:                 episode reward: -0.1627,                 loss: nan
agent1:                 episode reward: 0.1627,                 loss: 0.1713
Episode: 19541/101000 (19.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2827s / 2367.8865 s
agent0:                 episode reward: -0.3291,                 loss: nan
agent1:                 episode reward: 0.3291,                 loss: 0.1702
Episode: 19561/101000 (19.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0310s / 2374.9174 s
agent0:                 episode reward: -0.4907,                 loss: nan
agent1:                 episode reward: 0.4907,                 loss: 0.1708
Episode: 19581/101000 (19.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3991s / 2381.3165 s
agent0:                 episode reward: -0.0299,                 loss: nan
agent1:                 episode reward: 0.0299,                 loss: 0.1697
Episode: 19601/101000 (19.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2120s / 2387.5285 s
agent0:                 episode reward: -0.3704,                 loss: nan
agent1:                 episode reward: 0.3704,                 loss: 0.1694
Episode: 19621/101000 (19.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2354s / 2393.7639 s
agent0:                 episode reward: -0.0702,                 loss: nan
agent1:                 episode reward: 0.0702,                 loss: 0.1668
Episode: 19641/101000 (19.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9340s / 2401.6980 s
agent0:                 episode reward: -0.5120,                 loss: 0.1744
agent1:                 episode reward: 0.5120,                 loss: 0.1647
Score delta: 1.8340959300947042, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/19211_1.
Episode: 19661/101000 (19.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0893s / 2408.7872 s
agent0:                 episode reward: -0.1936,                 loss: 0.1699
agent1:                 episode reward: 0.1936,                 loss: nan
Episode: 19681/101000 (19.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8584s / 2415.6456 s
agent0:                 episode reward: 0.0436,                 loss: 0.1710
agent1:                 episode reward: -0.0436,                 loss: 0.1641
Score delta: 1.685902721323036, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/19245_0.
Episode: 19701/101000 (19.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8632s / 2421.5088 s
agent0:                 episode reward: -0.1750,                 loss: nan
agent1:                 episode reward: 0.1750,                 loss: 0.1615
Episode: 19721/101000 (19.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1637s / 2428.6725 s
agent0:                 episode reward: -0.1654,                 loss: nan
agent1:                 episode reward: 0.1654,                 loss: 0.1621
Episode: 19741/101000 (19.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1945s / 2434.8670 s
agent0:                 episode reward: -0.3157,                 loss: nan
agent1:                 episode reward: 0.3157,                 loss: 0.1617
Episode: 19761/101000 (19.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4432s / 2440.3102 s
agent0:                 episode reward: -0.3296,                 loss: nan
agent1:                 episode reward: 0.3296,                 loss: 0.1604
Episode: 19781/101000 (19.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5925s / 2447.9027 s
agent0:                 episode reward: -0.7096,                 loss: 0.1752
agent1:                 episode reward: 0.7096,                 loss: 0.1615
Score delta: 2.2963755082569586, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/19344_1.
Episode: 19801/101000 (19.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6686s / 2455.5713 s
agent0:                 episode reward: -0.2585,                 loss: 0.1734
agent1:                 episode reward: 0.2585,                 loss: nan
Episode: 19821/101000 (19.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1073s / 2460.6786 s
agent0:                 episode reward: 0.1161,                 loss: 0.1712
agent1:                 episode reward: -0.1161,                 loss: nan
Episode: 19841/101000 (19.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6878s / 2467.3663 s
agent0:                 episode reward: -0.1401,                 loss: 0.1754
agent1:                 episode reward: 0.1401,                 loss: nan
Episode: 19861/101000 (19.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0821s / 2472.4485 s
agent0:                 episode reward: -0.2624,                 loss: 0.1743
agent1:                 episode reward: 0.2624,                 loss: nan
Episode: 19881/101000 (19.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8880s / 2478.3365 s
agent0:                 episode reward: -0.1663,                 loss: 0.1729
agent1:                 episode reward: 0.1663,                 loss: nan
Episode: 19901/101000 (19.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2843s / 2485.6208 s
agent0:                 episode reward: 0.3149,                 loss: 0.1752
agent1:                 episode reward: -0.3149,                 loss: nan
Episode: 19921/101000 (19.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5589s / 2493.1798 s
agent0:                 episode reward: 0.1766,                 loss: 0.1744
agent1:                 episode reward: -0.1766,                 loss: nan
Episode: 19941/101000 (19.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1105s / 2498.2903 s
agent0:                 episode reward: 0.3565,                 loss: 0.1729
agent1:                 episode reward: -0.3565,                 loss: 0.2410
Score delta: 1.6031656284422393, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/19514_0.
Episode: 19961/101000 (19.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6670s / 2504.9572 s
agent0:                 episode reward: -0.2486,                 loss: nan
agent1:                 episode reward: 0.2486,                 loss: 0.2056
Episode: 19981/101000 (19.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7179s / 2510.6751 s
agent0:                 episode reward: -0.2086,                 loss: nan
agent1:                 episode reward: 0.2086,                 loss: 0.1939
Episode: 20001/101000 (19.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8277s / 2517.5028 s
agent0:                 episode reward: -0.2009,                 loss: nan
agent1:                 episode reward: 0.2009,                 loss: 0.1862
Episode: 20021/101000 (19.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0922s / 2522.5950 s
agent0:                 episode reward: 0.0047,                 loss: nan
agent1:                 episode reward: -0.0047,                 loss: 0.1838
Episode: 20041/101000 (19.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6403s / 2529.2353 s
agent0:                 episode reward: -0.0819,                 loss: nan
agent1:                 episode reward: 0.0819,                 loss: 0.1787
Episode: 20061/101000 (19.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8259s / 2536.0611 s
agent0:                 episode reward: -0.4815,                 loss: 0.1670
agent1:                 episode reward: 0.4815,                 loss: 0.1724
Score delta: 1.5632988115835513, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/19622_1.
Episode: 20081/101000 (19.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7102s / 2540.7714 s
agent0:                 episode reward: 0.0872,                 loss: 0.1680
agent1:                 episode reward: -0.0872,                 loss: nan
Episode: 20101/101000 (19.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3634s / 2545.1348 s
agent0:                 episode reward: -0.4957,                 loss: 0.1712
agent1:                 episode reward: 0.4957,                 loss: nan
Episode: 20121/101000 (19.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6319s / 2552.7667 s
agent0:                 episode reward: 0.0375,                 loss: 0.1708
agent1:                 episode reward: -0.0375,                 loss: nan
Episode: 20141/101000 (19.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 8.3732s / 2561.1399 s
agent0:                 episode reward: -0.1906,                 loss: 0.1734
agent1:                 episode reward: 0.1906,                 loss: nan
Episode: 20161/101000 (19.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8451s / 2566.9851 s
agent0:                 episode reward: -0.1912,                 loss: 0.1747
agent1:                 episode reward: 0.1912,                 loss: nan
Episode: 20181/101000 (19.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2919s / 2573.2770 s
agent0:                 episode reward: -0.2566,                 loss: 0.1744
agent1:                 episode reward: 0.2566,                 loss: nan
Episode: 20201/101000 (20.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7577s / 2579.0347 s
agent0:                 episode reward: -0.1862,                 loss: 0.1744
agent1:                 episode reward: 0.1862,                 loss: nan
Episode: 20221/101000 (20.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5587s / 2586.5934 s
agent0:                 episode reward: -0.1956,                 loss: 0.1759
agent1:                 episode reward: 0.1956,                 loss: nan
Episode: 20241/101000 (20.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9322s / 2594.5256 s
agent0:                 episode reward: -0.6000,                 loss: 0.1751
agent1:                 episode reward: 0.6000,                 loss: nan
Episode: 20261/101000 (20.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0835s / 2599.6091 s
agent0:                 episode reward: -0.0339,                 loss: 0.1725
agent1:                 episode reward: 0.0339,                 loss: nan
Episode: 20281/101000 (20.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5693s / 2606.1784 s
agent0:                 episode reward: -0.0265,                 loss: 0.1746
agent1:                 episode reward: 0.0265,                 loss: nan
Episode: 20301/101000 (20.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3877s / 2611.5660 s
agent0:                 episode reward: -0.4287,                 loss: 0.1742
agent1:                 episode reward: 0.4287,                 loss: nan
Episode: 20321/101000 (20.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4697s / 2618.0357 s
agent0:                 episode reward: -0.0742,                 loss: 0.1752
agent1:                 episode reward: 0.0742,                 loss: nan
Episode: 20341/101000 (20.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5288s / 2622.5645 s
agent0:                 episode reward: 0.1837,                 loss: 0.1755
agent1:                 episode reward: -0.1837,                 loss: nan
Episode: 20361/101000 (20.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8066s / 2628.3711 s
agent0:                 episode reward: 0.3107,                 loss: 0.1744
agent1:                 episode reward: -0.3107,                 loss: nan
Episode: 20381/101000 (20.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0581s / 2631.4292 s
agent0:                 episode reward: 0.0275,                 loss: 0.1742
agent1:                 episode reward: -0.0275,                 loss: nan
Episode: 20401/101000 (20.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8674s / 2635.2966 s
agent0:                 episode reward: 0.1161,                 loss: 0.1756
agent1:                 episode reward: -0.1161,                 loss: nan
Episode: 20421/101000 (20.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2330s / 2642.5296 s
agent0:                 episode reward: 0.0550,                 loss: 0.1754
agent1:                 episode reward: -0.0550,                 loss: nan
Episode: 20441/101000 (20.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4482s / 2648.9778 s
agent0:                 episode reward: -0.4985,                 loss: 0.1740
agent1:                 episode reward: 0.4985,                 loss: nan
Episode: 20461/101000 (20.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7428s / 2653.7206 s
agent0:                 episode reward: -0.1360,                 loss: 0.1763
agent1:                 episode reward: 0.1360,                 loss: nan
Episode: 20481/101000 (20.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5553s / 2661.2759 s
agent0:                 episode reward: -0.0673,                 loss: 0.1742
agent1:                 episode reward: 0.0673,                 loss: nan
Episode: 20501/101000 (20.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5993s / 2669.8752 s
agent0:                 episode reward: -0.0518,                 loss: 0.1752
agent1:                 episode reward: 0.0518,                 loss: nan
Episode: 20521/101000 (20.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 8.7455s / 2678.6207 s
agent0:                 episode reward: 0.1661,                 loss: 0.1741
agent1:                 episode reward: -0.1661,                 loss: nan
Episode: 20541/101000 (20.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1448s / 2686.7655 s
agent0:                 episode reward: -0.0585,                 loss: 0.1742
agent1:                 episode reward: 0.0585,                 loss: nan
Episode: 20561/101000 (20.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3350s / 2694.1005 s
agent0:                 episode reward: 0.0788,                 loss: 0.1750
agent1:                 episode reward: -0.0788,                 loss: nan
Episode: 20581/101000 (20.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9513s / 2702.0519 s
agent0:                 episode reward: 0.0292,                 loss: 0.1742
agent1:                 episode reward: -0.0292,                 loss: nan
Episode: 20601/101000 (20.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2237s / 2707.2755 s
agent0:                 episode reward: -0.0401,                 loss: 0.1763
agent1:                 episode reward: 0.0401,                 loss: nan
Episode: 20621/101000 (20.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6558s / 2714.9313 s
agent0:                 episode reward: -0.0843,                 loss: 0.1743
agent1:                 episode reward: 0.0843,                 loss: nan
Episode: 20641/101000 (20.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2145s / 2721.1458 s
agent0:                 episode reward: -0.1035,                 loss: 0.1739
agent1:                 episode reward: 0.1035,                 loss: nan
Episode: 20661/101000 (20.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2538s / 2729.3996 s
agent0:                 episode reward: -0.2084,                 loss: 0.1760
agent1:                 episode reward: 0.2084,                 loss: nan
Episode: 20681/101000 (20.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9093s / 2734.3090 s
agent0:                 episode reward: 0.0693,                 loss: 0.1745
agent1:                 episode reward: -0.0693,                 loss: nan
Episode: 20701/101000 (20.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7715s / 2741.0805 s
agent0:                 episode reward: -0.2470,                 loss: 0.1747
agent1:                 episode reward: 0.2470,                 loss: nan
Episode: 20721/101000 (20.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7656s / 2747.8460 s
agent0:                 episode reward: -0.3617,                 loss: 0.1761
agent1:                 episode reward: 0.3617,                 loss: nan
Episode: 20741/101000 (20.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7144s / 2752.5604 s
agent0:                 episode reward: -0.5387,                 loss: 0.1754
agent1:                 episode reward: 0.5387,                 loss: nan
Episode: 20761/101000 (20.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8327s / 2757.3931 s
agent0:                 episode reward: 0.3081,                 loss: 0.1744
agent1:                 episode reward: -0.3081,                 loss: nan
Episode: 20781/101000 (20.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1208s / 2765.5139 s
agent0:                 episode reward: -0.5629,                 loss: 0.1758
agent1:                 episode reward: 0.5629,                 loss: nan
Episode: 20801/101000 (20.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4455s / 2772.9594 s
agent0:                 episode reward: -0.7076,                 loss: 0.1794
agent1:                 episode reward: 0.7076,                 loss: nan
Episode: 20821/101000 (20.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1107s / 2780.0701 s
agent0:                 episode reward: 0.0275,                 loss: 0.1830
agent1:                 episode reward: -0.0275,                 loss: nan
Episode: 20841/101000 (20.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5404s / 2788.6105 s
agent0:                 episode reward: -0.3736,                 loss: 0.1815
agent1:                 episode reward: 0.3736,                 loss: nan
Episode: 20861/101000 (20.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4893s / 2794.0998 s
agent0:                 episode reward: -0.0349,                 loss: 0.1833
agent1:                 episode reward: 0.0349,                 loss: nan
Episode: 20881/101000 (20.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8063s / 2802.9061 s
agent0:                 episode reward: 0.1503,                 loss: 0.1831
agent1:                 episode reward: -0.1503,                 loss: nan
Episode: 20901/101000 (20.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3183s / 2808.2244 s
agent0:                 episode reward: -0.1516,                 loss: 0.1834
agent1:                 episode reward: 0.1516,                 loss: nan
Episode: 20921/101000 (20.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0572s / 2814.2816 s
agent0:                 episode reward: 0.1188,                 loss: 0.1820
agent1:                 episode reward: -0.1188,                 loss: nan
Episode: 20941/101000 (20.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0033s / 2821.2849 s
agent0:                 episode reward: -0.2006,                 loss: 0.1817
agent1:                 episode reward: 0.2006,                 loss: nan
Episode: 20961/101000 (20.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5639s / 2827.8489 s
agent0:                 episode reward: -0.1167,                 loss: 0.1818
agent1:                 episode reward: 0.1167,                 loss: nan
Episode: 20981/101000 (20.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0740s / 2834.9228 s
agent0:                 episode reward: 0.0812,                 loss: 0.1819
agent1:                 episode reward: -0.0812,                 loss: nan
Episode: 21001/101000 (20.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4592s / 2841.3820 s
agent0:                 episode reward: 0.0086,                 loss: 0.1821
agent1:                 episode reward: -0.0086,                 loss: nan
Episode: 21021/101000 (20.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5339s / 2848.9159 s
agent0:                 episode reward: -0.2580,                 loss: 0.1814
agent1:                 episode reward: 0.2580,                 loss: nan
Episode: 21041/101000 (20.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7632s / 2853.6791 s
agent0:                 episode reward: 0.2000,                 loss: 0.1830
agent1:                 episode reward: -0.2000,                 loss: nan
Episode: 21061/101000 (20.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5545s / 2860.2336 s
agent0:                 episode reward: 0.3557,                 loss: 0.1807
agent1:                 episode reward: -0.3557,                 loss: nan
Episode: 21081/101000 (20.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6129s / 2865.8465 s
agent0:                 episode reward: 0.1128,                 loss: 0.1798
agent1:                 episode reward: -0.1128,                 loss: nan
Episode: 21101/101000 (20.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2086s / 2872.0551 s
agent0:                 episode reward: -0.2418,                 loss: 0.1808
agent1:                 episode reward: 0.2418,                 loss: nan
Episode: 21121/101000 (20.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7983s / 2879.8534 s
agent0:                 episode reward: -0.0110,                 loss: 0.1819
agent1:                 episode reward: 0.0110,                 loss: nan
Episode: 21141/101000 (20.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2452s / 2887.0985 s
agent0:                 episode reward: 0.1179,                 loss: 0.1798
agent1:                 episode reward: -0.1179,                 loss: nan
Episode: 21161/101000 (20.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7031s / 2893.8016 s
agent0:                 episode reward: 0.0270,                 loss: 0.1805
agent1:                 episode reward: -0.0270,                 loss: 0.1725
Score delta: 1.509547623591319, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/20727_0.
Episode: 21181/101000 (20.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7034s / 2901.5050 s
agent0:                 episode reward: 0.0539,                 loss: nan
agent1:                 episode reward: -0.0539,                 loss: 0.1702
Episode: 21201/101000 (20.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8707s / 2907.3757 s
agent0:                 episode reward: -0.1351,                 loss: nan
agent1:                 episode reward: 0.1351,                 loss: 0.1684
Episode: 21221/101000 (21.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5370s / 2913.9127 s
agent0:                 episode reward: -0.2448,                 loss: nan
agent1:                 episode reward: 0.2448,                 loss: 0.1660
Episode: 21241/101000 (21.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3822s / 2920.2948 s
agent0:                 episode reward: -0.3118,                 loss: 0.1716
agent1:                 episode reward: 0.3118,                 loss: 0.1672
Score delta: 1.5274339299663897, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/20799_1.
Episode: 21261/101000 (21.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9496s / 2928.2445 s
agent0:                 episode reward: -0.3820,                 loss: 0.1720
agent1:                 episode reward: 0.3820,                 loss: nan
Episode: 21281/101000 (21.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5909s / 2934.8354 s
agent0:                 episode reward: -0.0544,                 loss: 0.1729
agent1:                 episode reward: 0.0544,                 loss: nan
Episode: 21301/101000 (21.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 7.5144s / 2942.3498 s
agent0:                 episode reward: -0.0644,                 loss: 0.1733
agent1:                 episode reward: 0.0644,                 loss: nan
Episode: 21321/101000 (21.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 7.8749s / 2950.2247 s
agent0:                 episode reward: -0.1850,                 loss: 0.1721
agent1:                 episode reward: 0.1850,                 loss: nan
Episode: 21341/101000 (21.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0167s / 2955.2414 s
agent0:                 episode reward: -0.0894,                 loss: 0.1739
agent1:                 episode reward: 0.0894,                 loss: 0.1654
Score delta: 1.5454851340582403, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/20905_0.
Episode: 21361/101000 (21.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1853s / 2961.4266 s
agent0:                 episode reward: 0.1981,                 loss: nan
agent1:                 episode reward: -0.1981,                 loss: 0.1633
Episode: 21381/101000 (21.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3316s / 2968.7582 s
agent0:                 episode reward: -0.2600,                 loss: nan
agent1:                 episode reward: 0.2600,                 loss: 0.1641
Episode: 21401/101000 (21.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6353s / 2976.3935 s
agent0:                 episode reward: -0.5365,                 loss: 0.1746
agent1:                 episode reward: 0.5365,                 loss: 0.1648
Score delta: 1.6286555091527415, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/20960_1.
Episode: 21421/101000 (21.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2754s / 2983.6689 s
agent0:                 episode reward: 0.0786,                 loss: 0.1711
agent1:                 episode reward: -0.0786,                 loss: nan
Episode: 21441/101000 (21.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 7.1255s / 2990.7944 s
agent0:                 episode reward: -0.5878,                 loss: 0.1712
agent1:                 episode reward: 0.5878,                 loss: nan
Episode: 21461/101000 (21.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 8.4974s / 2999.2917 s
agent0:                 episode reward: -0.0464,                 loss: 0.1718
agent1:                 episode reward: 0.0464,                 loss: nan
Episode: 21481/101000 (21.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6154s / 3005.9071 s
agent0:                 episode reward: 0.1056,                 loss: 0.1737
agent1:                 episode reward: -0.1056,                 loss: nan
Episode: 21501/101000 (21.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7359s / 3012.6430 s
agent0:                 episode reward: -0.0796,                 loss: 0.1712
agent1:                 episode reward: 0.0796,                 loss: nan
Episode: 21521/101000 (21.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 7.4335s / 3020.0765 s
agent0:                 episode reward: 0.0930,                 loss: 0.1732
agent1:                 episode reward: -0.0930,                 loss: nan
Episode: 21541/101000 (21.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3287s / 3025.4052 s
agent0:                 episode reward: -0.0012,                 loss: 0.1707
agent1:                 episode reward: 0.0012,                 loss: nan
Episode: 21561/101000 (21.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8469s / 3030.2521 s
agent0:                 episode reward: -0.3707,                 loss: 0.1733
agent1:                 episode reward: 0.3707,                 loss: nan
Episode: 21581/101000 (21.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3913s / 3037.6434 s
agent0:                 episode reward: 0.0105,                 loss: 0.1725
agent1:                 episode reward: -0.0105,                 loss: nan
Episode: 21601/101000 (21.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6652s / 3044.3086 s
agent0:                 episode reward: -0.4095,                 loss: 0.1782
agent1:                 episode reward: 0.4095,                 loss: nan
Episode: 21621/101000 (21.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1400s / 3053.4486 s
agent0:                 episode reward: -0.1879,                 loss: 0.1765
agent1:                 episode reward: 0.1879,                 loss: nan
Episode: 21641/101000 (21.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8655s / 3059.3141 s
agent0:                 episode reward: 0.2029,                 loss: 0.1752
agent1:                 episode reward: -0.2029,                 loss: nan
Episode: 21661/101000 (21.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1357s / 3065.4498 s
agent0:                 episode reward: -0.2115,                 loss: 0.1777
agent1:                 episode reward: 0.2115,                 loss: nan
Episode: 21681/101000 (21.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7451s / 3071.1949 s
agent0:                 episode reward: -0.0104,                 loss: 0.1781
agent1:                 episode reward: 0.0104,                 loss: nan
Episode: 21701/101000 (21.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 9.4715s / 3080.6663 s
agent0:                 episode reward: 0.0922,                 loss: 0.1760
agent1:                 episode reward: -0.0922,                 loss: nan
Episode: 21721/101000 (21.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2684s / 3088.9348 s
agent0:                 episode reward: 0.0158,                 loss: 0.1768
agent1:                 episode reward: -0.0158,                 loss: nan
Episode: 21741/101000 (21.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6400s / 3096.5747 s
agent0:                 episode reward: 0.2148,                 loss: 0.1759
agent1:                 episode reward: -0.2148,                 loss: nan
Episode: 21761/101000 (21.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1519s / 3104.7267 s
agent0:                 episode reward: -0.2564,                 loss: 0.1784
agent1:                 episode reward: 0.2564,                 loss: nan
Episode: 21781/101000 (21.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8685s / 3111.5952 s
agent0:                 episode reward: -0.1577,                 loss: 0.1763
agent1:                 episode reward: 0.1577,                 loss: nan
Episode: 21801/101000 (21.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9857s / 3118.5809 s
agent0:                 episode reward: -0.1594,                 loss: 0.1754
agent1:                 episode reward: 0.1594,                 loss: nan
Episode: 21821/101000 (21.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2580s / 3125.8389 s
agent0:                 episode reward: 0.3948,                 loss: 0.1772
agent1:                 episode reward: -0.3948,                 loss: nan
Episode: 21841/101000 (21.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 8.8535s / 3134.6924 s
agent0:                 episode reward: 0.3048,                 loss: 0.1772
agent1:                 episode reward: -0.3048,                 loss: nan
Episode: 21861/101000 (21.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8960s / 3140.5885 s
agent0:                 episode reward: -0.2415,                 loss: 0.1766
agent1:                 episode reward: 0.2415,                 loss: nan
Episode: 21881/101000 (21.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 8.9062s / 3149.4947 s
agent0:                 episode reward: 0.0615,                 loss: 0.1773
agent1:                 episode reward: -0.0615,                 loss: nan
Episode: 21901/101000 (21.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7241s / 3156.2189 s
agent0:                 episode reward: -0.5256,                 loss: 0.1759
agent1:                 episode reward: 0.5256,                 loss: nan
Episode: 21921/101000 (21.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7336s / 3162.9525 s
agent0:                 episode reward: -0.5741,                 loss: 0.1779
agent1:                 episode reward: 0.5741,                 loss: nan
Episode: 21941/101000 (21.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 7.3600s / 3170.3125 s
agent0:                 episode reward: 0.1565,                 loss: 0.1785
agent1:                 episode reward: -0.1565,                 loss: nan
Episode: 21961/101000 (21.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4971s / 3173.8096 s
agent0:                 episode reward: -0.5184,                 loss: 0.1775
agent1:                 episode reward: 0.5184,                 loss: nan
Episode: 21981/101000 (21.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6589s / 3181.4685 s
agent0:                 episode reward: -0.1719,                 loss: 0.1798
agent1:                 episode reward: 0.1719,                 loss: nan
Episode: 22001/101000 (21.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3811s / 3187.8496 s
agent0:                 episode reward: -0.2222,                 loss: 0.1768
agent1:                 episode reward: 0.2222,                 loss: nan
Episode: 22021/101000 (21.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0050s / 3194.8546 s
agent0:                 episode reward: -0.0629,                 loss: 0.1775
agent1:                 episode reward: 0.0629,                 loss: nan
Episode: 22041/101000 (21.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 8.1312s / 3202.9858 s
agent0:                 episode reward: -0.2463,                 loss: 0.1797
agent1:                 episode reward: 0.2463,                 loss: nan
Episode: 22061/101000 (21.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 7.6145s / 3210.6003 s
agent0:                 episode reward: -0.0589,                 loss: 0.1769
agent1:                 episode reward: 0.0589,                 loss: nan
Episode: 22081/101000 (21.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 6.8454s / 3217.4457 s
agent0:                 episode reward: -0.3006,                 loss: 0.1788
agent1:                 episode reward: 0.3006,                 loss: nan
Episode: 22101/101000 (21.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7064s / 3225.1522 s
agent0:                 episode reward: -0.3188,                 loss: 0.1768
agent1:                 episode reward: 0.3188,                 loss: nan
Episode: 22121/101000 (21.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3334s / 3231.4855 s
agent0:                 episode reward: 0.0201,                 loss: 0.1782
agent1:                 episode reward: -0.0201,                 loss: nan
Episode: 22141/101000 (21.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 7.9186s / 3239.4041 s
agent0:                 episode reward: 0.3707,                 loss: 0.1778
agent1:                 episode reward: -0.3707,                 loss: 0.1779
Score delta: 1.5644648013909468, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/21712_0.
Episode: 22161/101000 (21.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7281s / 3245.1322 s
agent0:                 episode reward: -0.3472,                 loss: nan
agent1:                 episode reward: 0.3472,                 loss: 0.1749
Episode: 22181/101000 (21.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2390s / 3251.3712 s
agent0:                 episode reward: -0.1946,                 loss: 0.1776
agent1:                 episode reward: 0.1946,                 loss: 0.1731
Score delta: 1.5614832729538985, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/21737_1.
Episode: 22201/101000 (21.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2546s / 3258.6258 s
agent0:                 episode reward: 0.0922,                 loss: 0.1801
agent1:                 episode reward: -0.0922,                 loss: nan
Episode: 22221/101000 (22.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 7.0451s / 3265.6709 s
agent0:                 episode reward: -0.1320,                 loss: 0.1778
agent1:                 episode reward: 0.1320,                 loss: nan
Episode: 22241/101000 (22.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6259s / 3271.2969 s
agent0:                 episode reward: 0.0039,                 loss: 0.1787
agent1:                 episode reward: -0.0039,                 loss: nan
Episode: 22261/101000 (22.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2869s / 3279.5837 s
agent0:                 episode reward: 0.0158,                 loss: 0.1795
agent1:                 episode reward: -0.0158,                 loss: nan
Episode: 22281/101000 (22.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 8.2272s / 3287.8109 s
agent0:                 episode reward: -0.0580,                 loss: 0.1785
agent1:                 episode reward: 0.0580,                 loss: nan
Episode: 22301/101000 (22.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 9.2522s / 3297.0631 s
agent0:                 episode reward: -0.3690,                 loss: 0.1789
agent1:                 episode reward: 0.3690,                 loss: nan
Episode: 22321/101000 (22.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 7.7517s / 3304.8148 s
agent0:                 episode reward: -0.5542,                 loss: 0.1817
agent1:                 episode reward: 0.5542,                 loss: nan
Episode: 22341/101000 (22.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 9.1743s / 3313.9891 s
agent0:                 episode reward: -0.0965,                 loss: 0.1800
agent1:                 episode reward: 0.0965,                 loss: nan
Episode: 22361/101000 (22.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 8.5533s / 3322.5424 s
agent0:                 episode reward: -0.2101,                 loss: 0.1799
agent1:                 episode reward: 0.2101,                 loss: nan
Episode: 22381/101000 (22.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 7.2534s / 3329.7958 s
agent0:                 episode reward: -0.4059,                 loss: 0.1806
agent1:                 episode reward: 0.4059,                 loss: nan
Episode: 22401/101000 (22.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 8.0387s / 3337.8345 s
agent0:                 episode reward: -0.4765,                 loss: 0.1788
agent1:                 episode reward: 0.4765,                 loss: nan
Episode: 22421/101000 (22.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 6.9093s / 3344.7438 s
agent0:                 episode reward: 0.0143,                 loss: 0.1808
agent1:                 episode reward: -0.0143,                 loss: nan
Episode: 22441/101000 (22.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9689s / 3347.7127 s
agent0:                 episode reward: 0.0281,                 loss: 0.1797
agent1:                 episode reward: -0.0281,                 loss: nan
Episode: 22461/101000 (22.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1304s / 3349.8431 s
agent0:                 episode reward: 0.1301,                 loss: 0.1787
agent1:                 episode reward: -0.1301,                 loss: nan
Episode: 22481/101000 (22.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6818s / 3352.5249 s
agent0:                 episode reward: -0.0980,                 loss: 0.1784
agent1:                 episode reward: 0.0980,                 loss: nan
Episode: 22501/101000 (22.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1093s / 3355.6341 s
agent0:                 episode reward: -0.0093,                 loss: 0.1805
agent1:                 episode reward: 0.0093,                 loss: nan
Episode: 22521/101000 (22.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1475s / 3357.7816 s
agent0:                 episode reward: 0.2042,                 loss: 0.1810
agent1:                 episode reward: -0.2042,                 loss: nan
Episode: 22541/101000 (22.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5721s / 3360.3537 s
agent0:                 episode reward: -0.3691,                 loss: 0.1794
agent1:                 episode reward: 0.3691,                 loss: nan
Episode: 22561/101000 (22.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8846s / 3363.2383 s
agent0:                 episode reward: -0.5020,                 loss: 0.1803
agent1:                 episode reward: 0.5020,                 loss: nan
Episode: 22581/101000 (22.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2084s / 3365.4467 s
agent0:                 episode reward: -0.2903,                 loss: 0.1800
agent1:                 episode reward: 0.2903,                 loss: nan
Episode: 22601/101000 (22.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8879s / 3368.3347 s
agent0:                 episode reward: -0.0307,                 loss: 0.1809
agent1:                 episode reward: 0.0307,                 loss: nan
Episode: 22621/101000 (22.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7226s / 3371.0572 s
agent0:                 episode reward: -0.1882,                 loss: 0.1809
agent1:                 episode reward: 0.1882,                 loss: nan
Episode: 22641/101000 (22.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2823s / 3374.3396 s
agent0:                 episode reward: 0.1182,                 loss: 0.1802
agent1:                 episode reward: -0.1182,                 loss: nan
Episode: 22661/101000 (22.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4805s / 3376.8200 s
agent0:                 episode reward: -0.2145,                 loss: 0.1789
agent1:                 episode reward: 0.2145,                 loss: nan
Episode: 22681/101000 (22.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4576s / 3379.2776 s
agent0:                 episode reward: -0.6445,                 loss: 0.1808
agent1:                 episode reward: 0.6445,                 loss: nan
Episode: 22701/101000 (22.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0481s / 3382.3258 s
agent0:                 episode reward: 0.2868,                 loss: 0.1787
agent1:                 episode reward: -0.2868,                 loss: nan
Score delta: 1.7975737068125537, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/22275_0.
Episode: 22721/101000 (22.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3254s / 3384.6511 s
agent0:                 episode reward: -0.3789,                 loss: nan
agent1:                 episode reward: 0.3789,                 loss: 0.2098
Episode: 22741/101000 (22.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9671s / 3387.6182 s
agent0:                 episode reward: 0.0581,                 loss: nan
agent1:                 episode reward: -0.0581,                 loss: 0.1919
Episode: 22761/101000 (22.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2390s / 3389.8572 s
agent0:                 episode reward: 0.1287,                 loss: nan
agent1:                 episode reward: -0.1287,                 loss: 0.1866
Episode: 22781/101000 (22.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3434s / 3392.2006 s
agent0:                 episode reward: -0.2374,                 loss: nan
agent1:                 episode reward: 0.2374,                 loss: 0.1817
Episode: 22801/101000 (22.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4737s / 3394.6743 s
agent0:                 episode reward: -0.2044,                 loss: nan
agent1:                 episode reward: 0.2044,                 loss: 0.1815
Episode: 22821/101000 (22.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5834s / 3397.2577 s
agent0:                 episode reward: -0.2430,                 loss: nan
agent1:                 episode reward: 0.2430,                 loss: 0.1787
Episode: 22841/101000 (22.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6366s / 3399.8943 s
agent0:                 episode reward: -0.2813,                 loss: 0.1789
agent1:                 episode reward: 0.2813,                 loss: 0.1749
Score delta: 1.5635501768518398, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/22408_1.
Episode: 22861/101000 (22.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3663s / 3403.2606 s
agent0:                 episode reward: 0.0830,                 loss: 0.1791
agent1:                 episode reward: -0.0830,                 loss: nan
Episode: 22881/101000 (22.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0759s / 3406.3366 s
agent0:                 episode reward: -0.6477,                 loss: 0.1758
agent1:                 episode reward: 0.6477,                 loss: nan
Episode: 22901/101000 (22.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3841s / 3409.7207 s
agent0:                 episode reward: -0.2317,                 loss: 0.1766
agent1:                 episode reward: 0.2317,                 loss: nan
Episode: 22921/101000 (22.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8042s / 3412.5248 s
agent0:                 episode reward: -0.3995,                 loss: 0.1765
agent1:                 episode reward: 0.3995,                 loss: nan
Episode: 22941/101000 (22.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8989s / 3415.4238 s
agent0:                 episode reward: 0.2267,                 loss: 0.1744
agent1:                 episode reward: -0.2267,                 loss: nan
Episode: 22961/101000 (22.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9956s / 3418.4193 s
agent0:                 episode reward: 0.0078,                 loss: 0.1758
agent1:                 episode reward: -0.0078,                 loss: nan
Episode: 22981/101000 (22.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5848s / 3421.0041 s
agent0:                 episode reward: -0.3331,                 loss: 0.1750
agent1:                 episode reward: 0.3331,                 loss: nan
Episode: 23001/101000 (22.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9392s / 3423.9433 s
agent0:                 episode reward: -0.0212,                 loss: 0.1766
agent1:                 episode reward: 0.0212,                 loss: nan
Episode: 23021/101000 (22.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5906s / 3426.5340 s
agent0:                 episode reward: -0.0375,                 loss: 0.1740
agent1:                 episode reward: 0.0375,                 loss: nan
Episode: 23041/101000 (22.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7066s / 3429.2405 s
agent0:                 episode reward: 0.0546,                 loss: 0.1753
agent1:                 episode reward: -0.0546,                 loss: nan
Episode: 23061/101000 (22.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6638s / 3431.9043 s
agent0:                 episode reward: 0.0361,                 loss: 0.1749
agent1:                 episode reward: -0.0361,                 loss: nan
Episode: 23081/101000 (22.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8024s / 3434.7067 s
agent0:                 episode reward: -0.4223,                 loss: 0.1801
agent1:                 episode reward: 0.4223,                 loss: nan
Episode: 23101/101000 (22.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9659s / 3437.6726 s
agent0:                 episode reward: -0.2269,                 loss: 0.1885
agent1:                 episode reward: 0.2269,                 loss: nan
Episode: 23121/101000 (22.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1521s / 3440.8247 s
agent0:                 episode reward: -0.4233,                 loss: 0.1896
agent1:                 episode reward: 0.4233,                 loss: nan
Episode: 23141/101000 (22.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4173s / 3444.2420 s
agent0:                 episode reward: -0.2837,                 loss: 0.1906
agent1:                 episode reward: 0.2837,                 loss: nan
Episode: 23161/101000 (22.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0545s / 3447.2965 s
agent0:                 episode reward: -0.0446,                 loss: 0.1881
agent1:                 episode reward: 0.0446,                 loss: nan
Episode: 23181/101000 (22.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4466s / 3450.7431 s
agent0:                 episode reward: 0.1262,                 loss: 0.1885
agent1:                 episode reward: -0.1262,                 loss: nan
Episode: 23201/101000 (22.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7817s / 3453.5248 s
agent0:                 episode reward: 0.0202,                 loss: 0.1876
agent1:                 episode reward: -0.0202,                 loss: nan
Episode: 23221/101000 (22.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5265s / 3457.0513 s
agent0:                 episode reward: -0.1099,                 loss: 0.1875
agent1:                 episode reward: 0.1099,                 loss: nan
Episode: 23241/101000 (23.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2717s / 3460.3230 s
agent0:                 episode reward: -0.0297,                 loss: 0.1889
agent1:                 episode reward: 0.0297,                 loss: nan
Episode: 23261/101000 (23.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5875s / 3462.9105 s
agent0:                 episode reward: 0.3176,                 loss: 0.1889
agent1:                 episode reward: -0.3176,                 loss: 0.2211
Score delta: 1.7152802829966045, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/22828_0.
Episode: 23281/101000 (23.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8204s / 3465.7309 s
agent0:                 episode reward: -0.0629,                 loss: nan
agent1:                 episode reward: 0.0629,                 loss: 0.1946
Episode: 23301/101000 (23.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9291s / 3468.6599 s
agent0:                 episode reward: -0.3721,                 loss: 0.1797
agent1:                 episode reward: 0.3721,                 loss: 0.1775
Score delta: 1.6424086564324043, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/22871_1.
Episode: 23321/101000 (23.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7486s / 3471.4086 s
agent0:                 episode reward: -0.0245,                 loss: 0.1800
agent1:                 episode reward: 0.0245,                 loss: nan
Episode: 23341/101000 (23.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5268s / 3473.9353 s
agent0:                 episode reward: -0.2727,                 loss: 0.1816
agent1:                 episode reward: 0.2727,                 loss: nan
Episode: 23361/101000 (23.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5837s / 3476.5190 s
agent0:                 episode reward: 0.0083,                 loss: 0.1799
agent1:                 episode reward: -0.0083,                 loss: nan
Episode: 23381/101000 (23.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6499s / 3479.1689 s
agent0:                 episode reward: 0.0518,                 loss: 0.1815
agent1:                 episode reward: -0.0518,                 loss: nan
Episode: 23401/101000 (23.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6723s / 3481.8412 s
agent0:                 episode reward: -0.4975,                 loss: 0.1790
agent1:                 episode reward: 0.4975,                 loss: nan
Episode: 23421/101000 (23.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7655s / 3484.6067 s
agent0:                 episode reward: -0.2624,                 loss: 0.1790
agent1:                 episode reward: 0.2624,                 loss: nan
Episode: 23441/101000 (23.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5977s / 3487.2044 s
agent0:                 episode reward: 0.1660,                 loss: 0.1792
agent1:                 episode reward: -0.1660,                 loss: nan
Episode: 23461/101000 (23.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4787s / 3490.6831 s
agent0:                 episode reward: -0.1642,                 loss: 0.1855
agent1:                 episode reward: 0.1642,                 loss: nan
Episode: 23481/101000 (23.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9949s / 3493.6780 s
agent0:                 episode reward: -0.3940,                 loss: 0.1924
agent1:                 episode reward: 0.3940,                 loss: nan
Episode: 23501/101000 (23.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3225s / 3496.0005 s
agent0:                 episode reward: -0.5840,                 loss: 0.1911
agent1:                 episode reward: 0.5840,                 loss: nan
Episode: 23521/101000 (23.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9238s / 3498.9243 s
agent0:                 episode reward: -0.1334,                 loss: 0.1920
agent1:                 episode reward: 0.1334,                 loss: nan
Episode: 23541/101000 (23.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6771s / 3501.6014 s
agent0:                 episode reward: 0.2822,                 loss: 0.1918
agent1:                 episode reward: -0.2822,                 loss: nan
Episode: 23561/101000 (23.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7019s / 3504.3034 s
agent0:                 episode reward: 0.0911,                 loss: 0.1905
agent1:                 episode reward: -0.0911,                 loss: nan
Episode: 23581/101000 (23.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6442s / 3506.9476 s
agent0:                 episode reward: -0.1446,                 loss: 0.1914
agent1:                 episode reward: 0.1446,                 loss: nan
Episode: 23601/101000 (23.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3050s / 3509.2525 s
agent0:                 episode reward: 0.3518,                 loss: 0.1931
agent1:                 episode reward: -0.3518,                 loss: 0.1657
Score delta: 2.1071573788433113, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/23160_0.
Episode: 23621/101000 (23.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5886s / 3511.8412 s
agent0:                 episode reward: -0.7874,                 loss: 0.1746
agent1:                 episode reward: 0.7874,                 loss: 0.1618
Score delta: 2.0081873428341126, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/23194_1.
Episode: 23641/101000 (23.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3228s / 3514.1640 s
agent0:                 episode reward: -0.4885,                 loss: 0.1792
agent1:                 episode reward: 0.4885,                 loss: nan
Episode: 23661/101000 (23.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7570s / 3516.9209 s
agent0:                 episode reward: -0.5093,                 loss: 0.1803
agent1:                 episode reward: 0.5093,                 loss: nan
Episode: 23681/101000 (23.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0069s / 3520.9279 s
agent0:                 episode reward: -0.3434,                 loss: 0.1802
agent1:                 episode reward: 0.3434,                 loss: nan
Episode: 23701/101000 (23.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6521s / 3523.5800 s
agent0:                 episode reward: -0.4795,                 loss: 0.1803
agent1:                 episode reward: 0.4795,                 loss: nan
Episode: 23721/101000 (23.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0002s / 3526.5803 s
agent0:                 episode reward: 0.1493,                 loss: 0.1820
agent1:                 episode reward: -0.1493,                 loss: 0.1688
Score delta: 1.5194561417514063, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/23284_0.
Episode: 23741/101000 (23.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5291s / 3529.1093 s
agent0:                 episode reward: -0.4765,                 loss: nan
agent1:                 episode reward: 0.4765,                 loss: 0.1661
Episode: 23761/101000 (23.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4719s / 3531.5812 s
agent0:                 episode reward: -0.2558,                 loss: nan
agent1:                 episode reward: 0.2558,                 loss: 0.1652
Episode: 23781/101000 (23.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9633s / 3534.5445 s
agent0:                 episode reward: -0.2126,                 loss: 0.1874
agent1:                 episode reward: 0.2126,                 loss: 0.1633
Score delta: 1.8909566323851492, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/23341_1.
Episode: 23801/101000 (23.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3545s / 3536.8991 s
agent0:                 episode reward: 0.2685,                 loss: 0.1856
agent1:                 episode reward: -0.2685,                 loss: 0.1655
Score delta: 1.6179593916348547, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/23371_0.
Episode: 23821/101000 (23.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2761s / 3539.1752 s
agent0:                 episode reward: -0.1856,                 loss: nan
agent1:                 episode reward: 0.1856,                 loss: 0.1625
Episode: 23841/101000 (23.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8995s / 3542.0747 s
agent0:                 episode reward: -0.1045,                 loss: nan
agent1:                 episode reward: 0.1045,                 loss: 0.1633
Episode: 23861/101000 (23.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1294s / 3545.2041 s
agent0:                 episode reward: -0.3543,                 loss: nan
agent1:                 episode reward: 0.3543,                 loss: 0.1610
Episode: 23881/101000 (23.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1861s / 3547.3902 s
agent0:                 episode reward: -0.3463,                 loss: nan
agent1:                 episode reward: 0.3463,                 loss: 0.1622
Episode: 23901/101000 (23.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5296s / 3549.9198 s
agent0:                 episode reward: -0.0254,                 loss: nan
agent1:                 episode reward: 0.0254,                 loss: 0.1612
Episode: 23921/101000 (23.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6147s / 3552.5344 s
agent0:                 episode reward: -0.7258,                 loss: 0.1983
agent1:                 episode reward: 0.7258,                 loss: 0.1612
Score delta: 1.8930419894953996, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/23481_1.
Episode: 23941/101000 (23.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2131s / 3555.7475 s
agent0:                 episode reward: -0.9744,                 loss: 0.1758
agent1:                 episode reward: 0.9744,                 loss: nan
Episode: 23961/101000 (23.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9123s / 3558.6598 s
agent0:                 episode reward: -0.6824,                 loss: 0.1645
agent1:                 episode reward: 0.6824,                 loss: nan
Episode: 23981/101000 (23.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6525s / 3561.3123 s
agent0:                 episode reward: -0.2547,                 loss: 0.1597
agent1:                 episode reward: 0.2547,                 loss: nan
Episode: 24001/101000 (23.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1754s / 3564.4877 s
agent0:                 episode reward: 0.1559,                 loss: 0.1728
agent1:                 episode reward: -0.1559,                 loss: nan
Episode: 24021/101000 (23.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3281s / 3567.8158 s
agent0:                 episode reward: -0.4020,                 loss: 0.1777
agent1:                 episode reward: 0.4020,                 loss: nan
Episode: 24041/101000 (23.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6016s / 3570.4174 s
agent0:                 episode reward: -0.2282,                 loss: 0.1754
agent1:                 episode reward: 0.2282,                 loss: nan
Episode: 24061/101000 (23.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1396s / 3573.5570 s
agent0:                 episode reward: -0.5415,                 loss: 0.1742
agent1:                 episode reward: 0.5415,                 loss: nan
Episode: 24081/101000 (23.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2122s / 3576.7692 s
agent0:                 episode reward: -0.1698,                 loss: 0.1734
agent1:                 episode reward: 0.1698,                 loss: nan
Episode: 24101/101000 (23.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4450s / 3579.2142 s
agent0:                 episode reward: -0.1083,                 loss: 0.1727
agent1:                 episode reward: 0.1083,                 loss: nan
Episode: 24121/101000 (23.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6368s / 3582.8510 s
agent0:                 episode reward: -0.6430,                 loss: 0.1729
agent1:                 episode reward: 0.6430,                 loss: nan
Episode: 24141/101000 (23.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0235s / 3585.8745 s
agent0:                 episode reward: 0.0475,                 loss: 0.1732
agent1:                 episode reward: -0.0475,                 loss: nan
Episode: 24161/101000 (23.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4635s / 3588.3380 s
agent0:                 episode reward: -0.4517,                 loss: 0.1796
agent1:                 episode reward: 0.4517,                 loss: 0.1627
Score delta: 1.855577944026754, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/23717_0.
Episode: 24181/101000 (23.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0343s / 3591.3723 s
agent0:                 episode reward: -0.3627,                 loss: nan
agent1:                 episode reward: 0.3627,                 loss: 0.1608
Episode: 24201/101000 (23.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0480s / 3594.4203 s
agent0:                 episode reward: -0.2871,                 loss: nan
agent1:                 episode reward: 0.2871,                 loss: 0.1606
Episode: 24221/101000 (23.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5940s / 3597.0143 s
agent0:                 episode reward: -0.6390,                 loss: 0.1898
agent1:                 episode reward: 0.6390,                 loss: 0.1617
Score delta: 1.5892909580210322, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/23782_1.
Episode: 24241/101000 (24.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5734s / 3599.5877 s
agent0:                 episode reward: 0.2509,                 loss: 0.1868
agent1:                 episode reward: -0.2509,                 loss: nan
Episode: 24261/101000 (24.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5579s / 3602.1455 s
agent0:                 episode reward: -0.3141,                 loss: 0.1849
agent1:                 episode reward: 0.3141,                 loss: 0.1687
Score delta: 1.5338991097761325, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/23817_0.
Episode: 24281/101000 (24.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4135s / 3604.5591 s
agent0:                 episode reward: -0.2669,                 loss: nan
agent1:                 episode reward: 0.2669,                 loss: 0.1666
Episode: 24301/101000 (24.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7369s / 3607.2959 s
agent0:                 episode reward: -0.2942,                 loss: nan
agent1:                 episode reward: 0.2942,                 loss: 0.1644
Episode: 24321/101000 (24.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6068s / 3609.9027 s
agent0:                 episode reward: -0.3509,                 loss: nan
agent1:                 episode reward: 0.3509,                 loss: 0.1611
Episode: 24341/101000 (24.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5240s / 3612.4267 s
agent0:                 episode reward: -0.1984,                 loss: nan
agent1:                 episode reward: 0.1984,                 loss: 0.1592
Episode: 24361/101000 (24.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3616s / 3614.7882 s
agent0:                 episode reward: -0.1819,                 loss: nan
agent1:                 episode reward: 0.1819,                 loss: 0.1594
Episode: 24381/101000 (24.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3883s / 3617.1766 s
agent0:                 episode reward: -0.2227,                 loss: nan
agent1:                 episode reward: 0.2227,                 loss: 0.1588
Episode: 24401/101000 (24.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5507s / 3619.7273 s
agent0:                 episode reward: -0.3397,                 loss: 0.1942
agent1:                 episode reward: 0.3397,                 loss: 0.1643
Score delta: 1.5315027082375843, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/23956_1.
Episode: 24421/101000 (24.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0893s / 3622.8167 s
agent0:                 episode reward: -0.5256,                 loss: 0.1912
agent1:                 episode reward: 0.5256,                 loss: nan
Episode: 24441/101000 (24.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6875s / 3625.5042 s
agent0:                 episode reward: 0.0855,                 loss: 0.1913
agent1:                 episode reward: -0.0855,                 loss: nan
Episode: 24461/101000 (24.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6201s / 3628.1243 s
agent0:                 episode reward: -0.2806,                 loss: 0.1930
agent1:                 episode reward: 0.2806,                 loss: nan
Episode: 24481/101000 (24.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3604s / 3631.4847 s
agent0:                 episode reward: -0.3659,                 loss: 0.1896
agent1:                 episode reward: 0.3659,                 loss: nan
Episode: 24501/101000 (24.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1678s / 3634.6525 s
agent0:                 episode reward: -0.5151,                 loss: 0.1899
agent1:                 episode reward: 0.5151,                 loss: nan
Episode: 24521/101000 (24.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6695s / 3637.3220 s
agent0:                 episode reward: -0.1397,                 loss: 0.1888
agent1:                 episode reward: 0.1397,                 loss: nan
Episode: 24541/101000 (24.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6430s / 3639.9650 s
agent0:                 episode reward: -0.2622,                 loss: 0.1830
agent1:                 episode reward: 0.2622,                 loss: nan
Episode: 24561/101000 (24.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3500s / 3642.3150 s
agent0:                 episode reward: -0.5301,                 loss: 0.1825
agent1:                 episode reward: 0.5301,                 loss: nan
Episode: 24581/101000 (24.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6088s / 3644.9237 s
agent0:                 episode reward: -0.2138,                 loss: 0.1809
agent1:                 episode reward: 0.2138,                 loss: nan
Episode: 24601/101000 (24.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7525s / 3648.6762 s
agent0:                 episode reward: -0.1719,                 loss: 0.1794
agent1:                 episode reward: 0.1719,                 loss: nan
Episode: 24621/101000 (24.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2021s / 3651.8783 s
agent0:                 episode reward: 0.0042,                 loss: 0.1808
agent1:                 episode reward: -0.0042,                 loss: nan
Episode: 24641/101000 (24.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2921s / 3655.1704 s
agent0:                 episode reward: -0.0788,                 loss: 0.1813
agent1:                 episode reward: 0.0788,                 loss: nan
Episode: 24661/101000 (24.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1649s / 3658.3353 s
agent0:                 episode reward: -0.2462,                 loss: 0.1822
agent1:                 episode reward: 0.2462,                 loss: nan
Episode: 24681/101000 (24.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2233s / 3661.5586 s
agent0:                 episode reward: 0.0676,                 loss: 0.1804
agent1:                 episode reward: -0.0676,                 loss: nan
Episode: 24701/101000 (24.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9342s / 3664.4928 s
agent0:                 episode reward: -0.3620,                 loss: 0.1816
agent1:                 episode reward: 0.3620,                 loss: nan
Episode: 24721/101000 (24.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1090s / 3667.6018 s
agent0:                 episode reward: -0.0346,                 loss: 0.1809
agent1:                 episode reward: 0.0346,                 loss: nan
Episode: 24741/101000 (24.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5100s / 3671.1118 s
agent0:                 episode reward: 0.2370,                 loss: 0.1824
agent1:                 episode reward: -0.2370,                 loss: nan
Episode: 24761/101000 (24.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2926s / 3674.4044 s
agent0:                 episode reward: -0.1876,                 loss: 0.1821
agent1:                 episode reward: 0.1876,                 loss: nan
Episode: 24781/101000 (24.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3901s / 3676.7944 s
agent0:                 episode reward: 0.1060,                 loss: 0.1801
agent1:                 episode reward: -0.1060,                 loss: nan
Episode: 24801/101000 (24.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.0052s / 3678.7996 s
agent0:                 episode reward: -0.0665,                 loss: 0.1822
agent1:                 episode reward: 0.0665,                 loss: nan
Episode: 24821/101000 (24.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9565s / 3681.7562 s
agent0:                 episode reward: 0.2925,                 loss: 0.1798
agent1:                 episode reward: -0.2925,                 loss: nan
Episode: 24841/101000 (24.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0471s / 3684.8033 s
agent0:                 episode reward: -0.1070,                 loss: 0.1815
agent1:                 episode reward: 0.1070,                 loss: nan
Episode: 24861/101000 (24.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7019s / 3688.5052 s
agent0:                 episode reward: -0.1001,                 loss: 0.1827
agent1:                 episode reward: 0.1001,                 loss: nan
Episode: 24881/101000 (24.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6339s / 3691.1392 s
agent0:                 episode reward: -0.0109,                 loss: 0.1805
agent1:                 episode reward: 0.0109,                 loss: nan
Episode: 24901/101000 (24.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6983s / 3693.8375 s
agent0:                 episode reward: -0.3101,                 loss: 0.1815
agent1:                 episode reward: 0.3101,                 loss: nan
Episode: 24921/101000 (24.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2648s / 3696.1022 s
agent0:                 episode reward: 0.0259,                 loss: 0.1804
agent1:                 episode reward: -0.0259,                 loss: nan
Episode: 24941/101000 (24.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7879s / 3698.8901 s
agent0:                 episode reward: -0.7165,                 loss: 0.1808
agent1:                 episode reward: 0.7165,                 loss: nan
Episode: 24961/101000 (24.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2183s / 3702.1084 s
agent0:                 episode reward: 0.0340,                 loss: 0.1822
agent1:                 episode reward: -0.0340,                 loss: nan
Episode: 24981/101000 (24.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4194s / 3704.5278 s
agent0:                 episode reward: -0.4657,                 loss: 0.1804
agent1:                 episode reward: 0.4657,                 loss: nan
Episode: 25001/101000 (24.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7652s / 3707.2929 s
agent0:                 episode reward: -0.3169,                 loss: 0.1802
agent1:                 episode reward: 0.3169,                 loss: nan
Episode: 25021/101000 (24.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9613s / 3710.2542 s
agent0:                 episode reward: 0.0177,                 loss: 0.1813
agent1:                 episode reward: -0.0177,                 loss: nan
Episode: 25041/101000 (24.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8522s / 3713.1064 s
agent0:                 episode reward: -0.2739,                 loss: 0.1811
agent1:                 episode reward: 0.2739,                 loss: nan
Episode: 25061/101000 (24.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2695s / 3716.3759 s
agent0:                 episode reward: -0.4049,                 loss: 0.1810
agent1:                 episode reward: 0.4049,                 loss: nan
Episode: 25081/101000 (24.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3845s / 3719.7604 s
agent0:                 episode reward: -0.0438,                 loss: 0.1822
agent1:                 episode reward: 0.0438,                 loss: nan
Episode: 25101/101000 (24.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7147s / 3722.4751 s
agent0:                 episode reward: -0.1259,                 loss: 0.1826
agent1:                 episode reward: 0.1259,                 loss: nan
Episode: 25121/101000 (24.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8976s / 3725.3727 s
agent0:                 episode reward: -0.2175,                 loss: 0.1809
agent1:                 episode reward: 0.2175,                 loss: nan
Episode: 25141/101000 (24.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9664s / 3728.3391 s
agent0:                 episode reward: 0.1489,                 loss: 0.1812
agent1:                 episode reward: -0.1489,                 loss: nan
Episode: 25161/101000 (24.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0833s / 3731.4224 s
agent0:                 episode reward: -0.4410,                 loss: 0.1811
agent1:                 episode reward: 0.4410,                 loss: nan
Episode: 25181/101000 (24.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0237s / 3734.4461 s
agent0:                 episode reward: -0.0442,                 loss: 0.1813
agent1:                 episode reward: 0.0442,                 loss: nan
Episode: 25201/101000 (24.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8463s / 3737.2924 s
agent0:                 episode reward: -0.0688,                 loss: 0.1819
agent1:                 episode reward: 0.0688,                 loss: nan
Episode: 25221/101000 (24.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6908s / 3739.9832 s
agent0:                 episode reward: 0.2223,                 loss: 0.1817
agent1:                 episode reward: -0.2223,                 loss: nan
Episode: 25241/101000 (24.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8662s / 3742.8493 s
agent0:                 episode reward: 0.0012,                 loss: 0.1810
agent1:                 episode reward: -0.0012,                 loss: nan
Episode: 25261/101000 (25.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9494s / 3745.7987 s
agent0:                 episode reward: -0.0987,                 loss: 0.1814
agent1:                 episode reward: 0.0987,                 loss: nan
Episode: 25281/101000 (25.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6650s / 3748.4637 s
agent0:                 episode reward: -0.2182,                 loss: 0.1823
agent1:                 episode reward: 0.2182,                 loss: nan
Episode: 25301/101000 (25.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4320s / 3750.8957 s
agent0:                 episode reward: 0.0291,                 loss: 0.1800
agent1:                 episode reward: -0.0291,                 loss: nan
Episode: 25321/101000 (25.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8523s / 3753.7480 s
agent0:                 episode reward: -0.1630,                 loss: 0.1835
agent1:                 episode reward: 0.1630,                 loss: nan
Episode: 25341/101000 (25.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4963s / 3757.2444 s
agent0:                 episode reward: -0.1785,                 loss: 0.1792
agent1:                 episode reward: 0.1785,                 loss: nan
Episode: 25361/101000 (25.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6721s / 3760.9165 s
agent0:                 episode reward: 0.4593,                 loss: 0.1794
agent1:                 episode reward: -0.4593,                 loss: nan
Episode: 25381/101000 (25.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2195s / 3764.1360 s
agent0:                 episode reward: -0.4322,                 loss: 0.1823
agent1:                 episode reward: 0.4322,                 loss: nan
Episode: 25401/101000 (25.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2217s / 3767.3577 s
agent0:                 episode reward: 0.0692,                 loss: 0.1820
agent1:                 episode reward: -0.0692,                 loss: nan
Episode: 25421/101000 (25.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2543s / 3770.6120 s
agent0:                 episode reward: -0.4474,                 loss: 0.1806
agent1:                 episode reward: 0.4474,                 loss: nan
Episode: 25441/101000 (25.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6860s / 3773.2980 s
agent0:                 episode reward: 0.0323,                 loss: 0.1811
agent1:                 episode reward: -0.0323,                 loss: 0.1633
Score delta: 1.5451652480087408, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/25011_0.
Episode: 25461/101000 (25.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1842s / 3776.4822 s
agent0:                 episode reward: -0.2893,                 loss: nan
agent1:                 episode reward: 0.2893,                 loss: 0.1632
Episode: 25481/101000 (25.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7032s / 3779.1854 s
agent0:                 episode reward: -0.3877,                 loss: 0.1943
agent1:                 episode reward: 0.3877,                 loss: 0.1646
Score delta: 1.9081266462221287, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/25042_1.
Episode: 25501/101000 (25.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8703s / 3782.0557 s
agent0:                 episode reward: -0.0179,                 loss: 0.1901
agent1:                 episode reward: 0.0179,                 loss: nan
Episode: 25521/101000 (25.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3008s / 3785.3564 s
agent0:                 episode reward: -0.0438,                 loss: 0.1879
agent1:                 episode reward: 0.0438,                 loss: nan
Episode: 25541/101000 (25.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9699s / 3788.3263 s
agent0:                 episode reward: 0.0758,                 loss: 0.1869
agent1:                 episode reward: -0.0758,                 loss: nan
Episode: 25561/101000 (25.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0604s / 3791.3867 s
agent0:                 episode reward: -0.0857,                 loss: 0.1874
agent1:                 episode reward: 0.0857,                 loss: nan
Episode: 25581/101000 (25.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4166s / 3794.8034 s
agent0:                 episode reward: 0.0859,                 loss: 0.1846
agent1:                 episode reward: -0.0859,                 loss: nan
Episode: 25601/101000 (25.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3623s / 3797.1657 s
agent0:                 episode reward: -0.1058,                 loss: 0.1830
agent1:                 episode reward: 0.1058,                 loss: nan
Episode: 25621/101000 (25.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0715s / 3800.2372 s
agent0:                 episode reward: 0.2823,                 loss: 0.1835
agent1:                 episode reward: -0.2823,                 loss: nan
Episode: 25641/101000 (25.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5322s / 3802.7694 s
agent0:                 episode reward: -0.3995,                 loss: 0.1822
agent1:                 episode reward: 0.3995,                 loss: nan
Episode: 25661/101000 (25.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6223s / 3805.3917 s
agent0:                 episode reward: -0.3115,                 loss: 0.1822
agent1:                 episode reward: 0.3115,                 loss: nan
Episode: 25681/101000 (25.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7400s / 3808.1317 s
agent0:                 episode reward: 0.0275,                 loss: 0.1809
agent1:                 episode reward: -0.0275,                 loss: nan
Episode: 25701/101000 (25.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5728s / 3811.7046 s
agent0:                 episode reward: 0.1594,                 loss: 0.1826
agent1:                 episode reward: -0.1594,                 loss: nan
Episode: 25721/101000 (25.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7894s / 3814.4940 s
agent0:                 episode reward: -0.1558,                 loss: 0.1839
agent1:                 episode reward: 0.1558,                 loss: nan
Episode: 25741/101000 (25.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5498s / 3818.0438 s
agent0:                 episode reward: -0.1216,                 loss: 0.1825
agent1:                 episode reward: 0.1216,                 loss: nan
Episode: 25761/101000 (25.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4419s / 3821.4857 s
agent0:                 episode reward: -0.3389,                 loss: 0.1819
agent1:                 episode reward: 0.3389,                 loss: nan
Episode: 25781/101000 (25.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0868s / 3824.5724 s
agent0:                 episode reward: 0.0103,                 loss: 0.1838
agent1:                 episode reward: -0.0103,                 loss: nan
Episode: 25801/101000 (25.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8710s / 3827.4435 s
agent0:                 episode reward: -0.1606,                 loss: 0.1821
agent1:                 episode reward: 0.1606,                 loss: nan
Episode: 25821/101000 (25.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1585s / 3830.6019 s
agent0:                 episode reward: -0.0652,                 loss: 0.1823
agent1:                 episode reward: 0.0652,                 loss: nan
Episode: 25841/101000 (25.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3504s / 3833.9523 s
agent0:                 episode reward: 0.0079,                 loss: 0.1792
agent1:                 episode reward: -0.0079,                 loss: nan
Episode: 25861/101000 (25.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4377s / 3836.3900 s
agent0:                 episode reward: -0.0670,                 loss: 0.1827
agent1:                 episode reward: 0.0670,                 loss: nan
Episode: 25881/101000 (25.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7161s / 3839.1062 s
agent0:                 episode reward: -0.6932,                 loss: 0.1820
agent1:                 episode reward: 0.6932,                 loss: nan
Episode: 25901/101000 (25.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8733s / 3842.9795 s
agent0:                 episode reward: 0.2014,                 loss: 0.1784
agent1:                 episode reward: -0.2014,                 loss: nan
Episode: 25921/101000 (25.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0396s / 3846.0191 s
agent0:                 episode reward: 0.1469,                 loss: 0.1771
agent1:                 episode reward: -0.1469,                 loss: nan
Episode: 25941/101000 (25.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6349s / 3848.6540 s
agent0:                 episode reward: -0.0649,                 loss: 0.1780
agent1:                 episode reward: 0.0649,                 loss: nan
Episode: 25961/101000 (25.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4214s / 3852.0753 s
agent0:                 episode reward: -0.4063,                 loss: 0.1781
agent1:                 episode reward: 0.4063,                 loss: nan
Episode: 25981/101000 (25.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6138s / 3855.6891 s
agent0:                 episode reward: 0.1934,                 loss: 0.1783
agent1:                 episode reward: -0.1934,                 loss: nan
Episode: 26001/101000 (25.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5157s / 3861.2048 s
agent0:                 episode reward: 0.1555,                 loss: 0.1788
agent1:                 episode reward: -0.1555,                 loss: nan
Episode: 26021/101000 (25.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4905s / 3863.6953 s
agent0:                 episode reward: -0.0282,                 loss: 0.1782
agent1:                 episode reward: 0.0282,                 loss: nan
Episode: 26041/101000 (25.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4228s / 3866.1181 s
agent0:                 episode reward: -0.3980,                 loss: 0.1785
agent1:                 episode reward: 0.3980,                 loss: nan
Episode: 26061/101000 (25.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0103s / 3869.1284 s
agent0:                 episode reward: -0.2559,                 loss: 0.1792
agent1:                 episode reward: 0.2559,                 loss: nan
Episode: 26081/101000 (25.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6475s / 3871.7759 s
agent0:                 episode reward: 0.1699,                 loss: 0.1761
agent1:                 episode reward: -0.1699,                 loss: nan
Episode: 26101/101000 (25.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7396s / 3874.5155 s
agent0:                 episode reward: -0.2653,                 loss: 0.1770
agent1:                 episode reward: 0.2653,                 loss: nan
Episode: 26121/101000 (25.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0426s / 3877.5581 s
agent0:                 episode reward: 0.1715,                 loss: 0.1786
agent1:                 episode reward: -0.1715,                 loss: nan
Episode: 26141/101000 (25.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5530s / 3880.1111 s
agent0:                 episode reward: 0.0263,                 loss: 0.1766
agent1:                 episode reward: -0.0263,                 loss: nan
Episode: 26161/101000 (25.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2000s / 3883.3111 s
agent0:                 episode reward: -0.2016,                 loss: 0.1772
agent1:                 episode reward: 0.2016,                 loss: nan
Episode: 26181/101000 (25.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3057s / 3885.6168 s
agent0:                 episode reward: -0.1701,                 loss: 0.1782
agent1:                 episode reward: 0.1701,                 loss: nan
Episode: 26201/101000 (25.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5472s / 3888.1641 s
agent0:                 episode reward: 0.1396,                 loss: 0.1788
agent1:                 episode reward: -0.1396,                 loss: nan
Episode: 26221/101000 (25.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3454s / 3891.5095 s
agent0:                 episode reward: 0.1672,                 loss: 0.1780
agent1:                 episode reward: -0.1672,                 loss: 0.1659
Score delta: 1.789214535800194, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/25784_0.
Episode: 26241/101000 (25.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8252s / 3894.3347 s
agent0:                 episode reward: -0.5642,                 loss: nan
agent1:                 episode reward: 0.5642,                 loss: 0.1674
Episode: 26261/101000 (26.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8402s / 3897.1749 s
agent0:                 episode reward: -0.0730,                 loss: nan
agent1:                 episode reward: 0.0730,                 loss: 0.1650
Episode: 26281/101000 (26.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7518s / 3899.9267 s
agent0:                 episode reward: -0.3329,                 loss: 0.1949
agent1:                 episode reward: 0.3329,                 loss: 0.1655
Score delta: 1.6439940385122327, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/25848_1.
Episode: 26301/101000 (26.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9150s / 3902.8417 s
agent0:                 episode reward: 0.1072,                 loss: 0.1740
agent1:                 episode reward: -0.1072,                 loss: nan
Episode: 26321/101000 (26.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7127s / 3906.5545 s
agent0:                 episode reward: -0.0896,                 loss: 0.1703
agent1:                 episode reward: 0.0896,                 loss: nan
Episode: 26341/101000 (26.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0798s / 3909.6342 s
agent0:                 episode reward: -0.1743,                 loss: 0.1711
agent1:                 episode reward: 0.1743,                 loss: nan
Episode: 26361/101000 (26.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6492s / 3912.2834 s
agent0:                 episode reward: 0.0575,                 loss: 0.1684
agent1:                 episode reward: -0.0575,                 loss: nan
Episode: 26381/101000 (26.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8705s / 3915.1539 s
agent0:                 episode reward: -0.1316,                 loss: 0.1705
agent1:                 episode reward: 0.1316,                 loss: nan
Episode: 26401/101000 (26.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8894s / 3919.0433 s
agent0:                 episode reward: -0.4198,                 loss: 0.1700
agent1:                 episode reward: 0.4198,                 loss: nan
Episode: 26421/101000 (26.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4920s / 3922.5354 s
agent0:                 episode reward: -0.0280,                 loss: 0.1688
agent1:                 episode reward: 0.0280,                 loss: nan
Episode: 26441/101000 (26.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0240s / 3925.5593 s
agent0:                 episode reward: -0.2814,                 loss: 0.1700
agent1:                 episode reward: 0.2814,                 loss: nan
Episode: 26461/101000 (26.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7289s / 3928.2882 s
agent0:                 episode reward: -0.1779,                 loss: 0.1691
agent1:                 episode reward: 0.1779,                 loss: nan
Episode: 26481/101000 (26.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1649s / 3930.4532 s
agent0:                 episode reward: -0.1744,                 loss: 0.1685
agent1:                 episode reward: 0.1744,                 loss: nan
Episode: 26501/101000 (26.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7142s / 3933.1674 s
agent0:                 episode reward: -0.0037,                 loss: 0.1677
agent1:                 episode reward: 0.0037,                 loss: nan
Episode: 26521/101000 (26.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7609s / 3935.9283 s
agent0:                 episode reward: -0.4199,                 loss: 0.1681
agent1:                 episode reward: 0.4199,                 loss: nan
Episode: 26541/101000 (26.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6620s / 3939.5903 s
agent0:                 episode reward: 0.0578,                 loss: 0.1675
agent1:                 episode reward: -0.0578,                 loss: nan
Episode: 26561/101000 (26.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2632s / 3942.8535 s
agent0:                 episode reward: -0.0773,                 loss: 0.1687
agent1:                 episode reward: 0.0773,                 loss: nan
Episode: 26581/101000 (26.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4536s / 3946.3071 s
agent0:                 episode reward: 0.2046,                 loss: 0.1689
agent1:                 episode reward: -0.2046,                 loss: nan
Episode: 26601/101000 (26.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5622s / 3948.8693 s
agent0:                 episode reward: -0.0750,                 loss: 0.1698
agent1:                 episode reward: 0.0750,                 loss: nan
Episode: 26621/101000 (26.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3947s / 3952.2641 s
agent0:                 episode reward: -0.0153,                 loss: 0.1692
agent1:                 episode reward: 0.0153,                 loss: nan
Episode: 26641/101000 (26.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0563s / 3955.3204 s
agent0:                 episode reward: -0.1016,                 loss: 0.1825
agent1:                 episode reward: 0.1016,                 loss: nan
Episode: 26661/101000 (26.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5797s / 3957.9000 s
agent0:                 episode reward: -0.3351,                 loss: 0.1815
agent1:                 episode reward: 0.3351,                 loss: nan
Episode: 26681/101000 (26.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3776s / 3961.2776 s
agent0:                 episode reward: -0.0133,                 loss: 0.1807
agent1:                 episode reward: 0.0133,                 loss: nan
Episode: 26701/101000 (26.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0044s / 3965.2820 s
agent0:                 episode reward: -0.3507,                 loss: 0.1815
agent1:                 episode reward: 0.3507,                 loss: nan
Episode: 26721/101000 (26.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2427s / 3968.5246 s
agent0:                 episode reward: 0.2393,                 loss: 0.1824
agent1:                 episode reward: -0.2393,                 loss: nan
Episode: 26741/101000 (26.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9064s / 3971.4310 s
agent0:                 episode reward: -0.3300,                 loss: 0.1816
agent1:                 episode reward: 0.3300,                 loss: nan
Episode: 26761/101000 (26.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7070s / 3974.1381 s
agent0:                 episode reward: -0.2843,                 loss: 0.1805
agent1:                 episode reward: 0.2843,                 loss: nan
Episode: 26781/101000 (26.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8565s / 3976.9946 s
agent0:                 episode reward: -0.4398,                 loss: 0.1807
agent1:                 episode reward: 0.4398,                 loss: nan
Episode: 26801/101000 (26.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2474s / 3980.2420 s
agent0:                 episode reward: -0.2328,                 loss: 0.1810
agent1:                 episode reward: 0.2328,                 loss: nan
Episode: 26821/101000 (26.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4827s / 3983.7247 s
agent0:                 episode reward: -0.2422,                 loss: 0.1830
agent1:                 episode reward: 0.2422,                 loss: nan
Episode: 26841/101000 (26.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8964s / 3986.6211 s
agent0:                 episode reward: -0.4139,                 loss: 0.1819
agent1:                 episode reward: 0.4139,                 loss: nan
Episode: 26861/101000 (26.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0055s / 3989.6266 s
agent0:                 episode reward: -0.3350,                 loss: 0.1811
agent1:                 episode reward: 0.3350,                 loss: nan
Episode: 26881/101000 (26.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7964s / 3992.4230 s
agent0:                 episode reward: -0.4381,                 loss: 0.1795
agent1:                 episode reward: 0.4381,                 loss: nan
Episode: 26901/101000 (26.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8159s / 3995.2389 s
agent0:                 episode reward: 0.4245,                 loss: 0.1811
agent1:                 episode reward: -0.4245,                 loss: nan
Episode: 26921/101000 (26.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8920s / 3998.1309 s
agent0:                 episode reward: -0.1086,                 loss: 0.1811
agent1:                 episode reward: 0.1086,                 loss: nan
Episode: 26941/101000 (26.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7643s / 4000.8953 s
agent0:                 episode reward: -0.1664,                 loss: 0.1817
agent1:                 episode reward: 0.1664,                 loss: nan
Episode: 26961/101000 (26.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6251s / 4004.5204 s
agent0:                 episode reward: -0.3160,                 loss: 0.1810
agent1:                 episode reward: 0.3160,                 loss: nan
Episode: 26981/101000 (26.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9244s / 4007.4448 s
agent0:                 episode reward: -0.1744,                 loss: 0.1838
agent1:                 episode reward: 0.1744,                 loss: nan
Episode: 27001/101000 (26.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9575s / 4010.4023 s
agent0:                 episode reward: -0.0742,                 loss: 0.1849
agent1:                 episode reward: 0.0742,                 loss: nan
Episode: 27021/101000 (26.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1246s / 4013.5269 s
agent0:                 episode reward: 0.0877,                 loss: 0.1853
agent1:                 episode reward: -0.0877,                 loss: nan
Episode: 27041/101000 (26.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2471s / 4017.7740 s
agent0:                 episode reward: -0.1890,                 loss: 0.1845
agent1:                 episode reward: 0.1890,                 loss: nan
Episode: 27061/101000 (26.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2768s / 4021.0508 s
agent0:                 episode reward: 0.0955,                 loss: 0.1864
agent1:                 episode reward: -0.0955,                 loss: nan
Episode: 27081/101000 (26.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4377s / 4024.4885 s
agent0:                 episode reward: -0.3445,                 loss: 0.1856
agent1:                 episode reward: 0.3445,                 loss: nan
Episode: 27101/101000 (26.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0393s / 4027.5278 s
agent0:                 episode reward: -0.0416,                 loss: 0.1842
agent1:                 episode reward: 0.0416,                 loss: nan
Episode: 27121/101000 (26.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8372s / 4030.3650 s
agent0:                 episode reward: 0.0169,                 loss: 0.1836
agent1:                 episode reward: -0.0169,                 loss: nan
Episode: 27141/101000 (26.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6809s / 4033.0459 s
agent0:                 episode reward: -0.0551,                 loss: 0.1834
agent1:                 episode reward: 0.0551,                 loss: nan
Episode: 27161/101000 (26.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1573s / 4036.2032 s
agent0:                 episode reward: 0.0986,                 loss: 0.1843
agent1:                 episode reward: -0.0986,                 loss: nan
Episode: 27181/101000 (26.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1691s / 4039.3723 s
agent0:                 episode reward: -0.4594,                 loss: 0.1856
agent1:                 episode reward: 0.4594,                 loss: nan
Episode: 27201/101000 (26.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4962s / 4042.8685 s
agent0:                 episode reward: -0.4049,                 loss: 0.1860
agent1:                 episode reward: 0.4049,                 loss: nan
Episode: 27221/101000 (26.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2782s / 4045.1467 s
agent0:                 episode reward: 0.1226,                 loss: 0.1848
agent1:                 episode reward: -0.1226,                 loss: nan
Episode: 27241/101000 (26.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4473s / 4047.5940 s
agent0:                 episode reward: 0.0509,                 loss: 0.1829
agent1:                 episode reward: -0.0509,                 loss: nan
Episode: 27261/101000 (26.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6970s / 4050.2911 s
agent0:                 episode reward: -0.5605,                 loss: 0.1845
agent1:                 episode reward: 0.5605,                 loss: nan
Episode: 27281/101000 (27.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7178s / 4053.0089 s
agent0:                 episode reward: -0.1542,                 loss: 0.1860
agent1:                 episode reward: 0.1542,                 loss: nan
Episode: 27301/101000 (27.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9583s / 4055.9672 s
agent0:                 episode reward: -0.1042,                 loss: 0.1820
agent1:                 episode reward: 0.1042,                 loss: nan
Episode: 27321/101000 (27.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7785s / 4058.7456 s
agent0:                 episode reward: -0.0093,                 loss: 0.1819
agent1:                 episode reward: 0.0093,                 loss: nan
Episode: 27341/101000 (27.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0594s / 4061.8050 s
agent0:                 episode reward: -0.0342,                 loss: 0.1831
agent1:                 episode reward: 0.0342,                 loss: nan
Episode: 27361/101000 (27.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7067s / 4065.5117 s
agent0:                 episode reward: 0.1226,                 loss: 0.1834
agent1:                 episode reward: -0.1226,                 loss: nan
Episode: 27381/101000 (27.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6013s / 4068.1130 s
agent0:                 episode reward: -0.0996,                 loss: 0.1858
agent1:                 episode reward: 0.0996,                 loss: 0.1632
Score delta: 1.5876926740539052, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/26937_0.
Episode: 27401/101000 (27.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6344s / 4070.7474 s
agent0:                 episode reward: -0.1933,                 loss: nan
agent1:                 episode reward: 0.1933,                 loss: 0.1610
Episode: 27421/101000 (27.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0440s / 4073.7914 s
agent0:                 episode reward: -0.0138,                 loss: nan
agent1:                 episode reward: 0.0138,                 loss: 0.1612
Episode: 27441/101000 (27.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8075s / 4076.5988 s
agent0:                 episode reward: -0.3403,                 loss: nan
agent1:                 episode reward: 0.3403,                 loss: 0.1613
Episode: 27461/101000 (27.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2221s / 4079.8209 s
agent0:                 episode reward: -0.0491,                 loss: nan
agent1:                 episode reward: 0.0491,                 loss: 0.1580
Episode: 27481/101000 (27.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2394s / 4083.0604 s
agent0:                 episode reward: -0.0646,                 loss: nan
agent1:                 episode reward: 0.0646,                 loss: 0.1584
Episode: 27501/101000 (27.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2589s / 4086.3193 s
agent0:                 episode reward: -0.4640,                 loss: 0.1867
agent1:                 episode reward: 0.4640,                 loss: 0.1575
Score delta: 1.5086281966986532, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/27063_1.
Episode: 27521/101000 (27.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2495s / 4089.5687 s
agent0:                 episode reward: -0.0238,                 loss: 0.1856
agent1:                 episode reward: 0.0238,                 loss: nan
Episode: 27541/101000 (27.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8696s / 4092.4383 s
agent0:                 episode reward: -0.5477,                 loss: 0.1850
agent1:                 episode reward: 0.5477,                 loss: nan
Episode: 27561/101000 (27.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8144s / 4095.2527 s
agent0:                 episode reward: -0.3828,                 loss: 0.1841
agent1:                 episode reward: 0.3828,                 loss: nan
Episode: 27581/101000 (27.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2183s / 4098.4710 s
agent0:                 episode reward: -0.2854,                 loss: 0.1846
agent1:                 episode reward: 0.2854,                 loss: nan
Episode: 27601/101000 (27.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9188s / 4101.3898 s
agent0:                 episode reward: -0.0389,                 loss: 0.1848
agent1:                 episode reward: 0.0389,                 loss: nan
Episode: 27621/101000 (27.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9592s / 4104.3490 s
agent0:                 episode reward: -0.2152,                 loss: 0.1830
agent1:                 episode reward: 0.2152,                 loss: nan
Episode: 27641/101000 (27.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9554s / 4107.3043 s
agent0:                 episode reward: 0.2023,                 loss: 0.1847
agent1:                 episode reward: -0.2023,                 loss: nan
Episode: 27661/101000 (27.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4918s / 4110.7961 s
agent0:                 episode reward: -0.2417,                 loss: 0.1831
agent1:                 episode reward: 0.2417,                 loss: nan
Episode: 27681/101000 (27.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4421s / 4114.2382 s
agent0:                 episode reward: -0.2291,                 loss: 0.1848
agent1:                 episode reward: 0.2291,                 loss: nan
Episode: 27701/101000 (27.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3092s / 4117.5474 s
agent0:                 episode reward: -0.0202,                 loss: 0.1838
agent1:                 episode reward: 0.0202,                 loss: nan
Episode: 27721/101000 (27.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9204s / 4120.4678 s
agent0:                 episode reward: -0.0175,                 loss: 0.1861
agent1:                 episode reward: 0.0175,                 loss: nan
Episode: 27741/101000 (27.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8526s / 4124.3204 s
agent0:                 episode reward: 0.1481,                 loss: 0.1824
agent1:                 episode reward: -0.1481,                 loss: nan
Episode: 27761/101000 (27.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2509s / 4127.5713 s
agent0:                 episode reward: -0.4532,                 loss: 0.1862
agent1:                 episode reward: 0.4532,                 loss: 0.1649
Score delta: 1.5271765445201713, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/27316_0.
Episode: 27781/101000 (27.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2052s / 4130.7765 s
agent0:                 episode reward: -0.4014,                 loss: nan
agent1:                 episode reward: 0.4014,                 loss: 0.1592
Episode: 27801/101000 (27.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4836s / 4133.2600 s
agent0:                 episode reward: -0.3534,                 loss: nan
agent1:                 episode reward: 0.3534,                 loss: 0.1556
Episode: 27821/101000 (27.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 1.9249s / 4135.1850 s
agent0:                 episode reward: -0.0803,                 loss: nan
agent1:                 episode reward: 0.0803,                 loss: 0.1561
Episode: 27841/101000 (27.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8798s / 4138.0648 s
agent0:                 episode reward: -0.3358,                 loss: nan
agent1:                 episode reward: 0.3358,                 loss: 0.1548
Episode: 27861/101000 (27.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4607s / 4141.5255 s
agent0:                 episode reward: -0.3844,                 loss: nan
agent1:                 episode reward: 0.3844,                 loss: 0.1543
Episode: 27881/101000 (27.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7063s / 4144.2318 s
agent0:                 episode reward: -0.3053,                 loss: nan
agent1:                 episode reward: 0.3053,                 loss: 0.1527
Episode: 27901/101000 (27.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3639s / 4147.5957 s
agent0:                 episode reward: -0.1888,                 loss: 0.1860
agent1:                 episode reward: 0.1888,                 loss: 0.1522
Score delta: 1.7781923883424717, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/27466_1.
Episode: 27921/101000 (27.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5625s / 4151.1583 s
agent0:                 episode reward: 0.6802,                 loss: 0.1726
agent1:                 episode reward: -0.6802,                 loss: nan
Episode: 27941/101000 (27.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5249s / 4153.6832 s
agent0:                 episode reward: -0.0664,                 loss: 0.1687
agent1:                 episode reward: 0.0664,                 loss: 0.1652
Score delta: 1.523997091769243, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/27500_0.
Episode: 27961/101000 (27.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2666s / 4156.9497 s
agent0:                 episode reward: -0.2359,                 loss: nan
agent1:                 episode reward: 0.2359,                 loss: 0.1635
Episode: 27981/101000 (27.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2493s / 4160.1990 s
agent0:                 episode reward: -0.3205,                 loss: nan
agent1:                 episode reward: 0.3205,                 loss: 0.1623
Episode: 28001/101000 (27.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1639s / 4163.3629 s
agent0:                 episode reward: -0.2141,                 loss: nan
agent1:                 episode reward: 0.2141,                 loss: 0.1646
Episode: 28021/101000 (27.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9094s / 4166.2723 s
agent0:                 episode reward: -0.4711,                 loss: nan
agent1:                 episode reward: 0.4711,                 loss: 0.1637
Episode: 28041/101000 (27.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6083s / 4168.8806 s
agent0:                 episode reward: -0.3524,                 loss: nan
agent1:                 episode reward: 0.3524,                 loss: 0.1639
Episode: 28061/101000 (27.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1344s / 4171.0150 s
agent0:                 episode reward: -0.5398,                 loss: 0.1811
agent1:                 episode reward: 0.5398,                 loss: 0.1628
Score delta: 1.657705772224188, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/27626_1.
Episode: 28081/101000 (27.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1337s / 4174.1487 s
agent0:                 episode reward: -0.4543,                 loss: 0.1816
agent1:                 episode reward: 0.4543,                 loss: nan
Episode: 28101/101000 (27.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2709s / 4177.4195 s
agent0:                 episode reward: -0.1247,                 loss: 0.1803
agent1:                 episode reward: 0.1247,                 loss: nan
Episode: 28121/101000 (27.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6561s / 4180.0757 s
agent0:                 episode reward: 0.0414,                 loss: 0.1824
agent1:                 episode reward: -0.0414,                 loss: nan
Episode: 28141/101000 (27.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7440s / 4182.8196 s
agent0:                 episode reward: -0.1215,                 loss: 0.1826
agent1:                 episode reward: 0.1215,                 loss: nan
Episode: 28161/101000 (27.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4504s / 4186.2700 s
agent0:                 episode reward: -0.3584,                 loss: 0.1831
agent1:                 episode reward: 0.3584,                 loss: nan
Episode: 28181/101000 (27.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4756s / 4189.7456 s
agent0:                 episode reward: -0.0700,                 loss: 0.1814
agent1:                 episode reward: 0.0700,                 loss: nan
Episode: 28201/101000 (27.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9597s / 4192.7053 s
agent0:                 episode reward: -0.2202,                 loss: 0.1833
agent1:                 episode reward: 0.2202,                 loss: nan
Episode: 28221/101000 (27.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4821s / 4195.1875 s
agent0:                 episode reward: -0.2560,                 loss: 0.1813
agent1:                 episode reward: 0.2560,                 loss: nan
Episode: 28241/101000 (27.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3364s / 4198.5239 s
agent0:                 episode reward: 0.0653,                 loss: 0.1819
agent1:                 episode reward: -0.0653,                 loss: nan
Episode: 28261/101000 (27.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5052s / 4201.0290 s
agent0:                 episode reward: 0.2566,                 loss: 0.1835
agent1:                 episode reward: -0.2566,                 loss: nan
Episode: 28281/101000 (28.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3993s / 4204.4283 s
agent0:                 episode reward: -0.2903,                 loss: 0.1811
agent1:                 episode reward: 0.2903,                 loss: nan
Episode: 28301/101000 (28.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1176s / 4207.5460 s
agent0:                 episode reward: -0.4462,                 loss: 0.1823
agent1:                 episode reward: 0.4462,                 loss: nan
Episode: 28321/101000 (28.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8867s / 4210.4327 s
agent0:                 episode reward: -0.0555,                 loss: 0.1836
agent1:                 episode reward: 0.0555,                 loss: nan
Episode: 28341/101000 (28.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3069s / 4213.7396 s
agent0:                 episode reward: -0.3256,                 loss: 0.1815
agent1:                 episode reward: 0.3256,                 loss: nan
Episode: 28361/101000 (28.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1413s / 4216.8809 s
agent0:                 episode reward: -0.1079,                 loss: 0.1822
agent1:                 episode reward: 0.1079,                 loss: nan
Episode: 28381/101000 (28.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4120s / 4219.2929 s
agent0:                 episode reward: -0.0604,                 loss: 0.1805
agent1:                 episode reward: 0.0604,                 loss: 0.1656
Score delta: 1.5161674782225112, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/27952_0.
Episode: 28401/101000 (28.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8146s / 4222.1076 s
agent0:                 episode reward: -0.2685,                 loss: nan
agent1:                 episode reward: 0.2685,                 loss: 0.1571
Episode: 28421/101000 (28.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9256s / 4225.0331 s
agent0:                 episode reward: -0.6956,                 loss: 0.2104
agent1:                 episode reward: 0.6956,                 loss: 0.1567
Score delta: 1.5407152114897857, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/27988_1.
Episode: 28441/101000 (28.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5592s / 4227.5923 s
agent0:                 episode reward: -0.6385,                 loss: 0.1815
agent1:                 episode reward: 0.6385,                 loss: nan
Episode: 28461/101000 (28.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4360s / 4231.0283 s
agent0:                 episode reward: -0.2938,                 loss: 0.1651
agent1:                 episode reward: 0.2938,                 loss: nan
Episode: 28481/101000 (28.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4432s / 4234.4715 s
agent0:                 episode reward: -0.5228,                 loss: 0.1600
agent1:                 episode reward: 0.5228,                 loss: nan
Episode: 28501/101000 (28.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7972s / 4237.2687 s
agent0:                 episode reward: -0.7638,                 loss: 0.1593
agent1:                 episode reward: 0.7638,                 loss: nan
Episode: 28521/101000 (28.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3178s / 4240.5865 s
agent0:                 episode reward: -0.3249,                 loss: 0.1557
agent1:                 episode reward: 0.3249,                 loss: nan
Episode: 28541/101000 (28.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6988s / 4243.2853 s
agent0:                 episode reward: -0.4789,                 loss: 0.1572
agent1:                 episode reward: 0.4789,                 loss: nan
Episode: 28561/101000 (28.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6780s / 4246.9633 s
agent0:                 episode reward: -0.3618,                 loss: 0.1539
agent1:                 episode reward: 0.3618,                 loss: nan
Episode: 28581/101000 (28.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1246s / 4249.0879 s
agent0:                 episode reward: -0.2478,                 loss: 0.1553
agent1:                 episode reward: 0.2478,                 loss: nan
Episode: 28601/101000 (28.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4360s / 4251.5238 s
agent0:                 episode reward: -0.5109,                 loss: 0.1532
agent1:                 episode reward: 0.5109,                 loss: nan
Episode: 28621/101000 (28.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2911s / 4255.8149 s
agent0:                 episode reward: -0.3969,                 loss: 0.1520
agent1:                 episode reward: 0.3969,                 loss: nan
Episode: 28641/101000 (28.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0183s / 4258.8332 s
agent0:                 episode reward: 0.1212,                 loss: 0.1529
agent1:                 episode reward: -0.1212,                 loss: nan
Episode: 28661/101000 (28.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7069s / 4261.5401 s
agent0:                 episode reward: -0.4779,                 loss: 0.1512
agent1:                 episode reward: 0.4779,                 loss: nan
Episode: 28681/101000 (28.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3348s / 4264.8749 s
agent0:                 episode reward: -0.3042,                 loss: 0.1512
agent1:                 episode reward: 0.3042,                 loss: nan
Episode: 28701/101000 (28.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2512s / 4267.1261 s
agent0:                 episode reward: -0.1161,                 loss: 0.1522
agent1:                 episode reward: 0.1161,                 loss: nan
Episode: 28721/101000 (28.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4538s / 4270.5799 s
agent0:                 episode reward: -0.2195,                 loss: 0.1522
agent1:                 episode reward: 0.2195,                 loss: nan
Episode: 28741/101000 (28.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8746s / 4273.4545 s
agent0:                 episode reward: -0.2282,                 loss: 0.1763
agent1:                 episode reward: 0.2282,                 loss: nan
Episode: 28761/101000 (28.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2611s / 4276.7155 s
agent0:                 episode reward: -0.3669,                 loss: 0.1793
agent1:                 episode reward: 0.3669,                 loss: nan
Episode: 28781/101000 (28.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7793s / 4279.4948 s
agent0:                 episode reward: -0.2428,                 loss: 0.1783
agent1:                 episode reward: 0.2428,                 loss: nan
Episode: 28801/101000 (28.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1755s / 4282.6703 s
agent0:                 episode reward: -0.8092,                 loss: 0.1784
agent1:                 episode reward: 0.8092,                 loss: nan
Episode: 28821/101000 (28.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8757s / 4285.5460 s
agent0:                 episode reward: -0.2611,                 loss: 0.1783
agent1:                 episode reward: 0.2611,                 loss: nan
Episode: 28841/101000 (28.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0852s / 4288.6312 s
agent0:                 episode reward: -0.2235,                 loss: 0.1774
agent1:                 episode reward: 0.2235,                 loss: nan
Episode: 28861/101000 (28.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6871s / 4291.3182 s
agent0:                 episode reward: -0.1034,                 loss: 0.1781
agent1:                 episode reward: 0.1034,                 loss: nan
Episode: 28881/101000 (28.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9069s / 4294.2251 s
agent0:                 episode reward: -0.3927,                 loss: 0.1782
agent1:                 episode reward: 0.3927,                 loss: nan
Episode: 28901/101000 (28.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7810s / 4297.0061 s
agent0:                 episode reward: 0.0197,                 loss: 0.1767
agent1:                 episode reward: -0.0197,                 loss: nan
Episode: 28921/101000 (28.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4150s / 4300.4210 s
agent0:                 episode reward: -0.2085,                 loss: 0.1771
agent1:                 episode reward: 0.2085,                 loss: nan
Episode: 28941/101000 (28.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7733s / 4303.1943 s
agent0:                 episode reward: -0.5880,                 loss: 0.1759
agent1:                 episode reward: 0.5880,                 loss: nan
Episode: 28961/101000 (28.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0991s / 4306.2934 s
agent0:                 episode reward: -0.2928,                 loss: 0.1752
agent1:                 episode reward: 0.2928,                 loss: nan
Episode: 28981/101000 (28.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2950s / 4309.5884 s
agent0:                 episode reward: -0.0254,                 loss: 0.1746
agent1:                 episode reward: 0.0254,                 loss: nan
Episode: 29001/101000 (28.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7732s / 4313.3617 s
agent0:                 episode reward: -0.3663,                 loss: 0.1752
agent1:                 episode reward: 0.3663,                 loss: nan
Episode: 29021/101000 (28.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5078s / 4316.8695 s
agent0:                 episode reward: -0.4296,                 loss: 0.1749
agent1:                 episode reward: 0.4296,                 loss: nan
Episode: 29041/101000 (28.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0913s / 4319.9608 s
agent0:                 episode reward: -0.8294,                 loss: 0.1746
agent1:                 episode reward: 0.8294,                 loss: nan
Episode: 29061/101000 (28.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2640s / 4322.2248 s
agent0:                 episode reward: 0.1903,                 loss: 0.1763
agent1:                 episode reward: -0.1903,                 loss: nan
Episode: 29081/101000 (28.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9142s / 4325.1390 s
agent0:                 episode reward: 0.0653,                 loss: 0.1854
agent1:                 episode reward: -0.0653,                 loss: nan
Episode: 29101/101000 (28.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3005s / 4328.4395 s
agent0:                 episode reward: -0.3849,                 loss: 0.1843
agent1:                 episode reward: 0.3849,                 loss: nan
Episode: 29121/101000 (28.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8462s / 4332.2857 s
agent0:                 episode reward: -0.0336,                 loss: 0.1832
agent1:                 episode reward: 0.0336,                 loss: nan
Episode: 29141/101000 (28.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8894s / 4335.1751 s
agent0:                 episode reward: -0.2129,                 loss: 0.1846
agent1:                 episode reward: 0.2129,                 loss: nan
Episode: 29161/101000 (28.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6260s / 4337.8011 s
agent0:                 episode reward: 0.0030,                 loss: 0.1835
agent1:                 episode reward: -0.0030,                 loss: nan
Episode: 29181/101000 (28.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4695s / 4341.2706 s
agent0:                 episode reward: -0.2957,                 loss: 0.1844
agent1:                 episode reward: 0.2957,                 loss: nan
Episode: 29201/101000 (28.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8857s / 4344.1562 s
agent0:                 episode reward: 0.0444,                 loss: 0.1835
agent1:                 episode reward: -0.0444,                 loss: nan
Episode: 29221/101000 (28.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5841s / 4346.7403 s
agent0:                 episode reward: -0.1318,                 loss: 0.1831
agent1:                 episode reward: 0.1318,                 loss: nan
Episode: 29241/101000 (28.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6781s / 4349.4184 s
agent0:                 episode reward: -0.0054,                 loss: 0.1828
agent1:                 episode reward: 0.0054,                 loss: nan
Episode: 29261/101000 (28.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6674s / 4352.0858 s
agent0:                 episode reward: -0.3681,                 loss: 0.1839
agent1:                 episode reward: 0.3681,                 loss: nan
Episode: 29281/101000 (28.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4789s / 4355.5647 s
agent0:                 episode reward: -0.0616,                 loss: 0.1830
agent1:                 episode reward: 0.0616,                 loss: nan
Episode: 29301/101000 (29.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6109s / 4359.1756 s
agent0:                 episode reward: -0.3993,                 loss: 0.1833
agent1:                 episode reward: 0.3993,                 loss: nan
Episode: 29321/101000 (29.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9266s / 4362.1021 s
agent0:                 episode reward: -0.0633,                 loss: 0.1838
agent1:                 episode reward: 0.0633,                 loss: nan
Episode: 29341/101000 (29.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2859s / 4365.3880 s
agent0:                 episode reward: 0.0202,                 loss: 0.1842
agent1:                 episode reward: -0.0202,                 loss: nan
Episode: 29361/101000 (29.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8737s / 4368.2617 s
agent0:                 episode reward: -0.4581,                 loss: 0.1825
agent1:                 episode reward: 0.4581,                 loss: nan
Episode: 29381/101000 (29.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6836s / 4371.9453 s
agent0:                 episode reward: -0.2514,                 loss: 0.1830
agent1:                 episode reward: 0.2514,                 loss: nan
Episode: 29401/101000 (29.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3071s / 4374.2524 s
agent0:                 episode reward: -0.3356,                 loss: 0.1846
agent1:                 episode reward: 0.3356,                 loss: nan
Episode: 29421/101000 (29.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3567s / 4376.6090 s
agent0:                 episode reward: -0.0710,                 loss: 0.1884
agent1:                 episode reward: 0.0710,                 loss: nan
Episode: 29441/101000 (29.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6649s / 4379.2739 s
agent0:                 episode reward: 0.0537,                 loss: 0.1865
agent1:                 episode reward: -0.0537,                 loss: nan
Episode: 29461/101000 (29.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3512s / 4381.6252 s
agent0:                 episode reward: -0.1634,                 loss: 0.1857
agent1:                 episode reward: 0.1634,                 loss: 0.1938
Score delta: 1.5187529378201483, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/29016_0.
Episode: 29481/101000 (29.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5627s / 4384.1878 s
agent0:                 episode reward: -0.1466,                 loss: nan
agent1:                 episode reward: 0.1466,                 loss: 0.1786
Episode: 29501/101000 (29.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7241s / 4386.9119 s
agent0:                 episode reward: -0.1830,                 loss: 0.1983
agent1:                 episode reward: 0.1830,                 loss: 0.1718
Score delta: 1.6392773149674453, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/29068_1.
Episode: 29521/101000 (29.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5224s / 4390.4343 s
agent0:                 episode reward: -0.7638,                 loss: 0.1888
agent1:                 episode reward: 0.7638,                 loss: nan
Episode: 29541/101000 (29.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8667s / 4393.3010 s
agent0:                 episode reward: 0.1042,                 loss: 0.1880
agent1:                 episode reward: -0.1042,                 loss: nan
Episode: 29561/101000 (29.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8029s / 4396.1039 s
agent0:                 episode reward: -0.2055,                 loss: 0.1862
agent1:                 episode reward: 0.2055,                 loss: nan
Episode: 29581/101000 (29.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5652s / 4398.6691 s
agent0:                 episode reward: 0.3088,                 loss: 0.1877
agent1:                 episode reward: -0.3088,                 loss: 0.1596
Score delta: 1.5218455360359582, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/29151_0.
Episode: 29601/101000 (29.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6734s / 4401.3425 s
agent0:                 episode reward: -0.3013,                 loss: nan
agent1:                 episode reward: 0.3013,                 loss: 0.1584
Episode: 29621/101000 (29.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8213s / 4404.1637 s
agent0:                 episode reward: -0.3515,                 loss: nan
agent1:                 episode reward: 0.3515,                 loss: 0.1574
Episode: 29641/101000 (29.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4839s / 4407.6476 s
agent0:                 episode reward: -0.4242,                 loss: 0.1850
agent1:                 episode reward: 0.4242,                 loss: 0.1575
Score delta: 1.800175103775657, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/29205_1.
Episode: 29661/101000 (29.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0155s / 4410.6631 s
agent0:                 episode reward: 0.1603,                 loss: 0.1867
agent1:                 episode reward: -0.1603,                 loss: nan
Episode: 29681/101000 (29.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2982s / 4413.9613 s
agent0:                 episode reward: -0.0178,                 loss: 0.1848
agent1:                 episode reward: 0.0178,                 loss: nan
Episode: 29701/101000 (29.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0961s / 4417.0574 s
agent0:                 episode reward: -0.3241,                 loss: 0.1876
agent1:                 episode reward: 0.3241,                 loss: nan
Episode: 29721/101000 (29.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6905s / 4419.7480 s
agent0:                 episode reward: 0.4628,                 loss: 0.1856
agent1:                 episode reward: -0.4628,                 loss: 0.1822
Score delta: 1.6867448318657694, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/29290_0.
Episode: 29741/101000 (29.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2732s / 4423.0212 s
agent0:                 episode reward: 0.0102,                 loss: nan
agent1:                 episode reward: -0.0102,                 loss: 0.1743
Episode: 29761/101000 (29.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9876s / 4426.0089 s
agent0:                 episode reward: -0.6355,                 loss: 0.1725
agent1:                 episode reward: 0.6355,                 loss: 0.1697
Score delta: 1.6419502932624774, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/29332_1.
Episode: 29781/101000 (29.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9502s / 4428.9591 s
agent0:                 episode reward: -0.3206,                 loss: 0.1758
agent1:                 episode reward: 0.3206,                 loss: nan
Episode: 29801/101000 (29.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2218s / 4432.1809 s
agent0:                 episode reward: -0.2553,                 loss: 0.1740
agent1:                 episode reward: 0.2553,                 loss: nan
Episode: 29821/101000 (29.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0662s / 4435.2471 s
agent0:                 episode reward: -0.1202,                 loss: 0.1757
agent1:                 episode reward: 0.1202,                 loss: nan
Episode: 29841/101000 (29.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3223s / 4438.5695 s
agent0:                 episode reward: -0.3832,                 loss: 0.1748
agent1:                 episode reward: 0.3832,                 loss: nan
Episode: 29861/101000 (29.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8178s / 4441.3873 s
agent0:                 episode reward: 0.0372,                 loss: 0.1755
agent1:                 episode reward: -0.0372,                 loss: nan
Episode: 29881/101000 (29.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7191s / 4445.1064 s
agent0:                 episode reward: -0.5743,                 loss: 0.1818
agent1:                 episode reward: 0.5743,                 loss: nan
Episode: 29901/101000 (29.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7458s / 4447.8523 s
agent0:                 episode reward: 0.1248,                 loss: 0.1843
agent1:                 episode reward: -0.1248,                 loss: nan
Episode: 29921/101000 (29.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3761s / 4451.2283 s
agent0:                 episode reward: -0.2432,                 loss: 0.1820
agent1:                 episode reward: 0.2432,                 loss: nan
Episode: 29941/101000 (29.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7388s / 4453.9671 s
agent0:                 episode reward: -0.3125,                 loss: 0.1825
agent1:                 episode reward: 0.3125,                 loss: nan
Episode: 29961/101000 (29.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8292s / 4456.7962 s
agent0:                 episode reward: -0.0700,                 loss: 0.1837
agent1:                 episode reward: 0.0700,                 loss: nan
Episode: 29981/101000 (29.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4867s / 4459.2829 s
agent0:                 episode reward: 0.3208,                 loss: 0.1825
agent1:                 episode reward: -0.3208,                 loss: nan
Episode: 30001/101000 (29.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5355s / 4461.8184 s
agent0:                 episode reward: 0.0530,                 loss: 0.1811
agent1:                 episode reward: -0.0530,                 loss: 0.1698
Score delta: 1.6758333809683326, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/29558_0.
Episode: 30021/101000 (29.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6707s / 4464.4891 s
agent0:                 episode reward: -0.0781,                 loss: nan
agent1:                 episode reward: 0.0781,                 loss: 0.1667
Episode: 30041/101000 (29.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9490s / 4467.4381 s
agent0:                 episode reward: -0.4538,                 loss: 0.1879
agent1:                 episode reward: 0.4538,                 loss: 0.1657
Score delta: 1.617919987778302, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/29614_1.
Episode: 30061/101000 (29.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8277s / 4470.2658 s
agent0:                 episode reward: -0.2166,                 loss: 0.1849
agent1:                 episode reward: 0.2166,                 loss: nan
Episode: 30081/101000 (29.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8458s / 4473.1116 s
agent0:                 episode reward: -0.1248,                 loss: 0.1840
agent1:                 episode reward: 0.1248,                 loss: nan
Episode: 30101/101000 (29.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8357s / 4475.9472 s
agent0:                 episode reward: 0.1757,                 loss: 0.1857
agent1:                 episode reward: -0.1757,                 loss: nan
Episode: 30121/101000 (29.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5729s / 4478.5201 s
agent0:                 episode reward: -0.4766,                 loss: 0.1849
agent1:                 episode reward: 0.4766,                 loss: nan
Episode: 30141/101000 (29.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5674s / 4481.0875 s
agent0:                 episode reward: -0.4704,                 loss: 0.1840
agent1:                 episode reward: 0.4704,                 loss: nan
Episode: 30161/101000 (29.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7908s / 4483.8782 s
agent0:                 episode reward: -0.1745,                 loss: 0.1849
agent1:                 episode reward: 0.1745,                 loss: nan
Episode: 30181/101000 (29.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3721s / 4486.2503 s
agent0:                 episode reward: 0.1113,                 loss: 0.1825
agent1:                 episode reward: -0.1113,                 loss: 0.1859
Score delta: 1.7824868688204751, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/29739_0.
Episode: 30201/101000 (29.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3007s / 4488.5509 s
agent0:                 episode reward: -0.2620,                 loss: nan
agent1:                 episode reward: 0.2620,                 loss: 0.1815
Episode: 30221/101000 (29.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1395s / 4491.6905 s
agent0:                 episode reward: -0.0887,                 loss: nan
agent1:                 episode reward: 0.0887,                 loss: 0.1814
Episode: 30241/101000 (29.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4303s / 4494.1208 s
agent0:                 episode reward: -0.5226,                 loss: 0.2028
agent1:                 episode reward: 0.5226,                 loss: 0.1782
Score delta: 1.5223570581060881, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/29803_1.
Episode: 30261/101000 (29.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6866s / 4496.8074 s
agent0:                 episode reward: -0.5337,                 loss: 0.1975
agent1:                 episode reward: 0.5337,                 loss: nan
Episode: 30281/101000 (29.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6688s / 4499.4762 s
agent0:                 episode reward: -0.5345,                 loss: 0.1924
agent1:                 episode reward: 0.5345,                 loss: nan
Episode: 30301/101000 (30.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6323s / 4502.1085 s
agent0:                 episode reward: -0.1515,                 loss: 0.1918
agent1:                 episode reward: 0.1515,                 loss: nan
Episode: 30321/101000 (30.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0674s / 4505.1759 s
agent0:                 episode reward: -0.3725,                 loss: 0.1929
agent1:                 episode reward: 0.3725,                 loss: nan
Episode: 30341/101000 (30.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4849s / 4508.6608 s
agent0:                 episode reward: -0.2336,                 loss: 0.1878
agent1:                 episode reward: 0.2336,                 loss: nan
Episode: 30361/101000 (30.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6177s / 4511.2785 s
agent0:                 episode reward: -0.3727,                 loss: 0.1879
agent1:                 episode reward: 0.3727,                 loss: nan
Episode: 30381/101000 (30.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7806s / 4514.0591 s
agent0:                 episode reward: -0.5525,                 loss: 0.1871
agent1:                 episode reward: 0.5525,                 loss: nan
Episode: 30401/101000 (30.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7164s / 4516.7755 s
agent0:                 episode reward: -0.2965,                 loss: 0.1879
agent1:                 episode reward: 0.2965,                 loss: nan
Episode: 30421/101000 (30.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0711s / 4519.8466 s
agent0:                 episode reward: 0.0439,                 loss: 0.1864
agent1:                 episode reward: -0.0439,                 loss: nan
Episode: 30441/101000 (30.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2734s / 4523.1200 s
agent0:                 episode reward: -0.2186,                 loss: 0.1877
agent1:                 episode reward: 0.2186,                 loss: nan
Episode: 30461/101000 (30.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8798s / 4525.9998 s
agent0:                 episode reward: -0.0904,                 loss: 0.1862
agent1:                 episode reward: 0.0904,                 loss: nan
Episode: 30481/101000 (30.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0146s / 4529.0144 s
agent0:                 episode reward: -0.1645,                 loss: 0.1860
agent1:                 episode reward: 0.1645,                 loss: nan
Episode: 30501/101000 (30.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9360s / 4531.9505 s
agent0:                 episode reward: -0.0583,                 loss: 0.1877
agent1:                 episode reward: 0.0583,                 loss: nan
Episode: 30521/101000 (30.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7950s / 4534.7454 s
agent0:                 episode reward: -0.2378,                 loss: 0.1880
agent1:                 episode reward: 0.2378,                 loss: nan
Episode: 30541/101000 (30.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7201s / 4537.4656 s
agent0:                 episode reward: -0.1349,                 loss: 0.1873
agent1:                 episode reward: 0.1349,                 loss: nan
Episode: 30561/101000 (30.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1013s / 4540.5669 s
agent0:                 episode reward: -0.3002,                 loss: 0.1886
agent1:                 episode reward: 0.3002,                 loss: nan
Episode: 30581/101000 (30.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1201s / 4543.6870 s
agent0:                 episode reward: -0.2310,                 loss: 0.1861
agent1:                 episode reward: 0.2310,                 loss: nan
Episode: 30601/101000 (30.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1032s / 4546.7902 s
agent0:                 episode reward: 0.1425,                 loss: 0.1877
agent1:                 episode reward: -0.1425,                 loss: nan
Episode: 30621/101000 (30.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7671s / 4549.5573 s
agent0:                 episode reward: -0.4040,                 loss: 0.1850
agent1:                 episode reward: 0.4040,                 loss: nan
Episode: 30641/101000 (30.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3678s / 4552.9251 s
agent0:                 episode reward: -0.1027,                 loss: 0.1862
agent1:                 episode reward: 0.1027,                 loss: nan
Episode: 30661/101000 (30.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2423s / 4556.1674 s
agent0:                 episode reward: 0.0529,                 loss: 0.1855
agent1:                 episode reward: -0.0529,                 loss: nan
Episode: 30681/101000 (30.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9857s / 4559.1531 s
agent0:                 episode reward: -0.1120,                 loss: 0.1802
agent1:                 episode reward: 0.1120,                 loss: nan
Episode: 30701/101000 (30.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5602s / 4561.7133 s
agent0:                 episode reward: -0.1999,                 loss: 0.1792
agent1:                 episode reward: 0.1999,                 loss: nan
Episode: 30721/101000 (30.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0782s / 4564.7915 s
agent0:                 episode reward: 0.1553,                 loss: 0.1806
agent1:                 episode reward: -0.1553,                 loss: nan
Episode: 30741/101000 (30.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7775s / 4567.5690 s
agent0:                 episode reward: -0.3057,                 loss: 0.1802
agent1:                 episode reward: 0.3057,                 loss: nan
Episode: 30761/101000 (30.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9556s / 4571.5246 s
agent0:                 episode reward: -0.1507,                 loss: 0.1806
agent1:                 episode reward: 0.1507,                 loss: nan
Episode: 30781/101000 (30.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9832s / 4574.5078 s
agent0:                 episode reward: -0.1486,                 loss: 0.1795
agent1:                 episode reward: 0.1486,                 loss: nan
Episode: 30801/101000 (30.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2927s / 4577.8005 s
agent0:                 episode reward: -0.0141,                 loss: 0.1812
agent1:                 episode reward: 0.0141,                 loss: nan
Episode: 30821/101000 (30.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4944s / 4581.2949 s
agent0:                 episode reward: -0.0917,                 loss: 0.1779
agent1:                 episode reward: 0.0917,                 loss: nan
Episode: 30841/101000 (30.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3339s / 4583.6288 s
agent0:                 episode reward: -0.1083,                 loss: 0.1796
agent1:                 episode reward: 0.1083,                 loss: nan
Episode: 30861/101000 (30.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5256s / 4586.1544 s
agent0:                 episode reward: -0.1831,                 loss: 0.1787
agent1:                 episode reward: 0.1831,                 loss: nan
Episode: 30881/101000 (30.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0800s / 4589.2344 s
agent0:                 episode reward: -0.1822,                 loss: 0.1788
agent1:                 episode reward: 0.1822,                 loss: nan
Episode: 30901/101000 (30.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7721s / 4592.0065 s
agent0:                 episode reward: -0.4731,                 loss: 0.1805
agent1:                 episode reward: 0.4731,                 loss: nan
Episode: 30921/101000 (30.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6129s / 4595.6194 s
agent0:                 episode reward: 0.1026,                 loss: 0.1813
agent1:                 episode reward: -0.1026,                 loss: nan
Episode: 30941/101000 (30.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0097s / 4598.6291 s
agent0:                 episode reward: 0.1545,                 loss: 0.1792
agent1:                 episode reward: -0.1545,                 loss: nan
Episode: 30961/101000 (30.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0101s / 4601.6392 s
agent0:                 episode reward: -0.1740,                 loss: 0.1781
agent1:                 episode reward: 0.1740,                 loss: nan
Episode: 30981/101000 (30.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9805s / 4604.6197 s
agent0:                 episode reward: -0.3710,                 loss: 0.1796
agent1:                 episode reward: 0.3710,                 loss: nan
Episode: 31001/101000 (30.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0344s / 4608.6541 s
agent0:                 episode reward: -0.0746,                 loss: 0.1828
agent1:                 episode reward: 0.0746,                 loss: nan
Episode: 31021/101000 (30.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0555s / 4611.7096 s
agent0:                 episode reward: -0.3795,                 loss: 0.1853
agent1:                 episode reward: 0.3795,                 loss: nan
Episode: 31041/101000 (30.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9093s / 4614.6189 s
agent0:                 episode reward: -0.0579,                 loss: 0.1854
agent1:                 episode reward: 0.0579,                 loss: nan
Episode: 31061/101000 (30.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5522s / 4617.1711 s
agent0:                 episode reward: 0.0758,                 loss: 0.1878
agent1:                 episode reward: -0.0758,                 loss: nan
Episode: 31081/101000 (30.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1340s / 4620.3051 s
agent0:                 episode reward: 0.0143,                 loss: 0.1855
agent1:                 episode reward: -0.0143,                 loss: nan
Episode: 31101/101000 (30.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5897s / 4622.8948 s
agent0:                 episode reward: -0.5456,                 loss: 0.1859
agent1:                 episode reward: 0.5456,                 loss: nan
Episode: 31121/101000 (30.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2166s / 4626.1114 s
agent0:                 episode reward: 0.1192,                 loss: 0.1852
agent1:                 episode reward: -0.1192,                 loss: nan
Episode: 31141/101000 (30.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9731s / 4629.0845 s
agent0:                 episode reward: -0.1713,                 loss: 0.1860
agent1:                 episode reward: 0.1713,                 loss: nan
Episode: 31161/101000 (30.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2020s / 4632.2865 s
agent0:                 episode reward: -0.2009,                 loss: 0.1868
agent1:                 episode reward: 0.2009,                 loss: nan
Episode: 31181/101000 (30.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2736s / 4635.5601 s
agent0:                 episode reward: -0.5035,                 loss: 0.1865
agent1:                 episode reward: 0.5035,                 loss: nan
Episode: 31201/101000 (30.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9075s / 4638.4676 s
agent0:                 episode reward: -0.1785,                 loss: 0.1852
agent1:                 episode reward: 0.1785,                 loss: nan
Episode: 31221/101000 (30.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5276s / 4641.9952 s
agent0:                 episode reward: -0.3289,                 loss: 0.1853
agent1:                 episode reward: 0.3289,                 loss: nan
Episode: 31241/101000 (30.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2131s / 4645.2083 s
agent0:                 episode reward: 0.3204,                 loss: 0.1862
agent1:                 episode reward: -0.3204,                 loss: nan
Episode: 31261/101000 (30.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2948s / 4648.5031 s
agent0:                 episode reward: 0.0059,                 loss: 0.1862
agent1:                 episode reward: -0.0059,                 loss: nan
Episode: 31281/101000 (30.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8144s / 4651.3175 s
agent0:                 episode reward: -0.2258,                 loss: 0.1855
agent1:                 episode reward: 0.2258,                 loss: nan
Episode: 31301/101000 (30.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9811s / 4654.2986 s
agent0:                 episode reward: 0.0252,                 loss: 0.1851
agent1:                 episode reward: -0.0252,                 loss: nan
Episode: 31321/101000 (31.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0442s / 4657.3429 s
agent0:                 episode reward: -0.2228,                 loss: 0.1870
agent1:                 episode reward: 0.2228,                 loss: nan
Episode: 31341/101000 (31.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9402s / 4660.2831 s
agent0:                 episode reward: -0.1380,                 loss: 0.1805
agent1:                 episode reward: 0.1380,                 loss: nan
Episode: 31361/101000 (31.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1405s / 4662.4236 s
agent0:                 episode reward: -0.4556,                 loss: 0.1799
agent1:                 episode reward: 0.4556,                 loss: nan
Episode: 31381/101000 (31.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4269s / 4665.8504 s
agent0:                 episode reward: 0.0484,                 loss: 0.1804
agent1:                 episode reward: -0.0484,                 loss: nan
Episode: 31401/101000 (31.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8670s / 4669.7174 s
agent0:                 episode reward: -0.2486,                 loss: 0.1814
agent1:                 episode reward: 0.2486,                 loss: nan
Episode: 31421/101000 (31.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0204s / 4672.7378 s
agent0:                 episode reward: 0.1448,                 loss: 0.1803
agent1:                 episode reward: -0.1448,                 loss: nan
Episode: 31441/101000 (31.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9729s / 4675.7106 s
agent0:                 episode reward: -0.0403,                 loss: 0.1801
agent1:                 episode reward: 0.0403,                 loss: nan
Episode: 31461/101000 (31.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2324s / 4678.9431 s
agent0:                 episode reward: -0.1490,                 loss: 0.1791
agent1:                 episode reward: 0.1490,                 loss: nan
Episode: 31481/101000 (31.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2380s / 4682.1810 s
agent0:                 episode reward: 0.1599,                 loss: 0.1786
agent1:                 episode reward: -0.1599,                 loss: nan
Episode: 31501/101000 (31.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9370s / 4685.1180 s
agent0:                 episode reward: -0.2797,                 loss: 0.1803
agent1:                 episode reward: 0.2797,                 loss: nan
Episode: 31521/101000 (31.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7889s / 4687.9069 s
agent0:                 episode reward: 0.0330,                 loss: 0.1814
agent1:                 episode reward: -0.0330,                 loss: nan
Episode: 31541/101000 (31.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8807s / 4690.7876 s
agent0:                 episode reward: -0.2767,                 loss: 0.1803
agent1:                 episode reward: 0.2767,                 loss: nan
Episode: 31561/101000 (31.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7761s / 4694.5637 s
agent0:                 episode reward: -0.5153,                 loss: 0.1800
agent1:                 episode reward: 0.5153,                 loss: nan
Episode: 31581/101000 (31.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5576s / 4697.1213 s
agent0:                 episode reward: -0.0162,                 loss: 0.1815
agent1:                 episode reward: 0.0162,                 loss: nan
Episode: 31601/101000 (31.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4026s / 4700.5238 s
agent0:                 episode reward: -0.0023,                 loss: 0.1798
agent1:                 episode reward: 0.0023,                 loss: nan
Episode: 31621/101000 (31.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2314s / 4703.7552 s
agent0:                 episode reward: 0.1998,                 loss: 0.1800
agent1:                 episode reward: -0.1998,                 loss: nan
Episode: 31641/101000 (31.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8024s / 4707.5576 s
agent0:                 episode reward: 0.1802,                 loss: 0.1812
agent1:                 episode reward: -0.1802,                 loss: nan
Episode: 31661/101000 (31.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7919s / 4711.3495 s
agent0:                 episode reward: -0.3281,                 loss: 0.1798
agent1:                 episode reward: 0.3281,                 loss: nan
Episode: 31681/101000 (31.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0277s / 4714.3772 s
agent0:                 episode reward: -0.4588,                 loss: 0.1800
agent1:                 episode reward: 0.4588,                 loss: nan
Episode: 31701/101000 (31.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0609s / 4717.4382 s
agent0:                 episode reward: -0.2848,                 loss: 0.1804
agent1:                 episode reward: 0.2848,                 loss: nan
Episode: 31721/101000 (31.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5582s / 4720.9964 s
agent0:                 episode reward: -0.2745,                 loss: 0.1780
agent1:                 episode reward: 0.2745,                 loss: nan
Episode: 31741/101000 (31.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4761s / 4723.4725 s
agent0:                 episode reward: -0.1712,                 loss: 0.1789
agent1:                 episode reward: 0.1712,                 loss: nan
Episode: 31761/101000 (31.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5215s / 4725.9940 s
agent0:                 episode reward: 0.0843,                 loss: 0.1797
agent1:                 episode reward: -0.0843,                 loss: 0.1884
Score delta: 1.544638896551487, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/31334_0.
Episode: 31781/101000 (31.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1410s / 4728.1350 s
agent0:                 episode reward: -0.3879,                 loss: nan
agent1:                 episode reward: 0.3879,                 loss: 0.1702
Episode: 31801/101000 (31.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4638s / 4730.5988 s
agent0:                 episode reward: -0.1092,                 loss: nan
agent1:                 episode reward: 0.1092,                 loss: 0.1663
Episode: 31821/101000 (31.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9124s / 4733.5113 s
agent0:                 episode reward: -0.3398,                 loss: 0.1729
agent1:                 episode reward: 0.3398,                 loss: 0.1668
Score delta: 1.7274374868197455, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/31385_1.
Episode: 31841/101000 (31.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3769s / 4736.8882 s
agent0:                 episode reward: 0.0040,                 loss: 0.1720
agent1:                 episode reward: -0.0040,                 loss: nan
Episode: 31861/101000 (31.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8594s / 4739.7476 s
agent0:                 episode reward: -0.2478,                 loss: 0.1708
agent1:                 episode reward: 0.2478,                 loss: nan
Episode: 31881/101000 (31.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 2.2397s / 4741.9873 s
agent0:                 episode reward: -0.2399,                 loss: 0.1738
agent1:                 episode reward: 0.2399,                 loss: nan
Episode: 31901/101000 (31.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6261s / 4745.6134 s
agent0:                 episode reward: -0.7481,                 loss: 0.1735
agent1:                 episode reward: 0.7481,                 loss: nan
Episode: 31921/101000 (31.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8019s / 4748.4154 s
agent0:                 episode reward: -0.2193,                 loss: 0.1696
agent1:                 episode reward: 0.2193,                 loss: nan
Episode: 31941/101000 (31.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3596s / 4750.7750 s
agent0:                 episode reward: 0.0179,                 loss: 0.1714
agent1:                 episode reward: -0.0179,                 loss: nan
Episode: 31961/101000 (31.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2515s / 4754.0264 s
agent0:                 episode reward: -0.4002,                 loss: 0.1720
agent1:                 episode reward: 0.4002,                 loss: nan
Episode: 31981/101000 (31.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2861s / 4757.3125 s
agent0:                 episode reward: -0.2288,                 loss: 0.1704
agent1:                 episode reward: 0.2288,                 loss: nan
Episode: 32001/101000 (31.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2270s / 4760.5395 s
agent0:                 episode reward: 0.0604,                 loss: 0.1706
agent1:                 episode reward: -0.0604,                 loss: nan
Episode: 32021/101000 (31.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9977s / 4763.5373 s
agent0:                 episode reward: 0.0876,                 loss: 0.1698
agent1:                 episode reward: -0.0876,                 loss: nan
Episode: 32041/101000 (31.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0215s / 4766.5588 s
agent0:                 episode reward: -0.2842,                 loss: 0.1704
agent1:                 episode reward: 0.2842,                 loss: nan
Episode: 32061/101000 (31.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9159s / 4769.4747 s
agent0:                 episode reward: 0.0177,                 loss: 0.1834
agent1:                 episode reward: -0.0177,                 loss: nan
Episode: 32081/101000 (31.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8672s / 4772.3419 s
agent0:                 episode reward: -0.2912,                 loss: 0.1840
agent1:                 episode reward: 0.2912,                 loss: nan
Episode: 32101/101000 (31.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8980s / 4775.2399 s
agent0:                 episode reward: -0.5070,                 loss: 0.1853
agent1:                 episode reward: 0.5070,                 loss: nan
Episode: 32121/101000 (31.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0854s / 4778.3253 s
agent0:                 episode reward: -0.5326,                 loss: 0.1844
agent1:                 episode reward: 0.5326,                 loss: nan
Episode: 32141/101000 (31.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6632s / 4780.9884 s
agent0:                 episode reward: -0.4831,                 loss: 0.1850
agent1:                 episode reward: 0.4831,                 loss: nan
Episode: 32161/101000 (31.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4591s / 4784.4475 s
agent0:                 episode reward: -0.2407,                 loss: 0.1844
agent1:                 episode reward: 0.2407,                 loss: nan
Episode: 32181/101000 (31.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8672s / 4788.3147 s
agent0:                 episode reward: -0.3927,                 loss: 0.1834
agent1:                 episode reward: 0.3927,                 loss: nan
Episode: 32201/101000 (31.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8176s / 4791.1323 s
agent0:                 episode reward: -0.2536,                 loss: 0.1839
agent1:                 episode reward: 0.2536,                 loss: nan
Episode: 32221/101000 (31.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7272s / 4793.8595 s
agent0:                 episode reward: -0.1747,                 loss: 0.1841
agent1:                 episode reward: 0.1747,                 loss: nan
Episode: 32241/101000 (31.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5342s / 4797.3937 s
agent0:                 episode reward: -0.3145,                 loss: 0.1838
agent1:                 episode reward: 0.3145,                 loss: nan
Episode: 32261/101000 (31.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5379s / 4800.9316 s
agent0:                 episode reward: -0.1026,                 loss: 0.1816
agent1:                 episode reward: 0.1026,                 loss: nan
Episode: 32281/101000 (31.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8980s / 4803.8296 s
agent0:                 episode reward: 0.2640,                 loss: 0.1811
agent1:                 episode reward: -0.2640,                 loss: nan
Episode: 32301/101000 (31.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8326s / 4807.6622 s
agent0:                 episode reward: 0.1266,                 loss: 0.1836
agent1:                 episode reward: -0.1266,                 loss: nan
Episode: 32321/101000 (32.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9731s / 4810.6353 s
agent0:                 episode reward: -0.2150,                 loss: 0.1823
agent1:                 episode reward: 0.2150,                 loss: nan
Episode: 32341/101000 (32.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3132s / 4812.9485 s
agent0:                 episode reward: -0.1486,                 loss: 0.1829
agent1:                 episode reward: 0.1486,                 loss: nan
Episode: 32361/101000 (32.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3738s / 4816.3223 s
agent0:                 episode reward: -0.0602,                 loss: 0.1840
agent1:                 episode reward: 0.0602,                 loss: nan
Episode: 32381/101000 (32.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6678s / 4818.9901 s
agent0:                 episode reward: -0.0552,                 loss: 0.1835
agent1:                 episode reward: 0.0552,                 loss: nan
Episode: 32401/101000 (32.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0246s / 4822.0147 s
agent0:                 episode reward: 0.0575,                 loss: 0.1834
agent1:                 episode reward: -0.0575,                 loss: nan
Episode: 32421/101000 (32.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5264s / 4825.5411 s
agent0:                 episode reward: -0.2064,                 loss: 0.1831
agent1:                 episode reward: 0.2064,                 loss: nan
Episode: 32441/101000 (32.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1289s / 4827.6700 s
agent0:                 episode reward: 0.0675,                 loss: 0.1825
agent1:                 episode reward: -0.0675,                 loss: nan
Episode: 32461/101000 (32.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6068s / 4830.2768 s
agent0:                 episode reward: -0.1122,                 loss: 0.1828
agent1:                 episode reward: 0.1122,                 loss: nan
Episode: 32481/101000 (32.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7425s / 4834.0192 s
agent0:                 episode reward: -0.0991,                 loss: 0.1830
agent1:                 episode reward: 0.0991,                 loss: nan
Episode: 32501/101000 (32.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6791s / 4836.6984 s
agent0:                 episode reward: -0.0503,                 loss: 0.1825
agent1:                 episode reward: 0.0503,                 loss: nan
Episode: 32521/101000 (32.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9182s / 4839.6165 s
agent0:                 episode reward: -0.1522,                 loss: 0.1820
agent1:                 episode reward: 0.1522,                 loss: nan
Episode: 32541/101000 (32.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0484s / 4842.6650 s
agent0:                 episode reward: -0.1193,                 loss: 0.1840
agent1:                 episode reward: 0.1193,                 loss: nan
Episode: 32561/101000 (32.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3661s / 4846.0311 s
agent0:                 episode reward: 0.1158,                 loss: 0.1831
agent1:                 episode reward: -0.1158,                 loss: nan
Episode: 32581/101000 (32.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3694s / 4849.4005 s
agent0:                 episode reward: 0.1182,                 loss: 0.1826
agent1:                 episode reward: -0.1182,                 loss: nan
Episode: 32601/101000 (32.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9745s / 4852.3750 s
agent0:                 episode reward: -0.0001,                 loss: 0.1810
agent1:                 episode reward: 0.0001,                 loss: nan
Episode: 32621/101000 (32.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4130s / 4855.7881 s
agent0:                 episode reward: 0.3130,                 loss: 0.1826
agent1:                 episode reward: -0.3130,                 loss: nan
Episode: 32641/101000 (32.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1992s / 4857.9872 s
agent0:                 episode reward: -0.2742,                 loss: 0.1823
agent1:                 episode reward: 0.2742,                 loss: nan
Episode: 32661/101000 (32.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4140s / 4860.4013 s
agent0:                 episode reward: 0.0899,                 loss: 0.1808
agent1:                 episode reward: -0.0899,                 loss: nan
Episode: 32681/101000 (32.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4446s / 4862.8459 s
agent0:                 episode reward: 0.1366,                 loss: 0.1817
agent1:                 episode reward: -0.1366,                 loss: nan
Episode: 32701/101000 (32.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7545s / 4865.6004 s
agent0:                 episode reward: -0.1343,                 loss: 0.1829
agent1:                 episode reward: 0.1343,                 loss: nan
Episode: 32721/101000 (32.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1936s / 4868.7940 s
agent0:                 episode reward: 0.1168,                 loss: 0.1806
agent1:                 episode reward: -0.1168,                 loss: nan
Episode: 32741/101000 (32.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9380s / 4871.7319 s
agent0:                 episode reward: -0.3301,                 loss: 0.1811
agent1:                 episode reward: 0.3301,                 loss: nan
Episode: 32761/101000 (32.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7874s / 4874.5193 s
agent0:                 episode reward: -0.4249,                 loss: 0.1819
agent1:                 episode reward: 0.4249,                 loss: nan
Episode: 32781/101000 (32.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0112s / 4877.5305 s
agent0:                 episode reward: -0.1129,                 loss: 0.1810
agent1:                 episode reward: 0.1129,                 loss: nan
Episode: 32801/101000 (32.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9242s / 4880.4547 s
agent0:                 episode reward: 0.5835,                 loss: 0.1818
agent1:                 episode reward: -0.5835,                 loss: 0.1618
Score delta: 1.6789242966242988, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/32367_0.
Episode: 32821/101000 (32.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3155s / 4882.7703 s
agent0:                 episode reward: -0.5944,                 loss: nan
agent1:                 episode reward: 0.5944,                 loss: 0.1610
Episode: 32841/101000 (32.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7331s / 4885.5034 s
agent0:                 episode reward: -0.1681,                 loss: nan
agent1:                 episode reward: 0.1681,                 loss: 0.1614
Episode: 32861/101000 (32.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3706s / 4888.8740 s
agent0:                 episode reward: -0.4260,                 loss: 0.2017
agent1:                 episode reward: 0.4260,                 loss: 0.1621
Score delta: 1.6533708218368564, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/32423_1.
Episode: 32881/101000 (32.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4609s / 4891.3349 s
agent0:                 episode reward: -0.2262,                 loss: 0.1955
agent1:                 episode reward: 0.2262,                 loss: nan
Episode: 32901/101000 (32.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3158s / 4894.6507 s
agent0:                 episode reward: -0.1349,                 loss: 0.1948
agent1:                 episode reward: 0.1349,                 loss: nan
Episode: 32921/101000 (32.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7819s / 4898.4326 s
agent0:                 episode reward: -0.3868,                 loss: 0.1930
agent1:                 episode reward: 0.3868,                 loss: nan
Episode: 32941/101000 (32.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3033s / 4901.7359 s
agent0:                 episode reward: -0.2670,                 loss: 0.1906
agent1:                 episode reward: 0.2670,                 loss: nan
Episode: 32961/101000 (32.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6569s / 4904.3928 s
agent0:                 episode reward: 0.0234,                 loss: 0.1920
agent1:                 episode reward: -0.0234,                 loss: nan
Episode: 32981/101000 (32.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4010s / 4907.7938 s
agent0:                 episode reward: -0.3331,                 loss: 0.1895
agent1:                 episode reward: 0.3331,                 loss: nan
Episode: 33001/101000 (32.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6927s / 4911.4864 s
agent0:                 episode reward: 0.0972,                 loss: 0.1908
agent1:                 episode reward: -0.0972,                 loss: nan
Episode: 33021/101000 (32.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8206s / 4914.3071 s
agent0:                 episode reward: 0.1466,                 loss: 0.1892
agent1:                 episode reward: -0.1466,                 loss: nan
Episode: 33041/101000 (32.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4240s / 4916.7311 s
agent0:                 episode reward: 0.0062,                 loss: 0.1909
agent1:                 episode reward: -0.0062,                 loss: nan
Episode: 33061/101000 (32.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8910s / 4919.6221 s
agent0:                 episode reward: -0.2197,                 loss: 0.1899
agent1:                 episode reward: 0.2197,                 loss: nan
Episode: 33081/101000 (32.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9144s / 4922.5365 s
agent0:                 episode reward: 0.1851,                 loss: 0.1900
agent1:                 episode reward: -0.1851,                 loss: nan
Episode: 33101/101000 (32.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0091s / 4926.5456 s
agent0:                 episode reward: -0.3697,                 loss: 0.1918
agent1:                 episode reward: 0.3697,                 loss: nan
Episode: 33121/101000 (32.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5748s / 4929.1205 s
agent0:                 episode reward: -0.1254,                 loss: 0.1966
agent1:                 episode reward: 0.1254,                 loss: nan
Episode: 33141/101000 (32.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0757s / 4932.1962 s
agent0:                 episode reward: -0.1225,                 loss: 0.1964
agent1:                 episode reward: 0.1225,                 loss: nan
Episode: 33161/101000 (32.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3822s / 4935.5784 s
agent0:                 episode reward: 0.0846,                 loss: 0.1969
agent1:                 episode reward: -0.0846,                 loss: nan
Episode: 33181/101000 (32.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7003s / 4938.2787 s
agent0:                 episode reward: 0.1999,                 loss: 0.1956
agent1:                 episode reward: -0.1999,                 loss: nan
Episode: 33201/101000 (32.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2473s / 4941.5260 s
agent0:                 episode reward: -0.0844,                 loss: 0.1942
agent1:                 episode reward: 0.0844,                 loss: nan
Episode: 33221/101000 (32.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7878s / 4944.3138 s
agent0:                 episode reward: 0.0607,                 loss: 0.1952
agent1:                 episode reward: -0.0607,                 loss: nan
Episode: 33241/101000 (32.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7105s / 4948.0243 s
agent0:                 episode reward: -0.3703,                 loss: 0.1966
agent1:                 episode reward: 0.3703,                 loss: nan
Episode: 33261/101000 (32.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4855s / 4951.5098 s
agent0:                 episode reward: 0.2026,                 loss: 0.1945
agent1:                 episode reward: -0.2026,                 loss: nan
Episode: 33281/101000 (32.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3200s / 4954.8298 s
agent0:                 episode reward: -0.2316,                 loss: 0.1956
agent1:                 episode reward: 0.2316,                 loss: nan
Episode: 33301/101000 (32.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6445s / 4957.4743 s
agent0:                 episode reward: -0.5828,                 loss: 0.1951
agent1:                 episode reward: 0.5828,                 loss: nan
Episode: 33321/101000 (32.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4102s / 4960.8845 s
agent0:                 episode reward: 0.0108,                 loss: 0.1960
agent1:                 episode reward: -0.0108,                 loss: nan
Episode: 33341/101000 (33.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9148s / 4963.7993 s
agent0:                 episode reward: 0.3372,                 loss: 0.1933
agent1:                 episode reward: -0.3372,                 loss: nan
Episode: 33361/101000 (33.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7100s / 4967.5093 s
agent0:                 episode reward: -0.5824,                 loss: 0.1923
agent1:                 episode reward: 0.5824,                 loss: nan
Episode: 33381/101000 (33.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1766s / 4970.6860 s
agent0:                 episode reward: 0.0934,                 loss: 0.1954
agent1:                 episode reward: -0.0934,                 loss: nan
Episode: 33401/101000 (33.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1263s / 4973.8123 s
agent0:                 episode reward: -0.0192,                 loss: 0.1946
agent1:                 episode reward: 0.0192,                 loss: nan
Episode: 33421/101000 (33.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3172s / 4976.1295 s
agent0:                 episode reward: -0.3533,                 loss: 0.1948
agent1:                 episode reward: 0.3533,                 loss: nan
Episode: 33441/101000 (33.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1098s / 4979.2393 s
agent0:                 episode reward: -0.1690,                 loss: 0.1904
agent1:                 episode reward: 0.1690,                 loss: nan
Episode: 33461/101000 (33.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5651s / 4982.8044 s
agent0:                 episode reward: -0.0554,                 loss: 0.1841
agent1:                 episode reward: 0.0554,                 loss: nan
Episode: 33481/101000 (33.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0172s / 4985.8216 s
agent0:                 episode reward: -0.0148,                 loss: 0.1838
agent1:                 episode reward: 0.0148,                 loss: nan
Episode: 33501/101000 (33.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7072s / 4989.5289 s
agent0:                 episode reward: -0.4961,                 loss: 0.1823
agent1:                 episode reward: 0.4961,                 loss: nan
Episode: 33521/101000 (33.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1323s / 4992.6611 s
agent0:                 episode reward: 0.0511,                 loss: 0.1830
agent1:                 episode reward: -0.0511,                 loss: nan
Episode: 33541/101000 (33.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8031s / 4996.4642 s
agent0:                 episode reward: 0.1483,                 loss: 0.1825
agent1:                 episode reward: -0.1483,                 loss: nan
Episode: 33561/101000 (33.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4417s / 4999.9059 s
agent0:                 episode reward: -0.0424,                 loss: 0.1839
agent1:                 episode reward: 0.0424,                 loss: nan
Episode: 33581/101000 (33.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0555s / 5002.9613 s
agent0:                 episode reward: -0.1616,                 loss: 0.1843
agent1:                 episode reward: 0.1616,                 loss: nan
Episode: 33601/101000 (33.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7830s / 5005.7444 s
agent0:                 episode reward: 0.3268,                 loss: 0.1838
agent1:                 episode reward: -0.3268,                 loss: nan
Episode: 33621/101000 (33.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9199s / 5008.6642 s
agent0:                 episode reward: 0.3087,                 loss: 0.1830
agent1:                 episode reward: -0.3087,                 loss: nan
Episode: 33641/101000 (33.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7658s / 5011.4300 s
agent0:                 episode reward: 0.1077,                 loss: 0.1834
agent1:                 episode reward: -0.1077,                 loss: nan
Episode: 33661/101000 (33.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2101s / 5014.6402 s
agent0:                 episode reward: -0.2967,                 loss: 0.1867
agent1:                 episode reward: 0.2967,                 loss: nan
Episode: 33681/101000 (33.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3905s / 5018.0307 s
agent0:                 episode reward: 0.3464,                 loss: 0.1841
agent1:                 episode reward: -0.3464,                 loss: nan
Episode: 33701/101000 (33.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0980s / 5021.1287 s
agent0:                 episode reward: -0.0038,                 loss: 0.1824
agent1:                 episode reward: 0.0038,                 loss: nan
Episode: 33721/101000 (33.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6470s / 5023.7757 s
agent0:                 episode reward: 0.2760,                 loss: 0.1832
agent1:                 episode reward: -0.2760,                 loss: 0.1667
Score delta: 1.8428122095127875, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/33287_0.
Episode: 33741/101000 (33.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7540s / 5026.5297 s
agent0:                 episode reward: 0.0246,                 loss: nan
agent1:                 episode reward: -0.0246,                 loss: 0.1629
Episode: 33761/101000 (33.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7478s / 5029.2775 s
agent0:                 episode reward: -0.3333,                 loss: nan
agent1:                 episode reward: 0.3333,                 loss: 0.1590
Episode: 33781/101000 (33.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3692s / 5032.6468 s
agent0:                 episode reward: -0.4910,                 loss: nan
agent1:                 episode reward: 0.4910,                 loss: 0.1596
Score delta: 1.7852145501607057, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/33355_1.
Episode: 33801/101000 (33.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4560s / 5035.1027 s
agent0:                 episode reward: -0.4737,                 loss: 0.1822
agent1:                 episode reward: 0.4737,                 loss: nan
Episode: 33821/101000 (33.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7228s / 5038.8256 s
agent0:                 episode reward: -0.4235,                 loss: 0.1828
agent1:                 episode reward: 0.4235,                 loss: nan
Episode: 33841/101000 (33.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1344s / 5041.9599 s
agent0:                 episode reward: 0.5043,                 loss: 0.1842
agent1:                 episode reward: -0.5043,                 loss: nan
Episode: 33861/101000 (33.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6783s / 5044.6382 s
agent0:                 episode reward: -0.3722,                 loss: 0.1795
agent1:                 episode reward: 0.3722,                 loss: 0.1768
Score delta: 1.7528269215764083, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/33416_0.
Episode: 33881/101000 (33.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4640s / 5047.1022 s
agent0:                 episode reward: -0.5562,                 loss: nan
agent1:                 episode reward: 0.5562,                 loss: 0.1775
Episode: 33901/101000 (33.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3082s / 5050.4104 s
agent0:                 episode reward: -0.3699,                 loss: 0.1902
agent1:                 episode reward: 0.3699,                 loss: 0.1766
Score delta: 1.60303444810897, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/33464_1.
Episode: 33921/101000 (33.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4224s / 5052.8329 s
agent0:                 episode reward: -0.4039,                 loss: 0.1886
agent1:                 episode reward: 0.4039,                 loss: nan
Episode: 33941/101000 (33.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.1926s / 5055.0255 s
agent0:                 episode reward: -0.5289,                 loss: 0.1866
agent1:                 episode reward: 0.5289,                 loss: nan
Episode: 33961/101000 (33.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1565s / 5058.1819 s
agent0:                 episode reward: -0.1806,                 loss: 0.1886
agent1:                 episode reward: 0.1806,                 loss: nan
Episode: 33981/101000 (33.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7757s / 5061.9576 s
agent0:                 episode reward: 0.0427,                 loss: 0.1898
agent1:                 episode reward: -0.0427,                 loss: nan
Episode: 34001/101000 (33.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6743s / 5064.6320 s
agent0:                 episode reward: -0.3703,                 loss: 0.1874
agent1:                 episode reward: 0.3703,                 loss: nan
Episode: 34021/101000 (33.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7646s / 5067.3966 s
agent0:                 episode reward: 0.2034,                 loss: 0.1889
agent1:                 episode reward: -0.2034,                 loss: nan
Episode: 34041/101000 (33.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4392s / 5070.8357 s
agent0:                 episode reward: 0.0356,                 loss: 0.1863
agent1:                 episode reward: -0.0356,                 loss: nan
Episode: 34061/101000 (33.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1631s / 5073.9988 s
agent0:                 episode reward: -0.1821,                 loss: 0.1885
agent1:                 episode reward: 0.1821,                 loss: nan
Episode: 34081/101000 (33.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1054s / 5077.1042 s
agent0:                 episode reward: 0.2432,                 loss: 0.1872
agent1:                 episode reward: -0.2432,                 loss: nan
Episode: 34101/101000 (33.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1605s / 5080.2647 s
agent0:                 episode reward: -0.2378,                 loss: 0.1867
agent1:                 episode reward: 0.2378,                 loss: nan
Episode: 34121/101000 (33.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4316s / 5083.6963 s
agent0:                 episode reward: -0.1482,                 loss: 0.1871
agent1:                 episode reward: 0.1482,                 loss: nan
Episode: 34141/101000 (33.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6249s / 5087.3212 s
agent0:                 episode reward: -0.5935,                 loss: 0.1878
agent1:                 episode reward: 0.5935,                 loss: nan
Episode: 34161/101000 (33.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1335s / 5091.4547 s
agent0:                 episode reward: -0.0633,                 loss: 0.1897
agent1:                 episode reward: 0.0633,                 loss: nan
Episode: 34181/101000 (33.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5233s / 5094.9780 s
agent0:                 episode reward: -0.1089,                 loss: 0.1886
agent1:                 episode reward: 0.1089,                 loss: nan
Episode: 34201/101000 (33.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9843s / 5097.9623 s
agent0:                 episode reward: -0.2152,                 loss: 0.1873
agent1:                 episode reward: 0.2152,                 loss: nan
Episode: 34221/101000 (33.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2731s / 5101.2354 s
agent0:                 episode reward: -0.0546,                 loss: 0.1957
agent1:                 episode reward: 0.0546,                 loss: nan
Episode: 34241/101000 (33.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3158s / 5103.5512 s
agent0:                 episode reward: -0.6115,                 loss: 0.2044
agent1:                 episode reward: 0.6115,                 loss: nan
Episode: 34261/101000 (33.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4356s / 5106.9868 s
agent0:                 episode reward: 0.3178,                 loss: 0.2020
agent1:                 episode reward: -0.3178,                 loss: nan
Episode: 34281/101000 (33.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7332s / 5109.7200 s
agent0:                 episode reward: -0.2811,                 loss: 0.2026
agent1:                 episode reward: 0.2811,                 loss: nan
Episode: 34301/101000 (33.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7007s / 5112.4206 s
agent0:                 episode reward: -0.4713,                 loss: 0.2015
agent1:                 episode reward: 0.4713,                 loss: nan
Episode: 34321/101000 (33.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6865s / 5116.1071 s
agent0:                 episode reward: 0.0696,                 loss: 0.2019
agent1:                 episode reward: -0.0696,                 loss: nan
Episode: 34341/101000 (34.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9515s / 5119.0587 s
agent0:                 episode reward: -0.2996,                 loss: 0.2014
agent1:                 episode reward: 0.2996,                 loss: nan
Episode: 34361/101000 (34.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2484s / 5122.3070 s
agent0:                 episode reward: -0.0133,                 loss: 0.2027
agent1:                 episode reward: 0.0133,                 loss: nan
Episode: 34381/101000 (34.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7191s / 5125.0262 s
agent0:                 episode reward: 0.1099,                 loss: 0.2011
agent1:                 episode reward: -0.1099,                 loss: nan
Episode: 34401/101000 (34.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4753s / 5128.5015 s
agent0:                 episode reward: -0.0084,                 loss: 0.2024
agent1:                 episode reward: 0.0084,                 loss: nan
Episode: 34421/101000 (34.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1113s / 5131.6128 s
agent0:                 episode reward: -0.1016,                 loss: 0.2008
agent1:                 episode reward: 0.1016,                 loss: nan
Episode: 34441/101000 (34.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7177s / 5134.3304 s
agent0:                 episode reward: -0.1758,                 loss: 0.2010
agent1:                 episode reward: 0.1758,                 loss: nan
Episode: 34461/101000 (34.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7704s / 5137.1008 s
agent0:                 episode reward: -0.2805,                 loss: 0.2012
agent1:                 episode reward: 0.2805,                 loss: nan
Episode: 34481/101000 (34.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1316s / 5140.2324 s
agent0:                 episode reward: -0.2090,                 loss: 0.2014
agent1:                 episode reward: 0.2090,                 loss: nan
Episode: 34501/101000 (34.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7325s / 5142.9649 s
agent0:                 episode reward: 0.2699,                 loss: 0.2013
agent1:                 episode reward: -0.2699,                 loss: nan
Episode: 34521/101000 (34.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1191s / 5146.0840 s
agent0:                 episode reward: 0.0453,                 loss: 0.2020
agent1:                 episode reward: -0.0453,                 loss: nan
Episode: 34541/101000 (34.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5973s / 5148.6813 s
agent0:                 episode reward: -0.1833,                 loss: 0.2008
agent1:                 episode reward: 0.1833,                 loss: nan
Episode: 34561/101000 (34.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8908s / 5151.5721 s
agent0:                 episode reward: -0.1904,                 loss: 0.1886
agent1:                 episode reward: 0.1904,                 loss: nan
Episode: 34581/101000 (34.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1672s / 5154.7393 s
agent0:                 episode reward: 0.1205,                 loss: 0.1825
agent1:                 episode reward: -0.1205,                 loss: nan
Episode: 34601/101000 (34.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4001s / 5158.1394 s
agent0:                 episode reward: -0.0888,                 loss: 0.1834
agent1:                 episode reward: 0.0888,                 loss: nan
Episode: 34621/101000 (34.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1498s / 5161.2892 s
agent0:                 episode reward: 0.1664,                 loss: 0.1861
agent1:                 episode reward: -0.1664,                 loss: nan
Episode: 34641/101000 (34.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7723s / 5164.0615 s
agent0:                 episode reward: 0.4780,                 loss: 0.1827
agent1:                 episode reward: -0.4780,                 loss: nan
Episode: 34661/101000 (34.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6191s / 5167.6806 s
agent0:                 episode reward: -0.1837,                 loss: 0.1833
agent1:                 episode reward: 0.1837,                 loss: nan
Episode: 34681/101000 (34.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2490s / 5170.9296 s
agent0:                 episode reward: -0.0707,                 loss: 0.1838
agent1:                 episode reward: 0.0707,                 loss: nan
Episode: 34701/101000 (34.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4578s / 5174.3874 s
agent0:                 episode reward: 0.1896,                 loss: 0.1826
agent1:                 episode reward: -0.1896,                 loss: nan
Episode: 34721/101000 (34.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2538s / 5177.6412 s
agent0:                 episode reward: -0.1779,                 loss: 0.1868
agent1:                 episode reward: 0.1779,                 loss: 0.1652
Score delta: 1.699720798369442, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/34277_0.
Episode: 34741/101000 (34.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0864s / 5180.7276 s
agent0:                 episode reward: -0.2320,                 loss: nan
agent1:                 episode reward: 0.2320,                 loss: 0.1600
Episode: 34761/101000 (34.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3810s / 5183.1086 s
agent0:                 episode reward: -0.2566,                 loss: nan
agent1:                 episode reward: 0.2566,                 loss: 0.1600
Episode: 34781/101000 (34.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9665s / 5186.0751 s
agent0:                 episode reward: -0.2553,                 loss: nan
agent1:                 episode reward: 0.2553,                 loss: 0.1587
Episode: 34801/101000 (34.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7457s / 5188.8208 s
agent0:                 episode reward: -0.2738,                 loss: nan
agent1:                 episode reward: 0.2738,                 loss: 0.1591
Episode: 34821/101000 (34.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4804s / 5192.3012 s
agent0:                 episode reward: -0.6699,                 loss: 0.1953
agent1:                 episode reward: 0.6699,                 loss: 0.1575
Score delta: 1.9278432912418357, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/34386_1.
Episode: 34841/101000 (34.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8896s / 5195.1908 s
agent0:                 episode reward: -0.1394,                 loss: 0.1940
agent1:                 episode reward: 0.1394,                 loss: nan
Episode: 34861/101000 (34.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3507s / 5198.5415 s
agent0:                 episode reward: -0.5360,                 loss: 0.1942
agent1:                 episode reward: 0.5360,                 loss: nan
Episode: 34881/101000 (34.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0471s / 5201.5886 s
agent0:                 episode reward: -0.1270,                 loss: 0.1932
agent1:                 episode reward: 0.1270,                 loss: nan
Episode: 34901/101000 (34.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9884s / 5204.5770 s
agent0:                 episode reward: -0.2263,                 loss: 0.1907
agent1:                 episode reward: 0.2263,                 loss: nan
Episode: 34921/101000 (34.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4451s / 5207.0221 s
agent0:                 episode reward: -0.2865,                 loss: 0.1932
agent1:                 episode reward: 0.2865,                 loss: nan
Episode: 34941/101000 (34.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6903s / 5210.7124 s
agent0:                 episode reward: 0.1734,                 loss: 0.1922
agent1:                 episode reward: -0.1734,                 loss: nan
Episode: 34961/101000 (34.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5529s / 5214.2653 s
agent0:                 episode reward: -0.0496,                 loss: 0.1892
agent1:                 episode reward: 0.0496,                 loss: nan
Episode: 34981/101000 (34.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6163s / 5216.8816 s
agent0:                 episode reward: 0.0212,                 loss: 0.1910
agent1:                 episode reward: -0.0212,                 loss: nan
Episode: 35001/101000 (34.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3268s / 5220.2084 s
agent0:                 episode reward: -0.0753,                 loss: 0.1962
agent1:                 episode reward: 0.0753,                 loss: nan
Episode: 35021/101000 (34.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1407s / 5223.3491 s
agent0:                 episode reward: -0.0841,                 loss: 0.1965
agent1:                 episode reward: 0.0841,                 loss: nan
Episode: 35041/101000 (34.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0020s / 5226.3511 s
agent0:                 episode reward: -0.0458,                 loss: 0.1964
agent1:                 episode reward: 0.0458,                 loss: nan
Episode: 35061/101000 (34.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1998s / 5229.5509 s
agent0:                 episode reward: 0.2624,                 loss: 0.1965
agent1:                 episode reward: -0.2624,                 loss: nan
Episode: 35081/101000 (34.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4701s / 5233.0209 s
agent0:                 episode reward: -0.3533,                 loss: 0.1951
agent1:                 episode reward: 0.3533,                 loss: nan
Episode: 35101/101000 (34.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5737s / 5236.5947 s
agent0:                 episode reward: -0.1891,                 loss: 0.1964
agent1:                 episode reward: 0.1891,                 loss: nan
Episode: 35121/101000 (34.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0173s / 5239.6119 s
agent0:                 episode reward: 0.2546,                 loss: 0.1959
agent1:                 episode reward: -0.2546,                 loss: nan
Episode: 35141/101000 (34.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5793s / 5243.1912 s
agent0:                 episode reward: 0.0143,                 loss: 0.1976
agent1:                 episode reward: -0.0143,                 loss: nan
Episode: 35161/101000 (34.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1487s / 5246.3400 s
agent0:                 episode reward: -0.3167,                 loss: 0.1944
agent1:                 episode reward: 0.3167,                 loss: nan
Episode: 35181/101000 (34.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2782s / 5249.6181 s
agent0:                 episode reward: -0.4474,                 loss: 0.1969
agent1:                 episode reward: 0.4474,                 loss: nan
Episode: 35201/101000 (34.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0827s / 5252.7008 s
agent0:                 episode reward: -0.3271,                 loss: 0.1967
agent1:                 episode reward: 0.3271,                 loss: nan
Episode: 35221/101000 (34.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2609s / 5255.9618 s
agent0:                 episode reward: -0.2211,                 loss: 0.1952
agent1:                 episode reward: 0.2211,                 loss: nan
Episode: 35241/101000 (34.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1483s / 5259.1101 s
agent0:                 episode reward: 0.3065,                 loss: 0.1938
agent1:                 episode reward: -0.3065,                 loss: nan
Episode: 35261/101000 (34.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6495s / 5262.7596 s
agent0:                 episode reward: -0.2641,                 loss: 0.1953
agent1:                 episode reward: 0.2641,                 loss: nan
Episode: 35281/101000 (34.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2172s / 5265.9768 s
agent0:                 episode reward: -0.3053,                 loss: 0.1962
agent1:                 episode reward: 0.3053,                 loss: nan
Episode: 35301/101000 (34.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3562s / 5269.3330 s
agent0:                 episode reward: -0.5150,                 loss: 0.1954
agent1:                 episode reward: 0.5150,                 loss: nan
Episode: 35321/101000 (34.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1817s / 5272.5148 s
agent0:                 episode reward: 0.0458,                 loss: 0.1973
agent1:                 episode reward: -0.0458,                 loss: nan
Episode: 35341/101000 (34.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9598s / 5275.4745 s
agent0:                 episode reward: -0.3736,                 loss: 0.1869
agent1:                 episode reward: 0.3736,                 loss: nan
Episode: 35361/101000 (35.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6076s / 5279.0822 s
agent0:                 episode reward: 0.1026,                 loss: 0.1818
agent1:                 episode reward: -0.1026,                 loss: nan
Episode: 35381/101000 (35.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0044s / 5282.0866 s
agent0:                 episode reward: -0.0729,                 loss: 0.1834
agent1:                 episode reward: 0.0729,                 loss: nan
Episode: 35401/101000 (35.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3023s / 5285.3889 s
agent0:                 episode reward: -0.4588,                 loss: 0.1827
agent1:                 episode reward: 0.4588,                 loss: nan
Episode: 35421/101000 (35.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8814s / 5288.2704 s
agent0:                 episode reward: -0.1252,                 loss: 0.1845
agent1:                 episode reward: 0.1252,                 loss: nan
Episode: 35441/101000 (35.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4716s / 5291.7420 s
agent0:                 episode reward: 0.0657,                 loss: 0.1810
agent1:                 episode reward: -0.0657,                 loss: nan
Episode: 35461/101000 (35.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3468s / 5295.0888 s
agent0:                 episode reward: -0.1241,                 loss: 0.1827
agent1:                 episode reward: 0.1241,                 loss: nan
Episode: 35481/101000 (35.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6664s / 5297.7552 s
agent0:                 episode reward: -0.0514,                 loss: 0.1827
agent1:                 episode reward: 0.0514,                 loss: nan
Episode: 35501/101000 (35.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5146s / 5301.2698 s
agent0:                 episode reward: -0.2051,                 loss: 0.1819
agent1:                 episode reward: 0.2051,                 loss: nan
Episode: 35521/101000 (35.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9058s / 5305.1756 s
agent0:                 episode reward: -0.1512,                 loss: 0.1821
agent1:                 episode reward: 0.1512,                 loss: nan
Episode: 35541/101000 (35.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8399s / 5308.0155 s
agent0:                 episode reward: -0.2665,                 loss: 0.1831
agent1:                 episode reward: 0.2665,                 loss: nan
Episode: 35561/101000 (35.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4670s / 5310.4824 s
agent0:                 episode reward: -0.2932,                 loss: 0.1831
agent1:                 episode reward: 0.2932,                 loss: nan
Episode: 35581/101000 (35.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3630s / 5313.8454 s
agent0:                 episode reward: 0.0451,                 loss: 0.1846
agent1:                 episode reward: -0.0451,                 loss: nan
Episode: 35601/101000 (35.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9502s / 5316.7956 s
agent0:                 episode reward: 0.0218,                 loss: 0.1823
agent1:                 episode reward: -0.0218,                 loss: nan
Episode: 35621/101000 (35.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2954s / 5320.0911 s
agent0:                 episode reward: -0.2938,                 loss: 0.1833
agent1:                 episode reward: 0.2938,                 loss: nan
Episode: 35641/101000 (35.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3515s / 5323.4426 s
agent0:                 episode reward: -0.3831,                 loss: 0.1841
agent1:                 episode reward: 0.3831,                 loss: nan
Episode: 35661/101000 (35.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1645s / 5326.6070 s
agent0:                 episode reward: -0.3659,                 loss: 0.1832
agent1:                 episode reward: 0.3659,                 loss: nan
Episode: 35681/101000 (35.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8658s / 5330.4728 s
agent0:                 episode reward: 0.0575,                 loss: 0.1818
agent1:                 episode reward: -0.0575,                 loss: nan
Episode: 35701/101000 (35.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9099s / 5334.3827 s
agent0:                 episode reward: -0.0773,                 loss: 0.1823
agent1:                 episode reward: 0.0773,                 loss: nan
Episode: 35721/101000 (35.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8234s / 5337.2061 s
agent0:                 episode reward: -0.2855,                 loss: 0.1824
agent1:                 episode reward: 0.2855,                 loss: nan
Episode: 35741/101000 (35.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7075s / 5339.9136 s
agent0:                 episode reward: 0.1532,                 loss: 0.1825
agent1:                 episode reward: -0.1532,                 loss: nan
Episode: 35761/101000 (35.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4598s / 5343.3734 s
agent0:                 episode reward: 0.0841,                 loss: 0.1803
agent1:                 episode reward: -0.0841,                 loss: nan
Episode: 35781/101000 (35.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5641s / 5346.9375 s
agent0:                 episode reward: 0.3769,                 loss: 0.1828
agent1:                 episode reward: -0.3769,                 loss: nan
Episode: 35801/101000 (35.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3665s / 5350.3040 s
agent0:                 episode reward: -0.2012,                 loss: 0.1824
agent1:                 episode reward: 0.2012,                 loss: nan
Episode: 35821/101000 (35.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2030s / 5353.5070 s
agent0:                 episode reward: 0.1050,                 loss: 0.1825
agent1:                 episode reward: -0.1050,                 loss: nan
Episode: 35841/101000 (35.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0180s / 5356.5250 s
agent0:                 episode reward: -0.0637,                 loss: 0.1839
agent1:                 episode reward: 0.0637,                 loss: nan
Episode: 35861/101000 (35.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1996s / 5359.7247 s
agent0:                 episode reward: 0.1009,                 loss: 0.1813
agent1:                 episode reward: -0.1009,                 loss: nan
Episode: 35881/101000 (35.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1276s / 5362.8523 s
agent0:                 episode reward: -0.0965,                 loss: 0.1849
agent1:                 episode reward: 0.0965,                 loss: 0.1649
Score delta: 1.8416746288168118, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/35439_0.
Episode: 35901/101000 (35.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8030s / 5365.6553 s
agent0:                 episode reward: -0.3951,                 loss: nan
agent1:                 episode reward: 0.3951,                 loss: 0.1624
Episode: 35921/101000 (35.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0430s / 5368.6984 s
agent0:                 episode reward: -0.4639,                 loss: 0.2155
agent1:                 episode reward: 0.4639,                 loss: 0.1626
Score delta: 1.6285825308027995, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/35486_1.
Episode: 35941/101000 (35.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4097s / 5371.1081 s
agent0:                 episode reward: -0.0548,                 loss: 0.2065
agent1:                 episode reward: 0.0548,                 loss: nan
Episode: 35961/101000 (35.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8690s / 5373.9771 s
agent0:                 episode reward: -0.3150,                 loss: 0.2038
agent1:                 episode reward: 0.3150,                 loss: nan
Episode: 35981/101000 (35.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9652s / 5376.9422 s
agent0:                 episode reward: -0.5466,                 loss: 0.2004
agent1:                 episode reward: 0.5466,                 loss: nan
Episode: 36001/101000 (35.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7911s / 5380.7333 s
agent0:                 episode reward: -0.1084,                 loss: 0.2016
agent1:                 episode reward: 0.1084,                 loss: nan
Episode: 36021/101000 (35.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3585s / 5384.0918 s
agent0:                 episode reward: -0.5161,                 loss: 0.1984
agent1:                 episode reward: 0.5161,                 loss: nan
Episode: 36041/101000 (35.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0418s / 5388.1337 s
agent0:                 episode reward: -0.2352,                 loss: 0.1988
agent1:                 episode reward: 0.2352,                 loss: nan
Episode: 36061/101000 (35.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6006s / 5391.7342 s
agent0:                 episode reward: -0.0114,                 loss: 0.1965
agent1:                 episode reward: 0.0114,                 loss: nan
Episode: 36081/101000 (35.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6243s / 5394.3585 s
agent0:                 episode reward: -0.1395,                 loss: 0.1973
agent1:                 episode reward: 0.1395,                 loss: nan
Episode: 36101/101000 (35.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3539s / 5397.7125 s
agent0:                 episode reward: -0.0791,                 loss: 0.1958
agent1:                 episode reward: 0.0791,                 loss: nan
Episode: 36121/101000 (35.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6816s / 5400.3940 s
agent0:                 episode reward: 0.2114,                 loss: 0.1957
agent1:                 episode reward: -0.2114,                 loss: nan
Episode: 36141/101000 (35.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5181s / 5403.9122 s
agent0:                 episode reward: -0.3922,                 loss: 0.1935
agent1:                 episode reward: 0.3922,                 loss: nan
Episode: 36161/101000 (35.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3362s / 5407.2483 s
agent0:                 episode reward: -0.3586,                 loss: 0.1940
agent1:                 episode reward: 0.3586,                 loss: nan
Episode: 36181/101000 (35.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0774s / 5410.3257 s
agent0:                 episode reward: 0.2016,                 loss: 0.1934
agent1:                 episode reward: -0.2016,                 loss: nan
Episode: 36201/101000 (35.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2802s / 5413.6060 s
agent0:                 episode reward: -0.1869,                 loss: 0.1942
agent1:                 episode reward: 0.1869,                 loss: nan
Episode: 36221/101000 (35.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9272s / 5417.5332 s
agent0:                 episode reward: 0.2493,                 loss: 0.1951
agent1:                 episode reward: -0.2493,                 loss: nan
Episode: 36241/101000 (35.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6855s / 5421.2187 s
agent0:                 episode reward: -0.0428,                 loss: 0.1929
agent1:                 episode reward: 0.0428,                 loss: nan
Episode: 36261/101000 (35.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2285s / 5424.4472 s
agent0:                 episode reward: 0.0450,                 loss: 0.1936
agent1:                 episode reward: -0.0450,                 loss: nan
Episode: 36281/101000 (35.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0644s / 5427.5115 s
agent0:                 episode reward: -0.0707,                 loss: 0.1942
agent1:                 episode reward: 0.0707,                 loss: nan
Episode: 36301/101000 (35.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1146s / 5431.6262 s
agent0:                 episode reward: -0.0298,                 loss: 0.1950
agent1:                 episode reward: 0.0298,                 loss: nan
Episode: 36321/101000 (35.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3238s / 5434.9500 s
agent0:                 episode reward: -0.1968,                 loss: 0.1951
agent1:                 episode reward: 0.1968,                 loss: nan
Episode: 36341/101000 (35.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0140s / 5437.9639 s
agent0:                 episode reward: -0.3065,                 loss: 0.1926
agent1:                 episode reward: 0.3065,                 loss: nan
Episode: 36361/101000 (36.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0525s / 5441.0164 s
agent0:                 episode reward: -0.3758,                 loss: 0.1934
agent1:                 episode reward: 0.3758,                 loss: nan
Episode: 36381/101000 (36.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8601s / 5443.8765 s
agent0:                 episode reward: -0.0226,                 loss: 0.1866
agent1:                 episode reward: 0.0226,                 loss: nan
Episode: 36401/101000 (36.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5681s / 5447.4447 s
agent0:                 episode reward: -0.2634,                 loss: 0.1835
agent1:                 episode reward: 0.2634,                 loss: nan
Episode: 36421/101000 (36.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1887s / 5450.6334 s
agent0:                 episode reward: -0.7413,                 loss: 0.1848
agent1:                 episode reward: 0.7413,                 loss: nan
Episode: 36441/101000 (36.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9349s / 5453.5683 s
agent0:                 episode reward: 0.0312,                 loss: 0.1812
agent1:                 episode reward: -0.0312,                 loss: nan
Episode: 36461/101000 (36.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5246s / 5457.0929 s
agent0:                 episode reward: -0.0155,                 loss: 0.1834
agent1:                 episode reward: 0.0155,                 loss: nan
Episode: 36481/101000 (36.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3519s / 5460.4448 s
agent0:                 episode reward: -0.1627,                 loss: 0.1826
agent1:                 episode reward: 0.1627,                 loss: nan
Episode: 36501/101000 (36.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9404s / 5463.3852 s
agent0:                 episode reward: -0.2230,                 loss: 0.1805
agent1:                 episode reward: 0.2230,                 loss: nan
Episode: 36521/101000 (36.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0551s / 5466.4403 s
agent0:                 episode reward: -0.0919,                 loss: 0.1840
agent1:                 episode reward: 0.0919,                 loss: nan
Episode: 36541/101000 (36.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4717s / 5469.9120 s
agent0:                 episode reward: -0.0442,                 loss: 0.1809
agent1:                 episode reward: 0.0442,                 loss: nan
Episode: 36561/101000 (36.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2970s / 5473.2090 s
agent0:                 episode reward: -0.5672,                 loss: 0.1799
agent1:                 episode reward: 0.5672,                 loss: 0.1772
Score delta: 1.5643553325012014, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/36118_0.
Episode: 36581/101000 (36.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7281s / 5475.9371 s
agent0:                 episode reward: -0.5264,                 loss: nan
agent1:                 episode reward: 0.5264,                 loss: 0.1723
Episode: 36601/101000 (36.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6763s / 5478.6134 s
agent0:                 episode reward: -0.3698,                 loss: nan
agent1:                 episode reward: 0.3698,                 loss: 0.1727
Episode: 36621/101000 (36.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3034s / 5481.9168 s
agent0:                 episode reward: -0.3033,                 loss: nan
agent1:                 episode reward: 0.3033,                 loss: 0.1598
Episode: 36641/101000 (36.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5556s / 5484.4724 s
agent0:                 episode reward: -0.4972,                 loss: 0.1901
agent1:                 episode reward: 0.4972,                 loss: 0.1560
Score delta: 1.5036225088851018, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/36206_1.
Episode: 36661/101000 (36.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0115s / 5487.4838 s
agent0:                 episode reward: -0.1711,                 loss: 0.1887
agent1:                 episode reward: 0.1711,                 loss: nan
Episode: 36681/101000 (36.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8069s / 5491.2907 s
agent0:                 episode reward: -0.3027,                 loss: 0.1889
agent1:                 episode reward: 0.3027,                 loss: nan
Episode: 36701/101000 (36.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9531s / 5494.2438 s
agent0:                 episode reward: -0.3083,                 loss: 0.1872
agent1:                 episode reward: 0.3083,                 loss: nan
Episode: 36721/101000 (36.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3036s / 5497.5474 s
agent0:                 episode reward: -0.4172,                 loss: 0.1896
agent1:                 episode reward: 0.4172,                 loss: nan
Episode: 36741/101000 (36.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5581s / 5501.1055 s
agent0:                 episode reward: -0.2421,                 loss: 0.1877
agent1:                 episode reward: 0.2421,                 loss: nan
Episode: 36761/101000 (36.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2854s / 5504.3909 s
agent0:                 episode reward: 0.1950,                 loss: 0.1881
agent1:                 episode reward: -0.1950,                 loss: nan
Episode: 36781/101000 (36.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9106s / 5507.3015 s
agent0:                 episode reward: -0.4162,                 loss: 0.1886
agent1:                 episode reward: 0.4162,                 loss: nan
Episode: 36801/101000 (36.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3011s / 5510.6026 s
agent0:                 episode reward: -0.5029,                 loss: 0.1920
agent1:                 episode reward: 0.5029,                 loss: nan
Episode: 36821/101000 (36.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2756s / 5513.8782 s
agent0:                 episode reward: -0.2323,                 loss: 0.1921
agent1:                 episode reward: 0.2323,                 loss: nan
Episode: 36841/101000 (36.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9950s / 5516.8731 s
agent0:                 episode reward: -0.0443,                 loss: 0.1921
agent1:                 episode reward: 0.0443,                 loss: nan
Episode: 36861/101000 (36.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1431s / 5520.0162 s
agent0:                 episode reward: -0.3622,                 loss: 0.1923
agent1:                 episode reward: 0.3622,                 loss: nan
Episode: 36881/101000 (36.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1303s / 5523.1465 s
agent0:                 episode reward: -0.1912,                 loss: 0.1918
agent1:                 episode reward: 0.1912,                 loss: nan
Episode: 36901/101000 (36.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6653s / 5525.8118 s
agent0:                 episode reward: -0.5346,                 loss: 0.1927
agent1:                 episode reward: 0.5346,                 loss: nan
Episode: 36921/101000 (36.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5604s / 5529.3722 s
agent0:                 episode reward: 0.1339,                 loss: 0.1905
agent1:                 episode reward: -0.1339,                 loss: nan
Episode: 36941/101000 (36.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6423s / 5533.0145 s
agent0:                 episode reward: -0.3700,                 loss: 0.1911
agent1:                 episode reward: 0.3700,                 loss: nan
Episode: 36961/101000 (36.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0201s / 5536.0346 s
agent0:                 episode reward: -0.0385,                 loss: 0.1915
agent1:                 episode reward: 0.0385,                 loss: nan
Episode: 36981/101000 (36.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0226s / 5539.0572 s
agent0:                 episode reward: -0.1984,                 loss: 0.1918
agent1:                 episode reward: 0.1984,                 loss: nan
Episode: 37001/101000 (36.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3942s / 5542.4514 s
agent0:                 episode reward: -0.0824,                 loss: 0.1924
agent1:                 episode reward: 0.0824,                 loss: nan
Episode: 37021/101000 (36.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2944s / 5545.7458 s
agent0:                 episode reward: 0.2775,                 loss: 0.1929
agent1:                 episode reward: -0.2775,                 loss: nan
Episode: 37041/101000 (36.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8659s / 5548.6117 s
agent0:                 episode reward: -0.1419,                 loss: 0.1902
agent1:                 episode reward: 0.1419,                 loss: nan
Episode: 37061/101000 (36.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0365s / 5551.6482 s
agent0:                 episode reward: 0.1208,                 loss: 0.1906
agent1:                 episode reward: -0.1208,                 loss: nan
Episode: 37081/101000 (36.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5290s / 5555.1772 s
agent0:                 episode reward: -0.4914,                 loss: 0.1906
agent1:                 episode reward: 0.4914,                 loss: nan
Episode: 37101/101000 (36.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7778s / 5558.9549 s
agent0:                 episode reward: -0.3681,                 loss: 0.1916
agent1:                 episode reward: 0.3681,                 loss: nan
Episode: 37121/101000 (36.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5321s / 5562.4870 s
agent0:                 episode reward: -0.5294,                 loss: 0.1932
agent1:                 episode reward: 0.5294,                 loss: nan
Episode: 37141/101000 (36.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1559s / 5565.6429 s
agent0:                 episode reward: 0.2621,                 loss: 0.1801
agent1:                 episode reward: -0.2621,                 loss: nan
Episode: 37161/101000 (36.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5431s / 5569.1861 s
agent0:                 episode reward: 0.4100,                 loss: 0.1770
agent1:                 episode reward: -0.4100,                 loss: nan
Episode: 37181/101000 (36.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4954s / 5571.6815 s
agent0:                 episode reward: -0.1639,                 loss: 0.1758
agent1:                 episode reward: 0.1639,                 loss: nan
Episode: 37201/101000 (36.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0407s / 5574.7222 s
agent0:                 episode reward: 0.1872,                 loss: 0.1778
agent1:                 episode reward: -0.1872,                 loss: nan
Episode: 37221/101000 (36.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9527s / 5577.6749 s
agent0:                 episode reward: -0.3441,                 loss: 0.1755
agent1:                 episode reward: 0.3441,                 loss: nan
Episode: 37241/101000 (36.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0553s / 5580.7302 s
agent0:                 episode reward: -0.3025,                 loss: 0.1769
agent1:                 episode reward: 0.3025,                 loss: nan
Episode: 37261/101000 (36.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3518s / 5584.0820 s
agent0:                 episode reward: -0.2460,                 loss: 0.1783
agent1:                 episode reward: 0.2460,                 loss: nan
Episode: 37281/101000 (36.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8328s / 5586.9148 s
agent0:                 episode reward: -0.0106,                 loss: 0.1779
agent1:                 episode reward: 0.0106,                 loss: nan
Episode: 37301/101000 (36.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5983s / 5590.5131 s
agent0:                 episode reward: -0.2229,                 loss: 0.1800
agent1:                 episode reward: 0.2229,                 loss: nan
Episode: 37321/101000 (36.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7667s / 5594.2798 s
agent0:                 episode reward: -0.0452,                 loss: 0.1784
agent1:                 episode reward: 0.0452,                 loss: 0.1703
Score delta: 1.7194829707480674, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/36894_0.
Episode: 37341/101000 (36.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3301s / 5597.6099 s
agent0:                 episode reward: -0.2279,                 loss: nan
agent1:                 episode reward: 0.2279,                 loss: 0.1636
Episode: 37361/101000 (36.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7671s / 5600.3769 s
agent0:                 episode reward: -0.3634,                 loss: nan
agent1:                 episode reward: 0.3634,                 loss: 0.1621
Episode: 37381/101000 (37.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0167s / 5603.3937 s
agent0:                 episode reward: -0.0936,                 loss: nan
agent1:                 episode reward: 0.0936,                 loss: 0.1618
Episode: 37401/101000 (37.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5933s / 5605.9870 s
agent0:                 episode reward: -0.5100,                 loss: 0.1757
agent1:                 episode reward: 0.5100,                 loss: 0.1610
Score delta: 1.554366143826098, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/36970_1.
Episode: 37421/101000 (37.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4325s / 5608.4195 s
agent0:                 episode reward: -0.2499,                 loss: 0.1737
agent1:                 episode reward: 0.2499,                 loss: nan
Episode: 37441/101000 (37.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4355s / 5611.8549 s
agent0:                 episode reward: 0.2302,                 loss: 0.1733
agent1:                 episode reward: -0.2302,                 loss: nan
Episode: 37461/101000 (37.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3830s / 5615.2379 s
agent0:                 episode reward: 0.3326,                 loss: 0.1724
agent1:                 episode reward: -0.3326,                 loss: nan
Episode: 37481/101000 (37.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1923s / 5618.4302 s
agent0:                 episode reward: 0.1261,                 loss: 0.1728
agent1:                 episode reward: -0.1261,                 loss: nan
Episode: 37501/101000 (37.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7683s / 5621.1985 s
agent0:                 episode reward: -0.5209,                 loss: 0.1738
agent1:                 episode reward: 0.5209,                 loss: nan
Episode: 37521/101000 (37.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9994s / 5624.1980 s
agent0:                 episode reward: -0.1942,                 loss: 0.1730
agent1:                 episode reward: 0.1942,                 loss: nan
Episode: 37541/101000 (37.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9008s / 5627.0988 s
agent0:                 episode reward: 0.0623,                 loss: 0.1778
agent1:                 episode reward: -0.0623,                 loss: nan
Episode: 37561/101000 (37.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1299s / 5630.2287 s
agent0:                 episode reward: -0.5295,                 loss: 0.1835
agent1:                 episode reward: 0.5295,                 loss: nan
Episode: 37581/101000 (37.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9730s / 5634.2018 s
agent0:                 episode reward: -0.4737,                 loss: 0.1841
agent1:                 episode reward: 0.4737,                 loss: nan
Episode: 37601/101000 (37.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0107s / 5637.2125 s
agent0:                 episode reward: -0.0259,                 loss: 0.1836
agent1:                 episode reward: 0.0259,                 loss: 0.1623
Score delta: 1.5278166013214831, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/37169_0.
Episode: 37621/101000 (37.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5891s / 5639.8016 s
agent0:                 episode reward: -0.1447,                 loss: nan
agent1:                 episode reward: 0.1447,                 loss: 0.1637
Episode: 37641/101000 (37.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6346s / 5643.4362 s
agent0:                 episode reward: -0.6968,                 loss: nan
agent1:                 episode reward: 0.6968,                 loss: 0.1609
Episode: 37661/101000 (37.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9078s / 5646.3440 s
agent0:                 episode reward: -0.1975,                 loss: 0.1896
agent1:                 episode reward: 0.1975,                 loss: 0.1633
Score delta: 2.0779514605648783, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/37219_1.
Episode: 37681/101000 (37.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2002s / 5649.5442 s
agent0:                 episode reward: -0.2305,                 loss: 0.1882
agent1:                 episode reward: 0.2305,                 loss: nan
Episode: 37701/101000 (37.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0381s / 5652.5823 s
agent0:                 episode reward: -0.0679,                 loss: 0.1874
agent1:                 episode reward: 0.0679,                 loss: nan
Episode: 37721/101000 (37.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8204s / 5655.4027 s
agent0:                 episode reward: -0.2290,                 loss: 0.1879
agent1:                 episode reward: 0.2290,                 loss: nan
Episode: 37741/101000 (37.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7449s / 5659.1476 s
agent0:                 episode reward: -0.3037,                 loss: 0.1856
agent1:                 episode reward: 0.3037,                 loss: nan
Episode: 37761/101000 (37.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4458s / 5662.5934 s
agent0:                 episode reward: -0.0869,                 loss: 0.1870
agent1:                 episode reward: 0.0869,                 loss: nan
Episode: 37781/101000 (37.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2150s / 5665.8084 s
agent0:                 episode reward: -0.3238,                 loss: 0.1864
agent1:                 episode reward: 0.3238,                 loss: nan
Episode: 37801/101000 (37.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8839s / 5668.6924 s
agent0:                 episode reward: -0.3500,                 loss: 0.1868
agent1:                 episode reward: 0.3500,                 loss: nan
Episode: 37821/101000 (37.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9974s / 5671.6898 s
agent0:                 episode reward: 0.2718,                 loss: 0.1864
agent1:                 episode reward: -0.2718,                 loss: nan
Episode: 37841/101000 (37.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2129s / 5674.9027 s
agent0:                 episode reward: 0.1896,                 loss: 0.1852
agent1:                 episode reward: -0.1896,                 loss: 0.1626
Score delta: 1.56013182635911, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/37401_0.
Episode: 37861/101000 (37.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0649s / 5678.9677 s
agent0:                 episode reward: -0.4547,                 loss: nan
agent1:                 episode reward: 0.4547,                 loss: 0.1627
Episode: 37881/101000 (37.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2451s / 5682.2128 s
agent0:                 episode reward: -0.3334,                 loss: nan
agent1:                 episode reward: 0.3334,                 loss: 0.1623
Episode: 37901/101000 (37.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4121s / 5685.6248 s
agent0:                 episode reward: -0.4007,                 loss: nan
agent1:                 episode reward: 0.4007,                 loss: 0.1610
Episode: 37921/101000 (37.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9502s / 5688.5751 s
agent0:                 episode reward: -0.3688,                 loss: 0.1834
agent1:                 episode reward: 0.3688,                 loss: 0.1605
Score delta: 1.5827192435719617, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/37494_1.
Episode: 37941/101000 (37.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0604s / 5691.6355 s
agent0:                 episode reward: -0.1436,                 loss: 0.1817
agent1:                 episode reward: 0.1436,                 loss: nan
Episode: 37961/101000 (37.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9133s / 5694.5488 s
agent0:                 episode reward: -0.4649,                 loss: 0.1827
agent1:                 episode reward: 0.4649,                 loss: nan
Episode: 37981/101000 (37.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7285s / 5698.2773 s
agent0:                 episode reward: -0.2295,                 loss: 0.1803
agent1:                 episode reward: 0.2295,                 loss: nan
Episode: 38001/101000 (37.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3665s / 5701.6438 s
agent0:                 episode reward: 0.1782,                 loss: 0.1820
agent1:                 episode reward: -0.1782,                 loss: nan
Episode: 38021/101000 (37.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0417s / 5705.6856 s
agent0:                 episode reward: -0.1148,                 loss: 0.1829
agent1:                 episode reward: 0.1148,                 loss: nan
Episode: 38041/101000 (37.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6215s / 5709.3071 s
agent0:                 episode reward: -0.6200,                 loss: 0.1818
agent1:                 episode reward: 0.6200,                 loss: nan
Episode: 38061/101000 (37.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1829s / 5712.4899 s
agent0:                 episode reward: -0.3453,                 loss: 0.1838
agent1:                 episode reward: 0.3453,                 loss: nan
Episode: 38081/101000 (37.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0485s / 5716.5384 s
agent0:                 episode reward: -0.1773,                 loss: 0.1829
agent1:                 episode reward: 0.1773,                 loss: nan
Episode: 38101/101000 (37.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2811s / 5719.8194 s
agent0:                 episode reward: -0.4005,                 loss: 0.1833
agent1:                 episode reward: 0.4005,                 loss: nan
Episode: 38121/101000 (37.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3311s / 5723.1505 s
agent0:                 episode reward: -0.1381,                 loss: 0.1844
agent1:                 episode reward: 0.1381,                 loss: nan
Episode: 38141/101000 (37.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4624s / 5726.6129 s
agent0:                 episode reward: -0.1407,                 loss: 0.1844
agent1:                 episode reward: 0.1407,                 loss: nan
Episode: 38161/101000 (37.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3642s / 5729.9771 s
agent0:                 episode reward: -0.0061,                 loss: 0.1822
agent1:                 episode reward: 0.0061,                 loss: nan
Episode: 38181/101000 (37.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9455s / 5732.9226 s
agent0:                 episode reward: -0.3940,                 loss: 0.1820
agent1:                 episode reward: 0.3940,                 loss: nan
Episode: 38201/101000 (37.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4404s / 5736.3629 s
agent0:                 episode reward: -0.6394,                 loss: 0.1831
agent1:                 episode reward: 0.6394,                 loss: nan
Episode: 38221/101000 (37.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5957s / 5738.9586 s
agent0:                 episode reward: -0.0767,                 loss: 0.1835
agent1:                 episode reward: 0.0767,                 loss: nan
Episode: 38241/101000 (37.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9338s / 5741.8924 s
agent0:                 episode reward: -0.0785,                 loss: 0.1828
agent1:                 episode reward: 0.0785,                 loss: nan
Episode: 38261/101000 (37.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6539s / 5745.5463 s
agent0:                 episode reward: -0.1253,                 loss: 0.1828
agent1:                 episode reward: 0.1253,                 loss: nan
Episode: 38281/101000 (37.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4379s / 5748.9843 s
agent0:                 episode reward: -0.2566,                 loss: 0.1826
agent1:                 episode reward: 0.2566,                 loss: nan
Episode: 38301/101000 (37.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4414s / 5753.4257 s
agent0:                 episode reward: 0.1688,                 loss: 0.1825
agent1:                 episode reward: -0.1688,                 loss: nan
Episode: 38321/101000 (37.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1705s / 5756.5962 s
agent0:                 episode reward: -0.1839,                 loss: 0.1815
agent1:                 episode reward: 0.1839,                 loss: nan
Episode: 38341/101000 (37.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0542s / 5759.6504 s
agent0:                 episode reward: 0.0126,                 loss: 0.1841
agent1:                 episode reward: -0.0126,                 loss: nan
Episode: 38361/101000 (37.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1866s / 5762.8371 s
agent0:                 episode reward: -0.0649,                 loss: 0.1801
agent1:                 episode reward: 0.0649,                 loss: nan
Episode: 38381/101000 (38.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3433s / 5766.1804 s
agent0:                 episode reward: -0.0295,                 loss: 0.1804
agent1:                 episode reward: 0.0295,                 loss: nan
Episode: 38401/101000 (38.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4855s / 5768.6659 s
agent0:                 episode reward: -0.1431,                 loss: 0.1809
agent1:                 episode reward: 0.1431,                 loss: nan
Episode: 38421/101000 (38.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1830s / 5771.8489 s
agent0:                 episode reward: -0.2152,                 loss: 0.1809
agent1:                 episode reward: 0.2152,                 loss: nan
Episode: 38441/101000 (38.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0032s / 5775.8521 s
agent0:                 episode reward: -0.0611,                 loss: 0.1811
agent1:                 episode reward: 0.0611,                 loss: nan
Episode: 38461/101000 (38.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8377s / 5778.6899 s
agent0:                 episode reward: 0.0148,                 loss: 0.1811
agent1:                 episode reward: -0.0148,                 loss: 0.1614
Score delta: 1.618276998283044, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/38022_0.
Episode: 38481/101000 (38.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4689s / 5782.1587 s
agent0:                 episode reward: -0.4007,                 loss: nan
agent1:                 episode reward: 0.4007,                 loss: 0.1626
Episode: 38501/101000 (38.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7784s / 5784.9371 s
agent0:                 episode reward: -0.1979,                 loss: nan
agent1:                 episode reward: 0.1979,                 loss: 0.1628
Episode: 38521/101000 (38.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1975s / 5789.1346 s
agent0:                 episode reward: -0.6342,                 loss: 0.1827
agent1:                 episode reward: 0.6342,                 loss: 0.1654
Score delta: 1.5632987341340725, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/38077_1.
Episode: 38541/101000 (38.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2674s / 5792.4020 s
agent0:                 episode reward: -0.2630,                 loss: 0.1810
agent1:                 episode reward: 0.2630,                 loss: nan
Episode: 38561/101000 (38.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5762s / 5795.9782 s
agent0:                 episode reward: -0.1348,                 loss: 0.1817
agent1:                 episode reward: 0.1348,                 loss: nan
Episode: 38581/101000 (38.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0534s / 5799.0316 s
agent0:                 episode reward: -0.4316,                 loss: 0.1803
agent1:                 episode reward: 0.4316,                 loss: nan
Episode: 38601/101000 (38.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0034s / 5802.0350 s
agent0:                 episode reward: 0.3528,                 loss: 0.1814
agent1:                 episode reward: -0.3528,                 loss: 0.1645
Score delta: 1.6545446891925626, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/38162_0.
Episode: 38621/101000 (38.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9863s / 5805.0213 s
agent0:                 episode reward: -0.2075,                 loss: nan
agent1:                 episode reward: 0.2075,                 loss: 0.1592
Episode: 38641/101000 (38.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4520s / 5808.4733 s
agent0:                 episode reward: -0.3089,                 loss: nan
agent1:                 episode reward: 0.3089,                 loss: 0.1541
Episode: 38661/101000 (38.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4593s / 5810.9326 s
agent0:                 episode reward: 0.0168,                 loss: 0.1895
agent1:                 episode reward: -0.0168,                 loss: 0.1541
Score delta: 1.7200200456870085, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/38216_1.
Episode: 38681/101000 (38.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1520s / 5814.0846 s
agent0:                 episode reward: -0.2778,                 loss: 0.1876
agent1:                 episode reward: 0.2778,                 loss: nan
Episode: 38701/101000 (38.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4582s / 5817.5428 s
agent0:                 episode reward: -0.0492,                 loss: 0.1904
agent1:                 episode reward: 0.0492,                 loss: nan
Episode: 38721/101000 (38.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2255s / 5820.7683 s
agent0:                 episode reward: -0.1906,                 loss: 0.1884
agent1:                 episode reward: 0.1906,                 loss: nan
Episode: 38741/101000 (38.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2062s / 5824.9745 s
agent0:                 episode reward: -0.4485,                 loss: 0.1879
agent1:                 episode reward: 0.4485,                 loss: nan
Episode: 38761/101000 (38.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0849s / 5829.0594 s
agent0:                 episode reward: -0.2144,                 loss: 0.1856
agent1:                 episode reward: 0.2144,                 loss: nan
Episode: 38781/101000 (38.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2788s / 5832.3382 s
agent0:                 episode reward: 0.0269,                 loss: 0.1886
agent1:                 episode reward: -0.0269,                 loss: nan
Episode: 38801/101000 (38.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5372s / 5835.8754 s
agent0:                 episode reward: 0.1454,                 loss: 0.1897
agent1:                 episode reward: -0.1454,                 loss: nan
Episode: 38821/101000 (38.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9780s / 5838.8534 s
agent0:                 episode reward: -0.0302,                 loss: 0.1889
agent1:                 episode reward: 0.0302,                 loss: nan
Episode: 38841/101000 (38.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4667s / 5842.3200 s
agent0:                 episode reward: -0.2890,                 loss: 0.1891
agent1:                 episode reward: 0.2890,                 loss: nan
Episode: 38861/101000 (38.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6879s / 5847.0079 s
agent0:                 episode reward: 0.0569,                 loss: 0.1905
agent1:                 episode reward: -0.0569,                 loss: nan
Episode: 38881/101000 (38.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1345s / 5851.1425 s
agent0:                 episode reward: -0.1274,                 loss: 0.1888
agent1:                 episode reward: 0.1274,                 loss: nan
Episode: 38901/101000 (38.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5141s / 5854.6566 s
agent0:                 episode reward: -0.4050,                 loss: 0.1890
agent1:                 episode reward: 0.4050,                 loss: nan
Episode: 38921/101000 (38.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6205s / 5857.2770 s
agent0:                 episode reward: -0.3412,                 loss: 0.1889
agent1:                 episode reward: 0.3412,                 loss: nan
Episode: 38941/101000 (38.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4331s / 5860.7102 s
agent0:                 episode reward: -0.2974,                 loss: 0.1900
agent1:                 episode reward: 0.2974,                 loss: nan
Episode: 38961/101000 (38.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2208s / 5863.9310 s
agent0:                 episode reward: -0.0225,                 loss: 0.1889
agent1:                 episode reward: 0.0225,                 loss: nan
Episode: 38981/101000 (38.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3005s / 5867.2316 s
agent0:                 episode reward: 0.1203,                 loss: 0.1908
agent1:                 episode reward: -0.1203,                 loss: nan
Episode: 39001/101000 (38.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8769s / 5870.1084 s
agent0:                 episode reward: 0.0586,                 loss: 0.1900
agent1:                 episode reward: -0.0586,                 loss: 0.1631
Score delta: 1.55579187262015, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/38565_0.
Episode: 39021/101000 (38.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0010s / 5873.1094 s
agent0:                 episode reward: -0.1003,                 loss: nan
agent1:                 episode reward: 0.1003,                 loss: 0.1603
Episode: 39041/101000 (38.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5129s / 5877.6223 s
agent0:                 episode reward: -0.4740,                 loss: nan
agent1:                 episode reward: 0.4740,                 loss: 0.1562
Episode: 39061/101000 (38.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8630s / 5880.4854 s
agent0:                 episode reward: -0.0826,                 loss: nan
agent1:                 episode reward: 0.0826,                 loss: 0.1559
Episode: 39081/101000 (38.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8624s / 5883.3478 s
agent0:                 episode reward: -0.2403,                 loss: nan
agent1:                 episode reward: 0.2403,                 loss: 0.1529
Episode: 39101/101000 (38.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6417s / 5886.9894 s
agent0:                 episode reward: -0.2245,                 loss: 0.1906
agent1:                 episode reward: 0.2245,                 loss: 0.1532
Score delta: 1.5379619229546717, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/38661_1.
Episode: 39121/101000 (38.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4797s / 5890.4692 s
agent0:                 episode reward: -0.3659,                 loss: 0.1885
agent1:                 episode reward: 0.3659,                 loss: nan
Episode: 39141/101000 (38.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7350s / 5894.2042 s
agent0:                 episode reward: -0.2560,                 loss: 0.1889
agent1:                 episode reward: 0.2560,                 loss: nan
Episode: 39161/101000 (38.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7390s / 5896.9432 s
agent0:                 episode reward: 0.2372,                 loss: 0.1883
agent1:                 episode reward: -0.2372,                 loss: 0.1560
Score delta: 1.972319692613074, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/38727_0.
Episode: 39181/101000 (38.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7162s / 5899.6594 s
agent0:                 episode reward: -0.2596,                 loss: nan
agent1:                 episode reward: 0.2596,                 loss: 0.1550
Episode: 39201/101000 (38.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6222s / 5902.2816 s
agent0:                 episode reward: -0.1081,                 loss: nan
agent1:                 episode reward: 0.1081,                 loss: 0.1534
Episode: 39221/101000 (38.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1863s / 5905.4679 s
agent0:                 episode reward: -0.0497,                 loss: nan
agent1:                 episode reward: 0.0497,                 loss: 0.1541
Episode: 39241/101000 (38.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8675s / 5908.3354 s
agent0:                 episode reward: -0.3272,                 loss: nan
agent1:                 episode reward: 0.3272,                 loss: 0.1532
Episode: 39261/101000 (38.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3414s / 5911.6768 s
agent0:                 episode reward: -0.2212,                 loss: nan
agent1:                 episode reward: 0.2212,                 loss: 0.1522
Episode: 39281/101000 (38.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7614s / 5914.4382 s
agent0:                 episode reward: -0.1194,                 loss: nan
agent1:                 episode reward: 0.1194,                 loss: 0.1527
Episode: 39301/101000 (38.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8162s / 5917.2543 s
agent0:                 episode reward: -0.1142,                 loss: nan
agent1:                 episode reward: 0.1142,                 loss: 0.1529
Episode: 39321/101000 (38.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5317s / 5920.7861 s
agent0:                 episode reward: -0.0039,                 loss: nan
agent1:                 episode reward: 0.0039,                 loss: 0.1517
Episode: 39341/101000 (38.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4633s / 5924.2493 s
agent0:                 episode reward: -0.5474,                 loss: 0.2012
agent1:                 episode reward: 0.5474,                 loss: 0.1535
Score delta: 1.7221101019439202, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/38913_1.
Episode: 39361/101000 (38.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5316s / 5927.7810 s
agent0:                 episode reward: -0.6731,                 loss: 0.1887
agent1:                 episode reward: 0.6731,                 loss: nan
Episode: 39381/101000 (38.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1695s / 5930.9505 s
agent0:                 episode reward: 0.0086,                 loss: 0.1874
agent1:                 episode reward: -0.0086,                 loss: nan
Episode: 39401/101000 (39.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4479s / 5934.3984 s
agent0:                 episode reward: -0.1811,                 loss: 0.1857
agent1:                 episode reward: 0.1811,                 loss: nan
Episode: 39421/101000 (39.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0393s / 5938.4377 s
agent0:                 episode reward: -0.1255,                 loss: 0.1878
agent1:                 episode reward: 0.1255,                 loss: nan
Episode: 39441/101000 (39.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5991s / 5942.0368 s
agent0:                 episode reward: -0.4694,                 loss: 0.1843
agent1:                 episode reward: 0.4694,                 loss: nan
Episode: 39461/101000 (39.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6592s / 5945.6960 s
agent0:                 episode reward: -0.0003,                 loss: 0.1872
agent1:                 episode reward: 0.0003,                 loss: nan
Episode: 39481/101000 (39.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3280s / 5949.0241 s
agent0:                 episode reward: -0.0920,                 loss: 0.1855
agent1:                 episode reward: 0.0920,                 loss: nan
Episode: 39501/101000 (39.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7174s / 5951.7415 s
agent0:                 episode reward: -0.5171,                 loss: 0.1852
agent1:                 episode reward: 0.5171,                 loss: nan
Episode: 39521/101000 (39.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6396s / 5954.3811 s
agent0:                 episode reward: -0.3329,                 loss: 0.1825
agent1:                 episode reward: 0.3329,                 loss: nan
Episode: 39541/101000 (39.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3836s / 5957.7647 s
agent0:                 episode reward: 0.1652,                 loss: 0.1844
agent1:                 episode reward: -0.1652,                 loss: nan
Episode: 39561/101000 (39.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0852s / 5961.8499 s
agent0:                 episode reward: -0.2327,                 loss: 0.1836
agent1:                 episode reward: 0.2327,                 loss: nan
Episode: 39581/101000 (39.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6401s / 5965.4900 s
agent0:                 episode reward: -0.2407,                 loss: 0.1839
agent1:                 episode reward: 0.2407,                 loss: nan
Episode: 39601/101000 (39.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1364s / 5969.6264 s
agent0:                 episode reward: -0.1399,                 loss: 0.1848
agent1:                 episode reward: 0.1399,                 loss: nan
Episode: 39621/101000 (39.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3668s / 5972.9931 s
agent0:                 episode reward: -0.5980,                 loss: 0.1857
agent1:                 episode reward: 0.5980,                 loss: nan
Episode: 39641/101000 (39.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6625s / 5976.6556 s
agent0:                 episode reward: 0.0715,                 loss: 0.1842
agent1:                 episode reward: -0.0715,                 loss: nan
Episode: 39661/101000 (39.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7345s / 5980.3901 s
agent0:                 episode reward: -0.1038,                 loss: 0.1846
agent1:                 episode reward: 0.1038,                 loss: nan
Episode: 39681/101000 (39.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6617s / 5984.0519 s
agent0:                 episode reward: 0.1198,                 loss: 0.1840
agent1:                 episode reward: -0.1198,                 loss: nan
Episode: 39701/101000 (39.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4473s / 5987.4992 s
agent0:                 episode reward: -0.0473,                 loss: 0.1841
agent1:                 episode reward: 0.0473,                 loss: nan
Episode: 39721/101000 (39.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2650s / 5990.7641 s
agent0:                 episode reward: -0.2171,                 loss: 0.1853
agent1:                 episode reward: 0.2171,                 loss: nan
Episode: 39741/101000 (39.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4090s / 5994.1731 s
agent0:                 episode reward: 0.3034,                 loss: 0.1840
agent1:                 episode reward: -0.3034,                 loss: nan
Episode: 39761/101000 (39.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3195s / 5997.4926 s
agent0:                 episode reward: -0.2877,                 loss: 0.1786
agent1:                 episode reward: 0.2877,                 loss: nan
Episode: 39781/101000 (39.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0549s / 6000.5475 s
agent0:                 episode reward: -0.2286,                 loss: 0.1782
agent1:                 episode reward: 0.2286,                 loss: nan
Episode: 39801/101000 (39.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6737s / 6004.2211 s
agent0:                 episode reward: -0.4162,                 loss: 0.1778
agent1:                 episode reward: 0.4162,                 loss: nan
Episode: 39821/101000 (39.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8086s / 6008.0298 s
agent0:                 episode reward: -0.3246,                 loss: 0.1803
agent1:                 episode reward: 0.3246,                 loss: nan
Episode: 39841/101000 (39.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7786s / 6011.8084 s
agent0:                 episode reward: -0.3354,                 loss: 0.1786
agent1:                 episode reward: 0.3354,                 loss: nan
Episode: 39861/101000 (39.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1362s / 6015.9447 s
agent0:                 episode reward: -0.1483,                 loss: 0.1793
agent1:                 episode reward: 0.1483,                 loss: nan
Episode: 39881/101000 (39.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7398s / 6019.6844 s
agent0:                 episode reward: -0.5126,                 loss: 0.1796
agent1:                 episode reward: 0.5126,                 loss: nan
Episode: 39901/101000 (39.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3164s / 6023.0008 s
agent0:                 episode reward: -0.1568,                 loss: 0.1791
agent1:                 episode reward: 0.1568,                 loss: nan
Episode: 39921/101000 (39.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3508s / 6025.3516 s
agent0:                 episode reward: 0.2516,                 loss: 0.1800
agent1:                 episode reward: -0.2516,                 loss: nan
Episode: 39941/101000 (39.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4766s / 6028.8282 s
agent0:                 episode reward: -0.2735,                 loss: 0.1779
agent1:                 episode reward: 0.2735,                 loss: nan
Episode: 39961/101000 (39.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2775s / 6032.1056 s
agent0:                 episode reward: -0.3074,                 loss: 0.1780
agent1:                 episode reward: 0.3074,                 loss: nan
Episode: 39981/101000 (39.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1314s / 6035.2370 s
agent0:                 episode reward: 0.1793,                 loss: 0.1794
agent1:                 episode reward: -0.1793,                 loss: nan
Episode: 40001/101000 (39.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8309s / 6038.0679 s
agent0:                 episode reward: -0.0333,                 loss: 0.1784
agent1:                 episode reward: 0.0333,                 loss: nan
Episode: 40021/101000 (39.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8346s / 6041.9025 s
agent0:                 episode reward: -0.0950,                 loss: 0.1788
agent1:                 episode reward: 0.0950,                 loss: nan
Episode: 40041/101000 (39.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4332s / 6044.3357 s
agent0:                 episode reward: -0.0035,                 loss: 0.1787
agent1:                 episode reward: 0.0035,                 loss: nan
Episode: 40061/101000 (39.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7052s / 6048.0409 s
agent0:                 episode reward: -0.1115,                 loss: 0.1806
agent1:                 episode reward: 0.1115,                 loss: nan
Episode: 40081/101000 (39.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0771s / 6051.1180 s
agent0:                 episode reward: 0.1871,                 loss: 0.1782
agent1:                 episode reward: -0.1871,                 loss: nan
Episode: 40101/101000 (39.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3152s / 6054.4332 s
agent0:                 episode reward: 0.4193,                 loss: 0.1775
agent1:                 episode reward: -0.4193,                 loss: 0.1625
Score delta: 1.5749984091664895, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/39672_0.
Episode: 40121/101000 (39.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0969s / 6057.5301 s
agent0:                 episode reward: -0.8092,                 loss: nan
agent1:                 episode reward: 0.8092,                 loss: 0.1610
Episode: 40141/101000 (39.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5829s / 6061.1130 s
agent0:                 episode reward: -0.5345,                 loss: nan
agent1:                 episode reward: 0.5345,                 loss: 0.1520
Episode: 40161/101000 (39.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0548s / 6064.1678 s
agent0:                 episode reward: -0.4353,                 loss: nan
agent1:                 episode reward: 0.4353,                 loss: 0.1495
Episode: 40181/101000 (39.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5066s / 6067.6744 s
agent0:                 episode reward: -0.3485,                 loss: 0.1754
agent1:                 episode reward: 0.3485,                 loss: 0.1448
Score delta: 1.7399836293300428, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/39736_1.
Episode: 40201/101000 (39.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6153s / 6071.2897 s
agent0:                 episode reward: -0.3732,                 loss: 0.1767
agent1:                 episode reward: 0.3732,                 loss: nan
Episode: 40221/101000 (39.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1867s / 6074.4764 s
agent0:                 episode reward: 0.1304,                 loss: 0.1750
agent1:                 episode reward: -0.1304,                 loss: nan
Episode: 40241/101000 (39.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2866s / 6077.7631 s
agent0:                 episode reward: -0.2030,                 loss: 0.1761
agent1:                 episode reward: 0.2030,                 loss: nan
Episode: 40261/101000 (39.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8454s / 6080.6085 s
agent0:                 episode reward: 0.0104,                 loss: 0.1756
agent1:                 episode reward: -0.0104,                 loss: nan
Episode: 40281/101000 (39.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6868s / 6084.2953 s
agent0:                 episode reward: -0.7389,                 loss: 0.1780
agent1:                 episode reward: 0.7389,                 loss: nan
Episode: 40301/101000 (39.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7504s / 6088.0457 s
agent0:                 episode reward: -0.2473,                 loss: 0.1772
agent1:                 episode reward: 0.2473,                 loss: nan
Episode: 40321/101000 (39.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2361s / 6091.2818 s
agent0:                 episode reward: 0.1154,                 loss: 0.1771
agent1:                 episode reward: -0.1154,                 loss: nan
Episode: 40341/101000 (39.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2371s / 6094.5189 s
agent0:                 episode reward: -0.2796,                 loss: 0.1774
agent1:                 episode reward: 0.2796,                 loss: nan
Episode: 40361/101000 (39.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0211s / 6097.5400 s
agent0:                 episode reward: -0.2921,                 loss: 0.1769
agent1:                 episode reward: 0.2921,                 loss: nan
Episode: 40381/101000 (39.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0515s / 6100.5915 s
agent0:                 episode reward: 0.0269,                 loss: 0.1748
agent1:                 episode reward: -0.0269,                 loss: nan
Episode: 40401/101000 (40.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0116s / 6104.6030 s
agent0:                 episode reward: -0.0215,                 loss: 0.1773
agent1:                 episode reward: 0.0215,                 loss: nan
Episode: 40421/101000 (40.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6076s / 6108.2106 s
agent0:                 episode reward: -0.2363,                 loss: 0.1761
agent1:                 episode reward: 0.2363,                 loss: nan
Episode: 40441/101000 (40.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7619s / 6110.9726 s
agent0:                 episode reward: -0.1858,                 loss: 0.1772
agent1:                 episode reward: 0.1858,                 loss: nan
Episode: 40461/101000 (40.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3002s / 6114.2727 s
agent0:                 episode reward: 0.0166,                 loss: 0.1763
agent1:                 episode reward: -0.0166,                 loss: nan
Episode: 40481/101000 (40.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2332s / 6117.5059 s
agent0:                 episode reward: -0.2554,                 loss: 0.1808
agent1:                 episode reward: 0.2554,                 loss: nan
Episode: 40501/101000 (40.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9406s / 6121.4466 s
agent0:                 episode reward: -0.2163,                 loss: 0.1805
agent1:                 episode reward: 0.2163,                 loss: nan
Episode: 40521/101000 (40.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8827s / 6125.3292 s
agent0:                 episode reward: -0.4212,                 loss: 0.1797
agent1:                 episode reward: 0.4212,                 loss: nan
Episode: 40541/101000 (40.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7178s / 6128.0471 s
agent0:                 episode reward: -0.3891,                 loss: 0.1793
agent1:                 episode reward: 0.3891,                 loss: nan
Episode: 40561/101000 (40.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9633s / 6131.0104 s
agent0:                 episode reward: -0.0039,                 loss: 0.1791
agent1:                 episode reward: 0.0039,                 loss: nan
Episode: 40581/101000 (40.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8099s / 6134.8203 s
agent0:                 episode reward: -0.1444,                 loss: 0.1797
agent1:                 episode reward: 0.1444,                 loss: nan
Episode: 40601/101000 (40.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8289s / 6137.6492 s
agent0:                 episode reward: 0.3857,                 loss: 0.1787
agent1:                 episode reward: -0.3857,                 loss: 0.1681
Score delta: 1.6361010567953005, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/40166_0.
Episode: 40621/101000 (40.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7485s / 6140.3977 s
agent0:                 episode reward: -0.0223,                 loss: nan
agent1:                 episode reward: 0.0223,                 loss: 0.1590
Episode: 40641/101000 (40.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7117s / 6143.1094 s
agent0:                 episode reward: -0.4575,                 loss: nan
agent1:                 episode reward: 0.4575,                 loss: 0.1568
Episode: 40661/101000 (40.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5999s / 6145.7093 s
agent0:                 episode reward: -0.2815,                 loss: nan
agent1:                 episode reward: 0.2815,                 loss: 0.1530
Episode: 40681/101000 (40.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9630s / 6148.6722 s
agent0:                 episode reward: -0.3651,                 loss: nan
agent1:                 episode reward: 0.3651,                 loss: 0.1539
Episode: 40701/101000 (40.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2438s / 6151.9161 s
agent0:                 episode reward: 0.0641,                 loss: nan
agent1:                 episode reward: -0.0641,                 loss: 0.1547
Episode: 40721/101000 (40.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9124s / 6155.8285 s
agent0:                 episode reward: -0.2346,                 loss: nan
agent1:                 episode reward: 0.2346,                 loss: 0.1529
Episode: 40741/101000 (40.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2101s / 6159.0385 s
agent0:                 episode reward: -0.1425,                 loss: nan
agent1:                 episode reward: 0.1425,                 loss: 0.1514
Episode: 40761/101000 (40.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3750s / 6162.4135 s
agent0:                 episode reward: -0.1556,                 loss: 0.1814
agent1:                 episode reward: 0.1556,                 loss: 0.1511
Score delta: 1.7459431543293185, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/40320_1.
Episode: 40781/101000 (40.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9115s / 6165.3251 s
agent0:                 episode reward: 0.4289,                 loss: 0.1792
agent1:                 episode reward: -0.4289,                 loss: nan
Episode: 40801/101000 (40.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1861s / 6168.5112 s
agent0:                 episode reward: -0.1360,                 loss: 0.1794
agent1:                 episode reward: 0.1360,                 loss: nan
Episode: 40821/101000 (40.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2910s / 6171.8022 s
agent0:                 episode reward: -0.1830,                 loss: 0.1803
agent1:                 episode reward: 0.1830,                 loss: nan
Episode: 40841/101000 (40.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2873s / 6176.0895 s
agent0:                 episode reward: -0.1675,                 loss: 0.1782
agent1:                 episode reward: 0.1675,                 loss: nan
Episode: 40861/101000 (40.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4130s / 6180.5025 s
agent0:                 episode reward: 0.3141,                 loss: 0.1787
agent1:                 episode reward: -0.3141,                 loss: nan
Episode: 40881/101000 (40.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8147s / 6184.3173 s
agent0:                 episode reward: -0.1105,                 loss: 0.1792
agent1:                 episode reward: 0.1105,                 loss: nan
Episode: 40901/101000 (40.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9140s / 6187.2313 s
agent0:                 episode reward: -0.0597,                 loss: 0.1809
agent1:                 episode reward: 0.0597,                 loss: nan
Episode: 40921/101000 (40.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3739s / 6190.6052 s
agent0:                 episode reward: -0.0837,                 loss: 0.1797
agent1:                 episode reward: 0.0837,                 loss: nan
Episode: 40941/101000 (40.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9576s / 6194.5628 s
agent0:                 episode reward: -0.1994,                 loss: 0.1821
agent1:                 episode reward: 0.1994,                 loss: nan
Episode: 40961/101000 (40.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2990s / 6197.8618 s
agent0:                 episode reward: -0.6136,                 loss: 0.1806
agent1:                 episode reward: 0.6136,                 loss: nan
Episode: 40981/101000 (40.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1977s / 6201.0595 s
agent0:                 episode reward: 0.2181,                 loss: 0.1836
agent1:                 episode reward: -0.2181,                 loss: nan
Episode: 41001/101000 (40.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7063s / 6203.7658 s
agent0:                 episode reward: -0.2210,                 loss: 0.1847
agent1:                 episode reward: 0.2210,                 loss: nan
Episode: 41021/101000 (40.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4312s / 6207.1970 s
agent0:                 episode reward: 0.1946,                 loss: 0.1822
agent1:                 episode reward: -0.1946,                 loss: nan
Episode: 41041/101000 (40.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0733s / 6210.2703 s
agent0:                 episode reward: 0.2672,                 loss: 0.1831
agent1:                 episode reward: -0.2672,                 loss: nan
Episode: 41061/101000 (40.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1048s / 6213.3751 s
agent0:                 episode reward: -0.6006,                 loss: 0.1821
agent1:                 episode reward: 0.6006,                 loss: nan
Episode: 41081/101000 (40.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2476s / 6216.6227 s
agent0:                 episode reward: 0.0499,                 loss: 0.1822
agent1:                 episode reward: -0.0499,                 loss: nan
Episode: 41101/101000 (40.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4314s / 6220.0541 s
agent0:                 episode reward: 0.0883,                 loss: 0.1826
agent1:                 episode reward: -0.0883,                 loss: nan
Episode: 41121/101000 (40.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1607s / 6223.2148 s
agent0:                 episode reward: -0.4736,                 loss: 0.1838
agent1:                 episode reward: 0.4736,                 loss: nan
Episode: 41141/101000 (40.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9569s / 6226.1717 s
agent0:                 episode reward: -0.1299,                 loss: 0.1838
agent1:                 episode reward: 0.1299,                 loss: nan
Episode: 41161/101000 (40.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8631s / 6230.0348 s
agent0:                 episode reward: 0.0494,                 loss: 0.1842
agent1:                 episode reward: -0.0494,                 loss: nan
Episode: 41181/101000 (40.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3763s / 6233.4112 s
agent0:                 episode reward: -0.1310,                 loss: 0.1853
agent1:                 episode reward: 0.1310,                 loss: nan
Episode: 41201/101000 (40.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1315s / 6236.5427 s
agent0:                 episode reward: -0.4498,                 loss: 0.1833
agent1:                 episode reward: 0.4498,                 loss: nan
Episode: 41221/101000 (40.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3077s / 6239.8504 s
agent0:                 episode reward: -0.6414,                 loss: 0.1818
agent1:                 episode reward: 0.6414,                 loss: nan
Episode: 41241/101000 (40.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7259s / 6243.5763 s
agent0:                 episode reward: -0.3542,                 loss: 0.1841
agent1:                 episode reward: 0.3542,                 loss: nan
Episode: 41261/101000 (40.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3543s / 6246.9306 s
agent0:                 episode reward: -0.1513,                 loss: 0.1837
agent1:                 episode reward: 0.1513,                 loss: nan
Episode: 41281/101000 (40.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3447s / 6250.2753 s
agent0:                 episode reward: -0.1246,                 loss: 0.1829
agent1:                 episode reward: 0.1246,                 loss: nan
Episode: 41301/101000 (40.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8055s / 6253.0808 s
agent0:                 episode reward: -0.2243,                 loss: 0.1838
agent1:                 episode reward: 0.2243,                 loss: nan
Episode: 41321/101000 (40.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6525s / 6256.7333 s
agent0:                 episode reward: -0.0935,                 loss: 0.1853
agent1:                 episode reward: 0.0935,                 loss: nan
Episode: 41341/101000 (40.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6015s / 6260.3348 s
agent0:                 episode reward: -0.2537,                 loss: 0.1867
agent1:                 episode reward: 0.2537,                 loss: nan
Episode: 41361/101000 (40.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0211s / 6264.3559 s
agent0:                 episode reward: 0.0426,                 loss: 0.1855
agent1:                 episode reward: -0.0426,                 loss: nan
Episode: 41381/101000 (40.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3309s / 6267.6868 s
agent0:                 episode reward: -0.3399,                 loss: 0.1861
agent1:                 episode reward: 0.3399,                 loss: nan
Episode: 41401/101000 (40.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0236s / 6270.7104 s
agent0:                 episode reward: -0.1630,                 loss: 0.1883
agent1:                 episode reward: 0.1630,                 loss: nan
Episode: 41421/101000 (41.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1804s / 6273.8909 s
agent0:                 episode reward: -0.3113,                 loss: 0.1877
agent1:                 episode reward: 0.3113,                 loss: nan
Episode: 41441/101000 (41.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1344s / 6277.0253 s
agent0:                 episode reward: 0.0007,                 loss: 0.1844
agent1:                 episode reward: -0.0007,                 loss: nan
Episode: 41461/101000 (41.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5183s / 6280.5435 s
agent0:                 episode reward: 0.0317,                 loss: 0.1868
agent1:                 episode reward: -0.0317,                 loss: nan
Episode: 41481/101000 (41.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4178s / 6284.9613 s
agent0:                 episode reward: -0.5193,                 loss: 0.1877
agent1:                 episode reward: 0.5193,                 loss: nan
Episode: 41501/101000 (41.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2718s / 6288.2331 s
agent0:                 episode reward: -0.4885,                 loss: 0.1862
agent1:                 episode reward: 0.4885,                 loss: nan
Episode: 41521/101000 (41.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8185s / 6292.0516 s
agent0:                 episode reward: -0.3694,                 loss: 0.1878
agent1:                 episode reward: 0.3694,                 loss: nan
Episode: 41541/101000 (41.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3421s / 6295.3937 s
agent0:                 episode reward: -0.3378,                 loss: 0.1880
agent1:                 episode reward: 0.3378,                 loss: nan
Episode: 41561/101000 (41.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6994s / 6299.0931 s
agent0:                 episode reward: 0.1321,                 loss: 0.1856
agent1:                 episode reward: -0.1321,                 loss: nan
Episode: 41581/101000 (41.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2856s / 6302.3787 s
agent0:                 episode reward: -0.0185,                 loss: 0.1862
agent1:                 episode reward: 0.0185,                 loss: nan
Episode: 41601/101000 (41.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2141s / 6305.5929 s
agent0:                 episode reward: -0.4490,                 loss: 0.1870
agent1:                 episode reward: 0.4490,                 loss: nan
Episode: 41621/101000 (41.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7834s / 6309.3762 s
agent0:                 episode reward: -0.5386,                 loss: 0.1866
agent1:                 episode reward: 0.5386,                 loss: nan
Episode: 41641/101000 (41.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0836s / 6312.4598 s
agent0:                 episode reward: -0.0314,                 loss: 0.1816
agent1:                 episode reward: 0.0314,                 loss: nan
Episode: 41661/101000 (41.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7373s / 6316.1971 s
agent0:                 episode reward: -0.2959,                 loss: 0.1800
agent1:                 episode reward: 0.2959,                 loss: nan
Episode: 41681/101000 (41.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1793s / 6319.3764 s
agent0:                 episode reward: -0.2275,                 loss: 0.1832
agent1:                 episode reward: 0.2275,                 loss: nan
Episode: 41701/101000 (41.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0775s / 6322.4539 s
agent0:                 episode reward: -0.0146,                 loss: 0.1809
agent1:                 episode reward: 0.0146,                 loss: nan
Episode: 41721/101000 (41.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7258s / 6326.1797 s
agent0:                 episode reward: 0.2335,                 loss: 0.1815
agent1:                 episode reward: -0.2335,                 loss: 0.1885
Score delta: 1.570262512200555, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/41294_0.
Episode: 41741/101000 (41.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9773s / 6329.1570 s
agent0:                 episode reward: -0.7665,                 loss: nan
agent1:                 episode reward: 0.7665,                 loss: 0.1725
Episode: 41761/101000 (41.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3642s / 6332.5212 s
agent0:                 episode reward: -0.2960,                 loss: nan
agent1:                 episode reward: 0.2960,                 loss: 0.1697
Episode: 41781/101000 (41.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0980s / 6335.6192 s
agent0:                 episode reward: -0.3521,                 loss: nan
agent1:                 episode reward: 0.3521,                 loss: 0.1666
Episode: 41801/101000 (41.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0141s / 6338.6333 s
agent0:                 episode reward: 0.0106,                 loss: nan
agent1:                 episode reward: -0.0106,                 loss: 0.1683
Episode: 41821/101000 (41.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0046s / 6342.6379 s
agent0:                 episode reward: -0.0571,                 loss: nan
agent1:                 episode reward: 0.0571,                 loss: 0.1667
Episode: 41841/101000 (41.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0139s / 6346.6518 s
agent0:                 episode reward: -0.3171,                 loss: nan
agent1:                 episode reward: 0.3171,                 loss: 0.1645
Episode: 41861/101000 (41.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0520s / 6349.7038 s
agent0:                 episode reward: -0.4921,                 loss: nan
agent1:                 episode reward: 0.4921,                 loss: 0.1656
Episode: 41881/101000 (41.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6909s / 6353.3946 s
agent0:                 episode reward: -0.0690,                 loss: 0.1844
agent1:                 episode reward: 0.0690,                 loss: 0.1575
Score delta: 1.506027138873717, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/41439_1.
Episode: 41901/101000 (41.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8154s / 6357.2100 s
agent0:                 episode reward: 0.0234,                 loss: 0.1855
agent1:                 episode reward: -0.0234,                 loss: nan
Episode: 41921/101000 (41.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6229s / 6360.8329 s
agent0:                 episode reward: -0.0857,                 loss: 0.1868
agent1:                 episode reward: 0.0857,                 loss: nan
Episode: 41941/101000 (41.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4626s / 6364.2955 s
agent0:                 episode reward: -0.3092,                 loss: 0.1853
agent1:                 episode reward: 0.3092,                 loss: nan
Episode: 41961/101000 (41.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 2.3515s / 6366.6471 s
agent0:                 episode reward: -0.2604,                 loss: 0.1820
agent1:                 episode reward: 0.2604,                 loss: nan
Episode: 41981/101000 (41.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9324s / 6370.5795 s
agent0:                 episode reward: -0.0490,                 loss: 0.1839
agent1:                 episode reward: 0.0490,                 loss: nan
Episode: 42001/101000 (41.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1289s / 6373.7085 s
agent0:                 episode reward: 0.0440,                 loss: 0.1851
agent1:                 episode reward: -0.0440,                 loss: nan
Episode: 42021/101000 (41.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3387s / 6378.0472 s
agent0:                 episode reward: 0.4655,                 loss: 0.1853
agent1:                 episode reward: -0.4655,                 loss: nan
Episode: 42041/101000 (41.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2819s / 6381.3290 s
agent0:                 episode reward: -0.3148,                 loss: 0.1834
agent1:                 episode reward: 0.3148,                 loss: nan
Episode: 42061/101000 (41.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2853s / 6384.6144 s
agent0:                 episode reward: -0.3903,                 loss: 0.1849
agent1:                 episode reward: 0.3903,                 loss: nan
Episode: 42081/101000 (41.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8761s / 6387.4905 s
agent0:                 episode reward: -0.2272,                 loss: 0.1857
agent1:                 episode reward: 0.2272,                 loss: nan
Episode: 42101/101000 (41.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3278s / 6391.8183 s
agent0:                 episode reward: -0.4664,                 loss: 0.1851
agent1:                 episode reward: 0.4664,                 loss: nan
Episode: 42121/101000 (41.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7341s / 6395.5524 s
agent0:                 episode reward: 0.1898,                 loss: 0.1873
agent1:                 episode reward: -0.1898,                 loss: nan
Episode: 42141/101000 (41.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9775s / 6399.5299 s
agent0:                 episode reward: -0.0021,                 loss: 0.1879
agent1:                 episode reward: 0.0021,                 loss: nan
Episode: 42161/101000 (41.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9296s / 6402.4595 s
agent0:                 episode reward: -0.1055,                 loss: 0.1880
agent1:                 episode reward: 0.1055,                 loss: nan
Episode: 42181/101000 (41.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6453s / 6406.1047 s
agent0:                 episode reward: -0.0137,                 loss: 0.1879
agent1:                 episode reward: 0.0137,                 loss: nan
Episode: 42201/101000 (41.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6410s / 6408.7457 s
agent0:                 episode reward: -0.2664,                 loss: 0.1866
agent1:                 episode reward: 0.2664,                 loss: nan
Episode: 42221/101000 (41.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4975s / 6412.2433 s
agent0:                 episode reward: -0.0392,                 loss: 0.1874
agent1:                 episode reward: 0.0392,                 loss: nan
Episode: 42241/101000 (41.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5365s / 6415.7798 s
agent0:                 episode reward: -0.2102,                 loss: 0.1881
agent1:                 episode reward: 0.2102,                 loss: nan
Episode: 42261/101000 (41.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6814s / 6419.4612 s
agent0:                 episode reward: 0.0641,                 loss: 0.1851
agent1:                 episode reward: -0.0641,                 loss: nan
Episode: 42281/101000 (41.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5812s / 6423.0423 s
agent0:                 episode reward: -0.1049,                 loss: 0.1877
agent1:                 episode reward: 0.1049,                 loss: nan
Episode: 42301/101000 (41.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6385s / 6426.6808 s
agent0:                 episode reward: 0.1924,                 loss: 0.1876
agent1:                 episode reward: -0.1924,                 loss: nan
Episode: 42321/101000 (41.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1670s / 6429.8478 s
agent0:                 episode reward: -0.3808,                 loss: 0.1856
agent1:                 episode reward: 0.3808,                 loss: nan
Episode: 42341/101000 (41.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3370s / 6434.1847 s
agent0:                 episode reward: -0.4861,                 loss: 0.1873
agent1:                 episode reward: 0.4861,                 loss: nan
Episode: 42361/101000 (41.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1261s / 6437.3108 s
agent0:                 episode reward: 0.4229,                 loss: 0.1878
agent1:                 episode reward: -0.4229,                 loss: nan
Episode: 42381/101000 (41.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0149s / 6441.3257 s
agent0:                 episode reward: -0.0021,                 loss: 0.1879
agent1:                 episode reward: 0.0021,                 loss: nan
Episode: 42401/101000 (41.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2840s / 6444.6098 s
agent0:                 episode reward: -0.2118,                 loss: 0.1866
agent1:                 episode reward: 0.2118,                 loss: nan
Episode: 42421/101000 (42.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2507s / 6447.8605 s
agent0:                 episode reward: -0.4266,                 loss: 0.1877
agent1:                 episode reward: 0.4266,                 loss: nan
Episode: 42441/101000 (42.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3145s / 6451.1750 s
agent0:                 episode reward: 0.1065,                 loss: 0.1821
agent1:                 episode reward: -0.1065,                 loss: nan
Episode: 42461/101000 (42.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2971s / 6454.4720 s
agent0:                 episode reward: -0.0328,                 loss: 0.1771
agent1:                 episode reward: 0.0328,                 loss: nan
Episode: 42481/101000 (42.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6079s / 6458.0799 s
agent0:                 episode reward: -0.0941,                 loss: 0.1772
agent1:                 episode reward: 0.0941,                 loss: nan
Episode: 42501/101000 (42.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6242s / 6460.7041 s
agent0:                 episode reward: -0.1074,                 loss: 0.1769
agent1:                 episode reward: 0.1074,                 loss: nan
Episode: 42521/101000 (42.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9184s / 6463.6225 s
agent0:                 episode reward: -0.2428,                 loss: 0.1755
agent1:                 episode reward: 0.2428,                 loss: nan
Episode: 42541/101000 (42.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0338s / 6466.6563 s
agent0:                 episode reward: -0.0073,                 loss: 0.1773
agent1:                 episode reward: 0.0073,                 loss: nan
Episode: 42561/101000 (42.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4374s / 6470.0937 s
agent0:                 episode reward: -0.8010,                 loss: 0.1780
agent1:                 episode reward: 0.8010,                 loss: nan
Episode: 42581/101000 (42.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8397s / 6472.9334 s
agent0:                 episode reward: 0.1873,                 loss: 0.1759
agent1:                 episode reward: -0.1873,                 loss: nan
Episode: 42601/101000 (42.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4480s / 6476.3814 s
agent0:                 episode reward: 0.1469,                 loss: 0.1765
agent1:                 episode reward: -0.1469,                 loss: nan
Episode: 42621/101000 (42.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7925s / 6480.1739 s
agent0:                 episode reward: -0.2460,                 loss: 0.1759
agent1:                 episode reward: 0.2460,                 loss: nan
Episode: 42641/101000 (42.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4086s / 6483.5825 s
agent0:                 episode reward: -0.3600,                 loss: 0.1771
agent1:                 episode reward: 0.3600,                 loss: nan
Episode: 42661/101000 (42.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1044s / 6487.6868 s
agent0:                 episode reward: -0.3755,                 loss: 0.1758
agent1:                 episode reward: 0.3755,                 loss: nan
Episode: 42681/101000 (42.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8129s / 6491.4998 s
agent0:                 episode reward: -0.2726,                 loss: 0.1770
agent1:                 episode reward: 0.2726,                 loss: nan
Episode: 42701/101000 (42.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9030s / 6495.4028 s
agent0:                 episode reward: -0.4725,                 loss: 0.1771
agent1:                 episode reward: 0.4725,                 loss: nan
Episode: 42721/101000 (42.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9405s / 6498.3433 s
agent0:                 episode reward: -0.1962,                 loss: 0.1759
agent1:                 episode reward: 0.1962,                 loss: nan
Episode: 42741/101000 (42.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8465s / 6501.1898 s
agent0:                 episode reward: 0.2395,                 loss: 0.1775
agent1:                 episode reward: -0.2395,                 loss: nan
Episode: 42761/101000 (42.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2369s / 6504.4267 s
agent0:                 episode reward: 0.0017,                 loss: 0.1781
agent1:                 episode reward: -0.0017,                 loss: nan
Episode: 42781/101000 (42.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4041s / 6507.8308 s
agent0:                 episode reward: -0.0605,                 loss: 0.1858
agent1:                 episode reward: 0.0605,                 loss: nan
Episode: 42801/101000 (42.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1161s / 6511.9469 s
agent0:                 episode reward: 0.1755,                 loss: 0.1865
agent1:                 episode reward: -0.1755,                 loss: nan
Episode: 42821/101000 (42.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8236s / 6515.7705 s
agent0:                 episode reward: -0.2176,                 loss: 0.1863
agent1:                 episode reward: 0.2176,                 loss: nan
Episode: 42841/101000 (42.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2191s / 6518.9896 s
agent0:                 episode reward: 0.2063,                 loss: 0.1862
agent1:                 episode reward: -0.2063,                 loss: nan
Episode: 42861/101000 (42.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7723s / 6521.7618 s
agent0:                 episode reward: 0.1065,                 loss: 0.1873
agent1:                 episode reward: -0.1065,                 loss: nan
Episode: 42881/101000 (42.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4838s / 6526.2457 s
agent0:                 episode reward: 0.0402,                 loss: 0.1871
agent1:                 episode reward: -0.0402,                 loss: 0.1679
Score delta: 1.5430426178075962, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/42445_0.
Episode: 42901/101000 (42.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5729s / 6529.8186 s
agent0:                 episode reward: -0.3088,                 loss: nan
agent1:                 episode reward: 0.3088,                 loss: 0.1638
Episode: 42921/101000 (42.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5628s / 6532.3814 s
agent0:                 episode reward: -0.3455,                 loss: nan
agent1:                 episode reward: 0.3455,                 loss: 0.1596
Episode: 42941/101000 (42.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9570s / 6535.3384 s
agent0:                 episode reward: -0.1638,                 loss: nan
agent1:                 episode reward: 0.1638,                 loss: 0.1596
Episode: 42961/101000 (42.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0556s / 6538.3940 s
agent0:                 episode reward: -0.2790,                 loss: nan
agent1:                 episode reward: 0.2790,                 loss: 0.1561
Episode: 42981/101000 (42.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5346s / 6540.9285 s
agent0:                 episode reward: -0.5284,                 loss: 0.1890
agent1:                 episode reward: 0.5284,                 loss: 0.1545
Score delta: 1.5388690853902243, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/42538_1.
Episode: 43001/101000 (42.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1838s / 6544.1123 s
agent0:                 episode reward: -0.3170,                 loss: 0.1863
agent1:                 episode reward: 0.3170,                 loss: nan
Episode: 43021/101000 (42.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9524s / 6547.0647 s
agent0:                 episode reward: -0.3591,                 loss: 0.1842
agent1:                 episode reward: 0.3591,                 loss: nan
Episode: 43041/101000 (42.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0712s / 6550.1359 s
agent0:                 episode reward: -0.3076,                 loss: 0.1854
agent1:                 episode reward: 0.3076,                 loss: nan
Episode: 43061/101000 (42.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8523s / 6553.9882 s
agent0:                 episode reward: -0.2082,                 loss: 0.1840
agent1:                 episode reward: 0.2082,                 loss: nan
Episode: 43081/101000 (42.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0647s / 6557.0529 s
agent0:                 episode reward: -0.2252,                 loss: 0.1833
agent1:                 episode reward: 0.2252,                 loss: nan
Episode: 43101/101000 (42.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6141s / 6560.6670 s
agent0:                 episode reward: -0.2334,                 loss: 0.1835
agent1:                 episode reward: 0.2334,                 loss: nan
Episode: 43121/101000 (42.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8983s / 6564.5654 s
agent0:                 episode reward: 0.2640,                 loss: 0.1826
agent1:                 episode reward: -0.2640,                 loss: 0.1621
Score delta: 1.6499832604049707, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/42687_0.
Episode: 43141/101000 (42.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1406s / 6567.7059 s
agent0:                 episode reward: -0.3733,                 loss: nan
agent1:                 episode reward: 0.3733,                 loss: 0.1608
Episode: 43161/101000 (42.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0213s / 6571.7273 s
agent0:                 episode reward: -0.1338,                 loss: nan
agent1:                 episode reward: 0.1338,                 loss: 0.1577
Episode: 43181/101000 (42.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7222s / 6575.4494 s
agent0:                 episode reward: -0.4436,                 loss: nan
agent1:                 episode reward: 0.4436,                 loss: 0.1574
Episode: 43201/101000 (42.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8733s / 6578.3227 s
agent0:                 episode reward: -0.6466,                 loss: 0.1833
agent1:                 episode reward: 0.6466,                 loss: 0.1565
Score delta: 1.7391484385196372, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/42770_1.
Episode: 43221/101000 (42.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2971s / 6581.6199 s
agent0:                 episode reward: -0.7384,                 loss: 0.1791
agent1:                 episode reward: 0.7384,                 loss: nan
Episode: 43241/101000 (42.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0031s / 6584.6230 s
agent0:                 episode reward: 0.0970,                 loss: 0.1775
agent1:                 episode reward: -0.0970,                 loss: nan
Episode: 43261/101000 (42.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8414s / 6588.4644 s
agent0:                 episode reward: -0.0316,                 loss: 0.1780
agent1:                 episode reward: 0.0316,                 loss: nan
Episode: 43281/101000 (42.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7431s / 6592.2075 s
agent0:                 episode reward: -0.7877,                 loss: 0.1800
agent1:                 episode reward: 0.7877,                 loss: nan
Episode: 43301/101000 (42.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0894s / 6596.2968 s
agent0:                 episode reward: -0.2882,                 loss: 0.1743
agent1:                 episode reward: 0.2882,                 loss: nan
Episode: 43321/101000 (42.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5691s / 6599.8659 s
agent0:                 episode reward: -0.2226,                 loss: 0.1743
agent1:                 episode reward: 0.2226,                 loss: nan
Episode: 43341/101000 (42.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6762s / 6602.5421 s
agent0:                 episode reward: -0.0485,                 loss: 0.1755
agent1:                 episode reward: 0.0485,                 loss: nan
Episode: 43361/101000 (42.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6644s / 6605.2065 s
agent0:                 episode reward: -0.5898,                 loss: 0.1737
agent1:                 episode reward: 0.5898,                 loss: nan
Episode: 43381/101000 (42.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6289s / 6608.8354 s
agent0:                 episode reward: -0.2502,                 loss: 0.1751
agent1:                 episode reward: 0.2502,                 loss: nan
Episode: 43401/101000 (42.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7764s / 6612.6118 s
agent0:                 episode reward: -0.5811,                 loss: 0.1760
agent1:                 episode reward: 0.5811,                 loss: nan
Episode: 43421/101000 (42.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6818s / 6616.2936 s
agent0:                 episode reward: 0.2111,                 loss: 0.1731
agent1:                 episode reward: -0.2111,                 loss: nan
Episode: 43441/101000 (43.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0320s / 6620.3256 s
agent0:                 episode reward: -0.3488,                 loss: 0.1740
agent1:                 episode reward: 0.3488,                 loss: nan
Episode: 43461/101000 (43.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6664s / 6623.9920 s
agent0:                 episode reward: -0.3933,                 loss: 0.1726
agent1:                 episode reward: 0.3933,                 loss: nan
Episode: 43481/101000 (43.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2408s / 6627.2329 s
agent0:                 episode reward: -0.6541,                 loss: 0.1726
agent1:                 episode reward: 0.6541,                 loss: nan
Episode: 43501/101000 (43.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2256s / 6630.4584 s
agent0:                 episode reward: -0.4170,                 loss: 0.1727
agent1:                 episode reward: 0.4170,                 loss: nan
Episode: 43521/101000 (43.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1742s / 6633.6326 s
agent0:                 episode reward: 0.0774,                 loss: 0.1729
agent1:                 episode reward: -0.0774,                 loss: nan
Episode: 43541/101000 (43.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5946s / 6636.2272 s
agent0:                 episode reward: -0.4823,                 loss: 0.1735
agent1:                 episode reward: 0.4823,                 loss: nan
Episode: 43561/101000 (43.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1453s / 6639.3725 s
agent0:                 episode reward: -0.4984,                 loss: 0.1748
agent1:                 episode reward: 0.4984,                 loss: nan
Episode: 43581/101000 (43.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6903s / 6643.0628 s
agent0:                 episode reward: -0.1455,                 loss: 0.1721
agent1:                 episode reward: 0.1455,                 loss: nan
Episode: 43601/101000 (43.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1979s / 6647.2607 s
agent0:                 episode reward: -0.0264,                 loss: 0.1726
agent1:                 episode reward: 0.0264,                 loss: nan
Episode: 43621/101000 (43.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5608s / 6650.8215 s
agent0:                 episode reward: -0.2242,                 loss: 0.1783
agent1:                 episode reward: 0.2242,                 loss: nan
Episode: 43641/101000 (43.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1296s / 6654.9511 s
agent0:                 episode reward: -0.3061,                 loss: 0.1772
agent1:                 episode reward: 0.3061,                 loss: nan
Episode: 43661/101000 (43.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7838s / 6658.7349 s
agent0:                 episode reward: -0.1278,                 loss: 0.1785
agent1:                 episode reward: 0.1278,                 loss: nan
Episode: 43681/101000 (43.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7105s / 6662.4454 s
agent0:                 episode reward: -0.3828,                 loss: 0.1778
agent1:                 episode reward: 0.3828,                 loss: nan
Episode: 43701/101000 (43.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5933s / 6665.0387 s
agent0:                 episode reward: -0.1524,                 loss: 0.1776
agent1:                 episode reward: 0.1524,                 loss: nan
Episode: 43721/101000 (43.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0171s / 6669.0558 s
agent0:                 episode reward: 0.0838,                 loss: 0.1775
agent1:                 episode reward: -0.0838,                 loss: nan
Episode: 43741/101000 (43.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7727s / 6671.8284 s
agent0:                 episode reward: -0.3715,                 loss: 0.1763
agent1:                 episode reward: 0.3715,                 loss: nan
Episode: 43761/101000 (43.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2222s / 6675.0507 s
agent0:                 episode reward: 0.0654,                 loss: 0.1778
agent1:                 episode reward: -0.0654,                 loss: nan
Episode: 43781/101000 (43.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1189s / 6679.1695 s
agent0:                 episode reward: -0.3339,                 loss: 0.1761
agent1:                 episode reward: 0.3339,                 loss: nan
Episode: 43801/101000 (43.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9735s / 6682.1430 s
agent0:                 episode reward: -0.0551,                 loss: 0.1769
agent1:                 episode reward: 0.0551,                 loss: nan
Episode: 43821/101000 (43.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7589s / 6685.9019 s
agent0:                 episode reward: -0.2175,                 loss: 0.1769
agent1:                 episode reward: 0.2175,                 loss: nan
Episode: 43841/101000 (43.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2814s / 6689.1834 s
agent0:                 episode reward: 0.0012,                 loss: 0.1778
agent1:                 episode reward: -0.0012,                 loss: nan
Episode: 43861/101000 (43.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4923s / 6692.6757 s
agent0:                 episode reward: 0.0881,                 loss: 0.1764
agent1:                 episode reward: -0.0881,                 loss: nan
Episode: 43881/101000 (43.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8172s / 6696.4928 s
agent0:                 episode reward: 0.2383,                 loss: 0.1766
agent1:                 episode reward: -0.2383,                 loss: nan
Episode: 43901/101000 (43.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5384s / 6700.0312 s
agent0:                 episode reward: 0.0247,                 loss: 0.1762
agent1:                 episode reward: -0.0247,                 loss: nan
Episode: 43921/101000 (43.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3183s / 6703.3495 s
agent0:                 episode reward: -0.3763,                 loss: 0.1772
agent1:                 episode reward: 0.3763,                 loss: nan
Episode: 43941/101000 (43.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7738s / 6707.1234 s
agent0:                 episode reward: -0.4787,                 loss: 0.1757
agent1:                 episode reward: 0.4787,                 loss: nan
Episode: 43961/101000 (43.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1830s / 6710.3064 s
agent0:                 episode reward: 0.0052,                 loss: 0.1851
agent1:                 episode reward: -0.0052,                 loss: nan
Episode: 43981/101000 (43.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3731s / 6713.6795 s
agent0:                 episode reward: -0.1770,                 loss: 0.1851
agent1:                 episode reward: 0.1770,                 loss: nan
Episode: 44001/101000 (43.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2659s / 6716.9454 s
agent0:                 episode reward: -0.4586,                 loss: 0.1855
agent1:                 episode reward: 0.4586,                 loss: nan
Episode: 44021/101000 (43.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1239s / 6720.0692 s
agent0:                 episode reward: -0.4670,                 loss: 0.1841
agent1:                 episode reward: 0.4670,                 loss: nan
Episode: 44041/101000 (43.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7195s / 6722.7888 s
agent0:                 episode reward: -0.1173,                 loss: 0.1846
agent1:                 episode reward: 0.1173,                 loss: nan
Episode: 44061/101000 (43.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8425s / 6726.6313 s
agent0:                 episode reward: 0.1978,                 loss: 0.1856
agent1:                 episode reward: -0.1978,                 loss: nan
Episode: 44081/101000 (43.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7583s / 6730.3895 s
agent0:                 episode reward: -0.3046,                 loss: 0.1848
agent1:                 episode reward: 0.3046,                 loss: nan
Episode: 44101/101000 (43.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5024s / 6733.8919 s
agent0:                 episode reward: -0.4360,                 loss: 0.1849
agent1:                 episode reward: 0.4360,                 loss: nan
Episode: 44121/101000 (43.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3486s / 6738.2405 s
agent0:                 episode reward: 0.1034,                 loss: 0.1841
agent1:                 episode reward: -0.1034,                 loss: nan
Episode: 44141/101000 (43.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3844s / 6741.6250 s
agent0:                 episode reward: -0.0714,                 loss: 0.1850
agent1:                 episode reward: 0.0714,                 loss: nan
Episode: 44161/101000 (43.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6798s / 6745.3047 s
agent0:                 episode reward: -0.2181,                 loss: 0.1836
agent1:                 episode reward: 0.2181,                 loss: nan
Episode: 44181/101000 (43.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2456s / 6748.5504 s
agent0:                 episode reward: -0.2691,                 loss: 0.1840
agent1:                 episode reward: 0.2691,                 loss: nan
Episode: 44201/101000 (43.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8782s / 6752.4285 s
agent0:                 episode reward: 0.2632,                 loss: 0.1841
agent1:                 episode reward: -0.2632,                 loss: nan
Episode: 44221/101000 (43.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4544s / 6755.8830 s
agent0:                 episode reward: 0.0187,                 loss: 0.1845
agent1:                 episode reward: -0.0187,                 loss: nan
Episode: 44241/101000 (43.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5424s / 6759.4254 s
agent0:                 episode reward: 0.0613,                 loss: 0.1828
agent1:                 episode reward: -0.0613,                 loss: nan
Episode: 44261/101000 (43.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5344s / 6762.9598 s
agent0:                 episode reward: -0.0870,                 loss: 0.1854
agent1:                 episode reward: 0.0870,                 loss: nan
Episode: 44281/101000 (43.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0873s / 6767.0471 s
agent0:                 episode reward: 0.3779,                 loss: 0.1851
agent1:                 episode reward: -0.3779,                 loss: nan
Episode: 44301/101000 (43.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7167s / 6770.7638 s
agent0:                 episode reward: -0.0975,                 loss: 0.1866
agent1:                 episode reward: 0.0975,                 loss: nan
Episode: 44321/101000 (43.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2276s / 6774.9914 s
agent0:                 episode reward: -0.1573,                 loss: 0.1863
agent1:                 episode reward: 0.1573,                 loss: nan
Episode: 44341/101000 (43.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3625s / 6778.3540 s
agent0:                 episode reward: -0.2617,                 loss: 0.1860
agent1:                 episode reward: 0.2617,                 loss: nan
Episode: 44361/101000 (43.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1338s / 6782.4877 s
agent0:                 episode reward: -0.0884,                 loss: 0.1881
agent1:                 episode reward: 0.0884,                 loss: nan
Episode: 44381/101000 (43.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0133s / 6786.5010 s
agent0:                 episode reward: 0.4669,                 loss: 0.1867
agent1:                 episode reward: -0.4669,                 loss: nan
Episode: 44401/101000 (43.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4051s / 6789.9061 s
agent0:                 episode reward: -0.4078,                 loss: 0.1852
agent1:                 episode reward: 0.4078,                 loss: nan
Episode: 44421/101000 (43.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3651s / 6793.2712 s
agent0:                 episode reward: -0.2520,                 loss: 0.1865
agent1:                 episode reward: 0.2520,                 loss: nan
Episode: 44441/101000 (44.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0524s / 6797.3236 s
agent0:                 episode reward: 0.2831,                 loss: 0.1851
agent1:                 episode reward: -0.2831,                 loss: nan
Episode: 44461/101000 (44.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5518s / 6800.8754 s
agent0:                 episode reward: -0.0092,                 loss: 0.1868
agent1:                 episode reward: 0.0092,                 loss: nan
Episode: 44481/101000 (44.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1078s / 6803.9833 s
agent0:                 episode reward: -0.0156,                 loss: 0.1860
agent1:                 episode reward: 0.0156,                 loss: 0.1647
Score delta: 1.51135717123226, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/44041_0.
Episode: 44501/101000 (44.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0163s / 6806.9996 s
agent0:                 episode reward: -0.4587,                 loss: nan
agent1:                 episode reward: 0.4587,                 loss: 0.1604
Episode: 44521/101000 (44.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8911s / 6809.8907 s
agent0:                 episode reward: -0.5598,                 loss: nan
agent1:                 episode reward: 0.5598,                 loss: 0.1587
Episode: 44541/101000 (44.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0972s / 6812.9879 s
agent0:                 episode reward: -0.2679,                 loss: nan
agent1:                 episode reward: 0.2679,                 loss: 0.1603
Episode: 44561/101000 (44.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1011s / 6816.0890 s
agent0:                 episode reward: 0.1510,                 loss: nan
agent1:                 episode reward: -0.1510,                 loss: 0.1593
Episode: 44581/101000 (44.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3069s / 6819.3959 s
agent0:                 episode reward: -0.7098,                 loss: 0.2123
agent1:                 episode reward: 0.7098,                 loss: 0.1604
Score delta: 1.5362013294399668, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/44145_1.
Episode: 44601/101000 (44.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2244s / 6822.6203 s
agent0:                 episode reward: -0.5141,                 loss: 0.1997
agent1:                 episode reward: 0.5141,                 loss: nan
Episode: 44621/101000 (44.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9901s / 6825.6104 s
agent0:                 episode reward: 0.1879,                 loss: 0.1980
agent1:                 episode reward: -0.1879,                 loss: nan
Episode: 44641/101000 (44.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8450s / 6829.4554 s
agent0:                 episode reward: 0.0016,                 loss: 0.1974
agent1:                 episode reward: -0.0016,                 loss: nan
Episode: 44661/101000 (44.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1618s / 6832.6172 s
agent0:                 episode reward: -0.0411,                 loss: 0.1962
agent1:                 episode reward: 0.0411,                 loss: nan
Episode: 44681/101000 (44.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6072s / 6836.2244 s
agent0:                 episode reward: -0.0742,                 loss: 0.1940
agent1:                 episode reward: 0.0742,                 loss: nan
Episode: 44701/101000 (44.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4130s / 6839.6374 s
agent0:                 episode reward: 0.0446,                 loss: 0.1946
agent1:                 episode reward: -0.0446,                 loss: nan
Episode: 44721/101000 (44.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6521s / 6843.2895 s
agent0:                 episode reward: -0.0648,                 loss: 0.1968
agent1:                 episode reward: 0.0648,                 loss: nan
Episode: 44741/101000 (44.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5758s / 6846.8653 s
agent0:                 episode reward: -0.5313,                 loss: 0.1908
agent1:                 episode reward: 0.5313,                 loss: nan
Episode: 44761/101000 (44.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8898s / 6850.7551 s
agent0:                 episode reward: -0.3328,                 loss: 0.1905
agent1:                 episode reward: 0.3328,                 loss: nan
Episode: 44781/101000 (44.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7233s / 6854.4784 s
agent0:                 episode reward: -0.8164,                 loss: 0.1907
agent1:                 episode reward: 0.8164,                 loss: nan
Episode: 44801/101000 (44.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0330s / 6858.5114 s
agent0:                 episode reward: 0.1304,                 loss: 0.1893
agent1:                 episode reward: -0.1304,                 loss: nan
Episode: 44821/101000 (44.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0010s / 6862.5124 s
agent0:                 episode reward: -0.4829,                 loss: 0.1889
agent1:                 episode reward: 0.4829,                 loss: nan
Episode: 44841/101000 (44.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7040s / 6866.2164 s
agent0:                 episode reward: -0.0537,                 loss: 0.1901
agent1:                 episode reward: 0.0537,                 loss: nan
Episode: 44861/101000 (44.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1165s / 6869.3330 s
agent0:                 episode reward: -0.3114,                 loss: 0.1866
agent1:                 episode reward: 0.3114,                 loss: nan
Episode: 44881/101000 (44.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3144s / 6872.6474 s
agent0:                 episode reward: -0.5360,                 loss: 0.1895
agent1:                 episode reward: 0.5360,                 loss: nan
Episode: 44901/101000 (44.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1306s / 6875.7780 s
agent0:                 episode reward: 0.0421,                 loss: 0.1886
agent1:                 episode reward: -0.0421,                 loss: nan
Episode: 44921/101000 (44.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5880s / 6879.3660 s
agent0:                 episode reward: -0.1796,                 loss: 0.1881
agent1:                 episode reward: 0.1796,                 loss: nan
Episode: 44941/101000 (44.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8072s / 6883.1732 s
agent0:                 episode reward: -0.0631,                 loss: 0.1888
agent1:                 episode reward: 0.0631,                 loss: nan
Episode: 44961/101000 (44.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0644s / 6887.2376 s
agent0:                 episode reward: 0.2252,                 loss: 0.1877
agent1:                 episode reward: -0.2252,                 loss: nan
Episode: 44981/101000 (44.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0524s / 6890.2900 s
agent0:                 episode reward: 0.0529,                 loss: 0.1908
agent1:                 episode reward: -0.0529,                 loss: nan
Episode: 45001/101000 (44.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9718s / 6893.2618 s
agent0:                 episode reward: 0.2076,                 loss: 0.1878
agent1:                 episode reward: -0.2076,                 loss: nan
Episode: 45021/101000 (44.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6269s / 6896.8887 s
agent0:                 episode reward: -0.2170,                 loss: 0.1893
agent1:                 episode reward: 0.2170,                 loss: nan
Episode: 45041/101000 (44.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1985s / 6900.0872 s
agent0:                 episode reward: -0.4009,                 loss: 0.1885
agent1:                 episode reward: 0.4009,                 loss: nan
Episode: 45061/101000 (44.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9131s / 6904.0004 s
agent0:                 episode reward: 0.0249,                 loss: 0.1829
agent1:                 episode reward: -0.0249,                 loss: nan
Episode: 45081/101000 (44.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6252s / 6907.6256 s
agent0:                 episode reward: -0.2623,                 loss: 0.1821
agent1:                 episode reward: 0.2623,                 loss: nan
Episode: 45101/101000 (44.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2989s / 6910.9245 s
agent0:                 episode reward: -0.0832,                 loss: 0.1800
agent1:                 episode reward: 0.0832,                 loss: nan
Episode: 45121/101000 (44.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0035s / 6913.9280 s
agent0:                 episode reward: 0.1530,                 loss: 0.1794
agent1:                 episode reward: -0.1530,                 loss: nan
Episode: 45141/101000 (44.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2444s / 6917.1724 s
agent0:                 episode reward: -0.2863,                 loss: 0.1807
agent1:                 episode reward: 0.2863,                 loss: nan
Episode: 45161/101000 (44.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4536s / 6920.6260 s
agent0:                 episode reward: 0.0984,                 loss: 0.1804
agent1:                 episode reward: -0.0984,                 loss: nan
Episode: 45181/101000 (44.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0718s / 6924.6977 s
agent0:                 episode reward: 0.3136,                 loss: 0.1790
agent1:                 episode reward: -0.3136,                 loss: nan
Episode: 45201/101000 (44.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3717s / 6928.0694 s
agent0:                 episode reward: -0.2013,                 loss: 0.1790
agent1:                 episode reward: 0.2013,                 loss: nan
Episode: 45221/101000 (44.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3578s / 6932.4272 s
agent0:                 episode reward: 0.1228,                 loss: 0.1801
agent1:                 episode reward: -0.1228,                 loss: nan
Episode: 45241/101000 (44.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7797s / 6936.2069 s
agent0:                 episode reward: -0.0657,                 loss: 0.1801
agent1:                 episode reward: 0.0657,                 loss: nan
Episode: 45261/101000 (44.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5252s / 6939.7321 s
agent0:                 episode reward: 0.0561,                 loss: 0.1820
agent1:                 episode reward: -0.0561,                 loss: nan
Episode: 45281/101000 (44.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4416s / 6943.1737 s
agent0:                 episode reward: 0.4272,                 loss: 0.1817
agent1:                 episode reward: -0.4272,                 loss: 0.1867
Score delta: 1.6549280008533689, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/44843_0.
Episode: 45301/101000 (44.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2564s / 6946.4301 s
agent0:                 episode reward: -0.5350,                 loss: nan
agent1:                 episode reward: 0.5350,                 loss: 0.1802
Episode: 45321/101000 (44.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9676s / 6949.3977 s
agent0:                 episode reward: -0.1264,                 loss: nan
agent1:                 episode reward: 0.1264,                 loss: 0.1765
Episode: 45341/101000 (44.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8953s / 6952.2930 s
agent0:                 episode reward: -0.8125,                 loss: 0.2352
agent1:                 episode reward: 0.8125,                 loss: 0.1486
Score delta: 1.5372980099949207, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/44908_1.
Episode: 45361/101000 (44.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9714s / 6956.2645 s
agent0:                 episode reward: -0.1798,                 loss: 0.2071
agent1:                 episode reward: 0.1798,                 loss: nan
Episode: 45381/101000 (44.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6485s / 6959.9130 s
agent0:                 episode reward: -0.2689,                 loss: 0.2023
agent1:                 episode reward: 0.2689,                 loss: nan
Episode: 45401/101000 (44.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5199s / 6962.4328 s
agent0:                 episode reward: -0.6431,                 loss: 0.2021
agent1:                 episode reward: 0.6431,                 loss: nan
Episode: 45421/101000 (44.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3481s / 6965.7809 s
agent0:                 episode reward: -0.1793,                 loss: 0.2003
agent1:                 episode reward: 0.1793,                 loss: nan
Episode: 45441/101000 (44.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1319s / 6969.9128 s
agent0:                 episode reward: -0.3470,                 loss: 0.1984
agent1:                 episode reward: 0.3470,                 loss: nan
Episode: 45461/101000 (45.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7197s / 6973.6326 s
agent0:                 episode reward: -0.2837,                 loss: 0.1959
agent1:                 episode reward: 0.2837,                 loss: nan
Episode: 45481/101000 (45.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3782s / 6977.0108 s
agent0:                 episode reward: -0.2361,                 loss: 0.1913
agent1:                 episode reward: 0.2361,                 loss: nan
Episode: 45501/101000 (45.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5189s / 6980.5297 s
agent0:                 episode reward: -0.1787,                 loss: 0.1891
agent1:                 episode reward: 0.1787,                 loss: nan
Episode: 45521/101000 (45.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6706s / 6984.2003 s
agent0:                 episode reward: 0.0989,                 loss: 0.1904
agent1:                 episode reward: -0.0989,                 loss: nan
Episode: 45541/101000 (45.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1248s / 6987.3251 s
agent0:                 episode reward: -0.1437,                 loss: 0.1894
agent1:                 episode reward: 0.1437,                 loss: nan
Episode: 45561/101000 (45.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7613s / 6991.0864 s
agent0:                 episode reward: -0.4758,                 loss: 0.1902
agent1:                 episode reward: 0.4758,                 loss: nan
Episode: 45581/101000 (45.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0442s / 6995.1306 s
agent0:                 episode reward: -0.0847,                 loss: 0.1898
agent1:                 episode reward: 0.0847,                 loss: nan
Episode: 45601/101000 (45.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0308s / 6998.1613 s
agent0:                 episode reward: -0.1509,                 loss: 0.1909
agent1:                 episode reward: 0.1509,                 loss: nan
Episode: 45621/101000 (45.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7179s / 7001.8792 s
agent0:                 episode reward: 0.0649,                 loss: 0.1888
agent1:                 episode reward: -0.0649,                 loss: nan
Episode: 45641/101000 (45.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 2.7204s / 7004.5996 s
agent0:                 episode reward: 0.0641,                 loss: 0.1893
agent1:                 episode reward: -0.0641,                 loss: nan
Episode: 45661/101000 (45.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8110s / 7008.4106 s
agent0:                 episode reward: -0.2785,                 loss: 0.1893
agent1:                 episode reward: 0.2785,                 loss: nan
Episode: 45681/101000 (45.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6174s / 7012.0280 s
agent0:                 episode reward: -0.5126,                 loss: 0.1890
agent1:                 episode reward: 0.5126,                 loss: nan
Episode: 45701/101000 (45.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2011s / 7016.2291 s
agent0:                 episode reward: 0.0044,                 loss: 0.1897
agent1:                 episode reward: -0.0044,                 loss: nan
Episode: 45721/101000 (45.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0171s / 7019.2462 s
agent0:                 episode reward: 0.1817,                 loss: 0.1886
agent1:                 episode reward: -0.1817,                 loss: nan
Episode: 45741/101000 (45.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8650s / 7023.1112 s
agent0:                 episode reward: -0.1141,                 loss: 0.1881
agent1:                 episode reward: 0.1141,                 loss: nan
Episode: 45761/101000 (45.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9565s / 7027.0677 s
agent0:                 episode reward: -0.4286,                 loss: 0.1891
agent1:                 episode reward: 0.4286,                 loss: nan
Episode: 45781/101000 (45.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1264s / 7030.1940 s
agent0:                 episode reward: -0.0530,                 loss: 0.1852
agent1:                 episode reward: 0.0530,                 loss: nan
Episode: 45801/101000 (45.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6532s / 7034.8472 s
agent0:                 episode reward: -0.3374,                 loss: 0.1805
agent1:                 episode reward: 0.3374,                 loss: nan
Episode: 45821/101000 (45.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0284s / 7038.8757 s
agent0:                 episode reward: -0.0240,                 loss: 0.1779
agent1:                 episode reward: 0.0240,                 loss: nan
Episode: 45841/101000 (45.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0088s / 7041.8844 s
agent0:                 episode reward: -0.0492,                 loss: 0.1777
agent1:                 episode reward: 0.0492,                 loss: nan
Episode: 45861/101000 (45.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1329s / 7046.0173 s
agent0:                 episode reward: -0.1939,                 loss: 0.1789
agent1:                 episode reward: 0.1939,                 loss: nan
Episode: 45881/101000 (45.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3583s / 7049.3756 s
agent0:                 episode reward: -0.2013,                 loss: 0.1786
agent1:                 episode reward: 0.2013,                 loss: nan
Episode: 45901/101000 (45.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2926s / 7052.6681 s
agent0:                 episode reward: -0.0285,                 loss: 0.1788
agent1:                 episode reward: 0.0285,                 loss: nan
Episode: 45921/101000 (45.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9151s / 7055.5832 s
agent0:                 episode reward: 0.1499,                 loss: 0.1778
agent1:                 episode reward: -0.1499,                 loss: nan
Episode: 45941/101000 (45.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4449s / 7059.0281 s
agent0:                 episode reward: -0.0552,                 loss: 0.1767
agent1:                 episode reward: 0.0552,                 loss: 0.1607
Score delta: 1.5666705500832117, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/45497_0.
Episode: 45961/101000 (45.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4560s / 7062.4841 s
agent0:                 episode reward: -0.4306,                 loss: nan
agent1:                 episode reward: 0.4306,                 loss: 0.1598
Episode: 45981/101000 (45.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8232s / 7066.3073 s
agent0:                 episode reward: -0.6815,                 loss: nan
agent1:                 episode reward: 0.6815,                 loss: 0.1587
Episode: 46001/101000 (45.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2514s / 7069.5587 s
agent0:                 episode reward: -0.0186,                 loss: nan
agent1:                 episode reward: 0.0186,                 loss: 0.1595
Episode: 46021/101000 (45.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9909s / 7073.5496 s
agent0:                 episode reward: -0.3563,                 loss: nan
agent1:                 episode reward: 0.3563,                 loss: 0.1572
Episode: 46041/101000 (45.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6312s / 7077.1808 s
agent0:                 episode reward: -0.1903,                 loss: nan
agent1:                 episode reward: 0.1903,                 loss: 0.1583
Episode: 46061/101000 (45.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1635s / 7080.3443 s
agent0:                 episode reward: -0.0789,                 loss: nan
agent1:                 episode reward: 0.0789,                 loss: 0.1584
Episode: 46081/101000 (45.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5378s / 7082.8821 s
agent0:                 episode reward: -0.1858,                 loss: 0.1890
agent1:                 episode reward: 0.1858,                 loss: 0.1596
Score delta: 1.6553194675969833, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/45642_1.
Episode: 46101/101000 (45.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0543s / 7086.9364 s
agent0:                 episode reward: 0.0718,                 loss: 0.1920
agent1:                 episode reward: -0.0718,                 loss: nan
Episode: 46121/101000 (45.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4822s / 7090.4186 s
agent0:                 episode reward: -0.0029,                 loss: 0.1927
agent1:                 episode reward: 0.0029,                 loss: nan
Episode: 46141/101000 (45.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1437s / 7094.5623 s
agent0:                 episode reward: 0.2470,                 loss: 0.1905
agent1:                 episode reward: -0.2470,                 loss: nan
Episode: 46161/101000 (45.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7441s / 7098.3064 s
agent0:                 episode reward: -0.1425,                 loss: 0.1920
agent1:                 episode reward: 0.1425,                 loss: nan
Episode: 46181/101000 (45.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3733s / 7101.6797 s
agent0:                 episode reward: -0.0315,                 loss: 0.1901
agent1:                 episode reward: 0.0315,                 loss: nan
Episode: 46201/101000 (45.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7296s / 7105.4093 s
agent0:                 episode reward: 0.1732,                 loss: 0.1907
agent1:                 episode reward: -0.1732,                 loss: nan
Episode: 46221/101000 (45.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3815s / 7108.7908 s
agent0:                 episode reward: 0.1805,                 loss: 0.1924
agent1:                 episode reward: -0.1805,                 loss: nan
Episode: 46241/101000 (45.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7152s / 7112.5061 s
agent0:                 episode reward: 0.3003,                 loss: 0.1910
agent1:                 episode reward: -0.3003,                 loss: nan
Episode: 46261/101000 (45.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6050s / 7116.1111 s
agent0:                 episode reward: 0.2625,                 loss: 0.1956
agent1:                 episode reward: -0.2625,                 loss: nan
Episode: 46281/101000 (45.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3017s / 7119.4128 s
agent0:                 episode reward: 0.1617,                 loss: 0.1943
agent1:                 episode reward: -0.1617,                 loss: nan
Episode: 46301/101000 (45.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5225s / 7122.9353 s
agent0:                 episode reward: 0.0028,                 loss: 0.1942
agent1:                 episode reward: -0.0028,                 loss: nan
Episode: 46321/101000 (45.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3262s / 7126.2615 s
agent0:                 episode reward: 0.2308,                 loss: 0.1938
agent1:                 episode reward: -0.2308,                 loss: nan
Episode: 46341/101000 (45.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6662s / 7129.9277 s
agent0:                 episode reward: 0.0210,                 loss: 0.1942
agent1:                 episode reward: -0.0210,                 loss: nan
Episode: 46361/101000 (45.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5685s / 7133.4963 s
agent0:                 episode reward: -0.3991,                 loss: 0.1966
agent1:                 episode reward: 0.3991,                 loss: nan
Episode: 46381/101000 (45.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9231s / 7136.4194 s
agent0:                 episode reward: 0.1751,                 loss: 0.1940
agent1:                 episode reward: -0.1751,                 loss: nan
Episode: 46401/101000 (45.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3239s / 7139.7433 s
agent0:                 episode reward: -0.1572,                 loss: 0.1917
agent1:                 episode reward: 0.1572,                 loss: nan
Episode: 46421/101000 (45.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0002s / 7142.7435 s
agent0:                 episode reward: -0.2738,                 loss: 0.1943
agent1:                 episode reward: 0.2738,                 loss: nan
Episode: 46441/101000 (45.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6360s / 7146.3794 s
agent0:                 episode reward: -0.0484,                 loss: 0.1913
agent1:                 episode reward: 0.0484,                 loss: nan
Episode: 46461/101000 (46.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1725s / 7150.5519 s
agent0:                 episode reward: -0.1895,                 loss: 0.1928
agent1:                 episode reward: 0.1895,                 loss: nan
Episode: 46481/101000 (46.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1021s / 7154.6541 s
agent0:                 episode reward: -0.4476,                 loss: 0.1928
agent1:                 episode reward: 0.4476,                 loss: nan
Episode: 46501/101000 (46.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3232s / 7157.9773 s
agent0:                 episode reward: 0.2806,                 loss: 0.1965
agent1:                 episode reward: -0.2806,                 loss: nan
Episode: 46521/101000 (46.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 2.5190s / 7160.4963 s
agent0:                 episode reward: 0.3393,                 loss: 0.1965
agent1:                 episode reward: -0.3393,                 loss: 0.1611
Score delta: 1.6501641164388363, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/46085_0.
Episode: 46541/101000 (46.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2028s / 7163.6992 s
agent0:                 episode reward: -0.4960,                 loss: nan
agent1:                 episode reward: 0.4960,                 loss: 0.1582
Episode: 46561/101000 (46.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6567s / 7167.3559 s
agent0:                 episode reward: -0.8588,                 loss: nan
agent1:                 episode reward: 0.8588,                 loss: 0.1597
Episode: 46581/101000 (46.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 2.6952s / 7170.0511 s
agent0:                 episode reward: -0.3616,                 loss: nan
agent1:                 episode reward: 0.3616,                 loss: 0.1598
Episode: 46601/101000 (46.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1918s / 7173.2429 s
agent0:                 episode reward: -0.5222,                 loss: 0.2086
agent1:                 episode reward: 0.5222,                 loss: 0.1567
Score delta: 1.5203084539444234, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/46164_1.
Episode: 46621/101000 (46.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8331s / 7177.0760 s
agent0:                 episode reward: 0.2875,                 loss: 0.2072
agent1:                 episode reward: -0.2875,                 loss: nan
Episode: 46641/101000 (46.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3996s / 7181.4756 s
agent0:                 episode reward: -0.1895,                 loss: 0.2060
agent1:                 episode reward: 0.1895,                 loss: nan
Episode: 46661/101000 (46.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9375s / 7184.4131 s
agent0:                 episode reward: -0.2393,                 loss: 0.2063
agent1:                 episode reward: 0.2393,                 loss: nan
Episode: 46681/101000 (46.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7729s / 7188.1860 s
agent0:                 episode reward: 0.2319,                 loss: 0.1929
agent1:                 episode reward: -0.2319,                 loss: nan
Episode: 46701/101000 (46.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7076s / 7191.8936 s
agent0:                 episode reward: -0.1190,                 loss: 0.1833
agent1:                 episode reward: 0.1190,                 loss: nan
Episode: 46721/101000 (46.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6928s / 7195.5863 s
agent0:                 episode reward: 0.0806,                 loss: 0.1837
agent1:                 episode reward: -0.0806,                 loss: nan
Episode: 46741/101000 (46.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3315s / 7198.9178 s
agent0:                 episode reward: 0.0642,                 loss: 0.1856
agent1:                 episode reward: -0.0642,                 loss: nan
Episode: 46761/101000 (46.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1475s / 7202.0653 s
agent0:                 episode reward: -0.4166,                 loss: 0.1840
agent1:                 episode reward: 0.4166,                 loss: nan
Episode: 46781/101000 (46.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4034s / 7204.4687 s
agent0:                 episode reward: 0.4541,                 loss: 0.1847
agent1:                 episode reward: -0.4541,                 loss: nan
Episode: 46801/101000 (46.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7193s / 7208.1880 s
agent0:                 episode reward: 0.2131,                 loss: 0.1841
agent1:                 episode reward: -0.2131,                 loss: nan
Episode: 46821/101000 (46.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8271s / 7212.0151 s
agent0:                 episode reward: -0.0704,                 loss: 0.1835
agent1:                 episode reward: 0.0704,                 loss: nan
Episode: 46841/101000 (46.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4472s / 7215.4623 s
agent0:                 episode reward: -0.0244,                 loss: 0.1843
agent1:                 episode reward: 0.0244,                 loss: nan
Episode: 46861/101000 (46.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4979s / 7218.9601 s
agent0:                 episode reward: -0.1232,                 loss: 0.1841
agent1:                 episode reward: 0.1232,                 loss: nan
Episode: 46881/101000 (46.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1750s / 7222.1351 s
agent0:                 episode reward: -0.0321,                 loss: 0.1835
agent1:                 episode reward: 0.0321,                 loss: nan
Episode: 46901/101000 (46.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5906s / 7226.7257 s
agent0:                 episode reward: -0.1572,                 loss: 0.1853
agent1:                 episode reward: 0.1572,                 loss: nan
Episode: 46921/101000 (46.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5328s / 7230.2585 s
agent0:                 episode reward: 0.0872,                 loss: 0.1842
agent1:                 episode reward: -0.0872,                 loss: nan
Episode: 46941/101000 (46.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1920s / 7234.4505 s
agent0:                 episode reward: -0.1303,                 loss: 0.1830
agent1:                 episode reward: 0.1303,                 loss: nan
Episode: 46961/101000 (46.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6816s / 7238.1321 s
agent0:                 episode reward: 0.1394,                 loss: 0.1840
agent1:                 episode reward: -0.1394,                 loss: nan
Episode: 46981/101000 (46.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1202s / 7242.2523 s
agent0:                 episode reward: 0.0447,                 loss: 0.1841
agent1:                 episode reward: -0.0447,                 loss: nan
Episode: 47001/101000 (46.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7326s / 7245.9849 s
agent0:                 episode reward: 0.0022,                 loss: 0.1846
agent1:                 episode reward: -0.0022,                 loss: nan
Episode: 47021/101000 (46.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2855s / 7250.2705 s
agent0:                 episode reward: 0.1585,                 loss: 0.1819
agent1:                 episode reward: -0.1585,                 loss: nan
Episode: 47041/101000 (46.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7350s / 7254.0054 s
agent0:                 episode reward: -0.2804,                 loss: 0.1827
agent1:                 episode reward: 0.2804,                 loss: nan
Episode: 47061/101000 (46.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2204s / 7257.2259 s
agent0:                 episode reward: 0.2292,                 loss: 0.1811
agent1:                 episode reward: -0.2292,                 loss: nan
Episode: 47081/101000 (46.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7529s / 7260.9787 s
agent0:                 episode reward: 0.0170,                 loss: 0.1845
agent1:                 episode reward: -0.0170,                 loss: nan
Episode: 47101/101000 (46.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7351s / 7264.7139 s
agent0:                 episode reward: -0.3219,                 loss: 0.1826
agent1:                 episode reward: 0.3219,                 loss: nan
Episode: 47121/101000 (46.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0212s / 7268.7351 s
agent0:                 episode reward: 0.0680,                 loss: 0.1816
agent1:                 episode reward: -0.0680,                 loss: nan
Episode: 47141/101000 (46.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0500s / 7272.7851 s
agent0:                 episode reward: 0.4393,                 loss: 0.1824
agent1:                 episode reward: -0.4393,                 loss: nan
Episode: 47161/101000 (46.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9349s / 7276.7199 s
agent0:                 episode reward: 0.2051,                 loss: 0.1822
agent1:                 episode reward: -0.2051,                 loss: nan
Episode: 47181/101000 (46.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1171s / 7280.8370 s
agent0:                 episode reward: -0.0762,                 loss: 0.1835
agent1:                 episode reward: 0.0762,                 loss: nan
Episode: 47201/101000 (46.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5321s / 7284.3691 s
agent0:                 episode reward: -0.0497,                 loss: 0.1828
agent1:                 episode reward: 0.0497,                 loss: nan
Episode: 47221/101000 (46.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0993s / 7287.4685 s
agent0:                 episode reward: -0.1373,                 loss: 0.1807
agent1:                 episode reward: 0.1373,                 loss: nan
Episode: 47241/101000 (46.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0234s / 7290.4918 s
agent0:                 episode reward: 0.2962,                 loss: 0.1827
agent1:                 episode reward: -0.2962,                 loss: nan
Episode: 47261/101000 (46.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1097s / 7295.6015 s
agent0:                 episode reward: -0.0188,                 loss: 0.1826
agent1:                 episode reward: 0.0188,                 loss: nan
Episode: 47281/101000 (46.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2026s / 7298.8042 s
agent0:                 episode reward: 0.1448,                 loss: 0.1806
agent1:                 episode reward: -0.1448,                 loss: nan
Episode: 47301/101000 (46.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7521s / 7302.5562 s
agent0:                 episode reward: -0.2852,                 loss: 0.1839
agent1:                 episode reward: 0.2852,                 loss: nan
Episode: 47321/101000 (46.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3449s / 7306.9011 s
agent0:                 episode reward: 0.0829,                 loss: 0.1823
agent1:                 episode reward: -0.0829,                 loss: nan
Episode: 47341/101000 (46.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1121s / 7311.0132 s
agent0:                 episode reward: -0.3456,                 loss: 0.1817
agent1:                 episode reward: 0.3456,                 loss: nan
Episode: 47361/101000 (46.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9491s / 7313.9622 s
agent0:                 episode reward: -0.2844,                 loss: 0.1830
agent1:                 episode reward: 0.2844,                 loss: nan
Episode: 47381/101000 (46.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3853s / 7317.3475 s
agent0:                 episode reward: -0.2170,                 loss: 0.1830
agent1:                 episode reward: 0.2170,                 loss: nan
Episode: 47401/101000 (46.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8144s / 7322.1619 s
agent0:                 episode reward: 0.3234,                 loss: 0.1833
agent1:                 episode reward: -0.3234,                 loss: 0.1624
Score delta: 1.5083076433489357, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/46974_0.
Episode: 47421/101000 (46.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0007s / 7325.1626 s
agent0:                 episode reward: -0.5341,                 loss: nan
agent1:                 episode reward: 0.5341,                 loss: 0.1573
Episode: 47441/101000 (46.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6625s / 7328.8251 s
agent0:                 episode reward: -0.3360,                 loss: nan
agent1:                 episode reward: 0.3360,                 loss: 0.1581
Episode: 47461/101000 (46.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4177s / 7332.2428 s
agent0:                 episode reward: -0.2466,                 loss: nan
agent1:                 episode reward: 0.2466,                 loss: 0.1582
Episode: 47481/101000 (47.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5932s / 7335.8360 s
agent0:                 episode reward: -0.1843,                 loss: nan
agent1:                 episode reward: 0.1843,                 loss: 0.1581
Episode: 47501/101000 (47.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2024s / 7339.0384 s
agent0:                 episode reward: -0.3310,                 loss: nan
agent1:                 episode reward: 0.3310,                 loss: 0.1565
Score delta: 1.5498413485993532, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/47075_1.
Episode: 47521/101000 (47.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5642s / 7342.6026 s
agent0:                 episode reward: -0.0851,                 loss: 0.1861
agent1:                 episode reward: 0.0851,                 loss: nan
Episode: 47541/101000 (47.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5280s / 7346.1306 s
agent0:                 episode reward: -0.1531,                 loss: 0.1833
agent1:                 episode reward: 0.1531,                 loss: nan
Episode: 47561/101000 (47.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8977s / 7350.0283 s
agent0:                 episode reward: 0.0008,                 loss: 0.1825
agent1:                 episode reward: -0.0008,                 loss: nan
Episode: 47581/101000 (47.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1402s / 7354.1686 s
agent0:                 episode reward: 0.1790,                 loss: 0.1840
agent1:                 episode reward: -0.1790,                 loss: nan
Episode: 47601/101000 (47.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5866s / 7357.7552 s
agent0:                 episode reward: -0.4614,                 loss: 0.1822
agent1:                 episode reward: 0.4614,                 loss: nan
Episode: 47621/101000 (47.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 2.4746s / 7360.2298 s
agent0:                 episode reward: -0.0577,                 loss: 0.1836
agent1:                 episode reward: 0.0577,                 loss: nan
Episode: 47641/101000 (47.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3624s / 7363.5922 s
agent0:                 episode reward: -0.3015,                 loss: 0.1865
agent1:                 episode reward: 0.3015,                 loss: nan
Episode: 47661/101000 (47.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3882s / 7366.9804 s
agent0:                 episode reward: -0.1068,                 loss: 0.1851
agent1:                 episode reward: 0.1068,                 loss: nan
Episode: 47681/101000 (47.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4937s / 7370.4741 s
agent0:                 episode reward: -0.0387,                 loss: 0.1849
agent1:                 episode reward: 0.0387,                 loss: nan
Episode: 47701/101000 (47.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4250s / 7373.8991 s
agent0:                 episode reward: 0.5483,                 loss: 0.1845
agent1:                 episode reward: -0.5483,                 loss: 0.1616
Score delta: 1.5279655516946749, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/47271_0.
Episode: 47721/101000 (47.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8372s / 7376.7363 s
agent0:                 episode reward: -0.6545,                 loss: nan
agent1:                 episode reward: 0.6545,                 loss: 0.1609
Episode: 47741/101000 (47.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6736s / 7380.4099 s
agent0:                 episode reward: -0.1395,                 loss: nan
agent1:                 episode reward: 0.1395,                 loss: 0.1594
Episode: 47761/101000 (47.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8536s / 7384.2635 s
agent0:                 episode reward: -0.3755,                 loss: nan
agent1:                 episode reward: 0.3755,                 loss: 0.1580
Episode: 47781/101000 (47.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2145s / 7388.4780 s
agent0:                 episode reward: -0.3784,                 loss: 0.1994
agent1:                 episode reward: 0.3784,                 loss: 0.1545
Score delta: 1.6974811717801739, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/47343_1.
Episode: 47801/101000 (47.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2352s / 7391.7133 s
agent0:                 episode reward: 0.0616,                 loss: 0.1954
agent1:                 episode reward: -0.0616,                 loss: nan
Episode: 47821/101000 (47.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9046s / 7394.6178 s
agent0:                 episode reward: 0.4162,                 loss: 0.1943
agent1:                 episode reward: -0.4162,                 loss: nan
Episode: 47841/101000 (47.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6561s / 7399.2740 s
agent0:                 episode reward: 0.1813,                 loss: 0.1946
agent1:                 episode reward: -0.1813,                 loss: nan
Episode: 47861/101000 (47.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6336s / 7402.9076 s
agent0:                 episode reward: 0.0649,                 loss: 0.1902
agent1:                 episode reward: -0.0649,                 loss: nan
Episode: 47881/101000 (47.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2111s / 7407.1187 s
agent0:                 episode reward: -0.4933,                 loss: 0.1889
agent1:                 episode reward: 0.4933,                 loss: nan
Episode: 47901/101000 (47.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3064s / 7411.4251 s
agent0:                 episode reward: 0.1235,                 loss: 0.1907
agent1:                 episode reward: -0.1235,                 loss: nan
Episode: 47921/101000 (47.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5558s / 7414.9808 s
agent0:                 episode reward: -0.1573,                 loss: 0.1908
agent1:                 episode reward: 0.1573,                 loss: nan
Episode: 47941/101000 (47.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0351s / 7418.0159 s
agent0:                 episode reward: 0.1503,                 loss: 0.1891
agent1:                 episode reward: -0.1503,                 loss: nan
Episode: 47961/101000 (47.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9349s / 7421.9508 s
agent0:                 episode reward: 0.1884,                 loss: 0.1905
agent1:                 episode reward: -0.1884,                 loss: nan
Episode: 47981/101000 (47.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5621s / 7425.5129 s
agent0:                 episode reward: -0.0444,                 loss: 0.1904
agent1:                 episode reward: 0.0444,                 loss: nan
Episode: 48001/101000 (47.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7264s / 7429.2393 s
agent0:                 episode reward: -0.0918,                 loss: 0.1888
agent1:                 episode reward: 0.0918,                 loss: nan
Episode: 48021/101000 (47.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9099s / 7433.1492 s
agent0:                 episode reward: 0.0390,                 loss: 0.1896
agent1:                 episode reward: -0.0390,                 loss: nan
Episode: 48041/101000 (47.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3725s / 7436.5217 s
agent0:                 episode reward: -0.2372,                 loss: 0.1894
agent1:                 episode reward: 0.2372,                 loss: nan
Episode: 48061/101000 (47.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8277s / 7440.3494 s
agent0:                 episode reward: 0.0039,                 loss: 0.1874
agent1:                 episode reward: -0.0039,                 loss: nan
Episode: 48081/101000 (47.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1311s / 7443.4805 s
agent0:                 episode reward: -0.2687,                 loss: 0.1906
agent1:                 episode reward: 0.2687,                 loss: nan
Episode: 48101/101000 (47.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4713s / 7447.9518 s
agent0:                 episode reward: -0.1614,                 loss: 0.1881
agent1:                 episode reward: 0.1614,                 loss: nan
Episode: 48121/101000 (47.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3538s / 7451.3056 s
agent0:                 episode reward: -0.1427,                 loss: 0.1881
agent1:                 episode reward: 0.1427,                 loss: nan
Episode: 48141/101000 (47.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2545s / 7455.5601 s
agent0:                 episode reward: -0.3736,                 loss: 0.1899
agent1:                 episode reward: 0.3736,                 loss: nan
Episode: 48161/101000 (47.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2133s / 7458.7734 s
agent0:                 episode reward: -0.0208,                 loss: 0.1887
agent1:                 episode reward: 0.0208,                 loss: nan
Episode: 48181/101000 (47.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9025s / 7462.6759 s
agent0:                 episode reward: -0.2236,                 loss: 0.1836
agent1:                 episode reward: 0.2236,                 loss: nan
Episode: 48201/101000 (47.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1543s / 7466.8302 s
agent0:                 episode reward: 0.0472,                 loss: 0.1735
agent1:                 episode reward: -0.0472,                 loss: nan
Episode: 48221/101000 (47.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2352s / 7471.0654 s
agent0:                 episode reward: -0.0384,                 loss: 0.1738
agent1:                 episode reward: 0.0384,                 loss: nan
Episode: 48241/101000 (47.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4531s / 7474.5185 s
agent0:                 episode reward: 0.0836,                 loss: 0.1737
agent1:                 episode reward: -0.0836,                 loss: nan
Episode: 48261/101000 (47.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7812s / 7478.2997 s
agent0:                 episode reward: -0.3433,                 loss: 0.1741
agent1:                 episode reward: 0.3433,                 loss: nan
Episode: 48281/101000 (47.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9871s / 7482.2868 s
agent0:                 episode reward: -0.3095,                 loss: 0.1737
agent1:                 episode reward: 0.3095,                 loss: nan
Episode: 48301/101000 (47.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2553s / 7485.5420 s
agent0:                 episode reward: -0.1576,                 loss: 0.1740
agent1:                 episode reward: 0.1576,                 loss: nan
Episode: 48321/101000 (47.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1241s / 7488.6661 s
agent0:                 episode reward: 0.0810,                 loss: 0.1722
agent1:                 episode reward: -0.0810,                 loss: nan
Episode: 48341/101000 (47.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6070s / 7492.2731 s
agent0:                 episode reward: 0.1487,                 loss: 0.1730
agent1:                 episode reward: -0.1487,                 loss: nan
Episode: 48361/101000 (47.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4578s / 7495.7309 s
agent0:                 episode reward: -0.2170,                 loss: 0.1716
agent1:                 episode reward: 0.2170,                 loss: nan
Episode: 48381/101000 (47.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8237s / 7499.5546 s
agent0:                 episode reward: 0.3030,                 loss: 0.1727
agent1:                 episode reward: -0.3030,                 loss: nan
Episode: 48401/101000 (47.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7115s / 7503.2661 s
agent0:                 episode reward: -0.2307,                 loss: 0.1720
agent1:                 episode reward: 0.2307,                 loss: nan
Episode: 48421/101000 (47.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6252s / 7506.8914 s
agent0:                 episode reward: -0.0900,                 loss: 0.1732
agent1:                 episode reward: 0.0900,                 loss: nan
Episode: 48441/101000 (47.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6297s / 7510.5211 s
agent0:                 episode reward: 0.0115,                 loss: 0.1711
agent1:                 episode reward: -0.0115,                 loss: nan
Episode: 48461/101000 (47.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4003s / 7513.9214 s
agent0:                 episode reward: -0.3126,                 loss: 0.1718
agent1:                 episode reward: 0.3126,                 loss: nan
Episode: 48481/101000 (48.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1191s / 7517.0405 s
agent0:                 episode reward: -0.4504,                 loss: 0.1727
agent1:                 episode reward: 0.4504,                 loss: nan
Episode: 48501/101000 (48.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1075s / 7521.1480 s
agent0:                 episode reward: -0.1166,                 loss: 0.1716
agent1:                 episode reward: 0.1166,                 loss: nan
Episode: 48521/101000 (48.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5473s / 7525.6953 s
agent0:                 episode reward: -0.1314,                 loss: 0.1746
agent1:                 episode reward: 0.1314,                 loss: nan
Episode: 48541/101000 (48.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8943s / 7529.5896 s
agent0:                 episode reward: 0.0220,                 loss: 0.1759
agent1:                 episode reward: -0.0220,                 loss: nan
Episode: 48561/101000 (48.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9311s / 7533.5207 s
agent0:                 episode reward: -0.0423,                 loss: 0.1773
agent1:                 episode reward: 0.0423,                 loss: nan
Episode: 48581/101000 (48.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6783s / 7537.1990 s
agent0:                 episode reward: -0.0417,                 loss: 0.1778
agent1:                 episode reward: 0.0417,                 loss: nan
Episode: 48601/101000 (48.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4228s / 7540.6218 s
agent0:                 episode reward: 0.0296,                 loss: 0.1789
agent1:                 episode reward: -0.0296,                 loss: nan
Episode: 48621/101000 (48.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5869s / 7544.2086 s
agent0:                 episode reward: 0.5551,                 loss: 0.1785
agent1:                 episode reward: -0.5551,                 loss: nan
Episode: 48641/101000 (48.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4456s / 7547.6542 s
agent0:                 episode reward: -0.0947,                 loss: 0.1777
agent1:                 episode reward: 0.0947,                 loss: 0.1585
Score delta: 1.6242820320784754, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/48202_0.
Episode: 48661/101000 (48.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7283s / 7551.3824 s
agent0:                 episode reward: -0.4401,                 loss: nan
agent1:                 episode reward: 0.4401,                 loss: 0.1572
Episode: 48681/101000 (48.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2962s / 7554.6786 s
agent0:                 episode reward: -0.1566,                 loss: nan
agent1:                 episode reward: 0.1566,                 loss: 0.1571
Episode: 48701/101000 (48.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9199s / 7557.5985 s
agent0:                 episode reward: -0.4225,                 loss: 0.1877
agent1:                 episode reward: 0.4225,                 loss: 0.1570
Score delta: 2.346583106235785, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/48269_1.
Episode: 48721/101000 (48.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1027s / 7560.7012 s
agent0:                 episode reward: -0.2309,                 loss: 0.1882
agent1:                 episode reward: 0.2309,                 loss: nan
Episode: 48741/101000 (48.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9133s / 7564.6145 s
agent0:                 episode reward: 0.1689,                 loss: 0.1853
agent1:                 episode reward: -0.1689,                 loss: nan
Episode: 48761/101000 (48.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8566s / 7568.4711 s
agent0:                 episode reward: 0.2578,                 loss: 0.1871
agent1:                 episode reward: -0.2578,                 loss: nan
Episode: 48781/101000 (48.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2654s / 7571.7365 s
agent0:                 episode reward: 0.1596,                 loss: 0.1858
agent1:                 episode reward: -0.1596,                 loss: nan
Episode: 48801/101000 (48.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8088s / 7575.5452 s
agent0:                 episode reward: 0.2310,                 loss: 0.1848
agent1:                 episode reward: -0.2310,                 loss: nan
Episode: 48821/101000 (48.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4437s / 7578.9889 s
agent0:                 episode reward: -0.0071,                 loss: 0.1848
agent1:                 episode reward: 0.0071,                 loss: nan
Episode: 48841/101000 (48.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5892s / 7582.5781 s
agent0:                 episode reward: 0.0895,                 loss: 0.1864
agent1:                 episode reward: -0.0895,                 loss: nan
Episode: 48861/101000 (48.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9866s / 7586.5647 s
agent0:                 episode reward: 0.0977,                 loss: 0.1848
agent1:                 episode reward: -0.0977,                 loss: nan
Episode: 48881/101000 (48.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4246s / 7589.9893 s
agent0:                 episode reward: 0.0983,                 loss: 0.1850
agent1:                 episode reward: -0.0983,                 loss: nan
Episode: 48901/101000 (48.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2002s / 7593.1895 s
agent0:                 episode reward: -0.0321,                 loss: 0.1859
agent1:                 episode reward: 0.0321,                 loss: nan
Episode: 48921/101000 (48.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8985s / 7597.0880 s
agent0:                 episode reward: -0.1052,                 loss: 0.1904
agent1:                 episode reward: 0.1052,                 loss: nan
Episode: 48941/101000 (48.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5793s / 7600.6672 s
agent0:                 episode reward: -0.1826,                 loss: 0.1906
agent1:                 episode reward: 0.1826,                 loss: nan
Episode: 48961/101000 (48.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7195s / 7604.3867 s
agent0:                 episode reward: -0.1305,                 loss: 0.1892
agent1:                 episode reward: 0.1305,                 loss: nan
Episode: 48981/101000 (48.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9008s / 7608.2875 s
agent0:                 episode reward: -0.0714,                 loss: 0.1889
agent1:                 episode reward: 0.0714,                 loss: nan
Episode: 49001/101000 (48.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4748s / 7611.7624 s
agent0:                 episode reward: 0.3913,                 loss: 0.1880
agent1:                 episode reward: -0.3913,                 loss: nan
Episode: 49021/101000 (48.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0036s / 7614.7659 s
agent0:                 episode reward: -0.5644,                 loss: 0.1909
agent1:                 episode reward: 0.5644,                 loss: 0.1592
Score delta: 1.5638315975771275, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/48577_0.
Episode: 49041/101000 (48.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3609s / 7618.1268 s
agent0:                 episode reward: -0.2257,                 loss: nan
agent1:                 episode reward: 0.2257,                 loss: 0.1589
Episode: 49061/101000 (48.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9526s / 7621.0794 s
agent0:                 episode reward: -0.4885,                 loss: nan
agent1:                 episode reward: 0.4885,                 loss: 0.1595
Episode: 49081/101000 (48.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0180s / 7624.0975 s
agent0:                 episode reward: -0.2505,                 loss: nan
agent1:                 episode reward: 0.2505,                 loss: 0.1578
Episode: 49101/101000 (48.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2209s / 7628.3183 s
agent0:                 episode reward: -0.3207,                 loss: 0.1837
agent1:                 episode reward: 0.3207,                 loss: 0.1576
Score delta: 1.989621212678702, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/48661_1.
Episode: 49121/101000 (48.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6682s / 7631.9865 s
agent0:                 episode reward: -0.2935,                 loss: 0.1838
agent1:                 episode reward: 0.2935,                 loss: nan
Episode: 49141/101000 (48.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1124s / 7635.0989 s
agent0:                 episode reward: 0.8781,                 loss: 0.1834
agent1:                 episode reward: -0.8781,                 loss: nan
Episode: 49161/101000 (48.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7608s / 7638.8598 s
agent0:                 episode reward: -0.0478,                 loss: 0.1825
agent1:                 episode reward: 0.0478,                 loss: nan
Episode: 49181/101000 (48.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9798s / 7641.8396 s
agent0:                 episode reward: -0.1290,                 loss: 0.1830
agent1:                 episode reward: 0.1290,                 loss: nan
Episode: 49201/101000 (48.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9869s / 7645.8264 s
agent0:                 episode reward: 0.1275,                 loss: 0.1837
agent1:                 episode reward: -0.1275,                 loss: 0.1647
Score delta: 1.7831058276267033, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/48761_0.
Episode: 49221/101000 (48.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4627s / 7649.2892 s
agent0:                 episode reward: -0.1359,                 loss: nan
agent1:                 episode reward: 0.1359,                 loss: 0.1601
Episode: 49241/101000 (48.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5212s / 7652.8103 s
agent0:                 episode reward: -0.5476,                 loss: nan
agent1:                 episode reward: 0.5476,                 loss: 0.1577
Episode: 49261/101000 (48.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4268s / 7656.2371 s
agent0:                 episode reward: -0.2025,                 loss: nan
agent1:                 episode reward: 0.2025,                 loss: 0.1561
Episode: 49281/101000 (48.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9184s / 7660.1556 s
agent0:                 episode reward: -0.6673,                 loss: 0.2025
agent1:                 episode reward: 0.6673,                 loss: 0.1566
Score delta: 1.5325245670061687, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/48854_1.
Episode: 49301/101000 (48.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6955s / 7663.8511 s
agent0:                 episode reward: -0.2624,                 loss: 0.1957
agent1:                 episode reward: 0.2624,                 loss: nan
Episode: 49321/101000 (48.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7342s / 7667.5853 s
agent0:                 episode reward: -0.1198,                 loss: 0.1955
agent1:                 episode reward: 0.1198,                 loss: nan
Episode: 49341/101000 (48.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1196s / 7671.7048 s
agent0:                 episode reward: 0.2843,                 loss: 0.1926
agent1:                 episode reward: -0.2843,                 loss: nan
Episode: 49361/101000 (48.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3909s / 7675.0957 s
agent0:                 episode reward: -0.2565,                 loss: 0.1917
agent1:                 episode reward: 0.2565,                 loss: nan
Episode: 49381/101000 (48.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2479s / 7678.3436 s
agent0:                 episode reward: 0.1764,                 loss: 0.1918
agent1:                 episode reward: -0.1764,                 loss: 0.1474
Score delta: 1.5389924876368486, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/48944_0.
Episode: 49401/101000 (48.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8099s / 7682.1536 s
agent0:                 episode reward: -0.3662,                 loss: nan
agent1:                 episode reward: 0.3662,                 loss: 0.1469
Episode: 49421/101000 (48.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9357s / 7686.0892 s
agent0:                 episode reward: -0.5113,                 loss: nan
agent1:                 episode reward: 0.5113,                 loss: 0.1480
Episode: 49441/101000 (48.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7400s / 7689.8293 s
agent0:                 episode reward: -0.4731,                 loss: nan
agent1:                 episode reward: 0.4731,                 loss: 0.1457
Episode: 49461/101000 (48.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3984s / 7693.2276 s
agent0:                 episode reward: -0.1265,                 loss: nan
agent1:                 episode reward: 0.1265,                 loss: 0.1458
Episode: 49481/101000 (48.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0245s / 7696.2521 s
agent0:                 episode reward: -0.6375,                 loss: 0.2004
agent1:                 episode reward: 0.6375,                 loss: 0.1448
Score delta: 1.5322936595466943, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/49048_1.
Episode: 49501/101000 (49.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4973s / 7699.7494 s
agent0:                 episode reward: -0.1107,                 loss: 0.1970
agent1:                 episode reward: 0.1107,                 loss: nan
Episode: 49521/101000 (49.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2907s / 7704.0401 s
agent0:                 episode reward: -0.3277,                 loss: 0.1953
agent1:                 episode reward: 0.3277,                 loss: nan
Episode: 49541/101000 (49.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0313s / 7708.0714 s
agent0:                 episode reward: -0.2267,                 loss: 0.1949
agent1:                 episode reward: 0.2267,                 loss: nan
Episode: 49561/101000 (49.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8587s / 7711.9301 s
agent0:                 episode reward: -0.2792,                 loss: 0.1920
agent1:                 episode reward: 0.2792,                 loss: nan
Episode: 49581/101000 (49.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6221s / 7715.5522 s
agent0:                 episode reward: 0.1445,                 loss: 0.1916
agent1:                 episode reward: -0.1445,                 loss: nan
Episode: 49601/101000 (49.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3165s / 7718.8687 s
agent0:                 episode reward: -0.3250,                 loss: 0.1888
agent1:                 episode reward: 0.3250,                 loss: nan
Episode: 49621/101000 (49.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1460s / 7723.0147 s
agent0:                 episode reward: 0.1448,                 loss: 0.1921
agent1:                 episode reward: -0.1448,                 loss: nan
Episode: 49641/101000 (49.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5328s / 7726.5475 s
agent0:                 episode reward: 0.2635,                 loss: 0.1900
agent1:                 episode reward: -0.2635,                 loss: nan
Episode: 49661/101000 (49.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3673s / 7730.9148 s
agent0:                 episode reward: 0.1351,                 loss: 0.1893
agent1:                 episode reward: -0.1351,                 loss: nan
Episode: 49681/101000 (49.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9314s / 7734.8461 s
agent0:                 episode reward: -0.0189,                 loss: 0.1886
agent1:                 episode reward: 0.0189,                 loss: nan
Episode: 49701/101000 (49.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3220s / 7738.1681 s
agent0:                 episode reward: -0.2119,                 loss: 0.1905
agent1:                 episode reward: 0.2119,                 loss: 0.1591
Score delta: 1.6641254000026975, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/49260_0.
Episode: 49721/101000 (49.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2822s / 7742.4503 s
agent0:                 episode reward: -0.1018,                 loss: nan
agent1:                 episode reward: 0.1018,                 loss: 0.1576
Episode: 49741/101000 (49.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3951s / 7745.8454 s
agent0:                 episode reward: -0.2800,                 loss: nan
agent1:                 episode reward: 0.2800,                 loss: 0.1591
Episode: 49761/101000 (49.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1281s / 7749.9735 s
agent0:                 episode reward: -0.4301,                 loss: nan
agent1:                 episode reward: 0.4301,                 loss: 0.1563
Episode: 49781/101000 (49.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2582s / 7753.2317 s
agent0:                 episode reward: -0.4062,                 loss: nan
agent1:                 episode reward: 0.4062,                 loss: 0.1554
Score delta: 1.7732571316900878, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/49355_1.
Episode: 49801/101000 (49.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8252s / 7757.0568 s
agent0:                 episode reward: -0.1280,                 loss: 0.1934
agent1:                 episode reward: 0.1280,                 loss: nan
Episode: 49821/101000 (49.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2404s / 7761.2972 s
agent0:                 episode reward: 0.5909,                 loss: 0.1900
agent1:                 episode reward: -0.5909,                 loss: nan
Episode: 49841/101000 (49.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8332s / 7765.1304 s
agent0:                 episode reward: 0.0039,                 loss: 0.1925
agent1:                 episode reward: -0.0039,                 loss: nan
Episode: 49861/101000 (49.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5576s / 7768.6880 s
agent0:                 episode reward: 0.1138,                 loss: 0.1896
agent1:                 episode reward: -0.1138,                 loss: nan
Episode: 49881/101000 (49.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6817s / 7772.3697 s
agent0:                 episode reward: -0.4426,                 loss: 0.1880
agent1:                 episode reward: 0.4426,                 loss: nan
Episode: 49901/101000 (49.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5192s / 7775.8889 s
agent0:                 episode reward: -0.5213,                 loss: 0.1895
agent1:                 episode reward: 0.5213,                 loss: nan
Episode: 49921/101000 (49.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0129s / 7778.9018 s
agent0:                 episode reward: 0.3454,                 loss: 0.1904
agent1:                 episode reward: -0.3454,                 loss: nan
Episode: 49941/101000 (49.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2952s / 7782.1970 s
agent0:                 episode reward: 0.1639,                 loss: 0.1893
agent1:                 episode reward: -0.1639,                 loss: nan
Episode: 49961/101000 (49.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3859s / 7785.5829 s
agent0:                 episode reward: -0.3458,                 loss: 0.1846
agent1:                 episode reward: 0.3458,                 loss: nan
Episode: 49981/101000 (49.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6156s / 7789.1985 s
agent0:                 episode reward: 0.0355,                 loss: 0.1822
agent1:                 episode reward: -0.0355,                 loss: nan
Episode: 50001/101000 (49.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8166s / 7793.0151 s
agent0:                 episode reward: -0.1333,                 loss: 0.1825
agent1:                 episode reward: 0.1333,                 loss: nan
Episode: 50021/101000 (49.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9096s / 7796.9247 s
agent0:                 episode reward: -0.3122,                 loss: 0.1830
agent1:                 episode reward: 0.3122,                 loss: nan
Episode: 50041/101000 (49.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2179s / 7800.1427 s
agent0:                 episode reward: 0.0115,                 loss: 0.1814
agent1:                 episode reward: -0.0115,                 loss: nan
Episode: 50061/101000 (49.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3339s / 7803.4766 s
agent0:                 episode reward: -0.1436,                 loss: 0.1814
agent1:                 episode reward: 0.1436,                 loss: nan
Episode: 50081/101000 (49.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0896s / 7807.5662 s
agent0:                 episode reward: 0.1188,                 loss: 0.1811
agent1:                 episode reward: -0.1188,                 loss: nan
Episode: 50101/101000 (49.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6170s / 7811.1831 s
agent0:                 episode reward: -0.0444,                 loss: 0.1802
agent1:                 episode reward: 0.0444,                 loss: nan
Episode: 50121/101000 (49.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0059s / 7815.1891 s
agent0:                 episode reward: 0.1856,                 loss: 0.1830
agent1:                 episode reward: -0.1856,                 loss: nan
Episode: 50141/101000 (49.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3336s / 7819.5226 s
agent0:                 episode reward: -0.0627,                 loss: 0.1831
agent1:                 episode reward: 0.0627,                 loss: nan
Episode: 50161/101000 (49.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3649s / 7823.8875 s
agent0:                 episode reward: 0.0149,                 loss: 0.1837
agent1:                 episode reward: -0.0149,                 loss: nan
Episode: 50181/101000 (49.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0652s / 7827.9527 s
agent0:                 episode reward: -0.2868,                 loss: 0.1821
agent1:                 episode reward: 0.2868,                 loss: nan
Episode: 50201/101000 (49.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8514s / 7831.8041 s
agent0:                 episode reward: -0.1352,                 loss: 0.1832
agent1:                 episode reward: 0.1352,                 loss: nan
Episode: 50221/101000 (49.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4039s / 7835.2080 s
agent0:                 episode reward: 0.4255,                 loss: 0.1807
agent1:                 episode reward: -0.4255,                 loss: nan
Episode: 50241/101000 (49.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5410s / 7838.7489 s
agent0:                 episode reward: 0.0484,                 loss: 0.1813
agent1:                 episode reward: -0.0484,                 loss: nan
Episode: 50261/101000 (49.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2019s / 7842.9508 s
agent0:                 episode reward: 0.2750,                 loss: 0.1793
agent1:                 episode reward: -0.2750,                 loss: nan
Episode: 50281/101000 (49.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2040s / 7847.1548 s
agent0:                 episode reward: 0.1650,                 loss: 0.1831
agent1:                 episode reward: -0.1650,                 loss: nan
Episode: 50301/101000 (49.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8357s / 7850.9905 s
agent0:                 episode reward: -0.0036,                 loss: 0.1830
agent1:                 episode reward: 0.0036,                 loss: nan
Episode: 50321/101000 (49.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0510s / 7855.0415 s
agent0:                 episode reward: -0.1529,                 loss: 0.1824
agent1:                 episode reward: 0.1529,                 loss: nan
Episode: 50341/101000 (49.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7955s / 7858.8370 s
agent0:                 episode reward: -0.0037,                 loss: 0.1851
agent1:                 episode reward: 0.0037,                 loss: 0.1614
Score delta: 1.641446373501514, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/49902_0.
Episode: 50361/101000 (49.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8266s / 7862.6636 s
agent0:                 episode reward: -0.2541,                 loss: nan
agent1:                 episode reward: 0.2541,                 loss: 0.1588
Episode: 50381/101000 (49.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6230s / 7866.2866 s
agent0:                 episode reward: 0.1136,                 loss: nan
agent1:                 episode reward: -0.1136,                 loss: 0.1580
Episode: 50401/101000 (49.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4061s / 7869.6927 s
agent0:                 episode reward: -0.1555,                 loss: nan
agent1:                 episode reward: 0.1555,                 loss: 0.1591
Episode: 50421/101000 (49.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9445s / 7873.6372 s
agent0:                 episode reward: 0.0150,                 loss: nan
agent1:                 episode reward: -0.0150,                 loss: 0.1562
Episode: 50441/101000 (49.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6482s / 7877.2854 s
agent0:                 episode reward: -0.4299,                 loss: nan
agent1:                 episode reward: 0.4299,                 loss: 0.1572
Episode: 50461/101000 (49.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2687s / 7881.5541 s
agent0:                 episode reward: -0.3367,                 loss: 0.1904
agent1:                 episode reward: 0.3367,                 loss: 0.1553
Score delta: 1.7030314494919245, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/50019_1.
Episode: 50481/101000 (49.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1774s / 7884.7315 s
agent0:                 episode reward: -0.2121,                 loss: 0.1871
agent1:                 episode reward: 0.2121,                 loss: nan
Episode: 50501/101000 (50.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0412s / 7889.7727 s
agent0:                 episode reward: -0.0913,                 loss: 0.1888
agent1:                 episode reward: 0.0913,                 loss: nan
Episode: 50521/101000 (50.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6411s / 7893.4138 s
agent0:                 episode reward: -0.0899,                 loss: 0.1889
agent1:                 episode reward: 0.0899,                 loss: nan
Episode: 50541/101000 (50.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2420s / 7896.6557 s
agent0:                 episode reward: -0.2345,                 loss: 0.1881
agent1:                 episode reward: 0.2345,                 loss: nan
Episode: 50561/101000 (50.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2785s / 7899.9342 s
agent0:                 episode reward: 0.1382,                 loss: 0.1884
agent1:                 episode reward: -0.1382,                 loss: nan
Episode: 50581/101000 (50.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9891s / 7902.9234 s
agent0:                 episode reward: 0.1253,                 loss: 0.1889
agent1:                 episode reward: -0.1253,                 loss: nan
Episode: 50601/101000 (50.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6965s / 7906.6198 s
agent0:                 episode reward: -0.0893,                 loss: 0.1892
agent1:                 episode reward: 0.0893,                 loss: nan
Episode: 50621/101000 (50.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3903s / 7911.0101 s
agent0:                 episode reward: -0.1478,                 loss: 0.1859
agent1:                 episode reward: 0.1478,                 loss: nan
Episode: 50641/101000 (50.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6819s / 7915.6920 s
agent0:                 episode reward: -0.0806,                 loss: 0.1861
agent1:                 episode reward: 0.0806,                 loss: nan
Episode: 50661/101000 (50.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4335s / 7919.1255 s
agent0:                 episode reward: 0.2203,                 loss: 0.1864
agent1:                 episode reward: -0.2203,                 loss: nan
Episode: 50681/101000 (50.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6913s / 7922.8169 s
agent0:                 episode reward: -0.2164,                 loss: 0.1870
agent1:                 episode reward: 0.2164,                 loss: nan
Episode: 50701/101000 (50.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7241s / 7926.5409 s
agent0:                 episode reward: -0.0022,                 loss: 0.1882
agent1:                 episode reward: 0.0022,                 loss: nan
Episode: 50721/101000 (50.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8641s / 7929.4050 s
agent0:                 episode reward: -0.0657,                 loss: 0.1896
agent1:                 episode reward: 0.0657,                 loss: nan
Episode: 50741/101000 (50.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9952s / 7934.4002 s
agent0:                 episode reward: -0.1924,                 loss: 0.1837
agent1:                 episode reward: 0.1924,                 loss: nan
Episode: 50761/101000 (50.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1794s / 7938.5796 s
agent0:                 episode reward: -0.2650,                 loss: 0.1793
agent1:                 episode reward: 0.2650,                 loss: nan
Episode: 50781/101000 (50.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8532s / 7942.4328 s
agent0:                 episode reward: 0.1902,                 loss: 0.1797
agent1:                 episode reward: -0.1902,                 loss: nan
Episode: 50801/101000 (50.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3756s / 7945.8084 s
agent0:                 episode reward: -0.1499,                 loss: 0.1770
agent1:                 episode reward: 0.1499,                 loss: nan
Episode: 50821/101000 (50.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0186s / 7949.8270 s
agent0:                 episode reward: -0.3606,                 loss: 0.1789
agent1:                 episode reward: 0.3606,                 loss: nan
Episode: 50841/101000 (50.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0110s / 7952.8380 s
agent0:                 episode reward: -0.0146,                 loss: 0.1782
agent1:                 episode reward: 0.0146,                 loss: nan
Episode: 50861/101000 (50.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7921s / 7956.6301 s
agent0:                 episode reward: -0.0241,                 loss: 0.1774
agent1:                 episode reward: 0.0241,                 loss: nan
Episode: 50881/101000 (50.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0627s / 7960.6928 s
agent0:                 episode reward: 0.3254,                 loss: 0.1774
agent1:                 episode reward: -0.3254,                 loss: nan
Episode: 50901/101000 (50.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4149s / 7964.1077 s
agent0:                 episode reward: -0.2498,                 loss: 0.1801
agent1:                 episode reward: 0.2498,                 loss: nan
Episode: 50921/101000 (50.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1359s / 7967.2436 s
agent0:                 episode reward: 0.1490,                 loss: 0.1784
agent1:                 episode reward: -0.1490,                 loss: nan
Episode: 50941/101000 (50.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4514s / 7970.6950 s
agent0:                 episode reward: 0.1873,                 loss: 0.1794
agent1:                 episode reward: -0.1873,                 loss: nan
Episode: 50961/101000 (50.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0108s / 7973.7058 s
agent0:                 episode reward: -0.3857,                 loss: 0.1779
agent1:                 episode reward: 0.3857,                 loss: nan
Episode: 50981/101000 (50.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8690s / 7977.5747 s
agent0:                 episode reward: 0.1263,                 loss: 0.1770
agent1:                 episode reward: -0.1263,                 loss: nan
Episode: 51001/101000 (50.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4846s / 7981.0594 s
agent0:                 episode reward: -0.1824,                 loss: 0.1782
agent1:                 episode reward: 0.1824,                 loss: nan
Episode: 51021/101000 (50.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7600s / 7984.8194 s
agent0:                 episode reward: -0.0792,                 loss: 0.1779
agent1:                 episode reward: 0.0792,                 loss: nan
Episode: 51041/101000 (50.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4591s / 7988.2785 s
agent0:                 episode reward: -0.0029,                 loss: 0.1792
agent1:                 episode reward: 0.0029,                 loss: nan
Episode: 51061/101000 (50.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7431s / 7993.0216 s
agent0:                 episode reward: -0.1304,                 loss: 0.1778
agent1:                 episode reward: 0.1304,                 loss: nan
Episode: 51081/101000 (50.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3927s / 7996.4144 s
agent0:                 episode reward: -0.3858,                 loss: 0.1765
agent1:                 episode reward: 0.3858,                 loss: nan
Episode: 51101/101000 (50.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4398s / 8000.8542 s
agent0:                 episode reward: 0.2528,                 loss: 0.1756
agent1:                 episode reward: -0.2528,                 loss: 0.1445
Score delta: 1.9103434061446911, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/50664_0.
Episode: 51121/101000 (50.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9010s / 8003.7552 s
agent0:                 episode reward: -0.0333,                 loss: nan
agent1:                 episode reward: 0.0333,                 loss: 0.1456
Episode: 51141/101000 (50.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0474s / 8006.8026 s
agent0:                 episode reward: -0.1952,                 loss: nan
agent1:                 episode reward: 0.1952,                 loss: 0.1492
Episode: 51161/101000 (50.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3358s / 8010.1384 s
agent0:                 episode reward: -0.0987,                 loss: nan
agent1:                 episode reward: 0.0987,                 loss: 0.1468
Episode: 51181/101000 (50.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6774s / 8013.8158 s
agent0:                 episode reward: 0.0706,                 loss: nan
agent1:                 episode reward: -0.0706,                 loss: 0.1485
Episode: 51201/101000 (50.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1597s / 8016.9755 s
agent0:                 episode reward: -0.1581,                 loss: nan
agent1:                 episode reward: 0.1581,                 loss: 0.1443
Episode: 51221/101000 (50.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2851s / 8021.2606 s
agent0:                 episode reward: -0.4731,                 loss: nan
agent1:                 episode reward: 0.4731,                 loss: 0.1465
Score delta: 1.5184526568641263, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/50795_1.
Episode: 51241/101000 (50.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7090s / 8025.9696 s
agent0:                 episode reward: -0.6197,                 loss: 0.2064
agent1:                 episode reward: 0.6197,                 loss: nan
Episode: 51261/101000 (50.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7883s / 8029.7578 s
agent0:                 episode reward: 0.0697,                 loss: 0.2015
agent1:                 episode reward: -0.0697,                 loss: nan
Episode: 51281/101000 (50.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9108s / 8033.6687 s
agent0:                 episode reward: -0.0288,                 loss: 0.2011
agent1:                 episode reward: 0.0288,                 loss: nan
Episode: 51301/101000 (50.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1630s / 8036.8317 s
agent0:                 episode reward: -0.1495,                 loss: 0.2018
agent1:                 episode reward: 0.1495,                 loss: nan
Episode: 51321/101000 (50.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6483s / 8040.4799 s
agent0:                 episode reward: -0.0477,                 loss: 0.2013
agent1:                 episode reward: 0.0477,                 loss: nan
Episode: 51341/101000 (50.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1116s / 8044.5915 s
agent0:                 episode reward: -0.2769,                 loss: 0.2025
agent1:                 episode reward: 0.2769,                 loss: nan
Episode: 51361/101000 (50.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7811s / 8048.3726 s
agent0:                 episode reward: -0.1239,                 loss: 0.1995
agent1:                 episode reward: 0.1239,                 loss: nan
Episode: 51381/101000 (50.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4462s / 8051.8189 s
agent0:                 episode reward: -0.2432,                 loss: 0.2000
agent1:                 episode reward: 0.2432,                 loss: nan
Episode: 51401/101000 (50.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5597s / 8055.3786 s
agent0:                 episode reward: -0.2438,                 loss: 0.1982
agent1:                 episode reward: 0.2438,                 loss: nan
Episode: 51421/101000 (50.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0094s / 8059.3880 s
agent0:                 episode reward: -0.0229,                 loss: 0.1983
agent1:                 episode reward: 0.0229,                 loss: nan
Episode: 51441/101000 (50.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2916s / 8062.6795 s
agent0:                 episode reward: -0.2394,                 loss: 0.2000
agent1:                 episode reward: 0.2394,                 loss: nan
Episode: 51461/101000 (50.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8997s / 8066.5793 s
agent0:                 episode reward: -0.1590,                 loss: 0.1997
agent1:                 episode reward: 0.1590,                 loss: nan
Episode: 51481/101000 (50.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8680s / 8070.4473 s
agent0:                 episode reward: -0.0467,                 loss: 0.1979
agent1:                 episode reward: 0.0467,                 loss: nan
Episode: 51501/101000 (50.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5829s / 8074.0302 s
agent0:                 episode reward: -0.0618,                 loss: 0.1975
agent1:                 episode reward: 0.0618,                 loss: nan
Episode: 51521/101000 (51.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3695s / 8077.3996 s
agent0:                 episode reward: -0.0425,                 loss: 0.2015
agent1:                 episode reward: 0.0425,                 loss: nan
Episode: 51541/101000 (51.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9100s / 8082.3097 s
agent0:                 episode reward: 0.1946,                 loss: 0.1982
agent1:                 episode reward: -0.1946,                 loss: nan
Episode: 51561/101000 (51.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5140s / 8085.8236 s
agent0:                 episode reward: 0.1313,                 loss: 0.1905
agent1:                 episode reward: -0.1313,                 loss: 0.2058
Score delta: 1.5381464266865736, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/51127_0.
Episode: 51581/101000 (51.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6747s / 8089.4983 s
agent0:                 episode reward: -0.2819,                 loss: nan
agent1:                 episode reward: 0.2819,                 loss: 0.1776
Episode: 51601/101000 (51.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4600s / 8093.9584 s
agent0:                 episode reward: -0.2826,                 loss: nan
agent1:                 episode reward: 0.2826,                 loss: 0.1720
Episode: 51621/101000 (51.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9288s / 8096.8872 s
agent0:                 episode reward: 0.0330,                 loss: nan
agent1:                 episode reward: -0.0330,                 loss: 0.1662
Episode: 51641/101000 (51.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5030s / 8100.3902 s
agent0:                 episode reward: -0.2051,                 loss: 0.1913
agent1:                 episode reward: 0.2051,                 loss: 0.1616
Score delta: 1.6380078660543531, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/51208_1.
Episode: 51661/101000 (51.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9023s / 8104.2925 s
agent0:                 episode reward: 0.1027,                 loss: 0.1896
agent1:                 episode reward: -0.1027,                 loss: nan
Episode: 51681/101000 (51.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5626s / 8107.8551 s
agent0:                 episode reward: -0.1511,                 loss: 0.1886
agent1:                 episode reward: 0.1511,                 loss: nan
Episode: 51701/101000 (51.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0309s / 8111.8860 s
agent0:                 episode reward: 0.2305,                 loss: 0.1870
agent1:                 episode reward: -0.2305,                 loss: nan
Episode: 51721/101000 (51.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3657s / 8116.2517 s
agent0:                 episode reward: 0.0413,                 loss: 0.1882
agent1:                 episode reward: -0.0413,                 loss: nan
Episode: 51741/101000 (51.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8196s / 8120.0713 s
agent0:                 episode reward: 0.3430,                 loss: 0.1865
agent1:                 episode reward: -0.3430,                 loss: nan
Episode: 51761/101000 (51.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8722s / 8123.9435 s
agent0:                 episode reward: -0.1662,                 loss: 0.1874
agent1:                 episode reward: 0.1662,                 loss: nan
Episode: 51781/101000 (51.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9775s / 8127.9210 s
agent0:                 episode reward: 0.2514,                 loss: 0.1861
agent1:                 episode reward: -0.2514,                 loss: nan
Episode: 51801/101000 (51.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4479s / 8132.3689 s
agent0:                 episode reward: -0.0591,                 loss: 0.1872
agent1:                 episode reward: 0.0591,                 loss: nan
Episode: 51821/101000 (51.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6766s / 8137.0454 s
agent0:                 episode reward: -0.0577,                 loss: 0.1857
agent1:                 episode reward: 0.0577,                 loss: nan
Episode: 51841/101000 (51.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2378s / 8141.2832 s
agent0:                 episode reward: -0.1108,                 loss: 0.1874
agent1:                 episode reward: 0.1108,                 loss: nan
Episode: 51861/101000 (51.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9411s / 8145.2243 s
agent0:                 episode reward: 0.1605,                 loss: 0.1890
agent1:                 episode reward: -0.1605,                 loss: nan
Episode: 51881/101000 (51.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4750s / 8148.6993 s
agent0:                 episode reward: 0.0122,                 loss: 0.1887
agent1:                 episode reward: -0.0122,                 loss: nan
Episode: 51901/101000 (51.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6307s / 8152.3300 s
agent0:                 episode reward: 0.3297,                 loss: 0.1869
agent1:                 episode reward: -0.3297,                 loss: 0.1584
Score delta: 1.6337494976002485, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/51474_0.
Episode: 51921/101000 (51.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0333s / 8155.3633 s
agent0:                 episode reward: -0.4495,                 loss: nan
agent1:                 episode reward: 0.4495,                 loss: 0.1587
Episode: 51941/101000 (51.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8672s / 8159.2304 s
agent0:                 episode reward: -0.4410,                 loss: nan
agent1:                 episode reward: 0.4410,                 loss: 0.1578
Episode: 51961/101000 (51.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6541s / 8162.8845 s
agent0:                 episode reward: -0.2528,                 loss: nan
agent1:                 episode reward: 0.2528,                 loss: 0.1573
Episode: 51981/101000 (51.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4896s / 8166.3741 s
agent0:                 episode reward: -0.0844,                 loss: nan
agent1:                 episode reward: 0.0844,                 loss: 0.1579
Episode: 52001/101000 (51.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0106s / 8170.3847 s
agent0:                 episode reward: -0.6367,                 loss: nan
agent1:                 episode reward: 0.6367,                 loss: 0.1562
Episode: 52021/101000 (51.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7679s / 8174.1526 s
agent0:                 episode reward: -0.3574,                 loss: 0.1924
agent1:                 episode reward: 0.3574,                 loss: 0.1538
Score delta: 1.693872538227631, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/51578_1.
Episode: 52041/101000 (51.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1705s / 8178.3232 s
agent0:                 episode reward: 0.2325,                 loss: 0.1904
agent1:                 episode reward: -0.2325,                 loss: nan
Episode: 52061/101000 (51.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9299s / 8182.2530 s
agent0:                 episode reward: 0.1837,                 loss: 0.1877
agent1:                 episode reward: -0.1837,                 loss: nan
Episode: 52081/101000 (51.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5199s / 8185.7729 s
agent0:                 episode reward: -0.1438,                 loss: 0.1832
agent1:                 episode reward: 0.1438,                 loss: nan
Episode: 52101/101000 (51.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6158s / 8189.3888 s
agent0:                 episode reward: 0.0391,                 loss: 0.1838
agent1:                 episode reward: -0.0391,                 loss: nan
Episode: 52121/101000 (51.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1022s / 8194.4909 s
agent0:                 episode reward: -0.0015,                 loss: 0.1828
agent1:                 episode reward: 0.0015,                 loss: nan
Episode: 52141/101000 (51.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3277s / 8198.8186 s
agent0:                 episode reward: 0.2626,                 loss: 0.1843
agent1:                 episode reward: -0.2626,                 loss: nan
Episode: 52161/101000 (51.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1828s / 8202.0015 s
agent0:                 episode reward: 0.2515,                 loss: 0.1835
agent1:                 episode reward: -0.2515,                 loss: nan
Episode: 52181/101000 (51.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0803s / 8206.0818 s
agent0:                 episode reward: 0.3641,                 loss: 0.1828
agent1:                 episode reward: -0.3641,                 loss: nan
Episode: 52201/101000 (51.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1290s / 8209.2108 s
agent0:                 episode reward: 0.0359,                 loss: 0.1781
agent1:                 episode reward: -0.0359,                 loss: 0.1497
Score delta: 1.5261883340225357, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/51764_0.
Episode: 52221/101000 (51.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7945s / 8213.0054 s
agent0:                 episode reward: 0.1092,                 loss: nan
agent1:                 episode reward: -0.1092,                 loss: 0.1466
Episode: 52241/101000 (51.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4543s / 8216.4596 s
agent0:                 episode reward: 0.0983,                 loss: nan
agent1:                 episode reward: -0.0983,                 loss: 0.1514
Episode: 52261/101000 (51.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6024s / 8220.0620 s
agent0:                 episode reward: -0.0581,                 loss: nan
agent1:                 episode reward: 0.0581,                 loss: 0.1574
Episode: 52281/101000 (51.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4463s / 8223.5083 s
agent0:                 episode reward: -0.1523,                 loss: nan
agent1:                 episode reward: 0.1523,                 loss: 0.1520
Episode: 52301/101000 (51.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8515s / 8228.3599 s
agent0:                 episode reward: -0.6146,                 loss: 0.1889
agent1:                 episode reward: 0.6146,                 loss: 0.1543
Score delta: 1.6080870644521437, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/51866_1.
Episode: 52321/101000 (51.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6621s / 8232.0219 s
agent0:                 episode reward: -0.1211,                 loss: 0.1884
agent1:                 episode reward: 0.1211,                 loss: nan
Episode: 52341/101000 (51.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0368s / 8235.0587 s
agent0:                 episode reward: 0.0410,                 loss: 0.1889
agent1:                 episode reward: -0.0410,                 loss: nan
Episode: 52361/101000 (51.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4089s / 8239.4676 s
agent0:                 episode reward: -0.1862,                 loss: 0.1876
agent1:                 episode reward: 0.1862,                 loss: nan
Episode: 52381/101000 (51.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5544s / 8244.0221 s
agent0:                 episode reward: -0.2173,                 loss: 0.1891
agent1:                 episode reward: 0.2173,                 loss: nan
Episode: 52401/101000 (51.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8386s / 8247.8606 s
agent0:                 episode reward: -0.4300,                 loss: 0.1873
agent1:                 episode reward: 0.4300,                 loss: nan
Episode: 52421/101000 (51.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1715s / 8252.0321 s
agent0:                 episode reward: -0.0611,                 loss: 0.1866
agent1:                 episode reward: 0.0611,                 loss: nan
Episode: 52441/101000 (51.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1558s / 8256.1880 s
agent0:                 episode reward: -0.0994,                 loss: 0.1870
agent1:                 episode reward: 0.0994,                 loss: nan
Episode: 52461/101000 (51.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2575s / 8259.4455 s
agent0:                 episode reward: 0.0026,                 loss: 0.1862
agent1:                 episode reward: -0.0026,                 loss: nan
Episode: 52481/101000 (51.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6682s / 8263.1137 s
agent0:                 episode reward: -0.3092,                 loss: 0.1881
agent1:                 episode reward: 0.3092,                 loss: nan
Episode: 52501/101000 (51.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1077s / 8267.2214 s
agent0:                 episode reward: 0.2135,                 loss: 0.1777
agent1:                 episode reward: -0.2135,                 loss: nan
Episode: 52521/101000 (52.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1588s / 8270.3802 s
agent0:                 episode reward: -0.0020,                 loss: 0.1738
agent1:                 episode reward: 0.0020,                 loss: nan
Episode: 52541/101000 (52.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6555s / 8274.0357 s
agent0:                 episode reward: 0.2909,                 loss: 0.1751
agent1:                 episode reward: -0.2909,                 loss: nan
Episode: 52561/101000 (52.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7691s / 8277.8048 s
agent0:                 episode reward: 0.0618,                 loss: 0.1741
agent1:                 episode reward: -0.0618,                 loss: nan
Episode: 52581/101000 (52.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3258s / 8282.1306 s
agent0:                 episode reward: 0.0859,                 loss: 0.1751
agent1:                 episode reward: -0.0859,                 loss: nan
Episode: 52601/101000 (52.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0594s / 8287.1900 s
agent0:                 episode reward: -0.0517,                 loss: 0.1750
agent1:                 episode reward: 0.0517,                 loss: nan
Episode: 52621/101000 (52.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3353s / 8291.5253 s
agent0:                 episode reward: -0.1986,                 loss: 0.1746
agent1:                 episode reward: 0.1986,                 loss: nan
Episode: 52641/101000 (52.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3453s / 8295.8706 s
agent0:                 episode reward: 0.1929,                 loss: 0.1744
agent1:                 episode reward: -0.1929,                 loss: nan
Episode: 52661/101000 (52.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6357s / 8299.5063 s
agent0:                 episode reward: -0.4562,                 loss: 0.1746
agent1:                 episode reward: 0.4562,                 loss: nan
Episode: 52681/101000 (52.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9348s / 8303.4411 s
agent0:                 episode reward: 0.2606,                 loss: 0.1753
agent1:                 episode reward: -0.2606,                 loss: 0.1438
Score delta: 1.616701929651922, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/52253_0.
Episode: 52701/101000 (52.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8498s / 8306.2910 s
agent0:                 episode reward: -0.3231,                 loss: nan
agent1:                 episode reward: 0.3231,                 loss: 0.1468
Episode: 52721/101000 (52.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7368s / 8310.0278 s
agent0:                 episode reward: -0.2398,                 loss: nan
agent1:                 episode reward: 0.2398,                 loss: 0.1456
Episode: 52741/101000 (52.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3117s / 8313.3394 s
agent0:                 episode reward: -0.2845,                 loss: nan
agent1:                 episode reward: 0.2845,                 loss: 0.1445
Episode: 52761/101000 (52.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9591s / 8317.2986 s
agent0:                 episode reward: -0.2516,                 loss: nan
agent1:                 episode reward: 0.2516,                 loss: 0.1442
Episode: 52781/101000 (52.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1325s / 8320.4311 s
agent0:                 episode reward: 0.2087,                 loss: nan
agent1:                 episode reward: -0.2087,                 loss: 0.1440
Episode: 52801/101000 (52.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5750s / 8324.0061 s
agent0:                 episode reward: -0.5808,                 loss: 0.1915
agent1:                 episode reward: 0.5808,                 loss: 0.1437
Score delta: 1.6671139628322575, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/52363_1.
Episode: 52821/101000 (52.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1970s / 8328.2031 s
agent0:                 episode reward: -0.3790,                 loss: 0.1866
agent1:                 episode reward: 0.3790,                 loss: nan
Episode: 52841/101000 (52.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1926s / 8331.3957 s
agent0:                 episode reward: -0.0982,                 loss: 0.1876
agent1:                 episode reward: 0.0982,                 loss: nan
Episode: 52861/101000 (52.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4259s / 8334.8215 s
agent0:                 episode reward: -0.0782,                 loss: 0.1874
agent1:                 episode reward: 0.0782,                 loss: nan
Episode: 52881/101000 (52.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8645s / 8338.6860 s
agent0:                 episode reward: -0.3651,                 loss: 0.1881
agent1:                 episode reward: 0.3651,                 loss: nan
Episode: 52901/101000 (52.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5404s / 8342.2265 s
agent0:                 episode reward: -0.1289,                 loss: 0.1877
agent1:                 episode reward: 0.1289,                 loss: nan
Episode: 52921/101000 (52.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7068s / 8345.9333 s
agent0:                 episode reward: -0.1129,                 loss: 0.1883
agent1:                 episode reward: 0.1129,                 loss: nan
Episode: 52941/101000 (52.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4127s / 8350.3460 s
agent0:                 episode reward: 0.2053,                 loss: 0.1855
agent1:                 episode reward: -0.2053,                 loss: 0.1431
Score delta: 1.5496019984132414, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/52509_0.
Episode: 52961/101000 (52.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4147s / 8354.7607 s
agent0:                 episode reward: -0.3784,                 loss: nan
agent1:                 episode reward: 0.3784,                 loss: 0.1460
Episode: 52981/101000 (52.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2600s / 8358.0207 s
agent0:                 episode reward: -0.5164,                 loss: nan
agent1:                 episode reward: 0.5164,                 loss: 0.1456
Episode: 53001/101000 (52.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2048s / 8361.2256 s
agent0:                 episode reward: -0.4121,                 loss: nan
agent1:                 episode reward: 0.4121,                 loss: 0.1442
Episode: 53021/101000 (52.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4281s / 8364.6537 s
agent0:                 episode reward: -0.0744,                 loss: nan
agent1:                 episode reward: 0.0744,                 loss: 0.1439
Episode: 53041/101000 (52.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0650s / 8368.7186 s
agent0:                 episode reward: -0.2115,                 loss: nan
agent1:                 episode reward: 0.2115,                 loss: 0.1442
Episode: 53061/101000 (52.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9425s / 8371.6612 s
agent0:                 episode reward: -0.1814,                 loss: nan
agent1:                 episode reward: 0.1814,                 loss: 0.1439
Episode: 53081/101000 (52.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1165s / 8375.7776 s
agent0:                 episode reward: -0.7056,                 loss: 0.2329
agent1:                 episode reward: 0.7056,                 loss: 0.1446
Score delta: 1.6232784203872654, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/52639_1.
Episode: 53101/101000 (52.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1805s / 8378.9581 s
agent0:                 episode reward: -0.1872,                 loss: 0.2172
agent1:                 episode reward: 0.1872,                 loss: nan
Episode: 53121/101000 (52.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1071s / 8383.0652 s
agent0:                 episode reward: -0.2762,                 loss: 0.2138
agent1:                 episode reward: 0.2762,                 loss: nan
Episode: 53141/101000 (52.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4813s / 8387.5465 s
agent0:                 episode reward: -0.2474,                 loss: 0.2131
agent1:                 episode reward: 0.2474,                 loss: nan
Episode: 53161/101000 (52.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3553s / 8390.9018 s
agent0:                 episode reward: -0.4331,                 loss: 0.2136
agent1:                 episode reward: 0.4331,                 loss: nan
Episode: 53181/101000 (52.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8382s / 8394.7400 s
agent0:                 episode reward: -0.4260,                 loss: 0.2124
agent1:                 episode reward: 0.4260,                 loss: nan
Episode: 53201/101000 (52.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5511s / 8398.2911 s
agent0:                 episode reward: -0.3307,                 loss: 0.2124
agent1:                 episode reward: 0.3307,                 loss: nan
Episode: 53221/101000 (52.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6457s / 8401.9368 s
agent0:                 episode reward: -0.4005,                 loss: 0.2112
agent1:                 episode reward: 0.4005,                 loss: nan
Episode: 53241/101000 (52.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3378s / 8405.2746 s
agent0:                 episode reward: -0.0389,                 loss: 0.2092
agent1:                 episode reward: 0.0389,                 loss: nan
Episode: 53261/101000 (52.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1490s / 8409.4235 s
agent0:                 episode reward: -0.1928,                 loss: 0.2111
agent1:                 episode reward: 0.1928,                 loss: nan
Episode: 53281/101000 (52.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2905s / 8412.7140 s
agent0:                 episode reward: -0.2005,                 loss: 0.2114
agent1:                 episode reward: 0.2005,                 loss: nan
Episode: 53301/101000 (52.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4085s / 8416.1225 s
agent0:                 episode reward: -0.1010,                 loss: 0.2111
agent1:                 episode reward: 0.1010,                 loss: nan
Episode: 53321/101000 (52.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3052s / 8420.4276 s
agent0:                 episode reward: 0.2258,                 loss: 0.2117
agent1:                 episode reward: -0.2258,                 loss: nan
Episode: 53341/101000 (52.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7948s / 8424.2224 s
agent0:                 episode reward: 0.1168,                 loss: 0.2092
agent1:                 episode reward: -0.1168,                 loss: nan
Episode: 53361/101000 (52.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8861s / 8428.1085 s
agent0:                 episode reward: 0.2892,                 loss: 0.2100
agent1:                 episode reward: -0.2892,                 loss: nan
Episode: 53381/101000 (52.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6900s / 8431.7984 s
agent0:                 episode reward: 0.2285,                 loss: 0.2088
agent1:                 episode reward: -0.2285,                 loss: nan
Episode: 53401/101000 (52.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0675s / 8434.8659 s
agent0:                 episode reward: 0.2436,                 loss: 0.2105
agent1:                 episode reward: -0.2436,                 loss: nan
Episode: 53421/101000 (52.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5225s / 8439.3884 s
agent0:                 episode reward: 0.4361,                 loss: 0.2059
agent1:                 episode reward: -0.4361,                 loss: 0.1501
Score delta: 1.7270555071521216, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/52991_0.
Episode: 53441/101000 (52.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3270s / 8442.7155 s
agent0:                 episode reward: -0.6546,                 loss: nan
agent1:                 episode reward: 0.6546,                 loss: 0.1496
Episode: 53461/101000 (52.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6003s / 8446.3158 s
agent0:                 episode reward: -0.5179,                 loss: nan
agent1:                 episode reward: 0.5179,                 loss: 0.1505
Episode: 53481/101000 (52.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1509s / 8449.4667 s
agent0:                 episode reward: 0.1618,                 loss: nan
agent1:                 episode reward: -0.1618,                 loss: 0.1486
Episode: 53501/101000 (52.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6129s / 8453.0797 s
agent0:                 episode reward: -0.6447,                 loss: 0.1895
agent1:                 episode reward: 0.6447,                 loss: 0.1475
Score delta: 1.6136658025922734, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/53071_1.
Episode: 53521/101000 (52.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6339s / 8456.7136 s
agent0:                 episode reward: -0.3724,                 loss: 0.1879
agent1:                 episode reward: 0.3724,                 loss: nan
Episode: 53541/101000 (53.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0745s / 8459.7881 s
agent0:                 episode reward: 0.0328,                 loss: 0.1879
agent1:                 episode reward: -0.0328,                 loss: nan
Episode: 53561/101000 (53.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6803s / 8463.4684 s
agent0:                 episode reward: -0.4094,                 loss: 0.1875
agent1:                 episode reward: 0.4094,                 loss: nan
Episode: 53581/101000 (53.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4262s / 8466.8946 s
agent0:                 episode reward: -0.6286,                 loss: 0.1879
agent1:                 episode reward: 0.6286,                 loss: nan
Episode: 53601/101000 (53.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3979s / 8471.2926 s
agent0:                 episode reward: -0.5309,                 loss: 0.1864
agent1:                 episode reward: 0.5309,                 loss: nan
Episode: 53621/101000 (53.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9801s / 8475.2727 s
agent0:                 episode reward: 0.1235,                 loss: 0.1859
agent1:                 episode reward: -0.1235,                 loss: nan
Episode: 53641/101000 (53.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7532s / 8480.0260 s
agent0:                 episode reward: 0.0651,                 loss: 0.1884
agent1:                 episode reward: -0.0651,                 loss: nan
Episode: 53661/101000 (53.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9126s / 8483.9385 s
agent0:                 episode reward: 0.2190,                 loss: 0.1871
agent1:                 episode reward: -0.2190,                 loss: nan
Episode: 53681/101000 (53.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4739s / 8488.4124 s
agent0:                 episode reward: -0.1045,                 loss: 0.1892
agent1:                 episode reward: 0.1045,                 loss: nan
Episode: 53701/101000 (53.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0712s / 8492.4836 s
agent0:                 episode reward: 0.2272,                 loss: 0.1871
agent1:                 episode reward: -0.2272,                 loss: 0.1453
Score delta: 1.599873604138787, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/53266_0.
Episode: 53721/101000 (53.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0388s / 8495.5224 s
agent0:                 episode reward: -0.7268,                 loss: nan
agent1:                 episode reward: 0.7268,                 loss: 0.1466
Episode: 53741/101000 (53.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8447s / 8498.3671 s
agent0:                 episode reward: -0.3168,                 loss: nan
agent1:                 episode reward: 0.3168,                 loss: 0.1473
Episode: 53761/101000 (53.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 2.8965s / 8501.2636 s
agent0:                 episode reward: -0.3026,                 loss: nan
agent1:                 episode reward: 0.3026,                 loss: 0.1440
Episode: 53781/101000 (53.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2131s / 8505.4767 s
agent0:                 episode reward: -0.5150,                 loss: 0.2029
agent1:                 episode reward: 0.5150,                 loss: 0.1451
Score delta: 1.6566642135241179, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/53351_1.
Episode: 53801/101000 (53.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2233s / 8508.7000 s
agent0:                 episode reward: 0.2171,                 loss: 0.1970
agent1:                 episode reward: -0.2171,                 loss: nan
Episode: 53821/101000 (53.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8903s / 8512.5902 s
agent0:                 episode reward: 0.1839,                 loss: 0.1948
agent1:                 episode reward: -0.1839,                 loss: nan
Episode: 53841/101000 (53.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3124s / 8516.9026 s
agent0:                 episode reward: -0.3703,                 loss: 0.1935
agent1:                 episode reward: 0.3703,                 loss: nan
Episode: 53861/101000 (53.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1068s / 8522.0094 s
agent0:                 episode reward: -0.1536,                 loss: 0.1945
agent1:                 episode reward: 0.1536,                 loss: nan
Episode: 53881/101000 (53.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5160s / 8525.5255 s
agent0:                 episode reward: -0.0787,                 loss: 0.1931
agent1:                 episode reward: 0.0787,                 loss: nan
Episode: 53901/101000 (53.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1775s / 8529.7029 s
agent0:                 episode reward: 0.0544,                 loss: 0.1870
agent1:                 episode reward: -0.0544,                 loss: nan
Episode: 53921/101000 (53.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2839s / 8532.9868 s
agent0:                 episode reward: 0.1142,                 loss: 0.1827
agent1:                 episode reward: -0.1142,                 loss: nan
Episode: 53941/101000 (53.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8265s / 8536.8133 s
agent0:                 episode reward: 0.3755,                 loss: 0.1810
agent1:                 episode reward: -0.3755,                 loss: nan
Episode: 53961/101000 (53.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2254s / 8540.0388 s
agent0:                 episode reward: -0.6304,                 loss: 0.1860
agent1:                 episode reward: 0.6304,                 loss: 0.1538
Score delta: 1.5163356797979093, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/53516_0.
Episode: 53981/101000 (53.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0554s / 8544.0942 s
agent0:                 episode reward: -0.1217,                 loss: nan
agent1:                 episode reward: 0.1217,                 loss: 0.1546
Episode: 54001/101000 (53.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4143s / 8548.5085 s
agent0:                 episode reward: -0.0800,                 loss: nan
agent1:                 episode reward: 0.0800,                 loss: 0.1546
Episode: 54021/101000 (53.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1884s / 8551.6969 s
agent0:                 episode reward: -0.0162,                 loss: nan
agent1:                 episode reward: 0.0162,                 loss: 0.1548
Episode: 54041/101000 (53.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3343s / 8555.0313 s
agent0:                 episode reward: -0.7023,                 loss: 0.2077
agent1:                 episode reward: 0.7023,                 loss: 0.1550
Score delta: 1.5110629064983792, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/53605_1.
Episode: 54061/101000 (53.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7527s / 8558.7840 s
agent0:                 episode reward: 0.2650,                 loss: 0.1987
agent1:                 episode reward: -0.2650,                 loss: nan
Episode: 54081/101000 (53.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5549s / 8563.3389 s
agent0:                 episode reward: 0.2486,                 loss: 0.1989
agent1:                 episode reward: -0.2486,                 loss: nan
Episode: 54101/101000 (53.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1928s / 8567.5317 s
agent0:                 episode reward: -0.1996,                 loss: 0.1998
agent1:                 episode reward: 0.1996,                 loss: nan
Episode: 54121/101000 (53.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7571s / 8571.2888 s
agent0:                 episode reward: 0.1964,                 loss: 0.1990
agent1:                 episode reward: -0.1964,                 loss: nan
Episode: 54141/101000 (53.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9866s / 8575.2754 s
agent0:                 episode reward: 0.2465,                 loss: 0.1983
agent1:                 episode reward: -0.2465,                 loss: nan
Episode: 54161/101000 (53.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4648s / 8579.7402 s
agent0:                 episode reward: 0.4614,                 loss: 0.1964
agent1:                 episode reward: -0.4614,                 loss: nan
Episode: 54181/101000 (53.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0299s / 8583.7700 s
agent0:                 episode reward: 0.0749,                 loss: 0.1888
agent1:                 episode reward: -0.0749,                 loss: 0.1601
Score delta: 1.5181973983165866, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/53737_0.
Episode: 54201/101000 (53.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1059s / 8587.8760 s
agent0:                 episode reward: -0.6005,                 loss: nan
agent1:                 episode reward: 0.6005,                 loss: 0.1574
Episode: 54221/101000 (53.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5204s / 8591.3964 s
agent0:                 episode reward: -0.5516,                 loss: nan
agent1:                 episode reward: 0.5516,                 loss: 0.1564
Episode: 54241/101000 (53.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0586s / 8594.4550 s
agent0:                 episode reward: -0.3302,                 loss: nan
agent1:                 episode reward: 0.3302,                 loss: 0.1547
Episode: 54261/101000 (53.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9613s / 8598.4163 s
agent0:                 episode reward: -0.0302,                 loss: 0.1968
agent1:                 episode reward: 0.0302,                 loss: 0.1575
Score delta: 1.557168397602657, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/53820_1.
Episode: 54281/101000 (53.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1502s / 8602.5665 s
agent0:                 episode reward: -0.0917,                 loss: 0.1934
agent1:                 episode reward: 0.0917,                 loss: nan
Episode: 54301/101000 (53.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3244s / 8605.8909 s
agent0:                 episode reward: -0.0364,                 loss: 0.1907
agent1:                 episode reward: 0.0364,                 loss: nan
Episode: 54321/101000 (53.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6672s / 8609.5581 s
agent0:                 episode reward: -0.2583,                 loss: 0.1889
agent1:                 episode reward: 0.2583,                 loss: nan
Episode: 54341/101000 (53.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6802s / 8613.2383 s
agent0:                 episode reward: -0.1722,                 loss: 0.1890
agent1:                 episode reward: 0.1722,                 loss: nan
Episode: 54361/101000 (53.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7214s / 8616.9597 s
agent0:                 episode reward: -0.3540,                 loss: 0.1886
agent1:                 episode reward: 0.3540,                 loss: nan
Episode: 54381/101000 (53.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9102s / 8620.8699 s
agent0:                 episode reward: 0.0091,                 loss: 0.1915
agent1:                 episode reward: -0.0091,                 loss: nan
Episode: 54401/101000 (53.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6688s / 8625.5387 s
agent0:                 episode reward: 0.4944,                 loss: 0.1895
agent1:                 episode reward: -0.4944,                 loss: nan
Episode: 54421/101000 (53.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3134s / 8629.8521 s
agent0:                 episode reward: -0.0651,                 loss: 0.1870
agent1:                 episode reward: 0.0651,                 loss: 0.1568
Score delta: 1.516963047724635, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/53976_0.
Episode: 54441/101000 (53.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6051s / 8633.4573 s
agent0:                 episode reward: -0.5168,                 loss: nan
agent1:                 episode reward: 0.5168,                 loss: 0.1516
Episode: 54461/101000 (53.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2491s / 8636.7064 s
agent0:                 episode reward: 0.0012,                 loss: nan
agent1:                 episode reward: -0.0012,                 loss: 0.1446
Episode: 54481/101000 (53.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2230s / 8639.9293 s
agent0:                 episode reward: -0.4209,                 loss: nan
agent1:                 episode reward: 0.4209,                 loss: 0.1450
Episode: 54501/101000 (53.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5810s / 8643.5103 s
agent0:                 episode reward: -0.3079,                 loss: nan
agent1:                 episode reward: 0.3079,                 loss: 0.1440
Episode: 54521/101000 (53.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4598s / 8646.9702 s
agent0:                 episode reward: -0.4870,                 loss: nan
agent1:                 episode reward: 0.4870,                 loss: 0.1439
Episode: 54541/101000 (54.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7985s / 8650.7687 s
agent0:                 episode reward: -0.2481,                 loss: nan
agent1:                 episode reward: 0.2481,                 loss: 0.1423
Episode: 54561/101000 (54.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8512s / 8654.6199 s
agent0:                 episode reward: -0.3603,                 loss: nan
agent1:                 episode reward: 0.3603,                 loss: 0.1433
Episode: 54581/101000 (54.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3944s / 8658.0142 s
agent0:                 episode reward: -0.3304,                 loss: nan
agent1:                 episode reward: 0.3304,                 loss: 0.1433
Episode: 54601/101000 (54.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7791s / 8661.7933 s
agent0:                 episode reward: -0.3665,                 loss: 0.1973
agent1:                 episode reward: 0.3665,                 loss: 0.1452
Score delta: 1.5146422705453648, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/54168_1.
Episode: 54621/101000 (54.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8829s / 8665.6762 s
agent0:                 episode reward: 0.3125,                 loss: 0.1987
agent1:                 episode reward: -0.3125,                 loss: nan
Episode: 54641/101000 (54.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4511s / 8669.1273 s
agent0:                 episode reward: 0.0631,                 loss: 0.1983
agent1:                 episode reward: -0.0631,                 loss: nan
Episode: 54661/101000 (54.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3981s / 8672.5254 s
agent0:                 episode reward: -0.0748,                 loss: 0.1973
agent1:                 episode reward: 0.0748,                 loss: nan
Episode: 54681/101000 (54.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6456s / 8676.1710 s
agent0:                 episode reward: 0.1235,                 loss: 0.1985
agent1:                 episode reward: -0.1235,                 loss: nan
Episode: 54701/101000 (54.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2170s / 8679.3880 s
agent0:                 episode reward: 0.1093,                 loss: 0.1970
agent1:                 episode reward: -0.1093,                 loss: nan
Episode: 54721/101000 (54.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8233s / 8684.2113 s
agent0:                 episode reward: 0.3293,                 loss: 0.1985
agent1:                 episode reward: -0.3293,                 loss: nan
Episode: 54741/101000 (54.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3542s / 8687.5655 s
agent0:                 episode reward: 0.4394,                 loss: 0.1984
agent1:                 episode reward: -0.4394,                 loss: nan
Episode: 54761/101000 (54.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0054s / 8691.5709 s
agent0:                 episode reward: 0.2052,                 loss: 0.1994
agent1:                 episode reward: -0.2052,                 loss: nan
Episode: 54781/101000 (54.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8577s / 8695.4286 s
agent0:                 episode reward: -0.1384,                 loss: 0.1980
agent1:                 episode reward: 0.1384,                 loss: nan
Episode: 54801/101000 (54.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4888s / 8698.9174 s
agent0:                 episode reward: 0.0288,                 loss: 0.1981
agent1:                 episode reward: -0.0288,                 loss: nan
Episode: 54821/101000 (54.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9338s / 8702.8512 s
agent0:                 episode reward: 0.4751,                 loss: 0.1978
agent1:                 episode reward: -0.4751,                 loss: 0.1589
Score delta: 1.5436548102278147, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/54388_0.
Episode: 54841/101000 (54.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2085s / 8706.0597 s
agent0:                 episode reward: -0.2768,                 loss: nan
agent1:                 episode reward: 0.2768,                 loss: 0.1554
Episode: 54861/101000 (54.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4549s / 8709.5146 s
agent0:                 episode reward: -0.5324,                 loss: nan
agent1:                 episode reward: 0.5324,                 loss: 0.1531
Episode: 54881/101000 (54.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5309s / 8713.0455 s
agent0:                 episode reward: -0.5327,                 loss: nan
agent1:                 episode reward: 0.5327,                 loss: 0.1535
Episode: 54901/101000 (54.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1384s / 8716.1839 s
agent0:                 episode reward: -0.5129,                 loss: nan
agent1:                 episode reward: 0.5129,                 loss: 0.1519
Episode: 54921/101000 (54.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0421s / 8720.2260 s
agent0:                 episode reward: -0.3692,                 loss: 0.2037
agent1:                 episode reward: 0.3692,                 loss: 0.1472
Score delta: 1.967379884891764, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/54477_1.
Episode: 54941/101000 (54.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4181s / 8724.6440 s
agent0:                 episode reward: -0.4036,                 loss: 0.2024
agent1:                 episode reward: 0.4036,                 loss: nan
Episode: 54961/101000 (54.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0766s / 8729.7206 s
agent0:                 episode reward: 0.0757,                 loss: 0.2022
agent1:                 episode reward: -0.0757,                 loss: nan
Episode: 54981/101000 (54.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7917s / 8733.5123 s
agent0:                 episode reward: -0.3430,                 loss: 0.2033
agent1:                 episode reward: 0.3430,                 loss: nan
Episode: 55001/101000 (54.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9616s / 8737.4739 s
agent0:                 episode reward: 0.1163,                 loss: 0.2014
agent1:                 episode reward: -0.1163,                 loss: nan
Episode: 55021/101000 (54.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1876s / 8741.6616 s
agent0:                 episode reward: -0.0033,                 loss: 0.2013
agent1:                 episode reward: 0.0033,                 loss: nan
Episode: 55041/101000 (54.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3167s / 8745.9783 s
agent0:                 episode reward: -0.0008,                 loss: 0.1935
agent1:                 episode reward: 0.0008,                 loss: nan
Episode: 55061/101000 (54.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3899s / 8749.3682 s
agent0:                 episode reward: 0.1593,                 loss: 0.1933
agent1:                 episode reward: -0.1593,                 loss: nan
Episode: 55081/101000 (54.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7334s / 8753.1016 s
agent0:                 episode reward: 0.2190,                 loss: 0.1917
agent1:                 episode reward: -0.2190,                 loss: nan
Episode: 55101/101000 (54.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5733s / 8756.6749 s
agent0:                 episode reward: 0.2046,                 loss: 0.1934
agent1:                 episode reward: -0.2046,                 loss: nan
Episode: 55121/101000 (54.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8655s / 8760.5404 s
agent0:                 episode reward: 0.4201,                 loss: 0.1921
agent1:                 episode reward: -0.4201,                 loss: 0.2333
Score delta: 1.9547060264945018, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/54694_0.
Episode: 55141/101000 (54.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5215s / 8764.0619 s
agent0:                 episode reward: 0.4347,                 loss: nan
agent1:                 episode reward: -0.4347,                 loss: 0.1860
Episode: 55161/101000 (54.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5725s / 8767.6344 s
agent0:                 episode reward: -0.2482,                 loss: nan
agent1:                 episode reward: 0.2482,                 loss: 0.1713
Episode: 55181/101000 (54.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9159s / 8771.5503 s
agent0:                 episode reward: -0.2801,                 loss: nan
agent1:                 episode reward: 0.2801,                 loss: 0.1666
Episode: 55201/101000 (54.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7817s / 8775.3321 s
agent0:                 episode reward: -0.0223,                 loss: nan
agent1:                 episode reward: 0.0223,                 loss: 0.1608
Episode: 55221/101000 (54.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2511s / 8779.5831 s
agent0:                 episode reward: 0.0106,                 loss: nan
agent1:                 episode reward: -0.0106,                 loss: 0.1581
Episode: 55241/101000 (54.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9259s / 8782.5091 s
agent0:                 episode reward: -0.0763,                 loss: nan
agent1:                 episode reward: 0.0763,                 loss: 0.1501
Episode: 55261/101000 (54.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3765s / 8785.8856 s
agent0:                 episode reward: -0.3523,                 loss: nan
agent1:                 episode reward: 0.3523,                 loss: 0.1501
Episode: 55281/101000 (54.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7065s / 8789.5921 s
agent0:                 episode reward: 0.0137,                 loss: nan
agent1:                 episode reward: -0.0137,                 loss: 0.1477
Episode: 55301/101000 (54.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1970s / 8793.7891 s
agent0:                 episode reward: -0.1942,                 loss: 0.1871
agent1:                 episode reward: 0.1942,                 loss: 0.1487
Score delta: 1.7777774099567938, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/54864_1.
Episode: 55321/101000 (54.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9093s / 8797.6983 s
agent0:                 episode reward: -0.0589,                 loss: 0.1844
agent1:                 episode reward: 0.0589,                 loss: nan
Episode: 55341/101000 (54.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8492s / 8801.5476 s
agent0:                 episode reward: -0.0630,                 loss: 0.1855
agent1:                 episode reward: 0.0630,                 loss: nan
Episode: 55361/101000 (54.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6261s / 8805.1737 s
agent0:                 episode reward: -0.2483,                 loss: 0.1849
agent1:                 episode reward: 0.2483,                 loss: nan
Episode: 55381/101000 (54.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8081s / 8808.9817 s
agent0:                 episode reward: 0.3586,                 loss: 0.1838
agent1:                 episode reward: -0.3586,                 loss: nan
Episode: 55401/101000 (54.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7256s / 8813.7073 s
agent0:                 episode reward: 0.3638,                 loss: 0.1854
agent1:                 episode reward: -0.3638,                 loss: 0.1505
Score delta: 1.5547776044441792, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/54973_0.
Episode: 55421/101000 (54.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7988s / 8817.5061 s
agent0:                 episode reward: -0.1645,                 loss: nan
agent1:                 episode reward: 0.1645,                 loss: 0.1521
Episode: 55441/101000 (54.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8867s / 8821.3928 s
agent0:                 episode reward: -0.2989,                 loss: nan
agent1:                 episode reward: 0.2989,                 loss: 0.1520
Episode: 55461/101000 (54.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4742s / 8824.8670 s
agent0:                 episode reward: -0.3451,                 loss: nan
agent1:                 episode reward: 0.3451,                 loss: 0.1513
Episode: 55481/101000 (54.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4155s / 8828.2825 s
agent0:                 episode reward: -0.7340,                 loss: nan
agent1:                 episode reward: 0.7340,                 loss: 0.1518
Episode: 55501/101000 (54.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7701s / 8832.0526 s
agent0:                 episode reward: -0.4821,                 loss: nan
agent1:                 episode reward: 0.4821,                 loss: 0.1528
Episode: 55521/101000 (54.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2417s / 8835.2943 s
agent0:                 episode reward: -0.2269,                 loss: nan
agent1:                 episode reward: 0.2269,                 loss: 0.1506
Episode: 55541/101000 (54.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1555s / 8838.4498 s
agent0:                 episode reward: 0.0212,                 loss: 0.1845
agent1:                 episode reward: -0.0212,                 loss: 0.1494
Score delta: 1.5617721545596113, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/55100_1.
Episode: 55561/101000 (55.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8262s / 8842.2759 s
agent0:                 episode reward: 0.3308,                 loss: 0.1813
agent1:                 episode reward: -0.3308,                 loss: nan
Episode: 55581/101000 (55.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9156s / 8846.1915 s
agent0:                 episode reward: -0.0387,                 loss: 0.1799
agent1:                 episode reward: 0.0387,                 loss: nan
Episode: 55601/101000 (55.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7744s / 8850.9659 s
agent0:                 episode reward: 0.2779,                 loss: 0.1809
agent1:                 episode reward: -0.2779,                 loss: nan
Episode: 55621/101000 (55.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3267s / 8854.2926 s
agent0:                 episode reward: -0.0302,                 loss: 0.1828
agent1:                 episode reward: 0.0302,                 loss: nan
Episode: 55641/101000 (55.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4893s / 8857.7819 s
agent0:                 episode reward: -0.2621,                 loss: 0.1814
agent1:                 episode reward: 0.2621,                 loss: nan
Episode: 55661/101000 (55.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6248s / 8861.4067 s
agent0:                 episode reward: -0.1534,                 loss: 0.1838
agent1:                 episode reward: 0.1534,                 loss: nan
Episode: 55681/101000 (55.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3774s / 8865.7841 s
agent0:                 episode reward: 0.2645,                 loss: 0.1846
agent1:                 episode reward: -0.2645,                 loss: nan
Episode: 55701/101000 (55.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6333s / 8869.4174 s
agent0:                 episode reward: 0.0696,                 loss: 0.1839
agent1:                 episode reward: -0.0696,                 loss: nan
Episode: 55721/101000 (55.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9484s / 8874.3658 s
agent0:                 episode reward: 0.1930,                 loss: 0.1850
agent1:                 episode reward: -0.1930,                 loss: nan
Episode: 55741/101000 (55.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3818s / 8878.7476 s
agent0:                 episode reward: 0.2535,                 loss: 0.1850
agent1:                 episode reward: -0.2535,                 loss: nan
Episode: 55761/101000 (55.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4835s / 8883.2311 s
agent0:                 episode reward: -0.0666,                 loss: 0.1842
agent1:                 episode reward: 0.0666,                 loss: nan
Episode: 55781/101000 (55.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1915s / 8886.4226 s
agent0:                 episode reward: -0.0706,                 loss: 0.1852
agent1:                 episode reward: 0.0706,                 loss: nan
Episode: 55801/101000 (55.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6915s / 8891.1141 s
agent0:                 episode reward: -0.1103,                 loss: 0.1882
agent1:                 episode reward: 0.1103,                 loss: 0.1426
Score delta: 1.7025713738672477, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/55364_0.
Episode: 55821/101000 (55.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6893s / 8894.8034 s
agent0:                 episode reward: -0.2744,                 loss: nan
agent1:                 episode reward: 0.2744,                 loss: 0.1399
Episode: 55841/101000 (55.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4819s / 8899.2853 s
agent0:                 episode reward: 0.1270,                 loss: nan
agent1:                 episode reward: -0.1270,                 loss: 0.1399
Episode: 55861/101000 (55.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3811s / 8902.6663 s
agent0:                 episode reward: -0.1958,                 loss: nan
agent1:                 episode reward: 0.1958,                 loss: 0.1396
Episode: 55881/101000 (55.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0824s / 8905.7488 s
agent0:                 episode reward: -0.1606,                 loss: nan
agent1:                 episode reward: 0.1606,                 loss: 0.1409
Episode: 55901/101000 (55.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9110s / 8909.6598 s
agent0:                 episode reward: -0.4239,                 loss: 0.2005
agent1:                 episode reward: 0.4239,                 loss: 0.1388
Score delta: 1.6951109561348368, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/55470_1.
Episode: 55921/101000 (55.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7525s / 8913.4124 s
agent0:                 episode reward: 0.1121,                 loss: 0.1968
agent1:                 episode reward: -0.1121,                 loss: nan
Episode: 55941/101000 (55.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7089s / 8918.1213 s
agent0:                 episode reward: -0.2654,                 loss: 0.1958
agent1:                 episode reward: 0.2654,                 loss: nan
Episode: 55961/101000 (55.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8036s / 8921.9249 s
agent0:                 episode reward: 0.1823,                 loss: 0.1931
agent1:                 episode reward: -0.1823,                 loss: nan
Episode: 55981/101000 (55.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6817s / 8925.6067 s
agent0:                 episode reward: 0.4449,                 loss: 0.1932
agent1:                 episode reward: -0.4449,                 loss: nan
Episode: 56001/101000 (55.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9189s / 8929.5255 s
agent0:                 episode reward: -0.1648,                 loss: 0.1961
agent1:                 episode reward: 0.1648,                 loss: nan
Episode: 56021/101000 (55.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5931s / 8934.1186 s
agent0:                 episode reward: -0.1307,                 loss: 0.1933
agent1:                 episode reward: 0.1307,                 loss: nan
Episode: 56041/101000 (55.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6239s / 8937.7425 s
agent0:                 episode reward: 0.3506,                 loss: 0.1957
agent1:                 episode reward: -0.3506,                 loss: 0.1557
Score delta: 1.5086875785777036, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/55608_0.
Episode: 56061/101000 (55.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7746s / 8941.5172 s
agent0:                 episode reward: -0.6785,                 loss: nan
agent1:                 episode reward: 0.6785,                 loss: 0.1471
Episode: 56081/101000 (55.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5387s / 8945.0558 s
agent0:                 episode reward: -0.6296,                 loss: nan
agent1:                 episode reward: 0.6296,                 loss: 0.1412
Episode: 56101/101000 (55.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0360s / 8948.0919 s
agent0:                 episode reward: -0.2515,                 loss: nan
agent1:                 episode reward: 0.2515,                 loss: 0.1408
Episode: 56121/101000 (55.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3010s / 8952.3929 s
agent0:                 episode reward: -0.2054,                 loss: nan
agent1:                 episode reward: 0.2054,                 loss: 0.1388
Episode: 56141/101000 (55.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9204s / 8956.3133 s
agent0:                 episode reward: -0.0288,                 loss: nan
agent1:                 episode reward: 0.0288,                 loss: 0.1406
Episode: 56161/101000 (55.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9723s / 8959.2855 s
agent0:                 episode reward: -0.3780,                 loss: nan
agent1:                 episode reward: 0.3780,                 loss: 0.1396
Episode: 56181/101000 (55.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0480s / 8963.3335 s
agent0:                 episode reward: -0.1672,                 loss: nan
agent1:                 episode reward: 0.1672,                 loss: 0.1397
Episode: 56201/101000 (55.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2318s / 8967.5654 s
agent0:                 episode reward: -0.4691,                 loss: 0.1908
agent1:                 episode reward: 0.4691,                 loss: 0.1375
Score delta: 1.7117132594454862, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/55759_1.
Episode: 56221/101000 (55.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0973s / 8972.6626 s
agent0:                 episode reward: 0.0638,                 loss: 0.1921
agent1:                 episode reward: -0.0638,                 loss: nan
Episode: 56241/101000 (55.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8911s / 8976.5537 s
agent0:                 episode reward: -0.3319,                 loss: 0.1911
agent1:                 episode reward: 0.3319,                 loss: nan
Episode: 56261/101000 (55.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0967s / 8979.6505 s
agent0:                 episode reward: -0.1822,                 loss: 0.1881
agent1:                 episode reward: 0.1822,                 loss: nan
Episode: 56281/101000 (55.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3564s / 8984.0069 s
agent0:                 episode reward: 0.2641,                 loss: 0.1891
agent1:                 episode reward: -0.2641,                 loss: nan
Episode: 56301/101000 (55.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2723s / 8987.2792 s
agent0:                 episode reward: 0.0499,                 loss: 0.1884
agent1:                 episode reward: -0.0499,                 loss: nan
Episode: 56321/101000 (55.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6316s / 8990.9108 s
agent0:                 episode reward: -0.0290,                 loss: 0.1884
agent1:                 episode reward: 0.0290,                 loss: nan
Episode: 56341/101000 (55.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5213s / 8994.4320 s
agent0:                 episode reward: 0.3326,                 loss: 0.1878
agent1:                 episode reward: -0.3326,                 loss: nan
Episode: 56361/101000 (55.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0231s / 8998.4551 s
agent0:                 episode reward: 0.2612,                 loss: 0.1873
agent1:                 episode reward: -0.2612,                 loss: 0.1917
Score delta: 1.6076123469182453, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/55930_0.
Episode: 56381/101000 (55.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8561s / 9002.3113 s
agent0:                 episode reward: -0.4912,                 loss: nan
agent1:                 episode reward: 0.4912,                 loss: 0.1795
Episode: 56401/101000 (55.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0894s / 9005.4007 s
agent0:                 episode reward: 0.4524,                 loss: nan
agent1:                 episode reward: -0.4524,                 loss: 0.1747
Episode: 56421/101000 (55.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1223s / 9009.5230 s
agent0:                 episode reward: -0.4766,                 loss: nan
agent1:                 episode reward: 0.4766,                 loss: 0.1724
Episode: 56441/101000 (55.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7061s / 9013.2291 s
agent0:                 episode reward: -0.2900,                 loss: nan
agent1:                 episode reward: 0.2900,                 loss: 0.1725
Episode: 56461/101000 (55.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9942s / 9018.2233 s
agent0:                 episode reward: -0.0988,                 loss: nan
agent1:                 episode reward: 0.0988,                 loss: 0.1709
Episode: 56481/101000 (55.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8447s / 9022.0680 s
agent0:                 episode reward: -0.1007,                 loss: nan
agent1:                 episode reward: 0.1007,                 loss: 0.1706
Episode: 56501/101000 (55.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8293s / 9025.8973 s
agent0:                 episode reward: -0.2944,                 loss: nan
agent1:                 episode reward: 0.2944,                 loss: 0.1700
Episode: 56521/101000 (55.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6970s / 9029.5943 s
agent0:                 episode reward: -0.5713,                 loss: nan
agent1:                 episode reward: 0.5713,                 loss: 0.1678
Episode: 56541/101000 (55.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1271s / 9033.7214 s
agent0:                 episode reward: -0.3068,                 loss: nan
agent1:                 episode reward: 0.3068,                 loss: 0.1681
Episode: 56561/101000 (56.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1923s / 9037.9137 s
agent0:                 episode reward: -0.1664,                 loss: nan
agent1:                 episode reward: 0.1664,                 loss: 0.1595
Episode: 56581/101000 (56.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6649s / 9041.5786 s
agent0:                 episode reward: -0.3028,                 loss: nan
agent1:                 episode reward: 0.3028,                 loss: 0.1454
Episode: 56601/101000 (56.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8688s / 9045.4474 s
agent0:                 episode reward: -0.0872,                 loss: 0.2058
agent1:                 episode reward: 0.0872,                 loss: 0.1459
Score delta: 1.7645406299272668, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/56159_1.
Episode: 56621/101000 (56.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0036s / 9048.4511 s
agent0:                 episode reward: -0.3504,                 loss: 0.1955
agent1:                 episode reward: 0.3504,                 loss: nan
Episode: 56641/101000 (56.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5820s / 9052.0331 s
agent0:                 episode reward: -0.4560,                 loss: 0.1962
agent1:                 episode reward: 0.4560,                 loss: nan
Episode: 56661/101000 (56.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6669s / 9055.7000 s
agent0:                 episode reward: -0.3371,                 loss: 0.1950
agent1:                 episode reward: 0.3371,                 loss: nan
Episode: 56681/101000 (56.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2583s / 9058.9583 s
agent0:                 episode reward: -0.6321,                 loss: 0.1943
agent1:                 episode reward: 0.6321,                 loss: nan
Episode: 56701/101000 (56.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7010s / 9062.6593 s
agent0:                 episode reward: 0.0173,                 loss: 0.1933
agent1:                 episode reward: -0.0173,                 loss: nan
Episode: 56721/101000 (56.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0455s / 9066.7047 s
agent0:                 episode reward: 0.2842,                 loss: 0.1951
agent1:                 episode reward: -0.2842,                 loss: nan
Episode: 56741/101000 (56.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2484s / 9070.9532 s
agent0:                 episode reward: 0.2811,                 loss: 0.1904
agent1:                 episode reward: -0.2811,                 loss: nan
Episode: 56761/101000 (56.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2713s / 9075.2245 s
agent0:                 episode reward: 0.0336,                 loss: 0.1935
agent1:                 episode reward: -0.0336,                 loss: nan
Episode: 56781/101000 (56.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8825s / 9079.1069 s
agent0:                 episode reward: -0.2439,                 loss: 0.1924
agent1:                 episode reward: 0.2439,                 loss: nan
Episode: 56801/101000 (56.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6101s / 9082.7170 s
agent0:                 episode reward: 0.0317,                 loss: 0.2003
agent1:                 episode reward: -0.0317,                 loss: nan
Episode: 56821/101000 (56.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1585s / 9086.8755 s
agent0:                 episode reward: 0.2348,                 loss: 0.2062
agent1:                 episode reward: -0.2348,                 loss: nan
Episode: 56841/101000 (56.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8792s / 9090.7546 s
agent0:                 episode reward: 0.2969,                 loss: 0.2052
agent1:                 episode reward: -0.2969,                 loss: nan
Episode: 56861/101000 (56.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0641s / 9094.8188 s
agent0:                 episode reward: 0.1208,                 loss: 0.2052
agent1:                 episode reward: -0.1208,                 loss: nan
Episode: 56881/101000 (56.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2560s / 9099.0748 s
agent0:                 episode reward: 0.1410,                 loss: 0.2056
agent1:                 episode reward: -0.1410,                 loss: nan
Episode: 56901/101000 (56.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1047s / 9103.1795 s
agent0:                 episode reward: -0.1941,                 loss: 0.2040
agent1:                 episode reward: 0.1941,                 loss: 0.1705
Score delta: 1.5651407794328585, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/56464_0.
Episode: 56921/101000 (56.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1578s / 9107.3373 s
agent0:                 episode reward: -0.3032,                 loss: nan
agent1:                 episode reward: 0.3032,                 loss: 0.1706
Episode: 56941/101000 (56.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9757s / 9111.3130 s
agent0:                 episode reward: 0.1034,                 loss: nan
agent1:                 episode reward: -0.1034,                 loss: 0.1703
Episode: 56961/101000 (56.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2000s / 9115.5130 s
agent0:                 episode reward: -0.4512,                 loss: nan
agent1:                 episode reward: 0.4512,                 loss: 0.1686
Episode: 56981/101000 (56.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0352s / 9118.5483 s
agent0:                 episode reward: -0.3764,                 loss: nan
agent1:                 episode reward: 0.3764,                 loss: 0.1679
Episode: 57001/101000 (56.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0140s / 9122.5622 s
agent0:                 episode reward: -0.2990,                 loss: nan
agent1:                 episode reward: 0.2990,                 loss: 0.1678
Episode: 57021/101000 (56.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7671s / 9126.3293 s
agent0:                 episode reward: -0.2723,                 loss: 0.1945
agent1:                 episode reward: 0.2723,                 loss: 0.1697
Score delta: 1.6176216475308725, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/56589_1.
Episode: 57041/101000 (56.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4499s / 9129.7792 s
agent0:                 episode reward: 0.0363,                 loss: 0.1916
agent1:                 episode reward: -0.0363,                 loss: nan
Episode: 57061/101000 (56.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4208s / 9133.2000 s
agent0:                 episode reward: -0.0340,                 loss: 0.1920
agent1:                 episode reward: 0.0340,                 loss: nan
Episode: 57081/101000 (56.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6395s / 9136.8395 s
agent0:                 episode reward: 0.4454,                 loss: 0.1910
agent1:                 episode reward: -0.4454,                 loss: nan
Episode: 57101/101000 (56.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0778s / 9140.9173 s
agent0:                 episode reward: 0.1885,                 loss: 0.1920
agent1:                 episode reward: -0.1885,                 loss: nan
Episode: 57121/101000 (56.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6393s / 9144.5567 s
agent0:                 episode reward: -0.2473,                 loss: 0.1884
agent1:                 episode reward: 0.2473,                 loss: nan
Episode: 57141/101000 (56.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2676s / 9148.8242 s
agent0:                 episode reward: 0.0928,                 loss: 0.1903
agent1:                 episode reward: -0.0928,                 loss: nan
Episode: 57161/101000 (56.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9986s / 9152.8228 s
agent0:                 episode reward: 0.6433,                 loss: 0.1905
agent1:                 episode reward: -0.6433,                 loss: nan
Episode: 57181/101000 (56.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6046s / 9156.4274 s
agent0:                 episode reward: -0.2143,                 loss: 0.1873
agent1:                 episode reward: 0.2143,                 loss: 0.1537
Score delta: 1.948973779621743, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/56736_0.
Episode: 57201/101000 (56.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3073s / 9159.7347 s
agent0:                 episode reward: -0.1101,                 loss: nan
agent1:                 episode reward: 0.1101,                 loss: 0.1531
Episode: 57221/101000 (56.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4381s / 9163.1728 s
agent0:                 episode reward: -0.2321,                 loss: nan
agent1:                 episode reward: 0.2321,                 loss: 0.1533
Episode: 57241/101000 (56.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1215s / 9166.2942 s
agent0:                 episode reward: -0.3214,                 loss: nan
agent1:                 episode reward: 0.3214,                 loss: 0.1510
Episode: 57261/101000 (56.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5049s / 9169.7991 s
agent0:                 episode reward: -0.6840,                 loss: nan
agent1:                 episode reward: 0.6840,                 loss: 0.1511
Score delta: 1.645235640166873, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/56835_1.
Episode: 57281/101000 (56.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7768s / 9173.5759 s
agent0:                 episode reward: 0.0420,                 loss: 0.1858
agent1:                 episode reward: -0.0420,                 loss: nan
Episode: 57301/101000 (56.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9227s / 9177.4986 s
agent0:                 episode reward: 0.3158,                 loss: 0.1819
agent1:                 episode reward: -0.3158,                 loss: nan
Episode: 57321/101000 (56.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6178s / 9181.1164 s
agent0:                 episode reward: -0.2253,                 loss: 0.1826
agent1:                 episode reward: 0.2253,                 loss: nan
Episode: 57341/101000 (56.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7844s / 9184.9008 s
agent0:                 episode reward: 0.4982,                 loss: 0.1823
agent1:                 episode reward: -0.4982,                 loss: nan
Episode: 57361/101000 (56.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0629s / 9187.9637 s
agent0:                 episode reward: 0.1603,                 loss: 0.1834
agent1:                 episode reward: -0.1603,                 loss: nan
Episode: 57381/101000 (56.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9861s / 9191.9498 s
agent0:                 episode reward: -0.2697,                 loss: 0.1847
agent1:                 episode reward: 0.2697,                 loss: nan
Episode: 57401/101000 (56.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6905s / 9195.6403 s
agent0:                 episode reward: -0.3399,                 loss: 0.1848
agent1:                 episode reward: 0.3399,                 loss: nan
Episode: 57421/101000 (56.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4469s / 9200.0872 s
agent0:                 episode reward: -0.3295,                 loss: 0.1853
agent1:                 episode reward: 0.3295,                 loss: nan
Episode: 57441/101000 (56.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9303s / 9204.0175 s
agent0:                 episode reward: 0.1683,                 loss: 0.1831
agent1:                 episode reward: -0.1683,                 loss: nan
Episode: 57461/101000 (56.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2847s / 9208.3023 s
agent0:                 episode reward: 0.0168,                 loss: 0.1835
agent1:                 episode reward: -0.0168,                 loss: nan
Episode: 57481/101000 (56.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0240s / 9212.3263 s
agent0:                 episode reward: 0.0846,                 loss: 0.1870
agent1:                 episode reward: -0.0846,                 loss: 0.1699
Score delta: 1.584348462060643, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/57042_0.
Episode: 57501/101000 (56.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9524s / 9215.2786 s
agent0:                 episode reward: -0.3056,                 loss: nan
agent1:                 episode reward: 0.3056,                 loss: 0.1685
Episode: 57521/101000 (56.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2017s / 9218.4804 s
agent0:                 episode reward: -0.2676,                 loss: nan
agent1:                 episode reward: 0.2676,                 loss: 0.1694
Episode: 57541/101000 (56.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2034s / 9221.6837 s
agent0:                 episode reward: 0.2052,                 loss: nan
agent1:                 episode reward: -0.2052,                 loss: 0.1700
Episode: 57561/101000 (56.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2609s / 9225.9446 s
agent0:                 episode reward: -0.7295,                 loss: nan
agent1:                 episode reward: 0.7295,                 loss: 0.1506
Episode: 57581/101000 (57.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8859s / 9229.8305 s
agent0:                 episode reward: -0.2433,                 loss: 0.1863
agent1:                 episode reward: 0.2433,                 loss: 0.1419
Score delta: 1.59363979425471, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/57149_1.
Episode: 57601/101000 (57.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1333s / 9233.9638 s
agent0:                 episode reward: -0.0780,                 loss: 0.1845
agent1:                 episode reward: 0.0780,                 loss: nan
Episode: 57621/101000 (57.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7157s / 9238.6795 s
agent0:                 episode reward: 0.2944,                 loss: 0.1833
agent1:                 episode reward: -0.2944,                 loss: nan
Episode: 57641/101000 (57.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0760s / 9242.7554 s
agent0:                 episode reward: 0.3026,                 loss: 0.1845
agent1:                 episode reward: -0.3026,                 loss: nan
Episode: 57661/101000 (57.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2625s / 9246.0179 s
agent0:                 episode reward: -0.2394,                 loss: 0.1826
agent1:                 episode reward: 0.2394,                 loss: nan
Episode: 57681/101000 (57.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3834s / 9250.4013 s
agent0:                 episode reward: 0.3903,                 loss: 0.1826
agent1:                 episode reward: -0.3903,                 loss: nan
Episode: 57701/101000 (57.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8654s / 9254.2667 s
agent0:                 episode reward: 0.0299,                 loss: 0.1842
agent1:                 episode reward: -0.0299,                 loss: nan
Episode: 57721/101000 (57.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1626s / 9257.4293 s
agent0:                 episode reward: 0.2733,                 loss: 0.1837
agent1:                 episode reward: -0.2733,                 loss: 0.1543
Score delta: 2.140690737332227, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/57284_0.
Episode: 57741/101000 (57.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7994s / 9261.2287 s
agent0:                 episode reward: -0.2907,                 loss: nan
agent1:                 episode reward: 0.2907,                 loss: 0.1532
Episode: 57761/101000 (57.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9986s / 9265.2273 s
agent0:                 episode reward: -0.3620,                 loss: nan
agent1:                 episode reward: 0.3620,                 loss: 0.1533
Episode: 57781/101000 (57.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8045s / 9269.0318 s
agent0:                 episode reward: -0.5315,                 loss: nan
agent1:                 episode reward: 0.5315,                 loss: 0.1517
Episode: 57801/101000 (57.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6982s / 9272.7300 s
agent0:                 episode reward: -0.4468,                 loss: nan
agent1:                 episode reward: 0.4468,                 loss: 0.1520
Episode: 57821/101000 (57.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9143s / 9276.6443 s
agent0:                 episode reward: -0.2943,                 loss: 0.2047
agent1:                 episode reward: 0.2943,                 loss: 0.1521
Score delta: 1.6901479346363584, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/57386_1.
Episode: 57841/101000 (57.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4042s / 9281.0485 s
agent0:                 episode reward: -0.2273,                 loss: 0.2013
agent1:                 episode reward: 0.2273,                 loss: nan
Episode: 57861/101000 (57.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8536s / 9284.9021 s
agent0:                 episode reward: -0.3585,                 loss: 0.2007
agent1:                 episode reward: 0.3585,                 loss: nan
Episode: 57881/101000 (57.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5616s / 9288.4637 s
agent0:                 episode reward: 0.6417,                 loss: 0.1994
agent1:                 episode reward: -0.6417,                 loss: nan
Episode: 57901/101000 (57.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1080s / 9292.5717 s
agent0:                 episode reward: 0.4760,                 loss: 0.1956
agent1:                 episode reward: -0.4760,                 loss: nan
Episode: 57921/101000 (57.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0426s / 9296.6143 s
agent0:                 episode reward: -0.1194,                 loss: 0.1867
agent1:                 episode reward: 0.1194,                 loss: 0.1405
Score delta: 1.5771061319563084, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/57479_0.
Episode: 57941/101000 (57.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4082s / 9300.0225 s
agent0:                 episode reward: -0.2489,                 loss: nan
agent1:                 episode reward: 0.2489,                 loss: 0.1406
Episode: 57961/101000 (57.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3892s / 9303.4117 s
agent0:                 episode reward: -0.0186,                 loss: nan
agent1:                 episode reward: 0.0186,                 loss: 0.1416
Episode: 57981/101000 (57.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5507s / 9306.9624 s
agent0:                 episode reward: -0.2825,                 loss: nan
agent1:                 episode reward: 0.2825,                 loss: 0.1399
Episode: 58001/101000 (57.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0370s / 9310.9994 s
agent0:                 episode reward: 0.2624,                 loss: nan
agent1:                 episode reward: -0.2624,                 loss: 0.1390
Episode: 58021/101000 (57.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8597s / 9315.8591 s
agent0:                 episode reward: -0.1148,                 loss: 0.2054
agent1:                 episode reward: 0.1148,                 loss: 0.1381
Score delta: 1.5112324060180424, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/57586_1.
Episode: 58041/101000 (57.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0049s / 9319.8640 s
agent0:                 episode reward: 0.2305,                 loss: 0.2001
agent1:                 episode reward: -0.2305,                 loss: nan
Episode: 58061/101000 (57.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0078s / 9323.8718 s
agent0:                 episode reward: 0.0607,                 loss: 0.2008
agent1:                 episode reward: -0.0607,                 loss: nan
Episode: 58081/101000 (57.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0789s / 9328.9507 s
agent0:                 episode reward: 0.0345,                 loss: 0.1989
agent1:                 episode reward: -0.0345,                 loss: nan
Episode: 58101/101000 (57.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4196s / 9332.3703 s
agent0:                 episode reward: -0.1519,                 loss: 0.1997
agent1:                 episode reward: 0.1519,                 loss: nan
Episode: 58121/101000 (57.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2836s / 9335.6539 s
agent0:                 episode reward: -0.1820,                 loss: 0.1993
agent1:                 episode reward: 0.1820,                 loss: nan
Episode: 58141/101000 (57.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2522s / 9339.9061 s
agent0:                 episode reward: 0.2655,                 loss: 0.1987
agent1:                 episode reward: -0.2655,                 loss: nan
Episode: 58161/101000 (57.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3079s / 9344.2140 s
agent0:                 episode reward: 0.4241,                 loss: 0.1956
agent1:                 episode reward: -0.4241,                 loss: 0.1856
Score delta: 1.587944737149607, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/57722_0.
Episode: 58181/101000 (57.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8997s / 9348.1138 s
agent0:                 episode reward: -0.5940,                 loss: nan
agent1:                 episode reward: 0.5940,                 loss: 0.1769
Episode: 58201/101000 (57.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2690s / 9351.3827 s
agent0:                 episode reward: -0.2259,                 loss: nan
agent1:                 episode reward: 0.2259,                 loss: 0.1729
Episode: 58221/101000 (57.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6265s / 9355.0092 s
agent0:                 episode reward: -0.5102,                 loss: nan
agent1:                 episode reward: 0.5102,                 loss: 0.1733
Episode: 58241/101000 (57.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5444s / 9358.5537 s
agent0:                 episode reward: -0.2923,                 loss: nan
agent1:                 episode reward: 0.2923,                 loss: 0.1721
Episode: 58261/101000 (57.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4789s / 9362.0326 s
agent0:                 episode reward: -0.3618,                 loss: nan
agent1:                 episode reward: 0.3618,                 loss: 0.1480
Episode: 58281/101000 (57.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2874s / 9365.3200 s
agent0:                 episode reward: -0.1724,                 loss: nan
agent1:                 episode reward: 0.1724,                 loss: 0.1410
Episode: 58301/101000 (57.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1701s / 9368.4901 s
agent0:                 episode reward: -0.5502,                 loss: nan
agent1:                 episode reward: 0.5502,                 loss: 0.1430
Episode: 58321/101000 (57.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6300s / 9373.1201 s
agent0:                 episode reward: -0.0597,                 loss: nan
agent1:                 episode reward: 0.0597,                 loss: 0.1430
Episode: 58341/101000 (57.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4644s / 9376.5845 s
agent0:                 episode reward: -0.3863,                 loss: nan
agent1:                 episode reward: 0.3863,                 loss: 0.1420
Episode: 58361/101000 (57.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9140s / 9381.4984 s
agent0:                 episode reward: -0.0774,                 loss: nan
agent1:                 episode reward: 0.0774,                 loss: 0.1432
Episode: 58381/101000 (57.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3604s / 9384.8589 s
agent0:                 episode reward: -0.4429,                 loss: 0.2196
agent1:                 episode reward: 0.4429,                 loss: 0.1416
Score delta: 1.5155258819388666, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/57948_1.
Episode: 58401/101000 (57.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1473s / 9389.0062 s
agent0:                 episode reward: 0.1262,                 loss: 0.2049
agent1:                 episode reward: -0.1262,                 loss: nan
Episode: 58421/101000 (57.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5597s / 9393.5659 s
agent0:                 episode reward: 0.2899,                 loss: 0.2028
agent1:                 episode reward: -0.2899,                 loss: nan
Episode: 58441/101000 (57.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9798s / 9397.5457 s
agent0:                 episode reward: 0.0049,                 loss: 0.2025
agent1:                 episode reward: -0.0049,                 loss: nan
Episode: 58461/101000 (57.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3666s / 9401.9123 s
agent0:                 episode reward: 0.1065,                 loss: 0.2050
agent1:                 episode reward: -0.1065,                 loss: nan
Episode: 58481/101000 (57.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6052s / 9405.5175 s
agent0:                 episode reward: 0.1906,                 loss: 0.2008
agent1:                 episode reward: -0.1906,                 loss: nan
Episode: 58501/101000 (57.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8083s / 9409.3258 s
agent0:                 episode reward: 0.3600,                 loss: 0.2022
agent1:                 episode reward: -0.3600,                 loss: nan
Episode: 58521/101000 (57.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2608s / 9413.5866 s
agent0:                 episode reward: 0.2849,                 loss: 0.2061
agent1:                 episode reward: -0.2849,                 loss: 0.1561
Score delta: 1.8087108583219589, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/58080_0.
Episode: 58541/101000 (57.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0908s / 9416.6775 s
agent0:                 episode reward: 0.1908,                 loss: nan
agent1:                 episode reward: -0.1908,                 loss: 0.1493
Episode: 58561/101000 (57.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6943s / 9420.3717 s
agent0:                 episode reward: -0.0469,                 loss: nan
agent1:                 episode reward: 0.0469,                 loss: 0.1466
Episode: 58581/101000 (58.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0585s / 9424.4302 s
agent0:                 episode reward: -0.3226,                 loss: nan
agent1:                 episode reward: 0.3226,                 loss: 0.1449
Episode: 58601/101000 (58.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3616s / 9427.7918 s
agent0:                 episode reward: -0.4198,                 loss: nan
agent1:                 episode reward: 0.4198,                 loss: 0.1435
Episode: 58621/101000 (58.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0156s / 9431.8074 s
agent0:                 episode reward: -0.2421,                 loss: nan
agent1:                 episode reward: 0.2421,                 loss: 0.1429
Episode: 58641/101000 (58.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6378s / 9435.4452 s
agent0:                 episode reward: -0.4817,                 loss: nan
agent1:                 episode reward: 0.4817,                 loss: 0.1421
Episode: 58661/101000 (58.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9546s / 9440.3998 s
agent0:                 episode reward: -0.1097,                 loss: 0.1967
agent1:                 episode reward: 0.1097,                 loss: 0.1403
Score delta: 1.7675253999262417, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/58218_1.
Episode: 58681/101000 (58.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5196s / 9443.9194 s
agent0:                 episode reward: -0.0703,                 loss: 0.1940
agent1:                 episode reward: 0.0703,                 loss: nan
Episode: 58701/101000 (58.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6192s / 9447.5385 s
agent0:                 episode reward: -0.0688,                 loss: 0.1915
agent1:                 episode reward: 0.0688,                 loss: nan
Episode: 58721/101000 (58.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2645s / 9451.8031 s
agent0:                 episode reward: -0.1091,                 loss: 0.1903
agent1:                 episode reward: 0.1091,                 loss: nan
Episode: 58741/101000 (58.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4781s / 9455.2812 s
agent0:                 episode reward: 0.0991,                 loss: 0.1928
agent1:                 episode reward: -0.0991,                 loss: nan
Episode: 58761/101000 (58.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8127s / 9459.0939 s
agent0:                 episode reward: -0.1271,                 loss: 0.1909
agent1:                 episode reward: 0.1271,                 loss: nan
Episode: 58781/101000 (58.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1405s / 9464.2344 s
agent0:                 episode reward: 0.0924,                 loss: 0.1927
agent1:                 episode reward: -0.0924,                 loss: nan
Episode: 58801/101000 (58.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0496s / 9467.2841 s
agent0:                 episode reward: -0.1100,                 loss: 0.1909
agent1:                 episode reward: 0.1100,                 loss: nan
Episode: 58821/101000 (58.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1447s / 9471.4288 s
agent0:                 episode reward: 0.1209,                 loss: 0.1922
agent1:                 episode reward: -0.1209,                 loss: 0.1575
Score delta: 1.5615530551988568, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/58382_0.
Episode: 58841/101000 (58.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4171s / 9476.8459 s
agent0:                 episode reward: -0.2171,                 loss: nan
agent1:                 episode reward: 0.2171,                 loss: 0.1536
Episode: 58861/101000 (58.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2240s / 9481.0699 s
agent0:                 episode reward: -0.3877,                 loss: nan
agent1:                 episode reward: 0.3877,                 loss: 0.1512
Episode: 58881/101000 (58.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6587s / 9484.7286 s
agent0:                 episode reward: -0.2079,                 loss: nan
agent1:                 episode reward: 0.2079,                 loss: 0.1512
Episode: 58901/101000 (58.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0045s / 9488.7331 s
agent0:                 episode reward: -0.1055,                 loss: nan
agent1:                 episode reward: 0.1055,                 loss: 0.1475
Episode: 58921/101000 (58.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7544s / 9492.4875 s
agent0:                 episode reward: -0.2489,                 loss: nan
agent1:                 episode reward: 0.2489,                 loss: 0.1461
Episode: 58941/101000 (58.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5225s / 9496.0100 s
agent0:                 episode reward: 0.2551,                 loss: nan
agent1:                 episode reward: -0.2551,                 loss: 0.1446
Episode: 58961/101000 (58.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2967s / 9500.3067 s
agent0:                 episode reward: -0.1559,                 loss: nan
agent1:                 episode reward: 0.1559,                 loss: 0.1467
Episode: 58981/101000 (58.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4362s / 9504.7430 s
agent0:                 episode reward: -0.0461,                 loss: nan
agent1:                 episode reward: 0.0461,                 loss: 0.1442
Episode: 59001/101000 (58.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9918s / 9508.7348 s
agent0:                 episode reward: -0.1739,                 loss: nan
agent1:                 episode reward: 0.1739,                 loss: 0.1448
Episode: 59021/101000 (58.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9181s / 9512.6529 s
agent0:                 episode reward: -0.1729,                 loss: nan
agent1:                 episode reward: 0.1729,                 loss: 0.1446
Episode: 59041/101000 (58.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4980s / 9517.1510 s
agent0:                 episode reward: -0.5143,                 loss: nan
agent1:                 episode reward: 0.5143,                 loss: 0.1439
Episode: 59061/101000 (58.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3581s / 9520.5091 s
agent0:                 episode reward: -0.0249,                 loss: nan
agent1:                 episode reward: 0.0249,                 loss: 0.1417
Episode: 59081/101000 (58.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7198s / 9524.2288 s
agent0:                 episode reward: -0.1617,                 loss: nan
agent1:                 episode reward: 0.1617,                 loss: 0.1437
Episode: 59101/101000 (58.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8315s / 9528.0603 s
agent0:                 episode reward: -0.1238,                 loss: nan
agent1:                 episode reward: 0.1238,                 loss: 0.1427
Episode: 59121/101000 (58.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2930s / 9531.3533 s
agent0:                 episode reward: -0.2331,                 loss: 0.2036
agent1:                 episode reward: 0.2331,                 loss: 0.1448
Score delta: 1.6351197606836876, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/58683_1.
Episode: 59141/101000 (58.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1653s / 9535.5186 s
agent0:                 episode reward: 0.2034,                 loss: 0.2017
agent1:                 episode reward: -0.2034,                 loss: nan
Episode: 59161/101000 (58.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7911s / 9539.3097 s
agent0:                 episode reward: 0.0999,                 loss: 0.2009
agent1:                 episode reward: -0.0999,                 loss: nan
Episode: 59181/101000 (58.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5692s / 9543.8789 s
agent0:                 episode reward: 0.2357,                 loss: 0.2008
agent1:                 episode reward: -0.2357,                 loss: nan
Episode: 59201/101000 (58.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1425s / 9548.0214 s
agent0:                 episode reward: -0.2964,                 loss: 0.2017
agent1:                 episode reward: 0.2964,                 loss: nan
Episode: 59221/101000 (58.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0251s / 9552.0465 s
agent0:                 episode reward: -0.1772,                 loss: 0.2012
agent1:                 episode reward: 0.1772,                 loss: nan
Episode: 59241/101000 (58.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8341s / 9555.8806 s
agent0:                 episode reward: -0.1170,                 loss: 0.2021
agent1:                 episode reward: 0.1170,                 loss: nan
Episode: 59261/101000 (58.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8335s / 9559.7141 s
agent0:                 episode reward: 0.1958,                 loss: 0.1989
agent1:                 episode reward: -0.1958,                 loss: nan
Episode: 59281/101000 (58.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2422s / 9563.9563 s
agent0:                 episode reward: 0.4776,                 loss: 0.2016
agent1:                 episode reward: -0.4776,                 loss: nan
Episode: 59301/101000 (58.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4401s / 9568.3964 s
agent0:                 episode reward: 0.0671,                 loss: 0.1996
agent1:                 episode reward: -0.0671,                 loss: nan
Episode: 59321/101000 (58.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3569s / 9572.7533 s
agent0:                 episode reward: -0.1128,                 loss: 0.2001
agent1:                 episode reward: 0.1128,                 loss: nan
Episode: 59341/101000 (58.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6462s / 9577.3996 s
agent0:                 episode reward: 0.3888,                 loss: 0.1950
agent1:                 episode reward: -0.3888,                 loss: nan
Episode: 59361/101000 (58.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9799s / 9581.3794 s
agent0:                 episode reward: -0.0797,                 loss: 0.1883
agent1:                 episode reward: 0.0797,                 loss: nan
Episode: 59381/101000 (58.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5488s / 9584.9282 s
agent0:                 episode reward: -0.2261,                 loss: 0.1879
agent1:                 episode reward: 0.2261,                 loss: nan
Episode: 59401/101000 (58.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8082s / 9588.7364 s
agent0:                 episode reward: 0.3840,                 loss: 0.1883
agent1:                 episode reward: -0.3840,                 loss: 0.1536
Score delta: 1.8383343638579852, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/58962_0.
Episode: 59421/101000 (58.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3580s / 9593.0943 s
agent0:                 episode reward: -0.3487,                 loss: nan
agent1:                 episode reward: 0.3487,                 loss: 0.1536
Episode: 59441/101000 (58.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7848s / 9597.8792 s
agent0:                 episode reward: -0.5990,                 loss: nan
agent1:                 episode reward: 0.5990,                 loss: 0.1514
Episode: 59461/101000 (58.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5610s / 9601.4401 s
agent0:                 episode reward: -0.4087,                 loss: nan
agent1:                 episode reward: 0.4087,                 loss: 0.1492
Episode: 59481/101000 (58.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4229s / 9605.8630 s
agent0:                 episode reward: -0.3682,                 loss: nan
agent1:                 episode reward: 0.3682,                 loss: 0.1498
Episode: 59501/101000 (58.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7460s / 9610.6091 s
agent0:                 episode reward: -0.3480,                 loss: nan
agent1:                 episode reward: 0.3480,                 loss: 0.1457
Episode: 59521/101000 (58.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7856s / 9614.3947 s
agent0:                 episode reward: -0.2035,                 loss: 0.2295
agent1:                 episode reward: 0.2035,                 loss: 0.1452
Score delta: 1.5076902772218101, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/59079_1.
Episode: 59541/101000 (58.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3579s / 9618.7526 s
agent0:                 episode reward: -0.4758,                 loss: 0.2145
agent1:                 episode reward: 0.4758,                 loss: nan
Episode: 59561/101000 (58.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3496s / 9623.1022 s
agent0:                 episode reward: -0.1035,                 loss: 0.2123
agent1:                 episode reward: 0.1035,                 loss: nan
Episode: 59581/101000 (58.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0055s / 9626.1078 s
agent0:                 episode reward: -0.1491,                 loss: 0.2075
agent1:                 episode reward: 0.1491,                 loss: nan
Episode: 59601/101000 (59.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9020s / 9630.0097 s
agent0:                 episode reward: -0.6709,                 loss: 0.2113
agent1:                 episode reward: 0.6709,                 loss: nan
Episode: 59621/101000 (59.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1584s / 9634.1681 s
agent0:                 episode reward: 0.1913,                 loss: 0.2092
agent1:                 episode reward: -0.1913,                 loss: nan
Episode: 59641/101000 (59.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2917s / 9638.4598 s
agent0:                 episode reward: -0.3463,                 loss: 0.2086
agent1:                 episode reward: 0.3463,                 loss: nan
Episode: 59661/101000 (59.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7848s / 9642.2446 s
agent0:                 episode reward: -0.5031,                 loss: 0.2102
agent1:                 episode reward: 0.5031,                 loss: nan
Episode: 59681/101000 (59.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1830s / 9646.4275 s
agent0:                 episode reward: -0.0321,                 loss: 0.2086
agent1:                 episode reward: 0.0321,                 loss: nan
Episode: 59701/101000 (59.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1641s / 9650.5917 s
agent0:                 episode reward: 0.0324,                 loss: 0.2109
agent1:                 episode reward: -0.0324,                 loss: nan
Episode: 59721/101000 (59.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5847s / 9655.1763 s
agent0:                 episode reward: 0.0800,                 loss: 0.2082
agent1:                 episode reward: -0.0800,                 loss: nan
Episode: 59741/101000 (59.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8882s / 9659.0645 s
agent0:                 episode reward: -0.0454,                 loss: 0.2087
agent1:                 episode reward: 0.0454,                 loss: nan
Episode: 59761/101000 (59.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1158s / 9663.1803 s
agent0:                 episode reward: 0.0871,                 loss: 0.2076
agent1:                 episode reward: -0.0871,                 loss: nan
Episode: 59781/101000 (59.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4371s / 9666.6173 s
agent0:                 episode reward: -0.3674,                 loss: 0.2077
agent1:                 episode reward: 0.3674,                 loss: nan
Episode: 59801/101000 (59.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1363s / 9671.7537 s
agent0:                 episode reward: 0.0101,                 loss: 0.2161
agent1:                 episode reward: -0.0101,                 loss: nan
Episode: 59821/101000 (59.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8239s / 9675.5775 s
agent0:                 episode reward: 0.0051,                 loss: 0.2147
agent1:                 episode reward: -0.0051,                 loss: nan
Episode: 59841/101000 (59.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5789s / 9679.1564 s
agent0:                 episode reward: -0.0770,                 loss: 0.2150
agent1:                 episode reward: 0.0770,                 loss: nan
Episode: 59861/101000 (59.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4080s / 9682.5644 s
agent0:                 episode reward: 0.2848,                 loss: 0.2148
agent1:                 episode reward: -0.2848,                 loss: nan
Episode: 59881/101000 (59.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8389s / 9686.4033 s
agent0:                 episode reward: 0.5206,                 loss: 0.2134
agent1:                 episode reward: -0.5206,                 loss: 0.1566
Score delta: 2.08216688122798, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/59447_0.
Episode: 59901/101000 (59.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1152s / 9690.5185 s
agent0:                 episode reward: -0.2340,                 loss: nan
agent1:                 episode reward: 0.2340,                 loss: 0.1527
Episode: 59921/101000 (59.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2689s / 9694.7874 s
agent0:                 episode reward: -0.2131,                 loss: nan
agent1:                 episode reward: 0.2131,                 loss: 0.1530
Episode: 59941/101000 (59.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5263s / 9698.3138 s
agent0:                 episode reward: -0.7515,                 loss: nan
agent1:                 episode reward: 0.7515,                 loss: 0.1535
Episode: 59961/101000 (59.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1047s / 9702.4184 s
agent0:                 episode reward: -0.4515,                 loss: nan
agent1:                 episode reward: 0.4515,                 loss: 0.1519
Episode: 59981/101000 (59.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4350s / 9705.8534 s
agent0:                 episode reward: -0.2455,                 loss: nan
agent1:                 episode reward: 0.2455,                 loss: 0.1535
Episode: 60001/101000 (59.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1280s / 9708.9814 s
agent0:                 episode reward: -0.1548,                 loss: 0.1864
agent1:                 episode reward: 0.1548,                 loss: 0.1527
Score delta: 1.6021256091434466, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/59563_1.
Episode: 60021/101000 (59.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3396s / 9712.3211 s
agent0:                 episode reward: 0.2476,                 loss: 0.1814
agent1:                 episode reward: -0.2476,                 loss: nan
Episode: 60041/101000 (59.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9438s / 9716.2649 s
agent0:                 episode reward: 0.1834,                 loss: 0.1797
agent1:                 episode reward: -0.1834,                 loss: nan
Episode: 60061/101000 (59.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2585s / 9720.5233 s
agent0:                 episode reward: 0.0141,                 loss: 0.1766
agent1:                 episode reward: -0.0141,                 loss: nan
Episode: 60081/101000 (59.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3936s / 9724.9169 s
agent0:                 episode reward: 0.2757,                 loss: 0.1801
agent1:                 episode reward: -0.2757,                 loss: nan
Episode: 60101/101000 (59.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8094s / 9729.7262 s
agent0:                 episode reward: 0.4266,                 loss: 0.1801
agent1:                 episode reward: -0.4266,                 loss: nan
Episode: 60121/101000 (59.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2534s / 9733.9796 s
agent0:                 episode reward: 0.0152,                 loss: 0.1821
agent1:                 episode reward: -0.0152,                 loss: 0.1439
Score delta: 1.7759336075100414, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/59678_0.
Episode: 60141/101000 (59.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3023s / 9737.2819 s
agent0:                 episode reward: -0.1244,                 loss: nan
agent1:                 episode reward: 0.1244,                 loss: 0.1434
Episode: 60161/101000 (59.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6550s / 9740.9369 s
agent0:                 episode reward: 0.0832,                 loss: nan
agent1:                 episode reward: -0.0832,                 loss: 0.1445
Episode: 60181/101000 (59.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7574s / 9744.6943 s
agent0:                 episode reward: -0.6723,                 loss: nan
agent1:                 episode reward: 0.6723,                 loss: 0.1412
Episode: 60201/101000 (59.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4675s / 9749.1617 s
agent0:                 episode reward: -0.2196,                 loss: nan
agent1:                 episode reward: 0.2196,                 loss: 0.1440
Episode: 60221/101000 (59.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3545s / 9753.5163 s
agent0:                 episode reward: -0.1090,                 loss: nan
agent1:                 episode reward: 0.1090,                 loss: 0.1429
Episode: 60241/101000 (59.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2020s / 9756.7183 s
agent0:                 episode reward: -0.5587,                 loss: nan
agent1:                 episode reward: 0.5587,                 loss: 0.1425
Episode: 60261/101000 (59.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7657s / 9760.4840 s
agent0:                 episode reward: 0.2364,                 loss: 0.2014
agent1:                 episode reward: -0.2364,                 loss: 0.1427
Score delta: 1.5399825704731067, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/59816_1.
Episode: 60281/101000 (59.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3344s / 9764.8184 s
agent0:                 episode reward: -0.3011,                 loss: 0.2029
agent1:                 episode reward: 0.3011,                 loss: nan
Episode: 60301/101000 (59.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0821s / 9768.9005 s
agent0:                 episode reward: 0.0295,                 loss: 0.2019
agent1:                 episode reward: -0.0295,                 loss: nan
Episode: 60321/101000 (59.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5234s / 9772.4239 s
agent0:                 episode reward: 0.0547,                 loss: 0.2009
agent1:                 episode reward: -0.0547,                 loss: nan
Episode: 60341/101000 (59.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5665s / 9776.9904 s
agent0:                 episode reward: -0.0110,                 loss: 0.2022
agent1:                 episode reward: 0.0110,                 loss: nan
Episode: 60361/101000 (59.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5048s / 9781.4952 s
agent0:                 episode reward: 0.3042,                 loss: 0.2045
agent1:                 episode reward: -0.3042,                 loss: nan
Episode: 60381/101000 (59.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5446s / 9786.0398 s
agent0:                 episode reward: -0.0754,                 loss: 0.2141
agent1:                 episode reward: 0.0754,                 loss: nan
Episode: 60401/101000 (59.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9873s / 9790.0270 s
agent0:                 episode reward: -0.1332,                 loss: 0.2157
agent1:                 episode reward: 0.1332,                 loss: nan
Episode: 60421/101000 (59.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8686s / 9793.8957 s
agent0:                 episode reward: -0.2264,                 loss: 0.2174
agent1:                 episode reward: 0.2264,                 loss: nan
Episode: 60441/101000 (59.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2668s / 9798.1625 s
agent0:                 episode reward: -0.3573,                 loss: 0.2160
agent1:                 episode reward: 0.3573,                 loss: nan
Episode: 60461/101000 (59.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4597s / 9801.6222 s
agent0:                 episode reward: 0.5105,                 loss: 0.2169
agent1:                 episode reward: -0.5105,                 loss: 0.1599
Score delta: 1.5964224051929796, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/60024_0.
Episode: 60481/101000 (59.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9062s / 9805.5284 s
agent0:                 episode reward: 0.0744,                 loss: nan
agent1:                 episode reward: -0.0744,                 loss: 0.1563
Episode: 60501/101000 (59.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1212s / 9809.6496 s
agent0:                 episode reward: -0.2548,                 loss: nan
agent1:                 episode reward: 0.2548,                 loss: 0.1552
Episode: 60521/101000 (59.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1128s / 9813.7623 s
agent0:                 episode reward: -0.4897,                 loss: nan
agent1:                 episode reward: 0.4897,                 loss: 0.1499
Episode: 60541/101000 (59.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4582s / 9817.2206 s
agent0:                 episode reward: -0.5123,                 loss: nan
agent1:                 episode reward: 0.5123,                 loss: 0.1452
Episode: 60561/101000 (59.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9328s / 9822.1533 s
agent0:                 episode reward: -0.4191,                 loss: 0.1926
agent1:                 episode reward: 0.4191,                 loss: 0.1441
Score delta: 1.5156807659813298, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/60128_1.
Episode: 60581/101000 (59.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4256s / 9826.5789 s
agent0:                 episode reward: -0.0319,                 loss: 0.1833
agent1:                 episode reward: 0.0319,                 loss: nan
Episode: 60601/101000 (60.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6569s / 9830.2358 s
agent0:                 episode reward: -0.2683,                 loss: 0.1823
agent1:                 episode reward: 0.2683,                 loss: nan
Episode: 60621/101000 (60.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1607s / 9834.3965 s
agent0:                 episode reward: 0.2398,                 loss: 0.1809
agent1:                 episode reward: -0.2398,                 loss: nan
Episode: 60641/101000 (60.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8074s / 9838.2039 s
agent0:                 episode reward: -0.1278,                 loss: 0.1803
agent1:                 episode reward: 0.1278,                 loss: nan
Episode: 60661/101000 (60.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2099s / 9841.4137 s
agent0:                 episode reward: 0.1241,                 loss: 0.1795
agent1:                 episode reward: -0.1241,                 loss: nan
Episode: 60681/101000 (60.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2945s / 9845.7082 s
agent0:                 episode reward: -0.0156,                 loss: 0.1804
agent1:                 episode reward: 0.0156,                 loss: nan
Episode: 60701/101000 (60.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5854s / 9850.2936 s
agent0:                 episode reward: -0.6420,                 loss: 0.1808
agent1:                 episode reward: 0.6420,                 loss: nan
Episode: 60721/101000 (60.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6598s / 9853.9534 s
agent0:                 episode reward: -0.0014,                 loss: 0.1793
agent1:                 episode reward: 0.0014,                 loss: nan
Episode: 60741/101000 (60.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2337s / 9858.1872 s
agent0:                 episode reward: 0.2524,                 loss: 0.1800
agent1:                 episode reward: -0.2524,                 loss: nan
Episode: 60761/101000 (60.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4391s / 9861.6262 s
agent0:                 episode reward: 0.3989,                 loss: 0.1798
agent1:                 episode reward: -0.3989,                 loss: nan
Episode: 60781/101000 (60.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0705s / 9865.6968 s
agent0:                 episode reward: -0.0846,                 loss: 0.1791
agent1:                 episode reward: 0.0846,                 loss: nan
Episode: 60801/101000 (60.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1678s / 9870.8646 s
agent0:                 episode reward: 0.3980,                 loss: 0.1796
agent1:                 episode reward: -0.3980,                 loss: nan
Episode: 60821/101000 (60.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2287s / 9875.0933 s
agent0:                 episode reward: -0.6098,                 loss: 0.1964
agent1:                 episode reward: 0.6098,                 loss: nan
Episode: 60841/101000 (60.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1882s / 9879.2814 s
agent0:                 episode reward: 0.0497,                 loss: 0.1990
agent1:                 episode reward: -0.0497,                 loss: nan
Episode: 60861/101000 (60.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5375s / 9882.8189 s
agent0:                 episode reward: 0.4774,                 loss: 0.2018
agent1:                 episode reward: -0.4774,                 loss: 0.1522
Score delta: 1.6560497711671442, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/60434_0.
Episode: 60881/101000 (60.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5670s / 9887.3859 s
agent0:                 episode reward: -0.5397,                 loss: nan
agent1:                 episode reward: 0.5397,                 loss: 0.1522
Episode: 60901/101000 (60.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8780s / 9891.2639 s
agent0:                 episode reward: -0.2254,                 loss: nan
agent1:                 episode reward: 0.2254,                 loss: 0.1514
Episode: 60921/101000 (60.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5562s / 9895.8201 s
agent0:                 episode reward: -0.2949,                 loss: nan
agent1:                 episode reward: 0.2949,                 loss: 0.1520
Episode: 60941/101000 (60.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0487s / 9899.8688 s
agent0:                 episode reward: 0.0361,                 loss: nan
agent1:                 episode reward: -0.0361,                 loss: 0.1521
Episode: 60961/101000 (60.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1017s / 9903.9705 s
agent0:                 episode reward: -0.0122,                 loss: nan
agent1:                 episode reward: 0.0122,                 loss: 0.1514
Episode: 60981/101000 (60.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1030s / 9909.0735 s
agent0:                 episode reward: -0.3131,                 loss: 0.1887
agent1:                 episode reward: 0.3131,                 loss: 0.1522
Score delta: 1.5016971441944005, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/60543_1.
Episode: 61001/101000 (60.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8010s / 9913.8745 s
agent0:                 episode reward: 0.5885,                 loss: 0.1906
agent1:                 episode reward: -0.5885,                 loss: nan
Episode: 61021/101000 (60.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7460s / 9917.6204 s
agent0:                 episode reward: 0.1245,                 loss: 0.1862
agent1:                 episode reward: -0.1245,                 loss: nan
Episode: 61041/101000 (60.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7987s / 9921.4192 s
agent0:                 episode reward: 0.0893,                 loss: 0.1886
agent1:                 episode reward: -0.0893,                 loss: nan
Episode: 61061/101000 (60.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1306s / 9925.5498 s
agent0:                 episode reward: 0.1005,                 loss: 0.1872
agent1:                 episode reward: -0.1005,                 loss: nan
Episode: 61081/101000 (60.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3639s / 9928.9138 s
agent0:                 episode reward: -0.0002,                 loss: 0.1853
agent1:                 episode reward: 0.0002,                 loss: 0.1403
Score delta: 1.6535681230261479, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/60645_0.
Episode: 61101/101000 (60.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8263s / 9932.7401 s
agent0:                 episode reward: 0.1804,                 loss: nan
agent1:                 episode reward: -0.1804,                 loss: 0.1413
Episode: 61121/101000 (60.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1074s / 9936.8475 s
agent0:                 episode reward: 0.0331,                 loss: nan
agent1:                 episode reward: -0.0331,                 loss: 0.1416
Episode: 61141/101000 (60.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1308s / 9940.9783 s
agent0:                 episode reward: 0.1493,                 loss: nan
agent1:                 episode reward: -0.1493,                 loss: 0.1420
Episode: 61161/101000 (60.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6908s / 9944.6691 s
agent0:                 episode reward: -0.1822,                 loss: nan
agent1:                 episode reward: 0.1822,                 loss: 0.1415
Episode: 61181/101000 (60.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0546s / 9947.7238 s
agent0:                 episode reward: -0.1601,                 loss: nan
agent1:                 episode reward: 0.1601,                 loss: 0.1393
Episode: 61201/101000 (60.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2281s / 9950.9519 s
agent0:                 episode reward: -0.1704,                 loss: nan
agent1:                 episode reward: 0.1704,                 loss: 0.1388
Episode: 61221/101000 (60.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8755s / 9954.8273 s
agent0:                 episode reward: 0.0005,                 loss: nan
agent1:                 episode reward: -0.0005,                 loss: 0.1400
Episode: 61241/101000 (60.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2692s / 9959.0966 s
agent0:                 episode reward: -0.1729,                 loss: nan
agent1:                 episode reward: 0.1729,                 loss: 0.1384
Episode: 61261/101000 (60.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7516s / 9962.8482 s
agent0:                 episode reward: -0.0323,                 loss: nan
agent1:                 episode reward: 0.0323,                 loss: 0.1528
Episode: 61281/101000 (60.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0618s / 9966.9100 s
agent0:                 episode reward: -0.0947,                 loss: nan
agent1:                 episode reward: 0.0947,                 loss: 0.1585
Episode: 61301/101000 (60.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6535s / 9971.5635 s
agent0:                 episode reward: -0.4271,                 loss: nan
agent1:                 episode reward: 0.4271,                 loss: 0.1556
Episode: 61321/101000 (60.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9580s / 9975.5215 s
agent0:                 episode reward: -0.1867,                 loss: nan
agent1:                 episode reward: 0.1867,                 loss: 0.1548
Episode: 61341/101000 (60.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7474s / 9979.2689 s
agent0:                 episode reward: -0.3943,                 loss: nan
agent1:                 episode reward: 0.3943,                 loss: 0.1547
Episode: 61361/101000 (60.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1011s / 9983.3700 s
agent0:                 episode reward: -0.4869,                 loss: 0.2184
agent1:                 episode reward: 0.4869,                 loss: 0.1559
Score delta: 1.6034701689724655, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/60923_1.
Episode: 61381/101000 (60.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5836s / 9986.9536 s
agent0:                 episode reward: -0.1855,                 loss: 0.2016
agent1:                 episode reward: 0.1855,                 loss: nan
Episode: 61401/101000 (60.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7876s / 9991.7412 s
agent0:                 episode reward: -0.0681,                 loss: 0.1996
agent1:                 episode reward: 0.0681,                 loss: nan
Episode: 61421/101000 (60.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7894s / 9995.5306 s
agent0:                 episode reward: -0.4664,                 loss: 0.1998
agent1:                 episode reward: 0.4664,                 loss: nan
Episode: 61441/101000 (60.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9299s / 9999.4605 s
agent0:                 episode reward: -0.2533,                 loss: 0.1995
agent1:                 episode reward: 0.2533,                 loss: nan
Episode: 61461/101000 (60.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8444s / 10004.3049 s
agent0:                 episode reward: 0.0352,                 loss: 0.1976
agent1:                 episode reward: -0.0352,                 loss: nan
Episode: 61481/101000 (60.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6941s / 10007.9990 s
agent0:                 episode reward: 0.0048,                 loss: 0.1972
agent1:                 episode reward: -0.0048,                 loss: nan
Episode: 61501/101000 (60.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7798s / 10011.7788 s
agent0:                 episode reward: -0.2160,                 loss: 0.1969
agent1:                 episode reward: 0.2160,                 loss: nan
Episode: 61521/101000 (60.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1223s / 10016.9011 s
agent0:                 episode reward: 0.3841,                 loss: 0.1973
agent1:                 episode reward: -0.3841,                 loss: nan
Episode: 61541/101000 (60.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1661s / 10020.0671 s
agent0:                 episode reward: -0.0580,                 loss: 0.2110
agent1:                 episode reward: 0.0580,                 loss: nan
Episode: 61561/101000 (60.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9065s / 10023.9736 s
agent0:                 episode reward: -0.4118,                 loss: 0.2092
agent1:                 episode reward: 0.4118,                 loss: nan
Episode: 61581/101000 (60.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1422s / 10028.1158 s
agent0:                 episode reward: 0.3102,                 loss: 0.2084
agent1:                 episode reward: -0.3102,                 loss: nan
Score delta: 1.7078793224945596, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/61155_0.
Episode: 61601/101000 (60.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7739s / 10031.8897 s
agent0:                 episode reward: -0.4864,                 loss: nan
agent1:                 episode reward: 0.4864,                 loss: 0.1547
Episode: 61621/101000 (61.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8273s / 10035.7170 s
agent0:                 episode reward: -0.1032,                 loss: nan
agent1:                 episode reward: 0.1032,                 loss: 0.1534
Episode: 61641/101000 (61.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0114s / 10039.7284 s
agent0:                 episode reward: -0.4493,                 loss: nan
agent1:                 episode reward: 0.4493,                 loss: 0.1530
Episode: 61661/101000 (61.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5903s / 10044.3187 s
agent0:                 episode reward: -0.5568,                 loss: nan
agent1:                 episode reward: 0.5568,                 loss: 0.1513
Episode: 61681/101000 (61.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0291s / 10048.3478 s
agent0:                 episode reward: -0.5745,                 loss: nan
agent1:                 episode reward: 0.5745,                 loss: 0.1526
Episode: 61701/101000 (61.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9554s / 10052.3032 s
agent0:                 episode reward: 0.2984,                 loss: nan
agent1:                 episode reward: -0.2984,                 loss: 0.1518
Episode: 61721/101000 (61.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6041s / 10055.9073 s
agent0:                 episode reward: -0.1992,                 loss: nan
agent1:                 episode reward: 0.1992,                 loss: 0.1528
Episode: 61741/101000 (61.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4247s / 10059.3320 s
agent0:                 episode reward: 0.1990,                 loss: 0.2020
agent1:                 episode reward: -0.1990,                 loss: 0.1507
Score delta: 1.5254083970637886, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/61302_1.
Episode: 61761/101000 (61.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9927s / 10063.3247 s
agent0:                 episode reward: -0.0421,                 loss: 0.2032
agent1:                 episode reward: 0.0421,                 loss: nan
Episode: 61781/101000 (61.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5415s / 10066.8663 s
agent0:                 episode reward: -0.3844,                 loss: 0.2062
agent1:                 episode reward: 0.3844,                 loss: nan
Episode: 61801/101000 (61.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5633s / 10071.4296 s
agent0:                 episode reward: 0.1635,                 loss: 0.2046
agent1:                 episode reward: -0.1635,                 loss: nan
Episode: 61821/101000 (61.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2790s / 10075.7086 s
agent0:                 episode reward: -0.2992,                 loss: 0.2030
agent1:                 episode reward: 0.2992,                 loss: nan
Episode: 61841/101000 (61.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8140s / 10080.5226 s
agent0:                 episode reward: -0.1625,                 loss: 0.2025
agent1:                 episode reward: 0.1625,                 loss: nan
Episode: 61861/101000 (61.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0999s / 10084.6225 s
agent0:                 episode reward: 0.4625,                 loss: 0.2023
agent1:                 episode reward: -0.4625,                 loss: nan
Episode: 61881/101000 (61.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5316s / 10088.1541 s
agent0:                 episode reward: -0.3029,                 loss: 0.2021
agent1:                 episode reward: 0.3029,                 loss: 0.1528
Score delta: 1.831897468869012, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/61441_0.
Episode: 61901/101000 (61.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1204s / 10092.2744 s
agent0:                 episode reward: -0.3540,                 loss: nan
agent1:                 episode reward: 0.3540,                 loss: 0.1532
Episode: 61921/101000 (61.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7634s / 10096.0378 s
agent0:                 episode reward: 0.1699,                 loss: nan
agent1:                 episode reward: -0.1699,                 loss: 0.1532
Episode: 61941/101000 (61.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0858s / 10099.1236 s
agent0:                 episode reward: -0.3136,                 loss: nan
agent1:                 episode reward: 0.3136,                 loss: 0.1533
Episode: 61961/101000 (61.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2909s / 10103.4145 s
agent0:                 episode reward: -0.1187,                 loss: nan
agent1:                 episode reward: 0.1187,                 loss: 0.1486
Episode: 61981/101000 (61.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1488s / 10107.5634 s
agent0:                 episode reward: -0.2976,                 loss: nan
agent1:                 episode reward: 0.2976,                 loss: 0.1409
Episode: 62001/101000 (61.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8352s / 10111.3986 s
agent0:                 episode reward: -0.0748,                 loss: nan
agent1:                 episode reward: 0.0748,                 loss: 0.1413
Episode: 62021/101000 (61.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9066s / 10115.3052 s
agent0:                 episode reward: -0.4745,                 loss: nan
agent1:                 episode reward: 0.4745,                 loss: 0.1416
Score delta: 1.7379830147701554, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/61595_1.
Episode: 62041/101000 (61.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6193s / 10118.9245 s
agent0:                 episode reward: 0.0545,                 loss: 0.2093
agent1:                 episode reward: -0.0545,                 loss: nan
Episode: 62061/101000 (61.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0931s / 10123.0176 s
agent0:                 episode reward: 0.0413,                 loss: 0.2096
agent1:                 episode reward: -0.0413,                 loss: nan
Episode: 62081/101000 (61.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2388s / 10127.2564 s
agent0:                 episode reward: 0.5023,                 loss: 0.2093
agent1:                 episode reward: -0.5023,                 loss: nan
Episode: 62101/101000 (61.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3280s / 10131.5844 s
agent0:                 episode reward: 0.0071,                 loss: 0.2089
agent1:                 episode reward: -0.0071,                 loss: nan
Episode: 62121/101000 (61.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1933s / 10135.7778 s
agent0:                 episode reward: -0.3233,                 loss: 0.2115
agent1:                 episode reward: 0.3233,                 loss: nan
Episode: 62141/101000 (61.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8790s / 10140.6568 s
agent0:                 episode reward: 0.3902,                 loss: 0.2080
agent1:                 episode reward: -0.3902,                 loss: nan
Episode: 62161/101000 (61.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3089s / 10144.9657 s
agent0:                 episode reward: -0.1309,                 loss: 0.2088
agent1:                 episode reward: 0.1309,                 loss: nan
Episode: 62181/101000 (61.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6230s / 10148.5887 s
agent0:                 episode reward: -0.0348,                 loss: 0.1992
agent1:                 episode reward: 0.0348,                 loss: nan
Episode: 62201/101000 (61.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1752s / 10153.7639 s
agent0:                 episode reward: -0.1202,                 loss: 0.1992
agent1:                 episode reward: 0.1202,                 loss: nan
Episode: 62221/101000 (61.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7933s / 10157.5573 s
agent0:                 episode reward: -0.1910,                 loss: 0.1979
agent1:                 episode reward: 0.1910,                 loss: nan
Episode: 62241/101000 (61.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8236s / 10161.3809 s
agent0:                 episode reward: 0.4117,                 loss: 0.1965
agent1:                 episode reward: -0.4117,                 loss: nan
Episode: 62261/101000 (61.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5420s / 10165.9229 s
agent0:                 episode reward: -0.4853,                 loss: 0.1965
agent1:                 episode reward: 0.4853,                 loss: nan
Episode: 62281/101000 (61.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3731s / 10170.2961 s
agent0:                 episode reward: 0.0316,                 loss: 0.1968
agent1:                 episode reward: -0.0316,                 loss: nan
Episode: 62301/101000 (61.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5123s / 10173.8083 s
agent0:                 episode reward: 0.2375,                 loss: 0.1978
agent1:                 episode reward: -0.2375,                 loss: 0.1563
Score delta: 1.508295956938325, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/61873_0.
Episode: 62321/101000 (61.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3995s / 10177.2079 s
agent0:                 episode reward: 0.3793,                 loss: nan
agent1:                 episode reward: -0.3793,                 loss: 0.1482
Episode: 62341/101000 (61.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4633s / 10180.6711 s
agent0:                 episode reward: -0.3220,                 loss: nan
agent1:                 episode reward: 0.3220,                 loss: 0.1461
Episode: 62361/101000 (61.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7871s / 10185.4582 s
agent0:                 episode reward: -0.6449,                 loss: nan
agent1:                 episode reward: 0.6449,                 loss: 0.1426
Episode: 62381/101000 (61.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7116s / 10189.1698 s
agent0:                 episode reward: -0.1152,                 loss: nan
agent1:                 episode reward: 0.1152,                 loss: 0.1412
Episode: 62401/101000 (61.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0057s / 10193.1755 s
agent0:                 episode reward: -0.4338,                 loss: nan
agent1:                 episode reward: 0.4338,                 loss: 0.1425
Episode: 62421/101000 (61.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1182s / 10197.2936 s
agent0:                 episode reward: -0.4395,                 loss: nan
agent1:                 episode reward: 0.4395,                 loss: 0.1417
Episode: 62441/101000 (61.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6037s / 10200.8974 s
agent0:                 episode reward: -0.4068,                 loss: nan
agent1:                 episode reward: 0.4068,                 loss: 0.1402
Episode: 62461/101000 (61.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5653s / 10204.4627 s
agent0:                 episode reward: -0.2404,                 loss: nan
agent1:                 episode reward: 0.2404,                 loss: 0.1434
Episode: 62481/101000 (61.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8039s / 10208.2667 s
agent0:                 episode reward: -0.1133,                 loss: 0.1983
agent1:                 episode reward: 0.1133,                 loss: 0.1435
Score delta: 1.619149339187686, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/62040_1.
Episode: 62501/101000 (61.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2050s / 10212.4716 s
agent0:                 episode reward: -0.4430,                 loss: 0.1989
agent1:                 episode reward: 0.4430,                 loss: nan
Episode: 62521/101000 (61.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1897s / 10216.6613 s
agent0:                 episode reward: -0.1367,                 loss: 0.1993
agent1:                 episode reward: 0.1367,                 loss: nan
Episode: 62541/101000 (61.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2684s / 10220.9298 s
agent0:                 episode reward: 0.0205,                 loss: 0.1991
agent1:                 episode reward: -0.0205,                 loss: nan
Episode: 62561/101000 (61.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3869s / 10224.3167 s
agent0:                 episode reward: 0.2585,                 loss: 0.1972
agent1:                 episode reward: -0.2585,                 loss: nan
Episode: 62581/101000 (61.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6021s / 10228.9188 s
agent0:                 episode reward: -0.0329,                 loss: 0.1947
agent1:                 episode reward: 0.0329,                 loss: nan
Episode: 62601/101000 (61.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4309s / 10232.3497 s
agent0:                 episode reward: -0.0363,                 loss: 0.1980
agent1:                 episode reward: 0.0363,                 loss: nan
Episode: 62621/101000 (62.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3219s / 10236.6716 s
agent0:                 episode reward: -0.2691,                 loss: 0.1959
agent1:                 episode reward: 0.2691,                 loss: nan
Episode: 62641/101000 (62.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1987s / 10240.8703 s
agent0:                 episode reward: 0.0126,                 loss: 0.1972
agent1:                 episode reward: -0.0126,                 loss: nan
Episode: 62661/101000 (62.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9936s / 10244.8639 s
agent0:                 episode reward: 0.0294,                 loss: 0.1983
agent1:                 episode reward: -0.0294,                 loss: 0.1492
Score delta: 1.670944329305199, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/62219_0.
Episode: 62681/101000 (62.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5682s / 10248.4321 s
agent0:                 episode reward: -0.3047,                 loss: nan
agent1:                 episode reward: 0.3047,                 loss: 0.1515
Episode: 62701/101000 (62.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7879s / 10252.2199 s
agent0:                 episode reward: 0.2098,                 loss: nan
agent1:                 episode reward: -0.2098,                 loss: 0.1501
Episode: 62721/101000 (62.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5009s / 10255.7209 s
agent0:                 episode reward: 0.1106,                 loss: nan
agent1:                 episode reward: -0.1106,                 loss: 0.1479
Episode: 62741/101000 (62.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9768s / 10260.6977 s
agent0:                 episode reward: -0.0041,                 loss: nan
agent1:                 episode reward: 0.0041,                 loss: 0.1489
Episode: 62761/101000 (62.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6883s / 10265.3860 s
agent0:                 episode reward: -0.0892,                 loss: nan
agent1:                 episode reward: 0.0892,                 loss: 0.1475
Episode: 62781/101000 (62.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6654s / 10270.0514 s
agent0:                 episode reward: -0.6240,                 loss: 0.2187
agent1:                 episode reward: 0.6240,                 loss: 0.1442
Score delta: 1.5046143371477332, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/62353_1.
Episode: 62801/101000 (62.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6383s / 10274.6898 s
agent0:                 episode reward: -0.1394,                 loss: 0.2084
agent1:                 episode reward: 0.1394,                 loss: nan
Episode: 62821/101000 (62.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3824s / 10280.0722 s
agent0:                 episode reward: 0.1404,                 loss: 0.2021
agent1:                 episode reward: -0.1404,                 loss: nan
Episode: 62841/101000 (62.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1824s / 10284.2545 s
agent0:                 episode reward: -0.0237,                 loss: 0.1994
agent1:                 episode reward: 0.0237,                 loss: nan
Episode: 62861/101000 (62.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2438s / 10288.4984 s
agent0:                 episode reward: -0.1149,                 loss: 0.1981
agent1:                 episode reward: 0.1149,                 loss: nan
Episode: 62881/101000 (62.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4069s / 10292.9052 s
agent0:                 episode reward: 0.2620,                 loss: 0.2009
agent1:                 episode reward: -0.2620,                 loss: nan
Episode: 62901/101000 (62.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6365s / 10296.5418 s
agent0:                 episode reward: -0.2533,                 loss: 0.1979
agent1:                 episode reward: 0.2533,                 loss: nan
Episode: 62921/101000 (62.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0657s / 10300.6075 s
agent0:                 episode reward: 0.1093,                 loss: 0.1983
agent1:                 episode reward: -0.1093,                 loss: 0.1509
Score delta: 1.7508102286530456, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/62483_0.
Episode: 62941/101000 (62.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9978s / 10304.6053 s
agent0:                 episode reward: -0.2443,                 loss: nan
agent1:                 episode reward: 0.2443,                 loss: 0.1519
Episode: 62961/101000 (62.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0061s / 10308.6114 s
agent0:                 episode reward: -0.3948,                 loss: nan
agent1:                 episode reward: 0.3948,                 loss: 0.1502
Episode: 62981/101000 (62.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5012s / 10312.1126 s
agent0:                 episode reward: -0.4903,                 loss: nan
agent1:                 episode reward: 0.4903,                 loss: 0.1507
Episode: 63001/101000 (62.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2013s / 10315.3140 s
agent0:                 episode reward: -0.5720,                 loss: nan
agent1:                 episode reward: 0.5720,                 loss: 0.1503
Episode: 63021/101000 (62.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3532s / 10318.6672 s
agent0:                 episode reward: -0.0703,                 loss: nan
agent1:                 episode reward: 0.0703,                 loss: 0.1522
Episode: 63041/101000 (62.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9616s / 10322.6288 s
agent0:                 episode reward: 0.1439,                 loss: nan
agent1:                 episode reward: -0.1439,                 loss: 0.1514
Episode: 63061/101000 (62.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7150s / 10326.3437 s
agent0:                 episode reward: -0.1873,                 loss: nan
agent1:                 episode reward: 0.1873,                 loss: 0.1498
Episode: 63081/101000 (62.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6434s / 10330.9872 s
agent0:                 episode reward: -0.3008,                 loss: nan
agent1:                 episode reward: 0.3008,                 loss: 0.1505
Episode: 63101/101000 (62.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7563s / 10335.7434 s
agent0:                 episode reward: -0.1801,                 loss: nan
agent1:                 episode reward: 0.1801,                 loss: 0.1494
Episode: 63121/101000 (62.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8165s / 10339.5599 s
agent0:                 episode reward: -0.2695,                 loss: nan
agent1:                 episode reward: 0.2695,                 loss: 0.1509
Episode: 63141/101000 (62.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3889s / 10342.9488 s
agent0:                 episode reward: -0.4067,                 loss: 0.2053
agent1:                 episode reward: 0.4067,                 loss: 0.1510
Score delta: 1.6289166657139549, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/62704_1.
Episode: 63161/101000 (62.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1749s / 10347.1237 s
agent0:                 episode reward: -0.2026,                 loss: 0.2024
agent1:                 episode reward: 0.2026,                 loss: nan
Episode: 63181/101000 (62.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7475s / 10350.8713 s
agent0:                 episode reward: 0.4366,                 loss: 0.2037
agent1:                 episode reward: -0.4366,                 loss: nan
Episode: 63201/101000 (62.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8981s / 10354.7693 s
agent0:                 episode reward: -0.0444,                 loss: 0.2016
agent1:                 episode reward: 0.0444,                 loss: nan
Episode: 63221/101000 (62.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0120s / 10358.7813 s
agent0:                 episode reward: -0.3124,                 loss: 0.2014
agent1:                 episode reward: 0.3124,                 loss: nan
Episode: 63241/101000 (62.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8058s / 10363.5871 s
agent0:                 episode reward: -0.0229,                 loss: 0.2015
agent1:                 episode reward: 0.0229,                 loss: nan
Episode: 63261/101000 (62.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8188s / 10367.4060 s
agent0:                 episode reward: 0.0780,                 loss: 0.2021
agent1:                 episode reward: -0.0780,                 loss: nan
Episode: 63281/101000 (62.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8682s / 10371.2742 s
agent0:                 episode reward: -0.1223,                 loss: 0.2018
agent1:                 episode reward: 0.1223,                 loss: nan
Episode: 63301/101000 (62.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4782s / 10375.7524 s
agent0:                 episode reward: 0.1389,                 loss: 0.1996
agent1:                 episode reward: -0.1389,                 loss: nan
Episode: 63321/101000 (62.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0027s / 10379.7551 s
agent0:                 episode reward: 0.1996,                 loss: 0.1985
agent1:                 episode reward: -0.1996,                 loss: 0.1523
Score delta: 1.7513160072105898, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/62885_0.
Episode: 63341/101000 (62.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2556s / 10384.0107 s
agent0:                 episode reward: -0.4145,                 loss: nan
agent1:                 episode reward: 0.4145,                 loss: 0.1513
Episode: 63361/101000 (62.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4121s / 10387.4228 s
agent0:                 episode reward: -0.1627,                 loss: nan
agent1:                 episode reward: 0.1627,                 loss: 0.1512
Episode: 63381/101000 (62.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8007s / 10391.2236 s
agent0:                 episode reward: -0.0059,                 loss: nan
agent1:                 episode reward: 0.0059,                 loss: 0.1506
Episode: 63401/101000 (62.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9761s / 10395.1996 s
agent0:                 episode reward: -0.1967,                 loss: nan
agent1:                 episode reward: 0.1967,                 loss: 0.1511
Episode: 63421/101000 (62.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8813s / 10399.0809 s
agent0:                 episode reward: -0.2454,                 loss: nan
agent1:                 episode reward: 0.2454,                 loss: 0.1489
Episode: 63441/101000 (62.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0163s / 10403.0972 s
agent0:                 episode reward: -0.3388,                 loss: nan
agent1:                 episode reward: 0.3388,                 loss: 0.1488
Episode: 63461/101000 (62.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7893s / 10406.8865 s
agent0:                 episode reward: -0.2292,                 loss: nan
agent1:                 episode reward: 0.2292,                 loss: 0.1478
Episode: 63481/101000 (62.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9025s / 10410.7890 s
agent0:                 episode reward: -0.5450,                 loss: 0.1919
agent1:                 episode reward: 0.5450,                 loss: 0.1488
Score delta: 1.5002771698452422, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/63045_1.
Episode: 63501/101000 (62.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2698s / 10415.0588 s
agent0:                 episode reward: 0.3956,                 loss: 0.1940
agent1:                 episode reward: -0.3956,                 loss: nan
Episode: 63521/101000 (62.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4117s / 10419.4705 s
agent0:                 episode reward: -0.2697,                 loss: 0.1991
agent1:                 episode reward: 0.2697,                 loss: nan
Episode: 63541/101000 (62.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8581s / 10424.3286 s
agent0:                 episode reward: -0.0557,                 loss: 0.1984
agent1:                 episode reward: 0.0557,                 loss: nan
Episode: 63561/101000 (62.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8869s / 10429.2155 s
agent0:                 episode reward: -0.0890,                 loss: 0.1970
agent1:                 episode reward: 0.0890,                 loss: nan
Episode: 63581/101000 (62.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6514s / 10433.8669 s
agent0:                 episode reward: 0.0854,                 loss: 0.1968
agent1:                 episode reward: -0.0854,                 loss: nan
Episode: 63601/101000 (62.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6052s / 10437.4722 s
agent0:                 episode reward: 0.1673,                 loss: 0.1985
agent1:                 episode reward: -0.1673,                 loss: nan
Episode: 63621/101000 (62.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4518s / 10441.9239 s
agent0:                 episode reward: 0.1014,                 loss: 0.1970
agent1:                 episode reward: -0.1014,                 loss: 0.1422
Score delta: 1.611843395997416, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/63182_0.
Episode: 63641/101000 (63.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6921s / 10446.6161 s
agent0:                 episode reward: -0.2558,                 loss: nan
agent1:                 episode reward: 0.2558,                 loss: 0.1397
Episode: 63661/101000 (63.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3107s / 10450.9267 s
agent0:                 episode reward: 0.0721,                 loss: nan
agent1:                 episode reward: -0.0721,                 loss: 0.1405
Episode: 63681/101000 (63.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8014s / 10454.7282 s
agent0:                 episode reward: -0.1044,                 loss: nan
agent1:                 episode reward: 0.1044,                 loss: 0.1413
Episode: 63701/101000 (63.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3098s / 10459.0380 s
agent0:                 episode reward: -0.4627,                 loss: nan
agent1:                 episode reward: 0.4627,                 loss: 0.1394
Episode: 63721/101000 (63.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8083s / 10462.8463 s
agent0:                 episode reward: -0.1377,                 loss: nan
agent1:                 episode reward: 0.1377,                 loss: 0.1409
Episode: 63741/101000 (63.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7328s / 10466.5791 s
agent0:                 episode reward: 0.2418,                 loss: nan
agent1:                 episode reward: -0.2418,                 loss: 0.1410
Episode: 63761/101000 (63.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7834s / 10471.3625 s
agent0:                 episode reward: -0.3365,                 loss: nan
agent1:                 episode reward: 0.3365,                 loss: 0.1395
Episode: 63781/101000 (63.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6677s / 10476.0302 s
agent0:                 episode reward: -0.4084,                 loss: nan
agent1:                 episode reward: 0.4084,                 loss: 0.1385
Episode: 63801/101000 (63.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1061s / 10480.1363 s
agent0:                 episode reward: -0.1250,                 loss: nan
agent1:                 episode reward: 0.1250,                 loss: 0.1391
Episode: 63821/101000 (63.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6791s / 10483.8154 s
agent0:                 episode reward: -0.0585,                 loss: nan
agent1:                 episode reward: 0.0585,                 loss: 0.1381
Episode: 63841/101000 (63.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9358s / 10487.7512 s
agent0:                 episode reward: -0.1909,                 loss: nan
agent1:                 episode reward: 0.1909,                 loss: 0.1398
Episode: 63861/101000 (63.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8314s / 10491.5826 s
agent0:                 episode reward: -0.3802,                 loss: nan
agent1:                 episode reward: 0.3802,                 loss: 0.1423
Episode: 63881/101000 (63.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9519s / 10495.5345 s
agent0:                 episode reward: -0.3822,                 loss: nan
agent1:                 episode reward: 0.3822,                 loss: 0.1635
Episode: 63901/101000 (63.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0248s / 10499.5594 s
agent0:                 episode reward: -0.6347,                 loss: 0.2225
agent1:                 episode reward: 0.6347,                 loss: 0.1569
Score delta: 2.006275748114265, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/63472_1.
Episode: 63921/101000 (63.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5906s / 10504.1500 s
agent0:                 episode reward: -0.2017,                 loss: 0.2084
agent1:                 episode reward: 0.2017,                 loss: nan
Episode: 63941/101000 (63.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0784s / 10508.2284 s
agent0:                 episode reward: -0.3885,                 loss: 0.2050
agent1:                 episode reward: 0.3885,                 loss: nan
Episode: 63961/101000 (63.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5336s / 10512.7619 s
agent0:                 episode reward: 0.3492,                 loss: 0.2038
agent1:                 episode reward: -0.3492,                 loss: nan
Episode: 63981/101000 (63.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3067s / 10517.0686 s
agent0:                 episode reward: -0.0171,                 loss: 0.2035
agent1:                 episode reward: 0.0171,                 loss: nan
Episode: 64001/101000 (63.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5683s / 10520.6369 s
agent0:                 episode reward: 0.0469,                 loss: 0.2024
agent1:                 episode reward: -0.0469,                 loss: nan
Episode: 64021/101000 (63.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0594s / 10524.6964 s
agent0:                 episode reward: 0.0380,                 loss: 0.2029
agent1:                 episode reward: -0.0380,                 loss: nan
Episode: 64041/101000 (63.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4356s / 10528.1319 s
agent0:                 episode reward: 0.3061,                 loss: 0.2026
agent1:                 episode reward: -0.3061,                 loss: 0.1407
Score delta: 1.5756483963915386, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/63610_0.
Episode: 64061/101000 (63.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1558s / 10531.2877 s
agent0:                 episode reward: -0.4699,                 loss: nan
agent1:                 episode reward: 0.4699,                 loss: 0.1395
Episode: 64081/101000 (63.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2832s / 10534.5709 s
agent0:                 episode reward: -0.2568,                 loss: nan
agent1:                 episode reward: 0.2568,                 loss: 0.1399
Episode: 64101/101000 (63.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3471s / 10537.9179 s
agent0:                 episode reward: -0.2814,                 loss: nan
agent1:                 episode reward: 0.2814,                 loss: 0.1390
Episode: 64121/101000 (63.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5696s / 10542.4875 s
agent0:                 episode reward: -0.3780,                 loss: nan
agent1:                 episode reward: 0.3780,                 loss: 0.1375
Episode: 64141/101000 (63.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1760s / 10546.6634 s
agent0:                 episode reward: -0.4618,                 loss: nan
agent1:                 episode reward: 0.4618,                 loss: 0.1383
Episode: 64161/101000 (63.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2096s / 10550.8730 s
agent0:                 episode reward: -0.4096,                 loss: nan
agent1:                 episode reward: 0.4096,                 loss: 0.1392
Episode: 64181/101000 (63.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1343s / 10555.0074 s
agent0:                 episode reward: 0.0357,                 loss: 0.2050
agent1:                 episode reward: -0.0357,                 loss: 0.1361
Score delta: 1.6821041376481687, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/63739_1.
Episode: 64201/101000 (63.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2419s / 10559.2492 s
agent0:                 episode reward: 0.2770,                 loss: 0.2034
agent1:                 episode reward: -0.2770,                 loss: nan
Episode: 64221/101000 (63.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5260s / 10562.7752 s
agent0:                 episode reward: -0.2231,                 loss: 0.2024
agent1:                 episode reward: 0.2231,                 loss: nan
Episode: 64241/101000 (63.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3752s / 10568.1504 s
agent0:                 episode reward: -0.0747,                 loss: 0.2016
agent1:                 episode reward: 0.0747,                 loss: nan
Episode: 64261/101000 (63.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4245s / 10571.5748 s
agent0:                 episode reward: -0.0890,                 loss: 0.2053
agent1:                 episode reward: 0.0890,                 loss: nan
Episode: 64281/101000 (63.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8495s / 10575.4243 s
agent0:                 episode reward: 0.0847,                 loss: 0.2186
agent1:                 episode reward: -0.0847,                 loss: nan
Episode: 64301/101000 (63.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6844s / 10579.1087 s
agent0:                 episode reward: 0.4729,                 loss: 0.2202
agent1:                 episode reward: -0.4729,                 loss: 0.1518
Score delta: 2.0393554406805134, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/63869_0.
Episode: 64321/101000 (63.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2436s / 10583.3523 s
agent0:                 episode reward: -0.2154,                 loss: nan
agent1:                 episode reward: 0.2154,                 loss: 0.1482
Episode: 64341/101000 (63.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7829s / 10587.1352 s
agent0:                 episode reward: -0.3061,                 loss: nan
agent1:                 episode reward: 0.3061,                 loss: 0.1451
Episode: 64361/101000 (63.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1560s / 10591.2912 s
agent0:                 episode reward: 0.2001,                 loss: nan
agent1:                 episode reward: -0.2001,                 loss: 0.1441
Episode: 64381/101000 (63.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5195s / 10595.8107 s
agent0:                 episode reward: -0.1823,                 loss: nan
agent1:                 episode reward: 0.1823,                 loss: 0.1432
Episode: 64401/101000 (63.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5286s / 10599.3393 s
agent0:                 episode reward: -0.6874,                 loss: nan
agent1:                 episode reward: 0.6874,                 loss: 0.1432
Episode: 64421/101000 (63.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1464s / 10603.4857 s
agent0:                 episode reward: -0.1635,                 loss: 0.2183
agent1:                 episode reward: 0.1635,                 loss: 0.1441
Score delta: 1.5074652081695439, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/63981_1.
Episode: 64441/101000 (63.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3664s / 10607.8521 s
agent0:                 episode reward: -0.4454,                 loss: 0.2118
agent1:                 episode reward: 0.4454,                 loss: nan
Episode: 64461/101000 (63.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3154s / 10612.1675 s
agent0:                 episode reward: -0.7849,                 loss: 0.2090
agent1:                 episode reward: 0.7849,                 loss: nan
Episode: 64481/101000 (63.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6141s / 10616.7816 s
agent0:                 episode reward: -0.1607,                 loss: 0.2095
agent1:                 episode reward: 0.1607,                 loss: nan
Episode: 64501/101000 (63.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3203s / 10621.1019 s
agent0:                 episode reward: -0.1253,                 loss: 0.2093
agent1:                 episode reward: 0.1253,                 loss: nan
Episode: 64521/101000 (63.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0411s / 10625.1430 s
agent0:                 episode reward: 0.1063,                 loss: 0.2078
agent1:                 episode reward: -0.1063,                 loss: nan
Episode: 64541/101000 (63.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7152s / 10628.8582 s
agent0:                 episode reward: 0.6161,                 loss: 0.2089
agent1:                 episode reward: -0.6161,                 loss: 0.1587
Score delta: 1.7613974338132379, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/64105_0.
Episode: 64561/101000 (63.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0958s / 10632.9540 s
agent0:                 episode reward: -0.5179,                 loss: nan
agent1:                 episode reward: 0.5179,                 loss: 0.1561
Episode: 64581/101000 (63.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6609s / 10637.6149 s
agent0:                 episode reward: -0.1903,                 loss: nan
agent1:                 episode reward: 0.1903,                 loss: 0.1557
Episode: 64601/101000 (63.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7765s / 10641.3914 s
agent0:                 episode reward: -0.2444,                 loss: nan
agent1:                 episode reward: 0.2444,                 loss: 0.1506
Episode: 64621/101000 (63.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6994s / 10646.0909 s
agent0:                 episode reward: -0.3602,                 loss: nan
agent1:                 episode reward: 0.3602,                 loss: 0.1438
Episode: 64641/101000 (64.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0685s / 10650.1594 s
agent0:                 episode reward: -0.9696,                 loss: nan
agent1:                 episode reward: 0.9696,                 loss: 0.1439
Episode: 64661/101000 (64.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6777s / 10653.8371 s
agent0:                 episode reward: -0.0867,                 loss: nan
agent1:                 episode reward: 0.0867,                 loss: 0.1441
Episode: 64681/101000 (64.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1695s / 10659.0066 s
agent0:                 episode reward: -0.4192,                 loss: 0.1902
agent1:                 episode reward: 0.4192,                 loss: 0.1436
Score delta: 1.6378658743669068, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/64250_1.
Episode: 64701/101000 (64.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6625s / 10662.6691 s
agent0:                 episode reward: 0.1763,                 loss: 0.1907
agent1:                 episode reward: -0.1763,                 loss: nan
Episode: 64721/101000 (64.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5176s / 10666.1867 s
agent0:                 episode reward: -0.0863,                 loss: 0.1907
agent1:                 episode reward: 0.0863,                 loss: nan
Episode: 64741/101000 (64.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7980s / 10669.9847 s
agent0:                 episode reward: 0.0399,                 loss: 0.1902
agent1:                 episode reward: -0.0399,                 loss: nan
Episode: 64761/101000 (64.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8934s / 10674.8781 s
agent0:                 episode reward: 0.0498,                 loss: 0.1897
agent1:                 episode reward: -0.0498,                 loss: nan
Episode: 64781/101000 (64.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4763s / 10678.3544 s
agent0:                 episode reward: 0.3214,                 loss: 0.1910
agent1:                 episode reward: -0.3214,                 loss: nan
Episode: 64801/101000 (64.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0886s / 10682.4430 s
agent0:                 episode reward: 0.0703,                 loss: 0.1897
agent1:                 episode reward: -0.0703,                 loss: nan
Episode: 64821/101000 (64.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6093s / 10687.0523 s
agent0:                 episode reward: -0.2780,                 loss: 0.1899
agent1:                 episode reward: 0.2780,                 loss: nan
Episode: 64841/101000 (64.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6215s / 10690.6738 s
agent0:                 episode reward: -0.0884,                 loss: 0.1893
agent1:                 episode reward: 0.0884,                 loss: nan
Episode: 64861/101000 (64.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0444s / 10694.7181 s
agent0:                 episode reward: 0.1840,                 loss: 0.1977
agent1:                 episode reward: -0.1840,                 loss: nan
Episode: 64881/101000 (64.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0322s / 10698.7503 s
agent0:                 episode reward: -0.4671,                 loss: 0.2014
agent1:                 episode reward: 0.4671,                 loss: 0.1690
Score delta: 1.841704339746561, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/64436_0.
Episode: 64901/101000 (64.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0127s / 10702.7629 s
agent0:                 episode reward: -0.2998,                 loss: nan
agent1:                 episode reward: 0.2998,                 loss: 0.1672
Episode: 64921/101000 (64.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8287s / 10706.5917 s
agent0:                 episode reward: -0.3292,                 loss: nan
agent1:                 episode reward: 0.3292,                 loss: 0.1687
Episode: 64941/101000 (64.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2376s / 10710.8293 s
agent0:                 episode reward: -0.6750,                 loss: nan
agent1:                 episode reward: 0.6750,                 loss: 0.1683
Episode: 64961/101000 (64.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4006s / 10714.2299 s
agent0:                 episode reward: -0.0019,                 loss: nan
agent1:                 episode reward: 0.0019,                 loss: 0.1691
Episode: 64981/101000 (64.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5261s / 10718.7560 s
agent0:                 episode reward: -0.5541,                 loss: 0.2034
agent1:                 episode reward: 0.5541,                 loss: 0.1685
Score delta: 1.7588386023290787, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/64553_1.
Episode: 65001/101000 (64.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7905s / 10724.5465 s
agent0:                 episode reward: 0.0522,                 loss: 0.1972
agent1:                 episode reward: -0.0522,                 loss: nan
Episode: 65021/101000 (64.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7536s / 10728.3001 s
agent0:                 episode reward: -0.5908,                 loss: 0.1954
agent1:                 episode reward: 0.5908,                 loss: nan
Episode: 65041/101000 (64.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6120s / 10731.9121 s
agent0:                 episode reward: -0.1722,                 loss: 0.1926
agent1:                 episode reward: 0.1722,                 loss: nan
Episode: 65061/101000 (64.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0024s / 10735.9145 s
agent0:                 episode reward: -0.1462,                 loss: 0.1946
agent1:                 episode reward: 0.1462,                 loss: nan
Episode: 65081/101000 (64.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3264s / 10739.2409 s
agent0:                 episode reward: 0.5933,                 loss: 0.1943
agent1:                 episode reward: -0.5933,                 loss: nan
Episode: 65101/101000 (64.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3920s / 10743.6330 s
agent0:                 episode reward: 0.5232,                 loss: 0.1954
agent1:                 episode reward: -0.5232,                 loss: nan
Episode: 65121/101000 (64.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1944s / 10747.8273 s
agent0:                 episode reward: -0.2743,                 loss: 0.1992
agent1:                 episode reward: 0.2743,                 loss: 0.1469
Score delta: 1.7652273832080625, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/64676_0.
Episode: 65141/101000 (64.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4649s / 10752.2922 s
agent0:                 episode reward: -0.4452,                 loss: nan
agent1:                 episode reward: 0.4452,                 loss: 0.1477
Episode: 65161/101000 (64.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7964s / 10756.0886 s
agent0:                 episode reward: 0.0803,                 loss: nan
agent1:                 episode reward: -0.0803,                 loss: 0.1464
Episode: 65181/101000 (64.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9878s / 10760.0765 s
agent0:                 episode reward: -0.2325,                 loss: nan
agent1:                 episode reward: 0.2325,                 loss: 0.1477
Episode: 65201/101000 (64.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4198s / 10764.4962 s
agent0:                 episode reward: 0.1156,                 loss: nan
agent1:                 episode reward: -0.1156,                 loss: 0.1461
Episode: 65221/101000 (64.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1993s / 10768.6955 s
agent0:                 episode reward: -0.3806,                 loss: 0.1902
agent1:                 episode reward: 0.3806,                 loss: 0.1482
Score delta: 1.780712769085497, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/64791_1.
Episode: 65241/101000 (64.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6213s / 10772.3168 s
agent0:                 episode reward: 0.2006,                 loss: 0.1905
agent1:                 episode reward: -0.2006,                 loss: nan
Episode: 65261/101000 (64.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8432s / 10776.1600 s
agent0:                 episode reward: 0.2030,                 loss: 0.1887
agent1:                 episode reward: -0.2030,                 loss: nan
Episode: 65281/101000 (64.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9035s / 10780.0635 s
agent0:                 episode reward: -0.1231,                 loss: 0.1885
agent1:                 episode reward: 0.1231,                 loss: nan
Episode: 65301/101000 (64.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3004s / 10784.3639 s
agent0:                 episode reward: -0.0520,                 loss: 0.1869
agent1:                 episode reward: 0.0520,                 loss: nan
Episode: 65321/101000 (64.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4399s / 10788.8038 s
agent0:                 episode reward: -0.6288,                 loss: 0.1873
agent1:                 episode reward: 0.6288,                 loss: nan
Episode: 65341/101000 (64.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7286s / 10793.5324 s
agent0:                 episode reward: -0.1765,                 loss: 0.1879
agent1:                 episode reward: 0.1765,                 loss: nan
Episode: 65361/101000 (64.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2224s / 10797.7548 s
agent0:                 episode reward: -0.1305,                 loss: 0.1884
agent1:                 episode reward: 0.1305,                 loss: nan
Episode: 65381/101000 (64.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9764s / 10801.7313 s
agent0:                 episode reward: -0.2304,                 loss: 0.1884
agent1:                 episode reward: 0.2304,                 loss: nan
Episode: 65401/101000 (64.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9475s / 10805.6788 s
agent0:                 episode reward: 0.3499,                 loss: 0.1871
agent1:                 episode reward: -0.3499,                 loss: 0.1504
Score delta: 1.7961075536580844, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/64973_0.
Episode: 65421/101000 (64.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6544s / 10809.3332 s
agent0:                 episode reward: -0.1224,                 loss: nan
agent1:                 episode reward: 0.1224,                 loss: 0.1507
Episode: 65441/101000 (64.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4262s / 10812.7594 s
agent0:                 episode reward: -0.1533,                 loss: nan
agent1:                 episode reward: 0.1533,                 loss: 0.1497
Episode: 65461/101000 (64.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5531s / 10816.3125 s
agent0:                 episode reward: -0.1967,                 loss: nan
agent1:                 episode reward: 0.1967,                 loss: 0.1483
Episode: 65481/101000 (64.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1103s / 10820.4228 s
agent0:                 episode reward: -0.4576,                 loss: nan
agent1:                 episode reward: 0.4576,                 loss: 0.1476
Episode: 65501/101000 (64.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3991s / 10823.8219 s
agent0:                 episode reward: -0.2087,                 loss: nan
agent1:                 episode reward: 0.2087,                 loss: 0.1492
Episode: 65521/101000 (64.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6394s / 10827.4614 s
agent0:                 episode reward: -0.3521,                 loss: 0.2002
agent1:                 episode reward: 0.3521,                 loss: 0.1464
Score delta: 1.6351802708015932, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/65089_1.
Episode: 65541/101000 (64.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1578s / 10831.6191 s
agent0:                 episode reward: 0.0405,                 loss: 0.1993
agent1:                 episode reward: -0.0405,                 loss: nan
Episode: 65561/101000 (64.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9059s / 10836.5251 s
agent0:                 episode reward: -0.0623,                 loss: 0.1921
agent1:                 episode reward: 0.0623,                 loss: nan
Episode: 65581/101000 (64.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0911s / 10840.6161 s
agent0:                 episode reward: 0.6496,                 loss: 0.1916
agent1:                 episode reward: -0.6496,                 loss: nan
Episode: 65601/101000 (64.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2022s / 10844.8183 s
agent0:                 episode reward: -0.4869,                 loss: 0.1919
agent1:                 episode reward: 0.4869,                 loss: nan
Episode: 65621/101000 (64.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4576s / 10849.2759 s
agent0:                 episode reward: 0.0867,                 loss: 0.1924
agent1:                 episode reward: -0.0867,                 loss: nan
Episode: 65641/101000 (64.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2610s / 10853.5369 s
agent0:                 episode reward: -0.0397,                 loss: 0.1905
agent1:                 episode reward: 0.0397,                 loss: nan
Episode: 65661/101000 (65.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9362s / 10858.4730 s
agent0:                 episode reward: 0.2598,                 loss: 0.1899
agent1:                 episode reward: -0.2598,                 loss: 0.1438
Score delta: 1.593298694247816, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/65230_0.
Episode: 65681/101000 (65.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1865s / 10862.6595 s
agent0:                 episode reward: -0.6059,                 loss: nan
agent1:                 episode reward: 0.6059,                 loss: 0.1418
Episode: 65701/101000 (65.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2628s / 10866.9223 s
agent0:                 episode reward: -0.2136,                 loss: nan
agent1:                 episode reward: 0.2136,                 loss: 0.1435
Episode: 65721/101000 (65.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3124s / 10871.2347 s
agent0:                 episode reward: -0.0719,                 loss: nan
agent1:                 episode reward: 0.0719,                 loss: 0.1413
Episode: 65741/101000 (65.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0236s / 10874.2583 s
agent0:                 episode reward: -0.2875,                 loss: nan
agent1:                 episode reward: 0.2875,                 loss: 0.1421
Episode: 65761/101000 (65.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4886s / 10878.7469 s
agent0:                 episode reward: -0.1756,                 loss: nan
agent1:                 episode reward: 0.1756,                 loss: 0.1405
Episode: 65781/101000 (65.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2175s / 10882.9643 s
agent0:                 episode reward: -0.0944,                 loss: nan
agent1:                 episode reward: 0.0944,                 loss: 0.1427
Episode: 65801/101000 (65.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1631s / 10887.1274 s
agent0:                 episode reward: -0.3240,                 loss: nan
agent1:                 episode reward: 0.3240,                 loss: 0.1422
Episode: 65821/101000 (65.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9809s / 10892.1083 s
agent0:                 episode reward: -0.2023,                 loss: nan
agent1:                 episode reward: 0.2023,                 loss: 0.1416
Episode: 65841/101000 (65.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2155s / 10896.3238 s
agent0:                 episode reward: -0.1108,                 loss: nan
agent1:                 episode reward: 0.1108,                 loss: 0.1414
Episode: 65861/101000 (65.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6212s / 10900.9450 s
agent0:                 episode reward: -0.3288,                 loss: nan
agent1:                 episode reward: 0.3288,                 loss: 0.1409
Episode: 65881/101000 (65.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8969s / 10904.8419 s
agent0:                 episode reward: -0.3684,                 loss: nan
agent1:                 episode reward: 0.3684,                 loss: 0.1415
Episode: 65901/101000 (65.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9317s / 10908.7736 s
agent0:                 episode reward: -0.4883,                 loss: 0.1914
agent1:                 episode reward: 0.4883,                 loss: 0.1468
Score delta: 1.5699840698903604, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/65462_1.
Episode: 65921/101000 (65.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9797s / 10913.7533 s
agent0:                 episode reward: 0.2061,                 loss: 0.1889
agent1:                 episode reward: -0.2061,                 loss: nan
Episode: 65941/101000 (65.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4421s / 10918.1954 s
agent0:                 episode reward: -0.1170,                 loss: 0.1880
agent1:                 episode reward: 0.1170,                 loss: nan
Episode: 65961/101000 (65.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7688s / 10921.9642 s
agent0:                 episode reward: -0.0175,                 loss: 0.1875
agent1:                 episode reward: 0.0175,                 loss: nan
Episode: 65981/101000 (65.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4457s / 10926.4099 s
agent0:                 episode reward: -0.4993,                 loss: 0.1872
agent1:                 episode reward: 0.4993,                 loss: nan
Episode: 66001/101000 (65.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3034s / 10929.7133 s
agent0:                 episode reward: -0.0600,                 loss: 0.1880
agent1:                 episode reward: 0.0600,                 loss: nan
Episode: 66021/101000 (65.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0340s / 10933.7473 s
agent0:                 episode reward: -0.1632,                 loss: 0.1866
agent1:                 episode reward: 0.1632,                 loss: nan
Episode: 66041/101000 (65.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5818s / 10938.3291 s
agent0:                 episode reward: -0.3505,                 loss: 0.1870
agent1:                 episode reward: 0.3505,                 loss: nan
Episode: 66061/101000 (65.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5261s / 10942.8553 s
agent0:                 episode reward: -0.4672,                 loss: 0.1875
agent1:                 episode reward: 0.4672,                 loss: nan
Episode: 66081/101000 (65.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7671s / 10946.6224 s
agent0:                 episode reward: 0.0906,                 loss: 0.1888
agent1:                 episode reward: -0.0906,                 loss: nan
Episode: 66101/101000 (65.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7105s / 10950.3329 s
agent0:                 episode reward: -0.1669,                 loss: 0.1821
agent1:                 episode reward: 0.1669,                 loss: 0.1821
Score delta: 1.5655410242150793, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/65659_0.
Episode: 66121/101000 (65.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2145s / 10954.5474 s
agent0:                 episode reward: -0.0706,                 loss: nan
agent1:                 episode reward: 0.0706,                 loss: 0.1787
Episode: 66141/101000 (65.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0908s / 10959.6381 s
agent0:                 episode reward: -0.2009,                 loss: nan
agent1:                 episode reward: 0.2009,                 loss: 0.1751
Episode: 66161/101000 (65.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6783s / 10963.3165 s
agent0:                 episode reward: -0.0165,                 loss: nan
agent1:                 episode reward: 0.0165,                 loss: 0.1749
Episode: 66181/101000 (65.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9546s / 10967.2711 s
agent0:                 episode reward: 0.0337,                 loss: nan
agent1:                 episode reward: -0.0337,                 loss: 0.1705
Episode: 66201/101000 (65.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7306s / 10971.0017 s
agent0:                 episode reward: -0.5214,                 loss: nan
agent1:                 episode reward: 0.5214,                 loss: 0.1711
Episode: 66221/101000 (65.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4993s / 10974.5010 s
agent0:                 episode reward: -0.4707,                 loss: 0.2196
agent1:                 episode reward: 0.4707,                 loss: 0.1730
Score delta: 1.834128901218829, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/65777_1.
Episode: 66241/101000 (65.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5295s / 10979.0305 s
agent0:                 episode reward: 0.5445,                 loss: 0.2031
agent1:                 episode reward: -0.5445,                 loss: nan
Episode: 66261/101000 (65.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1292s / 10983.1597 s
agent0:                 episode reward: -0.3000,                 loss: 0.2019
agent1:                 episode reward: 0.3000,                 loss: nan
Episode: 66281/101000 (65.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9201s / 10988.0798 s
agent0:                 episode reward: -0.3186,                 loss: 0.2049
agent1:                 episode reward: 0.3186,                 loss: nan
Episode: 66301/101000 (65.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9086s / 10991.9883 s
agent0:                 episode reward: -0.0737,                 loss: 0.2029
agent1:                 episode reward: 0.0737,                 loss: nan
Episode: 66321/101000 (65.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4585s / 10995.4468 s
agent0:                 episode reward: -0.1940,                 loss: 0.2012
agent1:                 episode reward: 0.1940,                 loss: nan
Episode: 66341/101000 (65.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3489s / 10998.7957 s
agent0:                 episode reward: 0.3428,                 loss: 0.2019
agent1:                 episode reward: -0.3428,                 loss: 0.1346
Score delta: 1.6311857101472576, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/65913_0.
Episode: 66361/101000 (65.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5108s / 11003.3065 s
agent0:                 episode reward: -0.5335,                 loss: nan
agent1:                 episode reward: 0.5335,                 loss: 0.1408
Episode: 66381/101000 (65.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7010s / 11008.0075 s
agent0:                 episode reward: -0.0604,                 loss: nan
agent1:                 episode reward: 0.0604,                 loss: 0.1403
Episode: 66401/101000 (65.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0420s / 11012.0495 s
agent0:                 episode reward: 0.1642,                 loss: nan
agent1:                 episode reward: -0.1642,                 loss: 0.1405
Episode: 66421/101000 (65.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2018s / 11016.2512 s
agent0:                 episode reward: -0.3704,                 loss: nan
agent1:                 episode reward: 0.3704,                 loss: 0.1401
Episode: 66441/101000 (65.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2145s / 11019.4657 s
agent0:                 episode reward: -0.0899,                 loss: nan
agent1:                 episode reward: 0.0899,                 loss: 0.1399
Episode: 66461/101000 (65.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4051s / 11023.8708 s
agent0:                 episode reward: -0.3317,                 loss: nan
agent1:                 episode reward: 0.3317,                 loss: 0.1391
Episode: 66481/101000 (65.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1326s / 11028.0033 s
agent0:                 episode reward: -0.0597,                 loss: nan
agent1:                 episode reward: 0.0597,                 loss: 0.1382
Episode: 66501/101000 (65.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3233s / 11032.3266 s
agent0:                 episode reward: -0.0616,                 loss: nan
agent1:                 episode reward: 0.0616,                 loss: 0.1396
Episode: 66521/101000 (65.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1746s / 11036.5012 s
agent0:                 episode reward: -0.4579,                 loss: 0.1932
agent1:                 episode reward: 0.4579,                 loss: 0.1391
Score delta: 1.535358338500203, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/66089_1.
Episode: 66541/101000 (65.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1321s / 11040.6334 s
agent0:                 episode reward: 0.2773,                 loss: 0.1954
agent1:                 episode reward: -0.2773,                 loss: nan
Episode: 66561/101000 (65.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1464s / 11044.7797 s
agent0:                 episode reward: 0.0253,                 loss: 0.1948
agent1:                 episode reward: -0.0253,                 loss: nan
Episode: 66581/101000 (65.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0326s / 11048.8123 s
agent0:                 episode reward: -0.1487,                 loss: 0.1960
agent1:                 episode reward: 0.1487,                 loss: nan
Episode: 66601/101000 (65.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6922s / 11053.5045 s
agent0:                 episode reward: 0.0244,                 loss: 0.1948
agent1:                 episode reward: -0.0244,                 loss: nan
Episode: 66621/101000 (65.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5135s / 11058.0180 s
agent0:                 episode reward: 0.5525,                 loss: 0.1954
agent1:                 episode reward: -0.5525,                 loss: nan
Episode: 66641/101000 (65.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1088s / 11062.1268 s
agent0:                 episode reward: 0.0483,                 loss: 0.1949
agent1:                 episode reward: -0.0483,                 loss: nan
Episode: 66661/101000 (66.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9403s / 11066.0671 s
agent0:                 episode reward: 0.4164,                 loss: 0.1951
agent1:                 episode reward: -0.4164,                 loss: nan
Episode: 66681/101000 (66.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5910s / 11069.6581 s
agent0:                 episode reward: 0.3797,                 loss: 0.1934
agent1:                 episode reward: -0.3797,                 loss: nan
Episode: 66701/101000 (66.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5332s / 11074.1913 s
agent0:                 episode reward: -0.3784,                 loss: 0.1903
agent1:                 episode reward: 0.3784,                 loss: nan
Episode: 66721/101000 (66.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9616s / 11079.1529 s
agent0:                 episode reward: 0.1439,                 loss: 0.1919
agent1:                 episode reward: -0.1439,                 loss: nan
Episode: 66741/101000 (66.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9091s / 11084.0620 s
agent0:                 episode reward: 0.0476,                 loss: 0.2058
agent1:                 episode reward: -0.0476,                 loss: nan
Episode: 66761/101000 (66.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3898s / 11089.4518 s
agent0:                 episode reward: -0.0532,                 loss: 0.2089
agent1:                 episode reward: 0.0532,                 loss: nan
Episode: 66781/101000 (66.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5871s / 11094.0388 s
agent0:                 episode reward: 0.1571,                 loss: 0.2093
agent1:                 episode reward: -0.1571,                 loss: nan
Episode: 66801/101000 (66.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6233s / 11098.6621 s
agent0:                 episode reward: -0.0784,                 loss: 0.2097
agent1:                 episode reward: 0.0784,                 loss: nan
Episode: 66821/101000 (66.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8695s / 11102.5316 s
agent0:                 episode reward: 0.1294,                 loss: 0.2080
agent1:                 episode reward: -0.1294,                 loss: nan
Episode: 66841/101000 (66.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6791s / 11107.2107 s
agent0:                 episode reward: 0.3186,                 loss: 0.2097
agent1:                 episode reward: -0.3186,                 loss: nan
Episode: 66861/101000 (66.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1284s / 11111.3391 s
agent0:                 episode reward: 0.3433,                 loss: 0.2097
agent1:                 episode reward: -0.3433,                 loss: 0.1518
Score delta: 1.7532045519123869, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/66428_0.
Episode: 66881/101000 (66.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8433s / 11115.1825 s
agent0:                 episode reward: -0.3512,                 loss: nan
agent1:                 episode reward: 0.3512,                 loss: 0.1530
Episode: 66901/101000 (66.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8829s / 11119.0654 s
agent0:                 episode reward: -0.3244,                 loss: nan
agent1:                 episode reward: 0.3244,                 loss: 0.1510
Episode: 66921/101000 (66.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8382s / 11122.9035 s
agent0:                 episode reward: -0.5580,                 loss: nan
agent1:                 episode reward: 0.5580,                 loss: 0.1498
Episode: 66941/101000 (66.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0230s / 11126.9266 s
agent0:                 episode reward: -0.3296,                 loss: nan
agent1:                 episode reward: 0.3296,                 loss: 0.1489
Episode: 66961/101000 (66.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6514s / 11130.5780 s
agent0:                 episode reward: -0.7170,                 loss: nan
agent1:                 episode reward: 0.7170,                 loss: 0.1489
Episode: 66981/101000 (66.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0830s / 11134.6610 s
agent0:                 episode reward: -0.2352,                 loss: nan
agent1:                 episode reward: 0.2352,                 loss: 0.1495
Episode: 67001/101000 (66.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2920s / 11138.9530 s
agent0:                 episode reward: -0.5919,                 loss: 0.1943
agent1:                 episode reward: 0.5919,                 loss: 0.1506
Score delta: 1.6623084272015525, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/66570_1.
Episode: 67021/101000 (66.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7782s / 11143.7312 s
agent0:                 episode reward: 0.6837,                 loss: 0.1900
agent1:                 episode reward: -0.6837,                 loss: nan
Episode: 67041/101000 (66.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8749s / 11148.6060 s
agent0:                 episode reward: -0.0996,                 loss: 0.1890
agent1:                 episode reward: 0.0996,                 loss: nan
Episode: 67061/101000 (66.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7701s / 11152.3761 s
agent0:                 episode reward: 0.0664,                 loss: 0.1891
agent1:                 episode reward: -0.0664,                 loss: nan
Episode: 67081/101000 (66.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2361s / 11156.6123 s
agent0:                 episode reward: 0.5694,                 loss: 0.1885
agent1:                 episode reward: -0.5694,                 loss: nan
Episode: 67101/101000 (66.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2915s / 11159.9037 s
agent0:                 episode reward: 0.2365,                 loss: 0.1906
agent1:                 episode reward: -0.2365,                 loss: nan
Episode: 67121/101000 (66.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5258s / 11164.4295 s
agent0:                 episode reward: -0.0287,                 loss: 0.1870
agent1:                 episode reward: 0.0287,                 loss: nan
Episode: 67141/101000 (66.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5828s / 11169.0123 s
agent0:                 episode reward: 0.1603,                 loss: 0.1883
agent1:                 episode reward: -0.1603,                 loss: nan
Episode: 67161/101000 (66.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8748s / 11173.8871 s
agent0:                 episode reward: -0.0555,                 loss: 0.1887
agent1:                 episode reward: 0.0555,                 loss: 0.1504
Score delta: 1.6153361021210628, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/66722_0.
Episode: 67181/101000 (66.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6418s / 11178.5289 s
agent0:                 episode reward: -0.4144,                 loss: nan
agent1:                 episode reward: 0.4144,                 loss: 0.1511
Episode: 67201/101000 (66.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3375s / 11182.8665 s
agent0:                 episode reward: -0.1608,                 loss: nan
agent1:                 episode reward: 0.1608,                 loss: 0.1510
Episode: 67221/101000 (66.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4438s / 11187.3103 s
agent0:                 episode reward: -0.5157,                 loss: nan
agent1:                 episode reward: 0.5157,                 loss: 0.1504
Episode: 67241/101000 (66.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3030s / 11191.6133 s
agent0:                 episode reward: -0.1438,                 loss: nan
agent1:                 episode reward: 0.1438,                 loss: 0.1514
Episode: 67261/101000 (66.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3126s / 11194.9259 s
agent0:                 episode reward: -0.2178,                 loss: nan
agent1:                 episode reward: 0.2178,                 loss: 0.1522
Episode: 67281/101000 (66.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0752s / 11199.0011 s
agent0:                 episode reward: -0.2302,                 loss: nan
agent1:                 episode reward: 0.2302,                 loss: 0.1508
Episode: 67301/101000 (66.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9077s / 11202.9088 s
agent0:                 episode reward: -0.6475,                 loss: 0.2070
agent1:                 episode reward: 0.6475,                 loss: 0.1535
Score delta: 1.7607398964619296, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/66868_1.
Episode: 67321/101000 (66.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2856s / 11209.1944 s
agent0:                 episode reward: 0.0656,                 loss: 0.2092
agent1:                 episode reward: -0.0656,                 loss: nan
Episode: 67341/101000 (66.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7021s / 11212.8966 s
agent0:                 episode reward: 0.2927,                 loss: 0.2089
agent1:                 episode reward: -0.2927,                 loss: nan
Episode: 67361/101000 (66.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8848s / 11216.7814 s
agent0:                 episode reward: -0.4202,                 loss: 0.2082
agent1:                 episode reward: 0.4202,                 loss: nan
Episode: 67381/101000 (66.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4206s / 11221.2020 s
agent0:                 episode reward: 0.4782,                 loss: 0.2084
agent1:                 episode reward: -0.4782,                 loss: nan
Episode: 67401/101000 (66.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9693s / 11226.1713 s
agent0:                 episode reward: -0.4175,                 loss: 0.2060
agent1:                 episode reward: 0.4175,                 loss: nan
Episode: 67421/101000 (66.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5292s / 11230.7005 s
agent0:                 episode reward: 0.7468,                 loss: 0.2064
agent1:                 episode reward: -0.7468,                 loss: 0.1574
Score delta: 1.7953083821959315, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/66991_0.
Episode: 67441/101000 (66.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4684s / 11234.1689 s
agent0:                 episode reward: -0.6997,                 loss: nan
agent1:                 episode reward: 0.6997,                 loss: 0.1551
Episode: 67461/101000 (66.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6903s / 11237.8593 s
agent0:                 episode reward: -0.1641,                 loss: nan
agent1:                 episode reward: 0.1641,                 loss: 0.1550
Episode: 67481/101000 (66.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3404s / 11242.1997 s
agent0:                 episode reward: -0.2928,                 loss: nan
agent1:                 episode reward: 0.2928,                 loss: 0.1550
Episode: 67501/101000 (66.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4455s / 11246.6451 s
agent0:                 episode reward: -0.0324,                 loss: nan
agent1:                 episode reward: 0.0324,                 loss: 0.1545
Episode: 67521/101000 (66.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9123s / 11250.5574 s
agent0:                 episode reward: -0.5878,                 loss: nan
agent1:                 episode reward: 0.5878,                 loss: 0.1470
Episode: 67541/101000 (66.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8505s / 11254.4080 s
agent0:                 episode reward: -0.0196,                 loss: nan
agent1:                 episode reward: 0.0196,                 loss: 0.1463
Episode: 67561/101000 (66.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7714s / 11258.1794 s
agent0:                 episode reward: -0.6316,                 loss: 0.2002
agent1:                 episode reward: 0.6316,                 loss: 0.1448
Score delta: 1.5468793807105237, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/67126_1.
Episode: 67581/101000 (66.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6809s / 11261.8603 s
agent0:                 episode reward: -0.2793,                 loss: 0.1997
agent1:                 episode reward: 0.2793,                 loss: nan
Episode: 67601/101000 (66.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8248s / 11266.6851 s
agent0:                 episode reward: 0.2934,                 loss: 0.1976
agent1:                 episode reward: -0.2934,                 loss: nan
Episode: 67621/101000 (66.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1115s / 11270.7967 s
agent0:                 episode reward: 0.0798,                 loss: 0.1992
agent1:                 episode reward: -0.0798,                 loss: nan
Episode: 67641/101000 (66.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3973s / 11275.1939 s
agent0:                 episode reward: 0.1687,                 loss: 0.1981
agent1:                 episode reward: -0.1687,                 loss: nan
Episode: 67661/101000 (66.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8276s / 11280.0215 s
agent0:                 episode reward: -0.3562,                 loss: 0.1962
agent1:                 episode reward: 0.3562,                 loss: nan
Episode: 67681/101000 (67.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8996s / 11284.9211 s
agent0:                 episode reward: 0.0589,                 loss: 0.1973
agent1:                 episode reward: -0.0589,                 loss: nan
Episode: 67701/101000 (67.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7891s / 11289.7103 s
agent0:                 episode reward: -0.0287,                 loss: 0.1980
agent1:                 episode reward: 0.0287,                 loss: nan
Episode: 67721/101000 (67.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3319s / 11294.0422 s
agent0:                 episode reward: 0.4948,                 loss: 0.1977
agent1:                 episode reward: -0.4948,                 loss: nan
Episode: 67741/101000 (67.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6821s / 11297.7243 s
agent0:                 episode reward: -0.0580,                 loss: 0.1949
agent1:                 episode reward: 0.0580,                 loss: 0.1495
Score delta: 1.8437328993093025, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/67296_0.
Episode: 67761/101000 (67.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0744s / 11301.7987 s
agent0:                 episode reward: -0.4986,                 loss: nan
agent1:                 episode reward: 0.4986,                 loss: 0.1483
Episode: 67781/101000 (67.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1099s / 11305.9086 s
agent0:                 episode reward: -0.3949,                 loss: nan
agent1:                 episode reward: 0.3949,                 loss: 0.1482
Episode: 67801/101000 (67.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5631s / 11309.4717 s
agent0:                 episode reward: -0.4541,                 loss: nan
agent1:                 episode reward: 0.4541,                 loss: 0.1478
Episode: 67821/101000 (67.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8808s / 11313.3525 s
agent0:                 episode reward: -0.2408,                 loss: nan
agent1:                 episode reward: 0.2408,                 loss: 0.1492
Episode: 67841/101000 (67.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2009s / 11317.5534 s
agent0:                 episode reward: -0.8765,                 loss: nan
agent1:                 episode reward: 0.8765,                 loss: 0.1468
Episode: 67861/101000 (67.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2843s / 11321.8377 s
agent0:                 episode reward: -0.0758,                 loss: 0.2001
agent1:                 episode reward: 0.0758,                 loss: 0.1470
Score delta: 1.502082896747976, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/67422_1.
Episode: 67881/101000 (67.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1592s / 11325.9969 s
agent0:                 episode reward: 0.2842,                 loss: 0.1998
agent1:                 episode reward: -0.2842,                 loss: nan
Episode: 67901/101000 (67.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7824s / 11330.7793 s
agent0:                 episode reward: 0.0863,                 loss: 0.2007
agent1:                 episode reward: -0.0863,                 loss: nan
Episode: 67921/101000 (67.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2636s / 11335.0429 s
agent0:                 episode reward: 0.0316,                 loss: 0.1998
agent1:                 episode reward: -0.0316,                 loss: nan
Episode: 67941/101000 (67.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0407s / 11339.0836 s
agent0:                 episode reward: 0.2331,                 loss: 0.2009
agent1:                 episode reward: -0.2331,                 loss: nan
Episode: 67961/101000 (67.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4926s / 11343.5761 s
agent0:                 episode reward: -0.0627,                 loss: 0.2229
agent1:                 episode reward: 0.0627,                 loss: nan
Episode: 67981/101000 (67.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8889s / 11347.4651 s
agent0:                 episode reward: 0.3810,                 loss: 0.2199
agent1:                 episode reward: -0.3810,                 loss: nan
Episode: 68001/101000 (67.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5141s / 11351.9792 s
agent0:                 episode reward: -0.1267,                 loss: 0.2215
agent1:                 episode reward: 0.1267,                 loss: nan
Episode: 68021/101000 (67.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3579s / 11357.3371 s
agent0:                 episode reward: 0.3230,                 loss: 0.2199
agent1:                 episode reward: -0.3230,                 loss: nan
Episode: 68041/101000 (67.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5026s / 11361.8396 s
agent0:                 episode reward: -0.1044,                 loss: 0.2231
agent1:                 episode reward: 0.1044,                 loss: nan
Episode: 68061/101000 (67.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3288s / 11366.1684 s
agent0:                 episode reward: 0.1230,                 loss: 0.2210
agent1:                 episode reward: -0.1230,                 loss: nan
Episode: 68081/101000 (67.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0511s / 11371.2196 s
agent0:                 episode reward: 0.0634,                 loss: 0.2227
agent1:                 episode reward: -0.0634,                 loss: nan
Episode: 68101/101000 (67.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1653s / 11375.3848 s
agent0:                 episode reward: -0.2435,                 loss: 0.2265
agent1:                 episode reward: 0.2435,                 loss: 0.1552
Score delta: 1.6366919628935317, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/67658_0.
Episode: 68121/101000 (67.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6377s / 11379.0225 s
agent0:                 episode reward: -0.0386,                 loss: nan
agent1:                 episode reward: 0.0386,                 loss: 0.1560
Episode: 68141/101000 (67.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4087s / 11383.4313 s
agent0:                 episode reward: -0.5627,                 loss: nan
agent1:                 episode reward: 0.5627,                 loss: 0.1546
Episode: 68161/101000 (67.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2180s / 11387.6493 s
agent0:                 episode reward: 0.1026,                 loss: nan
agent1:                 episode reward: -0.1026,                 loss: 0.1515
Episode: 68181/101000 (67.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4795s / 11392.1288 s
agent0:                 episode reward: -0.2855,                 loss: nan
agent1:                 episode reward: 0.2855,                 loss: 0.1528
Episode: 68201/101000 (67.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0016s / 11396.1304 s
agent0:                 episode reward: -0.1510,                 loss: nan
agent1:                 episode reward: 0.1510,                 loss: 0.1540
Episode: 68221/101000 (67.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4049s / 11400.5352 s
agent0:                 episode reward: -0.4255,                 loss: 0.2404
agent1:                 episode reward: 0.4255,                 loss: 0.1530
Score delta: 1.6565802544799797, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/67792_1.
Episode: 68241/101000 (67.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0332s / 11405.5685 s
agent0:                 episode reward: -0.8425,                 loss: 0.2223
agent1:                 episode reward: 0.8425,                 loss: nan
Episode: 68261/101000 (67.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0824s / 11409.6509 s
agent0:                 episode reward: 0.2007,                 loss: 0.2227
agent1:                 episode reward: -0.2007,                 loss: nan
Episode: 68281/101000 (67.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2944s / 11413.9453 s
agent0:                 episode reward: -0.2729,                 loss: 0.2192
agent1:                 episode reward: 0.2729,                 loss: nan
Episode: 68301/101000 (67.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6133s / 11419.5586 s
agent0:                 episode reward: 0.2840,                 loss: 0.2190
agent1:                 episode reward: -0.2840,                 loss: nan
Episode: 68321/101000 (67.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9815s / 11423.5401 s
agent0:                 episode reward: 0.0287,                 loss: 0.2205
agent1:                 episode reward: -0.0287,                 loss: nan
Episode: 68341/101000 (67.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2304s / 11427.7704 s
agent0:                 episode reward: -0.0125,                 loss: 0.2202
agent1:                 episode reward: 0.0125,                 loss: nan
Episode: 68361/101000 (67.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9433s / 11432.7137 s
agent0:                 episode reward: 0.2795,                 loss: 0.2216
agent1:                 episode reward: -0.2795,                 loss: 0.1546
Score delta: 1.7481583846927804, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/67930_0.
Episode: 68381/101000 (67.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0492s / 11436.7629 s
agent0:                 episode reward: -0.4459,                 loss: nan
agent1:                 episode reward: 0.4459,                 loss: 0.1540
Episode: 68401/101000 (67.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7613s / 11440.5242 s
agent0:                 episode reward: -0.3570,                 loss: nan
agent1:                 episode reward: 0.3570,                 loss: 0.1501
Episode: 68421/101000 (67.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4760s / 11445.0002 s
agent0:                 episode reward: -0.1237,                 loss: nan
agent1:                 episode reward: 0.1237,                 loss: 0.1501
Episode: 68441/101000 (67.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5894s / 11449.5896 s
agent0:                 episode reward: -0.4564,                 loss: nan
agent1:                 episode reward: 0.4564,                 loss: 0.1487
Episode: 68461/101000 (67.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5546s / 11454.1442 s
agent0:                 episode reward: -0.0834,                 loss: nan
agent1:                 episode reward: 0.0834,                 loss: 0.1485
Episode: 68481/101000 (67.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3081s / 11458.4523 s
agent0:                 episode reward: -0.2099,                 loss: nan
agent1:                 episode reward: 0.2099,                 loss: 0.1498
Episode: 68501/101000 (67.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9686s / 11463.4209 s
agent0:                 episode reward: -0.2722,                 loss: 0.2004
agent1:                 episode reward: 0.2722,                 loss: 0.1519
Score delta: 1.8221840709355963, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/68057_1.
Episode: 68521/101000 (67.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4255s / 11468.8464 s
agent0:                 episode reward: -0.4196,                 loss: 0.1994
agent1:                 episode reward: 0.4196,                 loss: nan
Episode: 68541/101000 (67.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9741s / 11473.8205 s
agent0:                 episode reward: 0.0157,                 loss: 0.1954
agent1:                 episode reward: -0.0157,                 loss: nan
Episode: 68561/101000 (67.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7839s / 11478.6045 s
agent0:                 episode reward: 0.7580,                 loss: 0.1883
agent1:                 episode reward: -0.7580,                 loss: nan
Episode: 68581/101000 (67.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5929s / 11483.1974 s
agent0:                 episode reward: 0.3403,                 loss: 0.1875
agent1:                 episode reward: -0.3403,                 loss: nan
Episode: 68601/101000 (67.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0259s / 11487.2233 s
agent0:                 episode reward: 0.3151,                 loss: 0.1907
agent1:                 episode reward: -0.3151,                 loss: nan
Episode: 68621/101000 (67.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2250s / 11491.4483 s
agent0:                 episode reward: -0.1481,                 loss: 0.1889
agent1:                 episode reward: 0.1481,                 loss: nan
Episode: 68641/101000 (67.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1535s / 11495.6017 s
agent0:                 episode reward: -0.4931,                 loss: 0.1880
agent1:                 episode reward: 0.4931,                 loss: nan
Episode: 68661/101000 (67.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9995s / 11499.6012 s
agent0:                 episode reward: 0.2203,                 loss: 0.1886
agent1:                 episode reward: -0.2203,                 loss: nan
Episode: 68681/101000 (68.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8428s / 11505.4440 s
agent0:                 episode reward: -0.1435,                 loss: 0.1886
agent1:                 episode reward: 0.1435,                 loss: nan
Episode: 68701/101000 (68.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3107s / 11509.7546 s
agent0:                 episode reward: 0.2581,                 loss: 0.1857
agent1:                 episode reward: -0.2581,                 loss: nan
Episode: 68721/101000 (68.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5799s / 11514.3345 s
agent0:                 episode reward: -0.3692,                 loss: 0.1845
agent1:                 episode reward: 0.3692,                 loss: nan
Episode: 68741/101000 (68.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3269s / 11518.6614 s
agent0:                 episode reward: 0.4345,                 loss: 0.1824
agent1:                 episode reward: -0.4345,                 loss: 0.1577
Score delta: 1.6042642759404167, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/68308_0.
Episode: 68761/101000 (68.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1441s / 11522.8055 s
agent0:                 episode reward: -0.0182,                 loss: nan
agent1:                 episode reward: 0.0182,                 loss: 0.1571
Episode: 68781/101000 (68.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6121s / 11527.4176 s
agent0:                 episode reward: -0.2248,                 loss: nan
agent1:                 episode reward: 0.2248,                 loss: 0.1534
Episode: 68801/101000 (68.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4162s / 11531.8338 s
agent0:                 episode reward: -0.0051,                 loss: nan
agent1:                 episode reward: 0.0051,                 loss: 0.1557
Episode: 68821/101000 (68.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1646s / 11535.9984 s
agent0:                 episode reward: -0.3475,                 loss: nan
agent1:                 episode reward: 0.3475,                 loss: 0.1547
Episode: 68841/101000 (68.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1167s / 11540.1151 s
agent0:                 episode reward: 0.1827,                 loss: nan
agent1:                 episode reward: -0.1827,                 loss: 0.1532
Episode: 68861/101000 (68.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2269s / 11544.3420 s
agent0:                 episode reward: -0.6562,                 loss: 0.2025
agent1:                 episode reward: 0.6562,                 loss: 0.1539
Score delta: 1.5892462883431624, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/68434_1.
Episode: 68881/101000 (68.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1503s / 11548.4923 s
agent0:                 episode reward: -0.3493,                 loss: 0.2019
agent1:                 episode reward: 0.3493,                 loss: nan
Episode: 68901/101000 (68.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5462s / 11553.0385 s
agent0:                 episode reward: 0.0310,                 loss: 0.1993
agent1:                 episode reward: -0.0310,                 loss: nan
Episode: 68921/101000 (68.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2248s / 11557.2633 s
agent0:                 episode reward: -0.2494,                 loss: 0.1982
agent1:                 episode reward: 0.2494,                 loss: nan
Episode: 68941/101000 (68.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9378s / 11562.2012 s
agent0:                 episode reward: 0.0322,                 loss: 0.1983
agent1:                 episode reward: -0.0322,                 loss: nan
Episode: 68961/101000 (68.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4591s / 11566.6603 s
agent0:                 episode reward: -0.0261,                 loss: 0.1977
agent1:                 episode reward: 0.0261,                 loss: nan
Episode: 68981/101000 (68.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5479s / 11571.2082 s
agent0:                 episode reward: -0.2552,                 loss: 0.1978
agent1:                 episode reward: 0.2552,                 loss: nan
Episode: 69001/101000 (68.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2293s / 11575.4375 s
agent0:                 episode reward: -0.0303,                 loss: 0.2021
agent1:                 episode reward: 0.0303,                 loss: nan
Episode: 69021/101000 (68.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9119s / 11580.3493 s
agent0:                 episode reward: 0.2097,                 loss: 0.2045
agent1:                 episode reward: -0.2097,                 loss: nan
Episode: 69041/101000 (68.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4315s / 11583.7809 s
agent0:                 episode reward: 0.3287,                 loss: 0.2043
agent1:                 episode reward: -0.3287,                 loss: 0.1576
Score delta: 1.6468320427531793, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/68600_0.
Episode: 69061/101000 (68.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7444s / 11588.5253 s
agent0:                 episode reward: -0.2401,                 loss: nan
agent1:                 episode reward: 0.2401,                 loss: 0.1535
Episode: 69081/101000 (68.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0401s / 11592.5653 s
agent0:                 episode reward: -0.0360,                 loss: nan
agent1:                 episode reward: 0.0360,                 loss: 0.1528
Episode: 69101/101000 (68.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8673s / 11596.4327 s
agent0:                 episode reward: -0.7167,                 loss: nan
agent1:                 episode reward: 0.7167,                 loss: 0.1521
Episode: 69121/101000 (68.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9277s / 11600.3604 s
agent0:                 episode reward: -0.4948,                 loss: nan
agent1:                 episode reward: 0.4948,                 loss: 0.1516
Episode: 69141/101000 (68.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4404s / 11604.8008 s
agent0:                 episode reward: -0.2929,                 loss: nan
agent1:                 episode reward: 0.2929,                 loss: 0.1475
Episode: 69161/101000 (68.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9040s / 11608.7048 s
agent0:                 episode reward: -0.2209,                 loss: nan
agent1:                 episode reward: 0.2209,                 loss: 0.1446
Episode: 69181/101000 (68.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7072s / 11613.4120 s
agent0:                 episode reward: -0.0627,                 loss: 0.2053
agent1:                 episode reward: 0.0627,                 loss: 0.1442
Score delta: 1.9551859674379937, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/68741_1.
Episode: 69201/101000 (68.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8468s / 11617.2588 s
agent0:                 episode reward: 0.1045,                 loss: 0.2012
agent1:                 episode reward: -0.1045,                 loss: nan
Episode: 69221/101000 (68.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7787s / 11622.0374 s
agent0:                 episode reward: -0.0298,                 loss: 0.2022
agent1:                 episode reward: 0.0298,                 loss: nan
Episode: 69241/101000 (68.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6060s / 11626.6434 s
agent0:                 episode reward: 0.3197,                 loss: 0.2011
agent1:                 episode reward: -0.3197,                 loss: nan
Episode: 69261/101000 (68.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5292s / 11632.1726 s
agent0:                 episode reward: 0.0581,                 loss: 0.2006
agent1:                 episode reward: -0.0581,                 loss: nan
Episode: 69281/101000 (68.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4553s / 11636.6278 s
agent0:                 episode reward: 0.0008,                 loss: 0.1994
agent1:                 episode reward: -0.0008,                 loss: nan
Episode: 69301/101000 (68.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1799s / 11640.8077 s
agent0:                 episode reward: 0.1483,                 loss: 0.1980
agent1:                 episode reward: -0.1483,                 loss: nan
Episode: 69321/101000 (68.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2656s / 11645.0734 s
agent0:                 episode reward: -0.3154,                 loss: 0.1995
agent1:                 episode reward: 0.3154,                 loss: nan
Episode: 69341/101000 (68.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4353s / 11649.5087 s
agent0:                 episode reward: -0.0978,                 loss: 0.1991
agent1:                 episode reward: 0.0978,                 loss: nan
Episode: 69361/101000 (68.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0719s / 11653.5806 s
agent0:                 episode reward: 0.1215,                 loss: 0.1987
agent1:                 episode reward: -0.1215,                 loss: nan
Episode: 69381/101000 (68.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8173s / 11658.3979 s
agent0:                 episode reward: 0.1497,                 loss: 0.1965
agent1:                 episode reward: -0.1497,                 loss: 0.1562
Score delta: 1.501979832305271, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/68945_0.
Episode: 69401/101000 (68.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4894s / 11662.8873 s
agent0:                 episode reward: -0.3776,                 loss: nan
agent1:                 episode reward: 0.3776,                 loss: 0.1526
Episode: 69421/101000 (68.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2224s / 11667.1097 s
agent0:                 episode reward: -0.2802,                 loss: nan
agent1:                 episode reward: 0.2802,                 loss: 0.1516
Episode: 69441/101000 (68.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7091s / 11670.8188 s
agent0:                 episode reward: -0.4670,                 loss: nan
agent1:                 episode reward: 0.4670,                 loss: 0.1532
Episode: 69461/101000 (68.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6315s / 11675.4503 s
agent0:                 episode reward: 0.0702,                 loss: nan
agent1:                 episode reward: -0.0702,                 loss: 0.1518
Episode: 69481/101000 (68.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4667s / 11679.9170 s
agent0:                 episode reward: -0.1547,                 loss: nan
agent1:                 episode reward: 0.1547,                 loss: 0.1500
Episode: 69501/101000 (68.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4239s / 11684.3409 s
agent0:                 episode reward: -0.1225,                 loss: nan
agent1:                 episode reward: 0.1225,                 loss: 0.1490
Episode: 69521/101000 (68.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1018s / 11688.4427 s
agent0:                 episode reward: -0.6483,                 loss: 0.2111
agent1:                 episode reward: 0.6483,                 loss: 0.1512
Score delta: 1.5520095546107868, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/69086_1.
Episode: 69541/101000 (68.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9869s / 11693.4296 s
agent0:                 episode reward: -0.0643,                 loss: 0.2085
agent1:                 episode reward: 0.0643,                 loss: nan
Episode: 69561/101000 (68.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8516s / 11698.2812 s
agent0:                 episode reward: 0.3098,                 loss: 0.2136
agent1:                 episode reward: -0.3098,                 loss: nan
Episode: 69581/101000 (68.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6234s / 11702.9046 s
agent0:                 episode reward: 0.1465,                 loss: 0.2126
agent1:                 episode reward: -0.1465,                 loss: nan
Episode: 69601/101000 (68.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2394s / 11707.1440 s
agent0:                 episode reward: -0.0663,                 loss: 0.2108
agent1:                 episode reward: 0.0663,                 loss: nan
Episode: 69621/101000 (68.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0739s / 11712.2179 s
agent0:                 episode reward: -0.2619,                 loss: 0.2169
agent1:                 episode reward: 0.2619,                 loss: nan
Episode: 69641/101000 (68.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7925s / 11717.0104 s
agent0:                 episode reward: 0.0635,                 loss: 0.2169
agent1:                 episode reward: -0.0635,                 loss: nan
Episode: 69661/101000 (68.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7654s / 11720.7757 s
agent0:                 episode reward: -0.4660,                 loss: 0.2194
agent1:                 episode reward: 0.4660,                 loss: nan
Episode: 69681/101000 (68.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8546s / 11725.6303 s
agent0:                 episode reward: -0.2029,                 loss: 0.2198
agent1:                 episode reward: 0.2029,                 loss: nan
Episode: 69701/101000 (69.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3867s / 11730.0170 s
agent0:                 episode reward: -0.0671,                 loss: 0.2186
agent1:                 episode reward: 0.0671,                 loss: nan
Episode: 69721/101000 (69.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9137s / 11734.9308 s
agent0:                 episode reward: 0.2466,                 loss: 0.2172
agent1:                 episode reward: -0.2466,                 loss: nan
Episode: 69741/101000 (69.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0957s / 11740.0265 s
agent0:                 episode reward: 0.1431,                 loss: 0.2186
agent1:                 episode reward: -0.1431,                 loss: nan
Episode: 69761/101000 (69.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2361s / 11744.2625 s
agent0:                 episode reward: -0.1427,                 loss: 0.2162
agent1:                 episode reward: 0.1427,                 loss: nan
Episode: 69781/101000 (69.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9494s / 11749.2119 s
agent0:                 episode reward: -0.1398,                 loss: 0.2183
agent1:                 episode reward: 0.1398,                 loss: nan
Episode: 69801/101000 (69.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4300s / 11753.6419 s
agent0:                 episode reward: 0.2202,                 loss: 0.2186
agent1:                 episode reward: -0.2202,                 loss: nan
Episode: 69821/101000 (69.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9459s / 11758.5877 s
agent0:                 episode reward: -0.2811,                 loss: 0.2203
agent1:                 episode reward: 0.2811,                 loss: nan
Episode: 69841/101000 (69.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6298s / 11762.2176 s
agent0:                 episode reward: 0.1067,                 loss: 0.2196
agent1:                 episode reward: -0.1067,                 loss: nan
Episode: 69861/101000 (69.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7752s / 11766.9928 s
agent0:                 episode reward: 0.0657,                 loss: 0.2162
agent1:                 episode reward: -0.0657,                 loss: nan
Episode: 69881/101000 (69.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3815s / 11771.3743 s
agent0:                 episode reward: 0.0308,                 loss: 0.2189
agent1:                 episode reward: -0.0308,                 loss: nan
Episode: 69901/101000 (69.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6375s / 11775.0118 s
agent0:                 episode reward: 0.5880,                 loss: 0.2201
agent1:                 episode reward: -0.5880,                 loss: 0.1780
Score delta: 1.6947410415966258, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/69469_0.
Episode: 69921/101000 (69.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1638s / 11779.1756 s
agent0:                 episode reward: -0.0828,                 loss: nan
agent1:                 episode reward: 0.0828,                 loss: 0.1749
Episode: 69941/101000 (69.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4800s / 11783.6556 s
agent0:                 episode reward: -0.2989,                 loss: nan
agent1:                 episode reward: 0.2989,                 loss: 0.1739
Episode: 69961/101000 (69.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7497s / 11787.4053 s
agent0:                 episode reward: -0.7045,                 loss: nan
agent1:                 episode reward: 0.7045,                 loss: 0.1720
Episode: 69981/101000 (69.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0708s / 11791.4761 s
agent0:                 episode reward: -0.1491,                 loss: nan
agent1:                 episode reward: 0.1491,                 loss: 0.1719
Episode: 70001/101000 (69.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1527s / 11794.6288 s
agent0:                 episode reward: -0.0618,                 loss: nan
agent1:                 episode reward: 0.0618,                 loss: 0.1717
Episode: 70021/101000 (69.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0822s / 11798.7110 s
agent0:                 episode reward: 0.2154,                 loss: nan
agent1:                 episode reward: -0.2154,                 loss: 0.1720
Episode: 70041/101000 (69.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5783s / 11803.2893 s
agent0:                 episode reward: -0.1637,                 loss: 0.1987
agent1:                 episode reward: 0.1637,                 loss: 0.1720
Score delta: 1.6992403992722422, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/69610_1.
Episode: 70061/101000 (69.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6190s / 11807.9083 s
agent0:                 episode reward: -0.1441,                 loss: 0.1941
agent1:                 episode reward: 0.1441,                 loss: nan
Episode: 70081/101000 (69.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3633s / 11811.2716 s
agent0:                 episode reward: -0.3410,                 loss: 0.1943
agent1:                 episode reward: 0.3410,                 loss: nan
Episode: 70101/101000 (69.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5405s / 11815.8121 s
agent0:                 episode reward: 0.4766,                 loss: 0.2056
agent1:                 episode reward: -0.4766,                 loss: nan
Episode: 70121/101000 (69.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5980s / 11820.4101 s
agent0:                 episode reward: -0.2983,                 loss: 0.2051
agent1:                 episode reward: 0.2983,                 loss: nan
Episode: 70141/101000 (69.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0389s / 11824.4490 s
agent0:                 episode reward: 0.4198,                 loss: 0.2053
agent1:                 episode reward: -0.4198,                 loss: nan
Episode: 70161/101000 (69.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0197s / 11828.4687 s
agent0:                 episode reward: 0.1854,                 loss: 0.2051
agent1:                 episode reward: -0.1854,                 loss: nan
Episode: 70181/101000 (69.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0529s / 11833.5216 s
agent0:                 episode reward: -0.1244,                 loss: 0.2056
agent1:                 episode reward: 0.1244,                 loss: nan
Episode: 70201/101000 (69.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5109s / 11838.0325 s
agent0:                 episode reward: 0.4647,                 loss: 0.2047
agent1:                 episode reward: -0.4647,                 loss: nan
Episode: 70221/101000 (69.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1430s / 11843.1755 s
agent0:                 episode reward: 0.0599,                 loss: 0.2028
agent1:                 episode reward: -0.0599,                 loss: nan
Episode: 70241/101000 (69.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3210s / 11848.4965 s
agent0:                 episode reward: 0.0729,                 loss: 0.2023
agent1:                 episode reward: -0.0729,                 loss: nan
Episode: 70261/101000 (69.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6774s / 11853.1739 s
agent0:                 episode reward: -0.3316,                 loss: 0.2051
agent1:                 episode reward: 0.3316,                 loss: nan
Episode: 70281/101000 (69.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2797s / 11857.4536 s
agent0:                 episode reward: 0.0430,                 loss: 0.2045
agent1:                 episode reward: -0.0430,                 loss: nan
Episode: 70301/101000 (69.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5010s / 11861.9546 s
agent0:                 episode reward: -0.1342,                 loss: 0.2050
agent1:                 episode reward: 0.1342,                 loss: nan
Episode: 70321/101000 (69.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8557s / 11866.8103 s
agent0:                 episode reward: -0.1736,                 loss: 0.2026
agent1:                 episode reward: 0.1736,                 loss: nan
Episode: 70341/101000 (69.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0753s / 11870.8856 s
agent0:                 episode reward: -0.3722,                 loss: 0.2031
agent1:                 episode reward: 0.3722,                 loss: nan
Episode: 70361/101000 (69.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6802s / 11875.5658 s
agent0:                 episode reward: 0.4992,                 loss: 0.2019
agent1:                 episode reward: -0.4992,                 loss: 0.1545
Score delta: 1.6710524005449319, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/69929_0.
Episode: 70381/101000 (69.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5176s / 11879.0834 s
agent0:                 episode reward: -0.2563,                 loss: nan
agent1:                 episode reward: 0.2563,                 loss: 0.1488
Episode: 70401/101000 (69.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8731s / 11883.9565 s
agent0:                 episode reward: -0.2241,                 loss: nan
agent1:                 episode reward: 0.2241,                 loss: 0.1470
Episode: 70421/101000 (69.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1308s / 11888.0873 s
agent0:                 episode reward: -0.3504,                 loss: nan
agent1:                 episode reward: 0.3504,                 loss: 0.1455
Episode: 70441/101000 (69.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4679s / 11892.5552 s
agent0:                 episode reward: -0.4643,                 loss: nan
agent1:                 episode reward: 0.4643,                 loss: 0.1478
Episode: 70461/101000 (69.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2986s / 11896.8538 s
agent0:                 episode reward: -0.1177,                 loss: nan
agent1:                 episode reward: 0.1177,                 loss: 0.1471
Episode: 70481/101000 (69.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3854s / 11900.2392 s
agent0:                 episode reward: -0.1739,                 loss: nan
agent1:                 episode reward: 0.1739,                 loss: 0.1465
Episode: 70501/101000 (69.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3310s / 11904.5702 s
agent0:                 episode reward: -0.0549,                 loss: 0.1946
agent1:                 episode reward: 0.0549,                 loss: 0.1472
Score delta: 1.599842380135342, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/70064_1.
Episode: 70521/101000 (69.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1230s / 11908.6932 s
agent0:                 episode reward: -0.4887,                 loss: 0.1866
agent1:                 episode reward: 0.4887,                 loss: nan
Episode: 70541/101000 (69.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8477s / 11913.5409 s
agent0:                 episode reward: -0.3568,                 loss: 0.1840
agent1:                 episode reward: 0.3568,                 loss: nan
Episode: 70561/101000 (69.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2504s / 11918.7913 s
agent0:                 episode reward: -0.5440,                 loss: 0.1884
agent1:                 episode reward: 0.5440,                 loss: nan
Episode: 70581/101000 (69.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1882s / 11923.9794 s
agent0:                 episode reward: 0.1053,                 loss: 0.1927
agent1:                 episode reward: -0.1053,                 loss: nan
Episode: 70601/101000 (69.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8875s / 11928.8670 s
agent0:                 episode reward: -0.7638,                 loss: 0.1931
agent1:                 episode reward: 0.7638,                 loss: nan
Episode: 70621/101000 (69.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9871s / 11932.8540 s
agent0:                 episode reward: -0.2578,                 loss: 0.1896
agent1:                 episode reward: 0.2578,                 loss: nan
Episode: 70641/101000 (69.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4768s / 11937.3308 s
agent0:                 episode reward: -0.0949,                 loss: 0.1930
agent1:                 episode reward: 0.0949,                 loss: nan
Episode: 70661/101000 (69.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9496s / 11941.2804 s
agent0:                 episode reward: 0.7692,                 loss: 0.1927
agent1:                 episode reward: -0.7692,                 loss: 0.1585
Score delta: 1.7342976264321084, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/70226_0.
Episode: 70681/101000 (69.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4752s / 11945.7556 s
agent0:                 episode reward: -0.2470,                 loss: nan
agent1:                 episode reward: 0.2470,                 loss: 0.1554
Episode: 70701/101000 (70.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6687s / 11950.4243 s
agent0:                 episode reward: 0.0951,                 loss: nan
agent1:                 episode reward: -0.0951,                 loss: 0.1531
Episode: 70721/101000 (70.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5889s / 11955.0132 s
agent0:                 episode reward: -0.2180,                 loss: nan
agent1:                 episode reward: 0.2180,                 loss: 0.1527
Episode: 70741/101000 (70.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5538s / 11959.5670 s
agent0:                 episode reward: 0.0098,                 loss: nan
agent1:                 episode reward: -0.0098,                 loss: 0.1515
Episode: 70761/101000 (70.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0478s / 11963.6148 s
agent0:                 episode reward: -0.5013,                 loss: nan
agent1:                 episode reward: 0.5013,                 loss: 0.1500
Episode: 70781/101000 (70.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2968s / 11967.9116 s
agent0:                 episode reward: 0.1859,                 loss: nan
agent1:                 episode reward: -0.1859,                 loss: 0.1525
Episode: 70801/101000 (70.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8905s / 11972.8021 s
agent0:                 episode reward: -0.3977,                 loss: 0.2109
agent1:                 episode reward: 0.3977,                 loss: 0.1513
Score delta: 1.5018024884110843, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/70360_1.
Episode: 70821/101000 (70.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3998s / 11977.2020 s
agent0:                 episode reward: 0.0800,                 loss: 0.2111
agent1:                 episode reward: -0.0800,                 loss: nan
Episode: 70841/101000 (70.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3371s / 11981.5390 s
agent0:                 episode reward: 0.1156,                 loss: 0.2089
agent1:                 episode reward: -0.1156,                 loss: nan
Episode: 70861/101000 (70.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8106s / 11985.3497 s
agent0:                 episode reward: -0.3968,                 loss: 0.2096
agent1:                 episode reward: 0.3968,                 loss: nan
Episode: 70881/101000 (70.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6054s / 11989.9551 s
agent0:                 episode reward: 0.2262,                 loss: 0.2100
agent1:                 episode reward: -0.2262,                 loss: nan
Episode: 70901/101000 (70.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3782s / 11994.3333 s
agent0:                 episode reward: 0.2682,                 loss: 0.2096
agent1:                 episode reward: -0.2682,                 loss: nan
Episode: 70921/101000 (70.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3626s / 11998.6959 s
agent0:                 episode reward: 0.0539,                 loss: 0.2071
agent1:                 episode reward: -0.0539,                 loss: nan
Episode: 70941/101000 (70.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9686s / 12002.6645 s
agent0:                 episode reward: 0.0633,                 loss: 0.2097
agent1:                 episode reward: -0.0633,                 loss: nan
Episode: 70961/101000 (70.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5357s / 12007.2003 s
agent0:                 episode reward: 0.4119,                 loss: 0.2096
agent1:                 episode reward: -0.4119,                 loss: nan
Episode: 70981/101000 (70.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3444s / 12011.5446 s
agent0:                 episode reward: -0.0272,                 loss: 0.2077
agent1:                 episode reward: 0.0272,                 loss: nan
Episode: 71001/101000 (70.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8619s / 12016.4065 s
agent0:                 episode reward: 0.4536,                 loss: 0.2084
agent1:                 episode reward: -0.4536,                 loss: nan
Episode: 71021/101000 (70.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0188s / 12020.4254 s
agent0:                 episode reward: 0.3550,                 loss: 0.2078
agent1:                 episode reward: -0.3550,                 loss: 0.1486
Score delta: 1.5007273689129756, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/70583_0.
Episode: 71041/101000 (70.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8264s / 12024.2517 s
agent0:                 episode reward: -0.5330,                 loss: nan
agent1:                 episode reward: 0.5330,                 loss: 0.1473
Episode: 71061/101000 (70.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6991s / 12028.9508 s
agent0:                 episode reward: -0.2489,                 loss: nan
agent1:                 episode reward: 0.2489,                 loss: 0.1459
Episode: 71081/101000 (70.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2566s / 12033.2074 s
agent0:                 episode reward: 0.0803,                 loss: nan
agent1:                 episode reward: -0.0803,                 loss: 0.1463
Episode: 71101/101000 (70.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9769s / 12038.1843 s
agent0:                 episode reward: -0.3440,                 loss: nan
agent1:                 episode reward: 0.3440,                 loss: 0.1582
Episode: 71121/101000 (70.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0687s / 12042.2530 s
agent0:                 episode reward: -0.0291,                 loss: nan
agent1:                 episode reward: 0.0291,                 loss: 0.1585
Episode: 71141/101000 (70.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7043s / 12046.9573 s
agent0:                 episode reward: -0.0667,                 loss: nan
agent1:                 episode reward: 0.0667,                 loss: 0.1562
Episode: 71161/101000 (70.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8279s / 12050.7852 s
agent0:                 episode reward: -0.6514,                 loss: nan
agent1:                 episode reward: 0.6514,                 loss: 0.1580
Score delta: 1.6094569409780077, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/70735_1.
Episode: 71181/101000 (70.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3587s / 12055.1440 s
agent0:                 episode reward: 0.0379,                 loss: 0.2154
agent1:                 episode reward: -0.0379,                 loss: nan
Episode: 71201/101000 (70.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4170s / 12059.5609 s
agent0:                 episode reward: -0.0968,                 loss: 0.2104
agent1:                 episode reward: 0.0968,                 loss: nan
Episode: 71221/101000 (70.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3483s / 12063.9093 s
agent0:                 episode reward: -0.0063,                 loss: 0.2089
agent1:                 episode reward: 0.0063,                 loss: nan
Episode: 71241/101000 (70.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 2.9078s / 12066.8171 s
agent0:                 episode reward: 0.3714,                 loss: 0.2113
agent1:                 episode reward: -0.3714,                 loss: nan
Episode: 71261/101000 (70.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0832s / 12070.9002 s
agent0:                 episode reward: -0.3402,                 loss: 0.2093
agent1:                 episode reward: 0.3402,                 loss: nan
Episode: 71281/101000 (70.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6803s / 12075.5805 s
agent0:                 episode reward: 0.6317,                 loss: 0.2103
agent1:                 episode reward: -0.6317,                 loss: nan
Episode: 71301/101000 (70.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5913s / 12080.1717 s
agent0:                 episode reward: -0.2081,                 loss: 0.2096
agent1:                 episode reward: 0.2081,                 loss: nan
Episode: 71321/101000 (70.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3691s / 12084.5409 s
agent0:                 episode reward: -0.2494,                 loss: 0.2085
agent1:                 episode reward: 0.2494,                 loss: nan
Episode: 71341/101000 (70.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7926s / 12089.3334 s
agent0:                 episode reward: -0.0001,                 loss: 0.2084
agent1:                 episode reward: 0.0001,                 loss: nan
Episode: 71361/101000 (70.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1103s / 12093.4437 s
agent0:                 episode reward: 0.1401,                 loss: 0.2076
agent1:                 episode reward: -0.1401,                 loss: nan
Episode: 71381/101000 (70.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2209s / 12097.6646 s
agent0:                 episode reward: 0.0068,                 loss: 0.2079
agent1:                 episode reward: -0.0068,                 loss: nan
Episode: 71401/101000 (70.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3511s / 12102.0157 s
agent0:                 episode reward: 0.2033,                 loss: 0.2103
agent1:                 episode reward: -0.2033,                 loss: 0.1441
Score delta: 1.5005591184842522, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/70970_0.
Episode: 71421/101000 (70.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2685s / 12107.2842 s
agent0:                 episode reward: -0.2853,                 loss: nan
agent1:                 episode reward: 0.2853,                 loss: 0.1424
Episode: 71441/101000 (70.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9076s / 12112.1918 s
agent0:                 episode reward: -0.4012,                 loss: nan
agent1:                 episode reward: 0.4012,                 loss: 0.1421
Episode: 71461/101000 (70.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6254s / 12116.8172 s
agent0:                 episode reward: 0.0316,                 loss: nan
agent1:                 episode reward: -0.0316,                 loss: 0.1413
Episode: 71481/101000 (70.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1702s / 12120.9874 s
agent0:                 episode reward: -0.4531,                 loss: nan
agent1:                 episode reward: 0.4531,                 loss: 0.1420
Episode: 71501/101000 (70.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5819s / 12125.5693 s
agent0:                 episode reward: -0.4547,                 loss: nan
agent1:                 episode reward: 0.4547,                 loss: 0.1416
Episode: 71521/101000 (70.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3081s / 12129.8774 s
agent0:                 episode reward: -0.3169,                 loss: nan
agent1:                 episode reward: 0.3169,                 loss: 0.1419
Episode: 71541/101000 (70.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7118s / 12134.5892 s
agent0:                 episode reward: -0.2294,                 loss: nan
agent1:                 episode reward: 0.2294,                 loss: 0.1421
Episode: 71561/101000 (70.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5394s / 12139.1286 s
agent0:                 episode reward: -0.2585,                 loss: nan
agent1:                 episode reward: 0.2585,                 loss: 0.1420
Episode: 71581/101000 (70.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1776s / 12143.3062 s
agent0:                 episode reward: -0.4248,                 loss: nan
agent1:                 episode reward: 0.4248,                 loss: 0.1426
Score delta: 1.7809912420054015, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/71155_1.
Episode: 71601/101000 (70.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2662s / 12148.5724 s
agent0:                 episode reward: 0.3039,                 loss: 0.2147
agent1:                 episode reward: -0.3039,                 loss: nan
Episode: 71621/101000 (70.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8790s / 12152.4514 s
agent0:                 episode reward: 0.2731,                 loss: 0.2121
agent1:                 episode reward: -0.2731,                 loss: nan
Episode: 71641/101000 (70.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6327s / 12158.0841 s
agent0:                 episode reward: -0.1048,                 loss: 0.2116
agent1:                 episode reward: 0.1048,                 loss: nan
Episode: 71661/101000 (70.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7656s / 12161.8497 s
agent0:                 episode reward: -0.3395,                 loss: 0.2119
agent1:                 episode reward: 0.3395,                 loss: nan
Episode: 71681/101000 (70.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9132s / 12165.7629 s
agent0:                 episode reward: 0.2741,                 loss: 0.2091
agent1:                 episode reward: -0.2741,                 loss: nan
Episode: 71701/101000 (70.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9130s / 12169.6760 s
agent0:                 episode reward: 0.2134,                 loss: 0.2118
agent1:                 episode reward: -0.2134,                 loss: nan
Episode: 71721/101000 (71.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4217s / 12174.0977 s
agent0:                 episode reward: 0.2161,                 loss: 0.2091
agent1:                 episode reward: -0.2161,                 loss: nan
Episode: 71741/101000 (71.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7044s / 12178.8021 s
agent0:                 episode reward: 0.3000,                 loss: 0.2074
agent1:                 episode reward: -0.3000,                 loss: nan
Episode: 71761/101000 (71.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7679s / 12183.5700 s
agent0:                 episode reward: -0.0149,                 loss: 0.2071
agent1:                 episode reward: 0.0149,                 loss: nan
Episode: 71781/101000 (71.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9991s / 12187.5691 s
agent0:                 episode reward: 0.1112,                 loss: 0.2086
agent1:                 episode reward: -0.1112,                 loss: nan
Episode: 71801/101000 (71.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0288s / 12191.5978 s
agent0:                 episode reward: 0.1233,                 loss: 0.2086
agent1:                 episode reward: -0.1233,                 loss: 0.1543
Score delta: 1.5750069857902758, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/71364_0.
Episode: 71821/101000 (71.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6132s / 12195.2111 s
agent0:                 episode reward: -0.2347,                 loss: nan
agent1:                 episode reward: 0.2347,                 loss: 0.1564
Episode: 71841/101000 (71.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 3.0954s / 12198.3065 s
agent0:                 episode reward: 0.0032,                 loss: nan
agent1:                 episode reward: -0.0032,                 loss: 0.1536
Episode: 71861/101000 (71.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5057s / 12201.8122 s
agent0:                 episode reward: -0.1456,                 loss: nan
agent1:                 episode reward: 0.1456,                 loss: 0.1550
Episode: 71881/101000 (71.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9048s / 12206.7169 s
agent0:                 episode reward: -0.6515,                 loss: nan
agent1:                 episode reward: 0.6515,                 loss: 0.1586
Episode: 71901/101000 (71.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4539s / 12211.1709 s
agent0:                 episode reward: -0.4601,                 loss: nan
agent1:                 episode reward: 0.4601,                 loss: 0.1566
Episode: 71921/101000 (71.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1841s / 12215.3549 s
agent0:                 episode reward: -0.1752,                 loss: nan
agent1:                 episode reward: 0.1752,                 loss: 0.1546
Episode: 71941/101000 (71.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8512s / 12219.2061 s
agent0:                 episode reward: -0.4073,                 loss: 0.2029
agent1:                 episode reward: 0.4073,                 loss: 0.1583
Score delta: 1.6630265102715185, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/71499_1.
Episode: 71961/101000 (71.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5522s / 12223.7583 s
agent0:                 episode reward: 0.1358,                 loss: 0.1998
agent1:                 episode reward: -0.1358,                 loss: nan
Episode: 71981/101000 (71.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6944s / 12228.4527 s
agent0:                 episode reward: -0.4428,                 loss: 0.2005
agent1:                 episode reward: 0.4428,                 loss: nan
Episode: 72001/101000 (71.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8659s / 12233.3186 s
agent0:                 episode reward: -0.0412,                 loss: 0.2002
agent1:                 episode reward: 0.0412,                 loss: nan
Episode: 72021/101000 (71.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9432s / 12237.2618 s
agent0:                 episode reward: 0.2366,                 loss: 0.1962
agent1:                 episode reward: -0.2366,                 loss: nan
Episode: 72041/101000 (71.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3428s / 12240.6045 s
agent0:                 episode reward: 0.0479,                 loss: 0.1969
agent1:                 episode reward: -0.0479,                 loss: nan
Episode: 72061/101000 (71.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7253s / 12245.3298 s
agent0:                 episode reward: 0.0309,                 loss: 0.1978
agent1:                 episode reward: -0.0309,                 loss: nan
Episode: 72081/101000 (71.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8332s / 12250.1630 s
agent0:                 episode reward: -0.0835,                 loss: 0.1983
agent1:                 episode reward: 0.0835,                 loss: nan
Episode: 72101/101000 (71.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9345s / 12255.0974 s
agent0:                 episode reward: 0.1796,                 loss: 0.1964
agent1:                 episode reward: -0.1796,                 loss: nan
Episode: 72121/101000 (71.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9827s / 12259.0802 s
agent0:                 episode reward: -0.1007,                 loss: 0.1972
agent1:                 episode reward: 0.1007,                 loss: nan
Episode: 72141/101000 (71.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6441s / 12262.7243 s
agent0:                 episode reward: -0.2622,                 loss: 0.1959
agent1:                 episode reward: 0.2622,                 loss: nan
Episode: 72161/101000 (71.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7582s / 12267.4825 s
agent0:                 episode reward: 0.2925,                 loss: 0.2001
agent1:                 episode reward: -0.2925,                 loss: nan
Episode: 72181/101000 (71.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2762s / 12271.7587 s
agent0:                 episode reward: 0.3261,                 loss: 0.2061
agent1:                 episode reward: -0.3261,                 loss: nan
Episode: 72201/101000 (71.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0334s / 12276.7921 s
agent0:                 episode reward: 0.1091,                 loss: 0.2064
agent1:                 episode reward: -0.1091,                 loss: nan
Episode: 72221/101000 (71.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4524s / 12282.2445 s
agent0:                 episode reward: 0.2313,                 loss: 0.2057
agent1:                 episode reward: -0.2313,                 loss: nan
Episode: 72241/101000 (71.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6314s / 12286.8759 s
agent0:                 episode reward: -0.1003,                 loss: 0.2063
agent1:                 episode reward: 0.1003,                 loss: nan
Episode: 72261/101000 (71.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9328s / 12290.8086 s
agent0:                 episode reward: -0.0489,                 loss: 0.2068
agent1:                 episode reward: 0.0489,                 loss: nan
Episode: 72281/101000 (71.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3612s / 12295.1698 s
agent0:                 episode reward: 0.2582,                 loss: 0.2067
agent1:                 episode reward: -0.2582,                 loss: nan
Episode: 72301/101000 (71.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5048s / 12299.6746 s
agent0:                 episode reward: 0.2568,                 loss: 0.2028
agent1:                 episode reward: -0.2568,                 loss: nan
Episode: 72321/101000 (71.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8086s / 12304.4833 s
agent0:                 episode reward: 0.2560,                 loss: 0.2025
agent1:                 episode reward: -0.2560,                 loss: nan
Episode: 72341/101000 (71.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9740s / 12309.4572 s
agent0:                 episode reward: 0.1241,                 loss: 0.2065
agent1:                 episode reward: -0.1241,                 loss: nan
Episode: 72361/101000 (71.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6197s / 12314.0769 s
agent0:                 episode reward: 0.1190,                 loss: 0.2063
agent1:                 episode reward: -0.1190,                 loss: nan
Episode: 72381/101000 (71.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6805s / 12317.7575 s
agent0:                 episode reward: 0.0891,                 loss: 0.2040
agent1:                 episode reward: -0.0891,                 loss: 0.1445
Score delta: 1.7139663782148133, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/71945_0.
Episode: 72401/101000 (71.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9198s / 12322.6773 s
agent0:                 episode reward: -0.2348,                 loss: nan
agent1:                 episode reward: 0.2348,                 loss: 0.1447
Episode: 72421/101000 (71.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8793s / 12327.5566 s
agent0:                 episode reward: -0.5116,                 loss: nan
agent1:                 episode reward: 0.5116,                 loss: 0.1447
Episode: 72441/101000 (71.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1122s / 12331.6688 s
agent0:                 episode reward: -0.5137,                 loss: nan
agent1:                 episode reward: 0.5137,                 loss: 0.1446
Episode: 72461/101000 (71.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2738s / 12335.9426 s
agent0:                 episode reward: -0.5477,                 loss: nan
agent1:                 episode reward: 0.5477,                 loss: 0.1454
Episode: 72481/101000 (71.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5545s / 12339.4971 s
agent0:                 episode reward: -0.2100,                 loss: nan
agent1:                 episode reward: 0.2100,                 loss: 0.1438
Episode: 72501/101000 (71.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0216s / 12343.5187 s
agent0:                 episode reward: -0.0782,                 loss: nan
agent1:                 episode reward: 0.0782,                 loss: 0.1438
Episode: 72521/101000 (71.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5732s / 12348.0919 s
agent0:                 episode reward: -0.3477,                 loss: nan
agent1:                 episode reward: 0.3477,                 loss: 0.1444
Episode: 72541/101000 (71.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8876s / 12352.9795 s
agent0:                 episode reward: -0.1160,                 loss: 0.2209
agent1:                 episode reward: 0.1160,                 loss: 0.1403
Score delta: 1.6229632987851317, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/72097_1.
Episode: 72561/101000 (71.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1590s / 12357.1385 s
agent0:                 episode reward: -0.4140,                 loss: 0.2191
agent1:                 episode reward: 0.4140,                 loss: nan
Episode: 72581/101000 (71.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5976s / 12361.7360 s
agent0:                 episode reward: 0.0634,                 loss: 0.2160
agent1:                 episode reward: -0.0634,                 loss: nan
Episode: 72601/101000 (71.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2838s / 12367.0199 s
agent0:                 episode reward: 0.1983,                 loss: 0.2163
agent1:                 episode reward: -0.1983,                 loss: nan
Episode: 72621/101000 (71.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9810s / 12372.0008 s
agent0:                 episode reward: -0.4398,                 loss: 0.2151
agent1:                 episode reward: 0.4398,                 loss: nan
Episode: 72641/101000 (71.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0686s / 12376.0695 s
agent0:                 episode reward: 0.3064,                 loss: 0.2130
agent1:                 episode reward: -0.3064,                 loss: nan
Episode: 72661/101000 (71.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4074s / 12380.4768 s
agent0:                 episode reward: 0.1462,                 loss: 0.2197
agent1:                 episode reward: -0.1462,                 loss: nan
Episode: 72681/101000 (71.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8041s / 12384.2809 s
agent0:                 episode reward: 0.1388,                 loss: 0.2185
agent1:                 episode reward: -0.1388,                 loss: nan
Episode: 72701/101000 (71.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7604s / 12388.0413 s
agent0:                 episode reward: 0.1719,                 loss: 0.2171
agent1:                 episode reward: -0.1719,                 loss: nan
Episode: 72721/101000 (72.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0557s / 12393.0971 s
agent0:                 episode reward: -0.2544,                 loss: 0.2172
agent1:                 episode reward: 0.2544,                 loss: nan
Episode: 72741/101000 (72.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8875s / 12396.9845 s
agent0:                 episode reward: -0.4511,                 loss: 0.2177
agent1:                 episode reward: 0.4511,                 loss: nan
Episode: 72761/101000 (72.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2690s / 12401.2535 s
agent0:                 episode reward: -0.0416,                 loss: 0.2181
agent1:                 episode reward: 0.0416,                 loss: nan
Episode: 72781/101000 (72.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1532s / 12406.4067 s
agent0:                 episode reward: -0.0234,                 loss: 0.2192
agent1:                 episode reward: 0.0234,                 loss: nan
Episode: 72801/101000 (72.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8980s / 12411.3047 s
agent0:                 episode reward: -0.1635,                 loss: 0.2190
agent1:                 episode reward: 0.1635,                 loss: nan
Episode: 72821/101000 (72.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2601s / 12415.5648 s
agent0:                 episode reward: -0.4006,                 loss: 0.2186
agent1:                 episode reward: 0.4006,                 loss: nan
Episode: 72841/101000 (72.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1864s / 12420.7512 s
agent0:                 episode reward: 0.0013,                 loss: 0.2176
agent1:                 episode reward: -0.0013,                 loss: nan
Episode: 72861/101000 (72.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1068s / 12424.8580 s
agent0:                 episode reward: 0.3257,                 loss: 0.2179
agent1:                 episode reward: -0.3257,                 loss: nan
Episode: 72881/101000 (72.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0361s / 12428.8941 s
agent0:                 episode reward: 0.1142,                 loss: 0.2172
agent1:                 episode reward: -0.1142,                 loss: nan
Episode: 72901/101000 (72.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9052s / 12433.7993 s
agent0:                 episode reward: -0.1387,                 loss: 0.2172
agent1:                 episode reward: 0.1387,                 loss: nan
Episode: 72921/101000 (72.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6865s / 12438.4859 s
agent0:                 episode reward: 0.5568,                 loss: 0.2136
agent1:                 episode reward: -0.5568,                 loss: 0.1488
Score delta: 1.6745382698509996, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/72483_0.
Episode: 72941/101000 (72.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7158s / 12442.2016 s
agent0:                 episode reward: -0.2116,                 loss: nan
agent1:                 episode reward: 0.2116,                 loss: 0.1443
Episode: 72961/101000 (72.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6940s / 12446.8956 s
agent0:                 episode reward: -0.2656,                 loss: nan
agent1:                 episode reward: 0.2656,                 loss: 0.1448
Episode: 72981/101000 (72.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1618s / 12451.0574 s
agent0:                 episode reward: -0.4858,                 loss: nan
agent1:                 episode reward: 0.4858,                 loss: 0.1441
Episode: 73001/101000 (72.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7297s / 12454.7871 s
agent0:                 episode reward: -0.3565,                 loss: nan
agent1:                 episode reward: 0.3565,                 loss: 0.1423
Episode: 73021/101000 (72.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9270s / 12458.7141 s
agent0:                 episode reward: 0.0209,                 loss: nan
agent1:                 episode reward: -0.0209,                 loss: 0.1399
Episode: 73041/101000 (72.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1616s / 12462.8757 s
agent0:                 episode reward: 0.0184,                 loss: nan
agent1:                 episode reward: -0.0184,                 loss: 0.1610
Episode: 73061/101000 (72.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5036s / 12467.3793 s
agent0:                 episode reward: -0.1385,                 loss: nan
agent1:                 episode reward: 0.1385,                 loss: 0.1585
Episode: 73081/101000 (72.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9240s / 12472.3033 s
agent0:                 episode reward: -0.1731,                 loss: 0.1978
agent1:                 episode reward: 0.1731,                 loss: 0.1550
Score delta: 1.7005756741303268, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/72643_1.
Episode: 73101/101000 (72.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6785s / 12476.9818 s
agent0:                 episode reward: -0.0041,                 loss: 0.1978
agent1:                 episode reward: 0.0041,                 loss: nan
Episode: 73121/101000 (72.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2982s / 12482.2800 s
agent0:                 episode reward: 0.0288,                 loss: 0.1970
agent1:                 episode reward: -0.0288,                 loss: nan
Episode: 73141/101000 (72.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0069s / 12487.2869 s
agent0:                 episode reward: -0.3064,                 loss: 0.1975
agent1:                 episode reward: 0.3064,                 loss: nan
Episode: 73161/101000 (72.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2256s / 12492.5125 s
agent0:                 episode reward: -0.2920,                 loss: 0.2041
agent1:                 episode reward: 0.2920,                 loss: nan
Episode: 73181/101000 (72.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2170s / 12496.7295 s
agent0:                 episode reward: 0.2821,                 loss: 0.2037
agent1:                 episode reward: -0.2821,                 loss: nan
Episode: 73201/101000 (72.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9123s / 12501.6418 s
agent0:                 episode reward: 0.0368,                 loss: 0.2023
agent1:                 episode reward: -0.0368,                 loss: nan
Episode: 73221/101000 (72.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4096s / 12506.0513 s
agent0:                 episode reward: 0.0069,                 loss: 0.2006
agent1:                 episode reward: -0.0069,                 loss: nan
Episode: 73241/101000 (72.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7589s / 12510.8102 s
agent0:                 episode reward: 0.1568,                 loss: 0.2041
agent1:                 episode reward: -0.1568,                 loss: nan
Episode: 73261/101000 (72.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4391s / 12516.2493 s
agent0:                 episode reward: -0.5869,                 loss: 0.2028
agent1:                 episode reward: 0.5869,                 loss: nan
Episode: 73281/101000 (72.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6052s / 12520.8546 s
agent0:                 episode reward: -0.1734,                 loss: 0.2029
agent1:                 episode reward: 0.1734,                 loss: nan
Episode: 73301/101000 (72.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5113s / 12525.3659 s
agent0:                 episode reward: 0.3495,                 loss: 0.2014
agent1:                 episode reward: -0.3495,                 loss: nan
Episode: 73321/101000 (72.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7214s / 12530.0873 s
agent0:                 episode reward: 0.1128,                 loss: 0.2033
agent1:                 episode reward: -0.1128,                 loss: nan
Episode: 73341/101000 (72.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1588s / 12534.2461 s
agent0:                 episode reward: 0.3242,                 loss: 0.2014
agent1:                 episode reward: -0.3242,                 loss: nan
Episode: 73361/101000 (72.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0847s / 12538.3308 s
agent0:                 episode reward: 0.5216,                 loss: 0.1967
agent1:                 episode reward: -0.5216,                 loss: 0.1577
Score delta: 1.6207764404408498, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/72916_0.
Episode: 73381/101000 (72.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4935s / 12542.8243 s
agent0:                 episode reward: -0.3671,                 loss: nan
agent1:                 episode reward: 0.3671,                 loss: 0.1545
Episode: 73401/101000 (72.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5651s / 12547.3894 s
agent0:                 episode reward: -0.4918,                 loss: nan
agent1:                 episode reward: 0.4918,                 loss: 0.1533
Episode: 73421/101000 (72.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0307s / 12551.4201 s
agent0:                 episode reward: -0.2978,                 loss: nan
agent1:                 episode reward: 0.2978,                 loss: 0.1525
Episode: 73441/101000 (72.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4849s / 12555.9050 s
agent0:                 episode reward: -0.1024,                 loss: nan
agent1:                 episode reward: 0.1024,                 loss: 0.1523
Episode: 73461/101000 (72.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7518s / 12560.6568 s
agent0:                 episode reward: -0.3538,                 loss: nan
agent1:                 episode reward: 0.3538,                 loss: 0.1527
Episode: 73481/101000 (72.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3171s / 12564.9739 s
agent0:                 episode reward: 0.1581,                 loss: nan
agent1:                 episode reward: -0.1581,                 loss: 0.1522
Episode: 73501/101000 (72.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2049s / 12569.1787 s
agent0:                 episode reward: -0.1967,                 loss: nan
agent1:                 episode reward: 0.1967,                 loss: 0.1514
Episode: 73521/101000 (72.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1394s / 12573.3181 s
agent0:                 episode reward: -0.0718,                 loss: nan
agent1:                 episode reward: 0.0718,                 loss: 0.1533
Episode: 73541/101000 (72.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2962s / 12576.6144 s
agent0:                 episode reward: 0.1432,                 loss: nan
agent1:                 episode reward: -0.1432,                 loss: 0.1526
Episode: 73561/101000 (72.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8833s / 12582.4976 s
agent0:                 episode reward: -0.2038,                 loss: nan
agent1:                 episode reward: 0.2038,                 loss: 0.1517
Episode: 73581/101000 (72.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0556s / 12587.5532 s
agent0:                 episode reward: -0.2776,                 loss: nan
agent1:                 episode reward: 0.2776,                 loss: 0.1526
Episode: 73601/101000 (72.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0831s / 12591.6363 s
agent0:                 episode reward: -0.3435,                 loss: nan
agent1:                 episode reward: 0.3435,                 loss: 0.1537
Episode: 73621/101000 (72.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8385s / 12596.4749 s
agent0:                 episode reward: -0.3051,                 loss: nan
agent1:                 episode reward: 0.3051,                 loss: 0.1518
Episode: 73641/101000 (72.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2581s / 12600.7330 s
agent0:                 episode reward: -0.5474,                 loss: 0.2040
agent1:                 episode reward: 0.5474,                 loss: 0.1518
Score delta: 1.7156179420434374, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/73211_1.
Episode: 73661/101000 (72.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2304s / 12604.9634 s
agent0:                 episode reward: 0.1461,                 loss: 0.2032
agent1:                 episode reward: -0.1461,                 loss: nan
Episode: 73681/101000 (72.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7243s / 12609.6876 s
agent0:                 episode reward: -0.1170,                 loss: 0.2023
agent1:                 episode reward: 0.1170,                 loss: nan
Episode: 73701/101000 (72.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0189s / 12614.7065 s
agent0:                 episode reward: -0.0493,                 loss: 0.2014
agent1:                 episode reward: 0.0493,                 loss: nan
Episode: 73721/101000 (72.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7884s / 12619.4948 s
agent0:                 episode reward: 0.1118,                 loss: 0.1995
agent1:                 episode reward: -0.1118,                 loss: nan
Episode: 73741/101000 (73.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4901s / 12623.9849 s
agent0:                 episode reward: 0.0141,                 loss: 0.2002
agent1:                 episode reward: -0.0141,                 loss: nan
Episode: 73761/101000 (73.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5686s / 12628.5535 s
agent0:                 episode reward: -0.0830,                 loss: 0.1999
agent1:                 episode reward: 0.0830,                 loss: nan
Episode: 73781/101000 (73.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6122s / 12632.1657 s
agent0:                 episode reward: 0.2870,                 loss: 0.2029
agent1:                 episode reward: -0.2870,                 loss: nan
Episode: 73801/101000 (73.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5540s / 12636.7197 s
agent0:                 episode reward: -0.1408,                 loss: 0.2021
agent1:                 episode reward: 0.1408,                 loss: nan
Episode: 73821/101000 (73.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2474s / 12640.9671 s
agent0:                 episode reward: -0.1626,                 loss: 0.2041
agent1:                 episode reward: 0.1626,                 loss: nan
Episode: 73841/101000 (73.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4905s / 12645.4576 s
agent0:                 episode reward: 0.2494,                 loss: 0.2031
agent1:                 episode reward: -0.2494,                 loss: nan
Episode: 73861/101000 (73.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0127s / 12649.4703 s
agent0:                 episode reward: 0.0945,                 loss: 0.2014
agent1:                 episode reward: -0.0945,                 loss: nan
Episode: 73881/101000 (73.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1096s / 12653.5799 s
agent0:                 episode reward: -0.1126,                 loss: 0.2039
agent1:                 episode reward: 0.1126,                 loss: nan
Episode: 73901/101000 (73.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8083s / 12658.3882 s
agent0:                 episode reward: -0.2210,                 loss: 0.2039
agent1:                 episode reward: 0.2210,                 loss: nan
Episode: 73921/101000 (73.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7238s / 12663.1120 s
agent0:                 episode reward: 0.2011,                 loss: 0.2030
agent1:                 episode reward: -0.2011,                 loss: nan
Episode: 73941/101000 (73.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2936s / 12668.4057 s
agent0:                 episode reward: -0.2199,                 loss: 0.2031
agent1:                 episode reward: 0.2199,                 loss: nan
Episode: 73961/101000 (73.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4302s / 12672.8359 s
agent0:                 episode reward: 0.2124,                 loss: 0.2017
agent1:                 episode reward: -0.2124,                 loss: nan
Episode: 73981/101000 (73.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3804s / 12677.2163 s
agent0:                 episode reward: 0.2070,                 loss: 0.2022
agent1:                 episode reward: -0.2070,                 loss: nan
Episode: 74001/101000 (73.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2460s / 12681.4623 s
agent0:                 episode reward: 0.2719,                 loss: 0.2039
agent1:                 episode reward: -0.2719,                 loss: nan
Episode: 74021/101000 (73.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6167s / 12686.0790 s
agent0:                 episode reward: 0.5392,                 loss: 0.2023
agent1:                 episode reward: -0.5392,                 loss: nan
Score delta: 1.6214198685760581, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/73595_0.
Episode: 74041/101000 (73.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3296s / 12690.4086 s
agent0:                 episode reward: -0.1117,                 loss: nan
agent1:                 episode reward: 0.1117,                 loss: 0.1491
Episode: 74061/101000 (73.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4868s / 12694.8955 s
agent0:                 episode reward: -0.2333,                 loss: nan
agent1:                 episode reward: 0.2333,                 loss: 0.1470
Episode: 74081/101000 (73.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2179s / 12700.1133 s
agent0:                 episode reward: -0.4501,                 loss: nan
agent1:                 episode reward: 0.4501,                 loss: 0.1483
Episode: 74101/101000 (73.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1517s / 12704.2650 s
agent0:                 episode reward: 0.0775,                 loss: nan
agent1:                 episode reward: -0.0775,                 loss: 0.1466
Episode: 74121/101000 (73.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9082s / 12708.1732 s
agent0:                 episode reward: -0.1518,                 loss: nan
agent1:                 episode reward: 0.1518,                 loss: 0.1460
Episode: 74141/101000 (73.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3956s / 12711.5689 s
agent0:                 episode reward: -0.4601,                 loss: nan
agent1:                 episode reward: 0.4601,                 loss: 0.1464
Episode: 74161/101000 (73.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3084s / 12716.8773 s
agent0:                 episode reward: -0.1288,                 loss: nan
agent1:                 episode reward: 0.1288,                 loss: 0.1471
Episode: 74181/101000 (73.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2014s / 12721.0786 s
agent0:                 episode reward: -0.4555,                 loss: nan
agent1:                 episode reward: 0.4555,                 loss: 0.1469
Episode: 74201/101000 (73.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8082s / 12724.8868 s
agent0:                 episode reward: -0.2934,                 loss: 0.2115
agent1:                 episode reward: 0.2934,                 loss: 0.1442
Score delta: 1.577405433847048, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/73764_1.
Episode: 74221/101000 (73.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8978s / 12729.7846 s
agent0:                 episode reward: 0.1591,                 loss: 0.2119
agent1:                 episode reward: -0.1591,                 loss: nan
Episode: 74241/101000 (73.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1287s / 12734.9133 s
agent0:                 episode reward: -0.2237,                 loss: 0.2110
agent1:                 episode reward: 0.2237,                 loss: nan
Episode: 74261/101000 (73.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5741s / 12739.4874 s
agent0:                 episode reward: -0.3779,                 loss: 0.2122
agent1:                 episode reward: 0.3779,                 loss: nan
Episode: 74281/101000 (73.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6608s / 12744.1482 s
agent0:                 episode reward: -0.0490,                 loss: 0.2202
agent1:                 episode reward: 0.0490,                 loss: nan
Episode: 74301/101000 (73.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3759s / 12748.5241 s
agent0:                 episode reward: -0.0536,                 loss: 0.2225
agent1:                 episode reward: 0.0536,                 loss: nan
Episode: 74321/101000 (73.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1386s / 12752.6626 s
agent0:                 episode reward: -0.3355,                 loss: 0.2227
agent1:                 episode reward: 0.3355,                 loss: nan
Episode: 74341/101000 (73.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5291s / 12757.1917 s
agent0:                 episode reward: 0.3357,                 loss: 0.2200
agent1:                 episode reward: -0.3357,                 loss: nan
Episode: 74361/101000 (73.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3760s / 12760.5677 s
agent0:                 episode reward: -0.2220,                 loss: 0.2221
agent1:                 episode reward: 0.2220,                 loss: nan
Episode: 74381/101000 (73.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6582s / 12765.2260 s
agent0:                 episode reward: -0.0316,                 loss: 0.2238
agent1:                 episode reward: 0.0316,                 loss: nan
Episode: 74401/101000 (73.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9982s / 12770.2241 s
agent0:                 episode reward: 0.0338,                 loss: 0.2237
agent1:                 episode reward: -0.0338,                 loss: nan
Episode: 74421/101000 (73.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2912s / 12775.5153 s
agent0:                 episode reward: 0.4685,                 loss: 0.2226
agent1:                 episode reward: -0.4685,                 loss: 0.1499
Score delta: 1.7510309895180363, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/73980_0.
Episode: 74441/101000 (73.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4937s / 12780.0090 s
agent0:                 episode reward: -0.6114,                 loss: nan
agent1:                 episode reward: 0.6114,                 loss: 0.1487
Episode: 74461/101000 (73.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2778s / 12785.2868 s
agent0:                 episode reward: -0.4833,                 loss: nan
agent1:                 episode reward: 0.4833,                 loss: 0.1492
Episode: 74481/101000 (73.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3191s / 12789.6060 s
agent0:                 episode reward: -0.2815,                 loss: nan
agent1:                 episode reward: 0.2815,                 loss: 0.1473
Episode: 74501/101000 (73.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5493s / 12794.1553 s
agent0:                 episode reward: -0.6906,                 loss: nan
agent1:                 episode reward: 0.6906,                 loss: 0.1495
Episode: 74521/101000 (73.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2549s / 12798.4102 s
agent0:                 episode reward: -0.0672,                 loss: nan
agent1:                 episode reward: 0.0672,                 loss: 0.1478
Episode: 74541/101000 (73.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1233s / 12802.5335 s
agent0:                 episode reward: -0.3117,                 loss: nan
agent1:                 episode reward: 0.3117,                 loss: 0.1485
Episode: 74561/101000 (73.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5574s / 12807.0909 s
agent0:                 episode reward: -0.2644,                 loss: 0.2141
agent1:                 episode reward: 0.2644,                 loss: 0.1483
Score delta: 1.596004999766666, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/74123_1.
Episode: 74581/101000 (73.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4467s / 12812.5376 s
agent0:                 episode reward: -0.2815,                 loss: 0.2142
agent1:                 episode reward: 0.2815,                 loss: nan
Episode: 74601/101000 (73.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3938s / 12816.9314 s
agent0:                 episode reward: -0.1894,                 loss: 0.2151
agent1:                 episode reward: 0.1894,                 loss: nan
Episode: 74621/101000 (73.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4479s / 12822.3793 s
agent0:                 episode reward: -0.0439,                 loss: 0.2145
agent1:                 episode reward: 0.0439,                 loss: nan
Episode: 74641/101000 (73.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9024s / 12827.2817 s
agent0:                 episode reward: -0.1219,                 loss: 0.2162
agent1:                 episode reward: 0.1219,                 loss: nan
Episode: 74661/101000 (73.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7984s / 12832.0800 s
agent0:                 episode reward: 0.1390,                 loss: 0.2147
agent1:                 episode reward: -0.1390,                 loss: nan
Episode: 74681/101000 (73.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9854s / 12836.0655 s
agent0:                 episode reward: -0.0874,                 loss: 0.2137
agent1:                 episode reward: 0.0874,                 loss: nan
Episode: 74701/101000 (73.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5358s / 12839.6013 s
agent0:                 episode reward: 0.3428,                 loss: 0.2168
agent1:                 episode reward: -0.3428,                 loss: 0.1564
Score delta: 1.8004693576434558, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/74263_0.
Episode: 74721/101000 (73.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7931s / 12845.3943 s
agent0:                 episode reward: -0.7266,                 loss: nan
agent1:                 episode reward: 0.7266,                 loss: 0.1530
Episode: 74741/101000 (74.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7859s / 12849.1802 s
agent0:                 episode reward: -0.0878,                 loss: nan
agent1:                 episode reward: 0.0878,                 loss: 0.1478
Episode: 74761/101000 (74.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8949s / 12854.0751 s
agent0:                 episode reward: -0.6072,                 loss: nan
agent1:                 episode reward: 0.6072,                 loss: 0.1490
Episode: 74781/101000 (74.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4725s / 12858.5476 s
agent0:                 episode reward: -0.1006,                 loss: nan
agent1:                 episode reward: 0.1006,                 loss: 0.1472
Episode: 74801/101000 (74.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0852s / 12862.6329 s
agent0:                 episode reward: -0.2541,                 loss: nan
agent1:                 episode reward: 0.2541,                 loss: 0.1485
Episode: 74821/101000 (74.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8418s / 12866.4746 s
agent0:                 episode reward: -0.2778,                 loss: nan
agent1:                 episode reward: 0.2778,                 loss: 0.1447
Episode: 74841/101000 (74.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5105s / 12871.9851 s
agent0:                 episode reward: -0.4017,                 loss: nan
agent1:                 episode reward: 0.4017,                 loss: 0.1472
Episode: 74861/101000 (74.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7416s / 12876.7267 s
agent0:                 episode reward: 0.0195,                 loss: 0.1923
agent1:                 episode reward: -0.0195,                 loss: 0.1472
Score delta: 1.6098805836355066, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/74425_1.
Episode: 74881/101000 (74.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4408s / 12882.1675 s
agent0:                 episode reward: -0.0932,                 loss: 0.1931
agent1:                 episode reward: 0.0932,                 loss: nan
Episode: 74901/101000 (74.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8025s / 12886.9700 s
agent0:                 episode reward: 0.2380,                 loss: 0.1936
agent1:                 episode reward: -0.2380,                 loss: nan
Episode: 74921/101000 (74.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3782s / 12891.3482 s
agent0:                 episode reward: 0.2201,                 loss: 0.1965
agent1:                 episode reward: -0.2201,                 loss: nan
Episode: 74941/101000 (74.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3874s / 12895.7357 s
agent0:                 episode reward: 0.2525,                 loss: 0.1975
agent1:                 episode reward: -0.2525,                 loss: nan
Episode: 74961/101000 (74.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7805s / 12900.5161 s
agent0:                 episode reward: -0.0123,                 loss: 0.1976
agent1:                 episode reward: 0.0123,                 loss: nan
Episode: 74981/101000 (74.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6333s / 12905.1494 s
agent0:                 episode reward: -0.2257,                 loss: 0.1972
agent1:                 episode reward: 0.2257,                 loss: nan
Episode: 75001/101000 (74.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6458s / 12908.7952 s
agent0:                 episode reward: 0.0178,                 loss: 0.1944
agent1:                 episode reward: -0.0178,                 loss: nan
Episode: 75021/101000 (74.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8152s / 12913.6104 s
agent0:                 episode reward: 0.2170,                 loss: 0.1958
agent1:                 episode reward: -0.2170,                 loss: nan
Episode: 75041/101000 (74.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4529s / 12918.0633 s
agent0:                 episode reward: -0.0577,                 loss: 0.1978
agent1:                 episode reward: 0.0577,                 loss: nan
Episode: 75061/101000 (74.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7768s / 12921.8401 s
agent0:                 episode reward: 0.1400,                 loss: 0.1991
agent1:                 episode reward: -0.1400,                 loss: nan
Episode: 75081/101000 (74.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4764s / 12926.3165 s
agent0:                 episode reward: -0.3880,                 loss: 0.1973
agent1:                 episode reward: 0.3880,                 loss: nan
Episode: 75101/101000 (74.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5493s / 12930.8659 s
agent0:                 episode reward: 0.1653,                 loss: 0.1984
agent1:                 episode reward: -0.1653,                 loss: nan
Episode: 75121/101000 (74.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6767s / 12935.5426 s
agent0:                 episode reward: 0.1337,                 loss: 0.1964
agent1:                 episode reward: -0.1337,                 loss: nan
Episode: 75141/101000 (74.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8499s / 12940.3924 s
agent0:                 episode reward: 0.1563,                 loss: 0.1957
agent1:                 episode reward: -0.1563,                 loss: 0.1411
Score delta: 1.7476199891386952, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/74706_0.
Episode: 75161/101000 (74.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2435s / 12944.6360 s
agent0:                 episode reward: -0.1173,                 loss: nan
agent1:                 episode reward: 0.1173,                 loss: 0.1406
Episode: 75181/101000 (74.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6903s / 12948.3263 s
agent0:                 episode reward: -0.4251,                 loss: nan
agent1:                 episode reward: 0.4251,                 loss: 0.1389
Episode: 75201/101000 (74.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4794s / 12952.8057 s
agent0:                 episode reward: -0.2290,                 loss: nan
agent1:                 episode reward: 0.2290,                 loss: 0.1392
Episode: 75221/101000 (74.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0049s / 12956.8106 s
agent0:                 episode reward: -0.4649,                 loss: nan
agent1:                 episode reward: 0.4649,                 loss: 0.1416
Episode: 75241/101000 (74.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6351s / 12961.4457 s
agent0:                 episode reward: -0.2642,                 loss: nan
agent1:                 episode reward: 0.2642,                 loss: 0.1418
Episode: 75261/101000 (74.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3564s / 12965.8021 s
agent0:                 episode reward: -0.1057,                 loss: nan
agent1:                 episode reward: 0.1057,                 loss: 0.1394
Episode: 75281/101000 (74.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3035s / 12970.1056 s
agent0:                 episode reward: -0.5417,                 loss: 0.2287
agent1:                 episode reward: 0.5417,                 loss: 0.1399
Score delta: 1.5246897111513693, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/74851_1.
Episode: 75301/101000 (74.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2770s / 12974.3826 s
agent0:                 episode reward: -0.0212,                 loss: 0.2146
agent1:                 episode reward: 0.0212,                 loss: nan
Episode: 75321/101000 (74.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2409s / 12979.6235 s
agent0:                 episode reward: 0.5273,                 loss: 0.2156
agent1:                 episode reward: -0.5273,                 loss: nan
Episode: 75341/101000 (74.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4788s / 12984.1023 s
agent0:                 episode reward: 0.3240,                 loss: 0.2171
agent1:                 episode reward: -0.3240,                 loss: nan
Episode: 75361/101000 (74.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1589s / 12988.2612 s
agent0:                 episode reward: 0.3291,                 loss: 0.2150
agent1:                 episode reward: -0.3291,                 loss: nan
Episode: 75381/101000 (74.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8328s / 12992.0940 s
agent0:                 episode reward: 0.3832,                 loss: 0.2156
agent1:                 episode reward: -0.3832,                 loss: nan
Episode: 75401/101000 (74.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9803s / 12996.0743 s
agent0:                 episode reward: 0.3294,                 loss: 0.2117
agent1:                 episode reward: -0.3294,                 loss: nan
Episode: 75421/101000 (74.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2443s / 13000.3186 s
agent0:                 episode reward: 0.5136,                 loss: 0.2125
agent1:                 episode reward: -0.5136,                 loss: nan
Episode: 75441/101000 (74.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7950s / 13005.1136 s
agent0:                 episode reward: 0.1260,                 loss: 0.2135
agent1:                 episode reward: -0.1260,                 loss: nan
Episode: 75461/101000 (74.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0774s / 13009.1910 s
agent0:                 episode reward: -0.0652,                 loss: 0.2135
agent1:                 episode reward: 0.0652,                 loss: 0.1429
Score delta: 1.5169761724059447, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/75021_0.
Episode: 75481/101000 (74.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5773s / 13013.7683 s
agent0:                 episode reward: -0.3133,                 loss: nan
agent1:                 episode reward: 0.3133,                 loss: 0.1406
Episode: 75501/101000 (74.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7096s / 13018.4778 s
agent0:                 episode reward: 0.2697,                 loss: nan
agent1:                 episode reward: -0.2697,                 loss: 0.1576
Episode: 75521/101000 (74.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4491s / 13022.9269 s
agent0:                 episode reward: -0.5047,                 loss: nan
agent1:                 episode reward: 0.5047,                 loss: 0.1587
Episode: 75541/101000 (74.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3497s / 13027.2766 s
agent0:                 episode reward: -0.1896,                 loss: nan
agent1:                 episode reward: 0.1896,                 loss: 0.1581
Episode: 75561/101000 (74.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5554s / 13031.8320 s
agent0:                 episode reward: -0.6205,                 loss: nan
agent1:                 episode reward: 0.6205,                 loss: 0.1550
Episode: 75581/101000 (74.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0557s / 13036.8877 s
agent0:                 episode reward: 0.0189,                 loss: nan
agent1:                 episode reward: -0.0189,                 loss: 0.1538
Episode: 75601/101000 (74.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0650s / 13040.9527 s
agent0:                 episode reward: -0.3441,                 loss: nan
agent1:                 episode reward: 0.3441,                 loss: 0.1534
Episode: 75621/101000 (74.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3838s / 13046.3365 s
agent0:                 episode reward: -0.3330,                 loss: 0.2043
agent1:                 episode reward: 0.3330,                 loss: 0.1525
Score delta: 1.773764793169382, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/75178_1.
Episode: 75641/101000 (74.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3915s / 13050.7280 s
agent0:                 episode reward: -0.0268,                 loss: 0.2021
agent1:                 episode reward: 0.0268,                 loss: nan
Episode: 75661/101000 (74.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8101s / 13055.5381 s
agent0:                 episode reward: 0.1769,                 loss: 0.2020
agent1:                 episode reward: -0.1769,                 loss: nan
Episode: 75681/101000 (74.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6032s / 13060.1413 s
agent0:                 episode reward: -0.5912,                 loss: 0.2009
agent1:                 episode reward: 0.5912,                 loss: nan
Episode: 75701/101000 (74.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5039s / 13064.6451 s
agent0:                 episode reward: 0.5289,                 loss: 0.2004
agent1:                 episode reward: -0.5289,                 loss: nan
Episode: 75721/101000 (74.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3345s / 13069.9797 s
agent0:                 episode reward: 0.2633,                 loss: 0.2008
agent1:                 episode reward: -0.2633,                 loss: nan
Episode: 75741/101000 (74.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6148s / 13074.5945 s
agent0:                 episode reward: 0.5123,                 loss: 0.2008
agent1:                 episode reward: -0.5123,                 loss: nan
Episode: 75761/101000 (75.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3641s / 13078.9585 s
agent0:                 episode reward: 0.1793,                 loss: 0.2022
agent1:                 episode reward: -0.1793,                 loss: nan
Episode: 75781/101000 (75.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0455s / 13084.0040 s
agent0:                 episode reward: -0.2904,                 loss: 0.2011
agent1:                 episode reward: 0.2904,                 loss: nan
Episode: 75801/101000 (75.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4183s / 13089.4223 s
agent0:                 episode reward: -0.2032,                 loss: 0.1995
agent1:                 episode reward: 0.2032,                 loss: nan
Episode: 75821/101000 (75.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8343s / 13093.2566 s
agent0:                 episode reward: -0.0572,                 loss: 0.1995
agent1:                 episode reward: 0.0572,                 loss: nan
Episode: 75841/101000 (75.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3426s / 13097.5992 s
agent0:                 episode reward: -0.3328,                 loss: 0.2006
agent1:                 episode reward: 0.3328,                 loss: nan
Episode: 75861/101000 (75.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3539s / 13101.9531 s
agent0:                 episode reward: 0.3165,                 loss: 0.2012
agent1:                 episode reward: -0.3165,                 loss: nan
Score delta: 1.576059323578619, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/75435_0.
Episode: 75881/101000 (75.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1032s / 13106.0563 s
agent0:                 episode reward: -0.1672,                 loss: nan
agent1:                 episode reward: 0.1672,                 loss: 0.1588
Episode: 75901/101000 (75.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3577s / 13110.4139 s
agent0:                 episode reward: -0.1892,                 loss: nan
agent1:                 episode reward: 0.1892,                 loss: 0.1551
Episode: 75921/101000 (75.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7205s / 13115.1345 s
agent0:                 episode reward: -0.5041,                 loss: nan
agent1:                 episode reward: 0.5041,                 loss: 0.1548
Episode: 75941/101000 (75.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8388s / 13119.9732 s
agent0:                 episode reward: -0.4151,                 loss: nan
agent1:                 episode reward: 0.4151,                 loss: 0.1528
Episode: 75961/101000 (75.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7204s / 13124.6936 s
agent0:                 episode reward: -0.7406,                 loss: nan
agent1:                 episode reward: 0.7406,                 loss: 0.1543
Episode: 75981/101000 (75.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4253s / 13129.1189 s
agent0:                 episode reward: -0.1142,                 loss: nan
agent1:                 episode reward: 0.1142,                 loss: 0.1504
Episode: 76001/101000 (75.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9566s / 13133.0756 s
agent0:                 episode reward: -0.3606,                 loss: nan
agent1:                 episode reward: 0.3606,                 loss: 0.1522
Episode: 76021/101000 (75.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9257s / 13137.0013 s
agent0:                 episode reward: 0.1205,                 loss: nan
agent1:                 episode reward: -0.1205,                 loss: 0.1538
Episode: 76041/101000 (75.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7767s / 13141.7780 s
agent0:                 episode reward: -0.4229,                 loss: nan
agent1:                 episode reward: 0.4229,                 loss: 0.1510
Episode: 76061/101000 (75.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7364s / 13146.5144 s
agent0:                 episode reward: 0.0280,                 loss: 0.2123
agent1:                 episode reward: -0.0280,                 loss: 0.1544
Score delta: 1.7877504153878263, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/75617_1.
Episode: 76081/101000 (75.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0299s / 13151.5444 s
agent0:                 episode reward: -0.3468,                 loss: 0.2079
agent1:                 episode reward: 0.3468,                 loss: nan
Episode: 76101/101000 (75.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3735s / 13155.9178 s
agent0:                 episode reward: -0.1071,                 loss: 0.2085
agent1:                 episode reward: 0.1071,                 loss: nan
Episode: 76121/101000 (75.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1200s / 13160.0379 s
agent0:                 episode reward: 0.0737,                 loss: 0.2092
agent1:                 episode reward: -0.0737,                 loss: nan
Episode: 76141/101000 (75.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1428s / 13165.1807 s
agent0:                 episode reward: -0.0188,                 loss: 0.2083
agent1:                 episode reward: 0.0188,                 loss: nan
Episode: 76161/101000 (75.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5590s / 13169.7396 s
agent0:                 episode reward: 0.1738,                 loss: 0.2077
agent1:                 episode reward: -0.1738,                 loss: nan
Episode: 76181/101000 (75.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0119s / 13174.7515 s
agent0:                 episode reward: 0.1619,                 loss: 0.2066
agent1:                 episode reward: -0.1619,                 loss: nan
Episode: 76201/101000 (75.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0113s / 13178.7628 s
agent0:                 episode reward: 0.1202,                 loss: 0.2073
agent1:                 episode reward: -0.1202,                 loss: nan
Episode: 76221/101000 (75.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9070s / 13183.6698 s
agent0:                 episode reward: 0.0799,                 loss: 0.2067
agent1:                 episode reward: -0.0799,                 loss: nan
Episode: 76241/101000 (75.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0202s / 13187.6900 s
agent0:                 episode reward: 0.1127,                 loss: 0.2069
agent1:                 episode reward: -0.1127,                 loss: nan
Episode: 76261/101000 (75.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9816s / 13192.6715 s
agent0:                 episode reward: -0.0076,                 loss: 0.2064
agent1:                 episode reward: 0.0076,                 loss: nan
Episode: 76281/101000 (75.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3789s / 13198.0505 s
agent0:                 episode reward: -0.1883,                 loss: 0.2060
agent1:                 episode reward: 0.1883,                 loss: nan
Episode: 76301/101000 (75.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8181s / 13203.8686 s
agent0:                 episode reward: 0.2253,                 loss: 0.2076
agent1:                 episode reward: -0.2253,                 loss: nan
Episode: 76321/101000 (75.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3186s / 13208.1872 s
agent0:                 episode reward: -0.1882,                 loss: 0.2088
agent1:                 episode reward: 0.1882,                 loss: nan
Episode: 76341/101000 (75.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6298s / 13213.8169 s
agent0:                 episode reward: 0.4289,                 loss: 0.2062
agent1:                 episode reward: -0.4289,                 loss: 0.1554
Score delta: 1.5651284633600455, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/75914_0.
Episode: 76361/101000 (75.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1000s / 13218.9169 s
agent0:                 episode reward: -0.2348,                 loss: nan
agent1:                 episode reward: 0.2348,                 loss: 0.1584
Episode: 76381/101000 (75.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8634s / 13223.7804 s
agent0:                 episode reward: -0.0554,                 loss: nan
agent1:                 episode reward: 0.0554,                 loss: 0.1550
Episode: 76401/101000 (75.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2236s / 13228.0040 s
agent0:                 episode reward: 0.3349,                 loss: nan
agent1:                 episode reward: -0.3349,                 loss: 0.1519
Episode: 76421/101000 (75.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2122s / 13232.2162 s
agent0:                 episode reward: -0.1851,                 loss: nan
agent1:                 episode reward: 0.1851,                 loss: 0.1539
Episode: 76441/101000 (75.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5893s / 13236.8056 s
agent0:                 episode reward: -0.5275,                 loss: nan
agent1:                 episode reward: 0.5275,                 loss: 0.1552
Episode: 76461/101000 (75.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1933s / 13240.9989 s
agent0:                 episode reward: -0.1784,                 loss: nan
agent1:                 episode reward: 0.1784,                 loss: 0.1530
Episode: 76481/101000 (75.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9668s / 13245.9657 s
agent0:                 episode reward: -0.5097,                 loss: nan
agent1:                 episode reward: 0.5097,                 loss: 0.1524
Episode: 76501/101000 (75.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9694s / 13250.9351 s
agent0:                 episode reward: -0.1456,                 loss: nan
agent1:                 episode reward: 0.1456,                 loss: 0.1519
Episode: 76521/101000 (75.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5271s / 13255.4622 s
agent0:                 episode reward: -0.2553,                 loss: nan
agent1:                 episode reward: 0.2553,                 loss: 0.1543
Episode: 76541/101000 (75.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1148s / 13259.5770 s
agent0:                 episode reward: -0.3169,                 loss: nan
agent1:                 episode reward: 0.3169,                 loss: 0.1534
Episode: 76561/101000 (75.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2138s / 13263.7907 s
agent0:                 episode reward: -0.3791,                 loss: nan
agent1:                 episode reward: 0.3791,                 loss: 0.1531
Episode: 76581/101000 (75.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8010s / 13267.5918 s
agent0:                 episode reward: -0.0809,                 loss: nan
agent1:                 episode reward: 0.0809,                 loss: 0.1538
Episode: 76601/101000 (75.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6492s / 13271.2410 s
agent0:                 episode reward: -0.2108,                 loss: nan
agent1:                 episode reward: 0.2108,                 loss: 0.1542
Episode: 76621/101000 (75.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5230s / 13275.7639 s
agent0:                 episode reward: -0.0258,                 loss: nan
agent1:                 episode reward: 0.0258,                 loss: 0.1536
Episode: 76641/101000 (75.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6030s / 13280.3669 s
agent0:                 episode reward: -0.2508,                 loss: nan
agent1:                 episode reward: 0.2508,                 loss: 0.1531
Episode: 76661/101000 (75.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8706s / 13285.2375 s
agent0:                 episode reward: -0.4785,                 loss: 0.2066
agent1:                 episode reward: 0.4785,                 loss: 0.1527
Score delta: 1.786040469142145, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/76230_1.
Episode: 76681/101000 (75.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8468s / 13290.0843 s
agent0:                 episode reward: 0.5948,                 loss: 0.1995
agent1:                 episode reward: -0.5948,                 loss: nan
Episode: 76701/101000 (75.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4564s / 13294.5408 s
agent0:                 episode reward: 0.1680,                 loss: 0.1983
agent1:                 episode reward: -0.1680,                 loss: nan
Episode: 76721/101000 (75.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2955s / 13299.8362 s
agent0:                 episode reward: 0.3205,                 loss: 0.2010
agent1:                 episode reward: -0.3205,                 loss: nan
Episode: 76741/101000 (75.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6432s / 13304.4794 s
agent0:                 episode reward: -0.2639,                 loss: 0.2003
agent1:                 episode reward: 0.2639,                 loss: nan
Episode: 76761/101000 (76.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1854s / 13309.6648 s
agent0:                 episode reward: -0.1798,                 loss: 0.2002
agent1:                 episode reward: 0.1798,                 loss: nan
Episode: 76781/101000 (76.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3355s / 13315.0003 s
agent0:                 episode reward: 0.1577,                 loss: 0.2013
agent1:                 episode reward: -0.1577,                 loss: nan
Episode: 76801/101000 (76.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5922s / 13319.5925 s
agent0:                 episode reward: 0.2100,                 loss: 0.2010
agent1:                 episode reward: -0.2100,                 loss: nan
Episode: 76821/101000 (76.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3214s / 13323.9139 s
agent0:                 episode reward: -0.0360,                 loss: 0.2000
agent1:                 episode reward: 0.0360,                 loss: nan
Episode: 76841/101000 (76.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6617s / 13329.5756 s
agent0:                 episode reward: -0.2755,                 loss: 0.2002
agent1:                 episode reward: 0.2755,                 loss: nan
Episode: 76861/101000 (76.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3020s / 13333.8776 s
agent0:                 episode reward: 0.4233,                 loss: 0.1989
agent1:                 episode reward: -0.4233,                 loss: nan
Episode: 76881/101000 (76.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9245s / 13337.8021 s
agent0:                 episode reward: 0.1701,                 loss: 0.1995
agent1:                 episode reward: -0.1701,                 loss: nan
Episode: 76901/101000 (76.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9549s / 13342.7570 s
agent0:                 episode reward: 0.4045,                 loss: 0.1987
agent1:                 episode reward: -0.4045,                 loss: nan
Episode: 76921/101000 (76.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4411s / 13348.1981 s
agent0:                 episode reward: -0.0282,                 loss: 0.1973
agent1:                 episode reward: 0.0282,                 loss: nan
Episode: 76941/101000 (76.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6793s / 13353.8774 s
agent0:                 episode reward: -0.1552,                 loss: 0.1994
agent1:                 episode reward: 0.1552,                 loss: nan
Episode: 76961/101000 (76.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1896s / 13359.0670 s
agent0:                 episode reward: 0.1199,                 loss: 0.1971
agent1:                 episode reward: -0.1199,                 loss: 0.1537
Score delta: 1.5985170099392447, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/76519_0.
Episode: 76981/101000 (76.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7563s / 13363.8233 s
agent0:                 episode reward: -0.2437,                 loss: nan
agent1:                 episode reward: 0.2437,                 loss: 0.1522
Episode: 77001/101000 (76.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7185s / 13367.5418 s
agent0:                 episode reward: -0.2292,                 loss: nan
agent1:                 episode reward: 0.2292,                 loss: 0.1525
Episode: 77021/101000 (76.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9077s / 13372.4496 s
agent0:                 episode reward: -0.0456,                 loss: nan
agent1:                 episode reward: 0.0456,                 loss: 0.1534
Episode: 77041/101000 (76.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1739s / 13377.6235 s
agent0:                 episode reward: -0.0368,                 loss: nan
agent1:                 episode reward: 0.0368,                 loss: 0.1520
Episode: 77061/101000 (76.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2166s / 13381.8401 s
agent0:                 episode reward: -0.1703,                 loss: nan
agent1:                 episode reward: 0.1703,                 loss: 0.1521
Episode: 77081/101000 (76.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0589s / 13386.8990 s
agent0:                 episode reward: -0.6217,                 loss: nan
agent1:                 episode reward: 0.6217,                 loss: 0.1525
Episode: 77101/101000 (76.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1428s / 13391.0418 s
agent0:                 episode reward: -0.3883,                 loss: 0.2018
agent1:                 episode reward: 0.3883,                 loss: 0.1502
Score delta: 1.7276437488937497, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/76664_1.
Episode: 77121/101000 (76.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1953s / 13395.2371 s
agent0:                 episode reward: 0.3104,                 loss: 0.2006
agent1:                 episode reward: -0.3104,                 loss: nan
Episode: 77141/101000 (76.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0404s / 13399.2775 s
agent0:                 episode reward: -0.3323,                 loss: 0.2021
agent1:                 episode reward: 0.3323,                 loss: nan
Episode: 77161/101000 (76.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6571s / 13403.9347 s
agent0:                 episode reward: -0.1210,                 loss: 0.2019
agent1:                 episode reward: 0.1210,                 loss: nan
Episode: 77181/101000 (76.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3168s / 13409.2514 s
agent0:                 episode reward: -0.2751,                 loss: 0.2028
agent1:                 episode reward: 0.2751,                 loss: nan
Episode: 77201/101000 (76.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6907s / 13414.9421 s
agent0:                 episode reward: 0.0011,                 loss: 0.2058
agent1:                 episode reward: -0.0011,                 loss: nan
Episode: 77221/101000 (76.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2965s / 13420.2387 s
agent0:                 episode reward: 0.1988,                 loss: 0.2059
agent1:                 episode reward: -0.1988,                 loss: nan
Episode: 77241/101000 (76.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3285s / 13424.5672 s
agent0:                 episode reward: 0.2025,                 loss: 0.2031
agent1:                 episode reward: -0.2025,                 loss: nan
Episode: 77261/101000 (76.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0869s / 13429.6540 s
agent0:                 episode reward: 0.2281,                 loss: 0.2032
agent1:                 episode reward: -0.2281,                 loss: 0.1487
Score delta: 1.5929590035029808, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/76834_0.
Episode: 77281/101000 (76.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3268s / 13433.9808 s
agent0:                 episode reward: -0.4893,                 loss: nan
agent1:                 episode reward: 0.4893,                 loss: 0.1532
Episode: 77301/101000 (76.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0259s / 13438.0067 s
agent0:                 episode reward: -0.4769,                 loss: nan
agent1:                 episode reward: 0.4769,                 loss: 0.1529
Episode: 77321/101000 (76.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5337s / 13441.5404 s
agent0:                 episode reward: -0.0427,                 loss: nan
agent1:                 episode reward: 0.0427,                 loss: 0.1528
Episode: 77341/101000 (76.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9307s / 13445.4711 s
agent0:                 episode reward: -0.3184,                 loss: nan
agent1:                 episode reward: 0.3184,                 loss: 0.1532
Episode: 77361/101000 (76.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9163s / 13450.3873 s
agent0:                 episode reward: -0.2042,                 loss: nan
agent1:                 episode reward: 0.2042,                 loss: 0.1505
Episode: 77381/101000 (76.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2087s / 13455.5960 s
agent0:                 episode reward: 0.1373,                 loss: nan
agent1:                 episode reward: -0.1373,                 loss: 0.1530
Episode: 77401/101000 (76.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1956s / 13459.7917 s
agent0:                 episode reward: -0.3802,                 loss: nan
agent1:                 episode reward: 0.3802,                 loss: 0.1509
Episode: 77421/101000 (76.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9771s / 13464.7688 s
agent0:                 episode reward: -0.3355,                 loss: nan
agent1:                 episode reward: 0.3355,                 loss: 0.1513
Episode: 77441/101000 (76.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0729s / 13469.8416 s
agent0:                 episode reward: -0.1175,                 loss: nan
agent1:                 episode reward: 0.1175,                 loss: 0.1507
Episode: 77461/101000 (76.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8313s / 13474.6729 s
agent0:                 episode reward: -0.1086,                 loss: nan
agent1:                 episode reward: 0.1086,                 loss: 0.1521
Episode: 77481/101000 (76.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9970s / 13479.6699 s
agent0:                 episode reward: 0.0330,                 loss: nan
agent1:                 episode reward: -0.0330,                 loss: 0.1508
Episode: 77501/101000 (76.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6648s / 13483.3346 s
agent0:                 episode reward: 0.0497,                 loss: nan
agent1:                 episode reward: -0.0497,                 loss: 0.1515
Episode: 77521/101000 (76.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0591s / 13487.3937 s
agent0:                 episode reward: -0.1691,                 loss: nan
agent1:                 episode reward: 0.1691,                 loss: 0.1566
Episode: 77541/101000 (76.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7075s / 13492.1012 s
agent0:                 episode reward: -0.3000,                 loss: nan
agent1:                 episode reward: 0.3000,                 loss: 0.1548
Episode: 77561/101000 (76.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6848s / 13496.7860 s
agent0:                 episode reward: -0.2963,                 loss: nan
agent1:                 episode reward: 0.2963,                 loss: 0.1538
Episode: 77581/101000 (76.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1826s / 13500.9686 s
agent0:                 episode reward: -0.4524,                 loss: nan
agent1:                 episode reward: 0.4524,                 loss: 0.1547
Episode: 77601/101000 (76.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0820s / 13505.0506 s
agent0:                 episode reward: 0.0635,                 loss: nan
agent1:                 episode reward: -0.0635,                 loss: 0.1541
Episode: 77621/101000 (76.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5383s / 13509.5889 s
agent0:                 episode reward: -0.3489,                 loss: 0.2127
agent1:                 episode reward: 0.3489,                 loss: 0.1540
Score delta: 1.5054398290471762, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/77193_1.
Episode: 77641/101000 (76.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4429s / 13515.0318 s
agent0:                 episode reward: 0.5475,                 loss: 0.1967
agent1:                 episode reward: -0.5475,                 loss: nan
Episode: 77661/101000 (76.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9895s / 13519.0213 s
agent0:                 episode reward: 0.5315,                 loss: 0.1974
agent1:                 episode reward: -0.5315,                 loss: nan
Episode: 77681/101000 (76.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2555s / 13524.2768 s
agent0:                 episode reward: -0.1137,                 loss: 0.1974
agent1:                 episode reward: 0.1137,                 loss: nan
Episode: 77701/101000 (76.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3811s / 13528.6579 s
agent0:                 episode reward: 0.4183,                 loss: 0.1946
agent1:                 episode reward: -0.4183,                 loss: nan
Episode: 77721/101000 (76.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6391s / 13534.2971 s
agent0:                 episode reward: 0.0029,                 loss: 0.1946
agent1:                 episode reward: -0.0029,                 loss: nan
Episode: 77741/101000 (76.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0482s / 13538.3453 s
agent0:                 episode reward: -0.1927,                 loss: 0.1934
agent1:                 episode reward: 0.1927,                 loss: nan
Episode: 77761/101000 (76.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6073s / 13542.9526 s
agent0:                 episode reward: 0.4109,                 loss: 0.1927
agent1:                 episode reward: -0.4109,                 loss: nan
Episode: 77781/101000 (77.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5619s / 13547.5145 s
agent0:                 episode reward: 0.2901,                 loss: 0.1929
agent1:                 episode reward: -0.2901,                 loss: 0.1532
Score delta: 1.611069821713799, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/77348_0.
Episode: 77801/101000 (77.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4019s / 13551.9165 s
agent0:                 episode reward: -0.3086,                 loss: nan
agent1:                 episode reward: 0.3086,                 loss: 0.1538
Episode: 77821/101000 (77.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8263s / 13555.7428 s
agent0:                 episode reward: -0.1841,                 loss: nan
agent1:                 episode reward: 0.1841,                 loss: 0.1527
Episode: 77841/101000 (77.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9177s / 13560.6605 s
agent0:                 episode reward: -0.3835,                 loss: nan
agent1:                 episode reward: 0.3835,                 loss: 0.1535
Episode: 77861/101000 (77.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3660s / 13565.0265 s
agent0:                 episode reward: -0.3658,                 loss: nan
agent1:                 episode reward: 0.3658,                 loss: 0.1517
Episode: 77881/101000 (77.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9170s / 13569.9435 s
agent0:                 episode reward: -0.1163,                 loss: nan
agent1:                 episode reward: 0.1163,                 loss: 0.1510
Episode: 77901/101000 (77.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1353s / 13574.0788 s
agent0:                 episode reward: -0.4313,                 loss: nan
agent1:                 episode reward: 0.4313,                 loss: 0.1525
Episode: 77921/101000 (77.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7021s / 13577.7809 s
agent0:                 episode reward: -0.2658,                 loss: nan
agent1:                 episode reward: 0.2658,                 loss: 0.1531
Episode: 77941/101000 (77.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7813s / 13582.5622 s
agent0:                 episode reward: 0.0344,                 loss: nan
agent1:                 episode reward: -0.0344,                 loss: 0.1507
Episode: 77961/101000 (77.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8688s / 13586.4310 s
agent0:                 episode reward: -0.2470,                 loss: nan
agent1:                 episode reward: 0.2470,                 loss: 0.1508
Episode: 77981/101000 (77.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7833s / 13590.2143 s
agent0:                 episode reward: -0.1195,                 loss: nan
agent1:                 episode reward: 0.1195,                 loss: 0.1520
Episode: 78001/101000 (77.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9100s / 13594.1244 s
agent0:                 episode reward: -0.2549,                 loss: nan
agent1:                 episode reward: 0.2549,                 loss: 0.1519
Episode: 78021/101000 (77.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5008s / 13598.6252 s
agent0:                 episode reward: 0.0001,                 loss: nan
agent1:                 episode reward: -0.0001,                 loss: 0.1509
Episode: 78041/101000 (77.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0906s / 13603.7158 s
agent0:                 episode reward: -0.3080,                 loss: nan
agent1:                 episode reward: 0.3080,                 loss: 0.1506
Episode: 78061/101000 (77.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4369s / 13608.1527 s
agent0:                 episode reward: -0.4318,                 loss: nan
agent1:                 episode reward: 0.4318,                 loss: 0.1516
Episode: 78081/101000 (77.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5784s / 13611.7311 s
agent0:                 episode reward: -0.3539,                 loss: nan
agent1:                 episode reward: 0.3539,                 loss: 0.1511
Episode: 78101/101000 (77.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0408s / 13616.7719 s
agent0:                 episode reward: -0.0332,                 loss: nan
agent1:                 episode reward: 0.0332,                 loss: 0.1521
Episode: 78121/101000 (77.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3355s / 13621.1073 s
agent0:                 episode reward: -0.2472,                 loss: nan
agent1:                 episode reward: 0.2472,                 loss: 0.1509
Episode: 78141/101000 (77.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0509s / 13625.1582 s
agent0:                 episode reward: -0.1055,                 loss: nan
agent1:                 episode reward: 0.1055,                 loss: 0.1519
Episode: 78161/101000 (77.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1898s / 13629.3480 s
agent0:                 episode reward: -0.5333,                 loss: nan
agent1:                 episode reward: 0.5333,                 loss: 0.1521
Episode: 78181/101000 (77.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9967s / 13634.3447 s
agent0:                 episode reward: -0.0155,                 loss: 0.2070
agent1:                 episode reward: 0.0155,                 loss: 0.1520
Score delta: 1.7235362772778182, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/77742_1.
Episode: 78201/101000 (77.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3578s / 13639.7025 s
agent0:                 episode reward: 0.4132,                 loss: 0.2088
agent1:                 episode reward: -0.4132,                 loss: nan
Episode: 78221/101000 (77.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1851s / 13643.8876 s
agent0:                 episode reward: -0.4525,                 loss: 0.2080
agent1:                 episode reward: 0.4525,                 loss: nan
Episode: 78241/101000 (77.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5807s / 13648.4683 s
agent0:                 episode reward: -0.1087,                 loss: 0.2077
agent1:                 episode reward: 0.1087,                 loss: nan
Episode: 78261/101000 (77.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5239s / 13652.9922 s
agent0:                 episode reward: -0.0864,                 loss: 0.2092
agent1:                 episode reward: 0.0864,                 loss: nan
Episode: 78281/101000 (77.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3044s / 13657.2966 s
agent0:                 episode reward: -0.3116,                 loss: 0.2113
agent1:                 episode reward: 0.3116,                 loss: nan
Episode: 78301/101000 (77.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9196s / 13661.2162 s
agent0:                 episode reward: 0.1294,                 loss: 0.2145
agent1:                 episode reward: -0.1294,                 loss: nan
Episode: 78321/101000 (77.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0420s / 13665.2582 s
agent0:                 episode reward: 0.0068,                 loss: 0.2145
agent1:                 episode reward: -0.0068,                 loss: nan
Episode: 78341/101000 (77.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0082s / 13670.2663 s
agent0:                 episode reward: 0.5434,                 loss: 0.2157
agent1:                 episode reward: -0.5434,                 loss: 0.1579
Score delta: 1.5863554094280934, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/77913_0.
Episode: 78361/101000 (77.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4980s / 13674.7643 s
agent0:                 episode reward: -0.0901,                 loss: nan
agent1:                 episode reward: 0.0901,                 loss: 0.1587
Episode: 78381/101000 (77.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7425s / 13679.5068 s
agent0:                 episode reward: -0.1671,                 loss: nan
agent1:                 episode reward: 0.1671,                 loss: 0.1571
Episode: 78401/101000 (77.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8062s / 13684.3129 s
agent0:                 episode reward: -0.1525,                 loss: nan
agent1:                 episode reward: 0.1525,                 loss: 0.1550
Episode: 78421/101000 (77.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0464s / 13688.3593 s
agent0:                 episode reward: -0.3224,                 loss: nan
agent1:                 episode reward: 0.3224,                 loss: 0.1564
Episode: 78441/101000 (77.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2050s / 13692.5644 s
agent0:                 episode reward: -0.6496,                 loss: nan
agent1:                 episode reward: 0.6496,                 loss: 0.1557
Episode: 78461/101000 (77.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1093s / 13696.6737 s
agent0:                 episode reward: -0.2979,                 loss: nan
agent1:                 episode reward: 0.2979,                 loss: 0.1539
Episode: 78481/101000 (77.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3445s / 13703.0182 s
agent0:                 episode reward: -0.6137,                 loss: nan
agent1:                 episode reward: 0.6137,                 loss: 0.1534
Episode: 78501/101000 (77.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5702s / 13708.5884 s
agent0:                 episode reward: -0.2858,                 loss: nan
agent1:                 episode reward: 0.2858,                 loss: 0.1528
Episode: 78521/101000 (77.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7151s / 13713.3035 s
agent0:                 episode reward: -0.2273,                 loss: 0.2219
agent1:                 episode reward: 0.2273,                 loss: 0.1510
Score delta: 1.65777575745255, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/78086_1.
Episode: 78541/101000 (77.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6361s / 13717.9396 s
agent0:                 episode reward: -0.2113,                 loss: 0.2207
agent1:                 episode reward: 0.2113,                 loss: nan
Episode: 78561/101000 (77.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3757s / 13722.3153 s
agent0:                 episode reward: 0.3000,                 loss: 0.2184
agent1:                 episode reward: -0.3000,                 loss: nan
Episode: 78581/101000 (77.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9713s / 13727.2866 s
agent0:                 episode reward: 0.2350,                 loss: 0.2211
agent1:                 episode reward: -0.2350,                 loss: nan
Episode: 78601/101000 (77.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7405s / 13731.0271 s
agent0:                 episode reward: -0.1458,                 loss: 0.2186
agent1:                 episode reward: 0.1458,                 loss: nan
Episode: 78621/101000 (77.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7614s / 13735.7884 s
agent0:                 episode reward: 0.0549,                 loss: 0.2205
agent1:                 episode reward: -0.0549,                 loss: nan
Episode: 78641/101000 (77.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7725s / 13740.5609 s
agent0:                 episode reward: 0.3596,                 loss: 0.2201
agent1:                 episode reward: -0.3596,                 loss: nan
Episode: 78661/101000 (77.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9394s / 13745.5003 s
agent0:                 episode reward: 0.4615,                 loss: 0.2226
agent1:                 episode reward: -0.4615,                 loss: 0.1533
Score delta: 1.7447869217554246, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/78234_0.
Episode: 78681/101000 (77.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0255s / 13750.5257 s
agent0:                 episode reward: -0.1836,                 loss: nan
agent1:                 episode reward: 0.1836,                 loss: 0.1500
Episode: 78701/101000 (77.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2573s / 13754.7831 s
agent0:                 episode reward: -0.0140,                 loss: nan
agent1:                 episode reward: 0.0140,                 loss: 0.1480
Episode: 78721/101000 (77.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7433s / 13759.5264 s
agent0:                 episode reward: -0.2122,                 loss: nan
agent1:                 episode reward: 0.2122,                 loss: 0.1490
Episode: 78741/101000 (77.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2878s / 13764.8142 s
agent0:                 episode reward: 0.0177,                 loss: nan
agent1:                 episode reward: -0.0177,                 loss: 0.1494
Episode: 78761/101000 (77.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7484s / 13769.5626 s
agent0:                 episode reward: -0.3225,                 loss: nan
agent1:                 episode reward: 0.3225,                 loss: 0.1485
Episode: 78781/101000 (78.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8750s / 13773.4376 s
agent0:                 episode reward: 0.0429,                 loss: nan
agent1:                 episode reward: -0.0429,                 loss: 0.1473
Episode: 78801/101000 (78.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9397s / 13778.3773 s
agent0:                 episode reward: -0.1772,                 loss: nan
agent1:                 episode reward: 0.1772,                 loss: 0.1482
Episode: 78821/101000 (78.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5067s / 13783.8840 s
agent0:                 episode reward: -0.2348,                 loss: nan
agent1:                 episode reward: 0.2348,                 loss: 0.1487
Episode: 78841/101000 (78.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4895s / 13788.3734 s
agent0:                 episode reward: 0.0684,                 loss: nan
agent1:                 episode reward: -0.0684,                 loss: 0.1471
Episode: 78861/101000 (78.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5428s / 13792.9162 s
agent0:                 episode reward: -0.4766,                 loss: nan
agent1:                 episode reward: 0.4766,                 loss: 0.1481
Episode: 78881/101000 (78.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0751s / 13796.9913 s
agent0:                 episode reward: 0.2124,                 loss: nan
agent1:                 episode reward: -0.2124,                 loss: 0.1477
Episode: 78901/101000 (78.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3151s / 13801.3064 s
agent0:                 episode reward: 0.1761,                 loss: nan
agent1:                 episode reward: -0.1761,                 loss: 0.1468
Episode: 78921/101000 (78.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1853s / 13805.4917 s
agent0:                 episode reward: -0.3077,                 loss: nan
agent1:                 episode reward: 0.3077,                 loss: 0.1464
Episode: 78941/101000 (78.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2201s / 13808.7119 s
agent0:                 episode reward: 0.2999,                 loss: 0.2206
agent1:                 episode reward: -0.2999,                 loss: 0.1448
Score delta: 1.6001237430681763, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/78496_1.
Episode: 78961/101000 (78.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0635s / 13812.7753 s
agent0:                 episode reward: -0.0654,                 loss: 0.2175
agent1:                 episode reward: 0.0654,                 loss: nan
Episode: 78981/101000 (78.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8556s / 13817.6310 s
agent0:                 episode reward: -0.3839,                 loss: 0.2188
agent1:                 episode reward: 0.3839,                 loss: nan
Episode: 79001/101000 (78.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3039s / 13821.9349 s
agent0:                 episode reward: -0.1945,                 loss: 0.2208
agent1:                 episode reward: 0.1945,                 loss: nan
Episode: 79021/101000 (78.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3506s / 13826.2854 s
agent0:                 episode reward: 0.5233,                 loss: 0.2187
agent1:                 episode reward: -0.5233,                 loss: nan
Episode: 79041/101000 (78.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0386s / 13830.3240 s
agent0:                 episode reward: 0.1103,                 loss: 0.2224
agent1:                 episode reward: -0.1103,                 loss: nan
Episode: 79061/101000 (78.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4775s / 13834.8015 s
agent0:                 episode reward: 0.0243,                 loss: 0.2118
agent1:                 episode reward: -0.0243,                 loss: nan
Episode: 79081/101000 (78.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2415s / 13840.0430 s
agent0:                 episode reward: -0.0733,                 loss: 0.2130
agent1:                 episode reward: 0.0733,                 loss: nan
Episode: 79101/101000 (78.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7901s / 13844.8331 s
agent0:                 episode reward: 0.3540,                 loss: 0.2134
agent1:                 episode reward: -0.3540,                 loss: nan
Episode: 79121/101000 (78.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7082s / 13849.5413 s
agent0:                 episode reward: -0.1105,                 loss: 0.2118
agent1:                 episode reward: 0.1105,                 loss: nan
Episode: 79141/101000 (78.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9351s / 13854.4765 s
agent0:                 episode reward: 0.3104,                 loss: 0.2126
agent1:                 episode reward: -0.3104,                 loss: nan
Episode: 79161/101000 (78.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0452s / 13859.5217 s
agent0:                 episode reward: 0.1061,                 loss: 0.2126
agent1:                 episode reward: -0.1061,                 loss: nan
Episode: 79181/101000 (78.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3085s / 13864.8301 s
agent0:                 episode reward: -0.1494,                 loss: 0.2121
agent1:                 episode reward: 0.1494,                 loss: nan
Episode: 79201/101000 (78.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7456s / 13868.5757 s
agent0:                 episode reward: 0.1129,                 loss: 0.2137
agent1:                 episode reward: -0.1129,                 loss: nan
Episode: 79221/101000 (78.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0945s / 13872.6702 s
agent0:                 episode reward: 0.2559,                 loss: 0.2124
agent1:                 episode reward: -0.2559,                 loss: nan
Episode: 79241/101000 (78.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6363s / 13876.3065 s
agent0:                 episode reward: 0.0298,                 loss: 0.2120
agent1:                 episode reward: -0.0298,                 loss: nan
Episode: 79261/101000 (78.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5362s / 13880.8427 s
agent0:                 episode reward: -0.0478,                 loss: 0.2125
agent1:                 episode reward: 0.0478,                 loss: nan
Episode: 79281/101000 (78.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6253s / 13886.4679 s
agent0:                 episode reward: -0.2643,                 loss: 0.2108
agent1:                 episode reward: 0.2643,                 loss: nan
Episode: 79301/101000 (78.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1504s / 13890.6184 s
agent0:                 episode reward: 0.0196,                 loss: 0.2144
agent1:                 episode reward: -0.0196,                 loss: 0.1573
Score delta: 1.5033779707809738, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/78860_0.
Episode: 79321/101000 (78.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0616s / 13894.6800 s
agent0:                 episode reward: -0.6895,                 loss: nan
agent1:                 episode reward: 0.6895,                 loss: 0.1573
Episode: 79341/101000 (78.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5634s / 13899.2434 s
agent0:                 episode reward: -0.0991,                 loss: nan
agent1:                 episode reward: 0.0991,                 loss: 0.1550
Episode: 79361/101000 (78.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5584s / 13905.8018 s
agent0:                 episode reward: -0.1970,                 loss: nan
agent1:                 episode reward: 0.1970,                 loss: 0.1528
Episode: 79381/101000 (78.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0069s / 13910.8088 s
agent0:                 episode reward: -0.4728,                 loss: nan
agent1:                 episode reward: 0.4728,                 loss: 0.1486
Episode: 79401/101000 (78.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6579s / 13915.4666 s
agent0:                 episode reward: -0.3627,                 loss: nan
agent1:                 episode reward: 0.3627,                 loss: 0.1499
Episode: 79421/101000 (78.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2283s / 13920.6949 s
agent0:                 episode reward: -0.1490,                 loss: nan
agent1:                 episode reward: 0.1490,                 loss: 0.1481
Episode: 79441/101000 (78.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5979s / 13926.2928 s
agent0:                 episode reward: -0.2917,                 loss: nan
agent1:                 episode reward: 0.2917,                 loss: 0.1486
Episode: 79461/101000 (78.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0877s / 13931.3805 s
agent0:                 episode reward: -0.8404,                 loss: 0.2369
agent1:                 episode reward: 0.8404,                 loss: 0.1482
Score delta: 1.5182141845808446, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/79023_1.
Episode: 79481/101000 (78.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6622s / 13936.0427 s
agent0:                 episode reward: -0.4026,                 loss: 0.2236
agent1:                 episode reward: 0.4026,                 loss: nan
Episode: 79501/101000 (78.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6496s / 13940.6923 s
agent0:                 episode reward: -0.0517,                 loss: 0.2218
agent1:                 episode reward: 0.0517,                 loss: nan
Episode: 79521/101000 (78.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9068s / 13945.5990 s
agent0:                 episode reward: 0.0627,                 loss: 0.2189
agent1:                 episode reward: -0.0627,                 loss: nan
Episode: 79541/101000 (78.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 3.1099s / 13948.7090 s
agent0:                 episode reward: -0.0196,                 loss: 0.2210
agent1:                 episode reward: 0.0196,                 loss: nan
Episode: 79561/101000 (78.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6290s / 13952.3380 s
agent0:                 episode reward: -0.1229,                 loss: 0.2181
agent1:                 episode reward: 0.1229,                 loss: nan
Episode: 79581/101000 (78.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7761s / 13957.1141 s
agent0:                 episode reward: -0.1843,                 loss: 0.2179
agent1:                 episode reward: 0.1843,                 loss: nan
Episode: 79601/101000 (78.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1780s / 13961.2921 s
agent0:                 episode reward: 0.0339,                 loss: 0.2165
agent1:                 episode reward: -0.0339,                 loss: nan
Episode: 79621/101000 (78.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7958s / 13966.0878 s
agent0:                 episode reward: 0.0128,                 loss: 0.2166
agent1:                 episode reward: -0.0128,                 loss: nan
Episode: 79641/101000 (78.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9705s / 13971.0583 s
agent0:                 episode reward: -0.2713,                 loss: 0.2173
agent1:                 episode reward: 0.2713,                 loss: nan
Episode: 79661/101000 (78.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4757s / 13976.5341 s
agent0:                 episode reward: -0.0713,                 loss: 0.2150
agent1:                 episode reward: 0.0713,                 loss: nan
Episode: 79681/101000 (78.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8971s / 13982.4312 s
agent0:                 episode reward: -0.0216,                 loss: 0.2137
agent1:                 episode reward: 0.0216,                 loss: nan
Episode: 79701/101000 (78.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2195s / 13987.6507 s
agent0:                 episode reward: 0.2315,                 loss: 0.2147
agent1:                 episode reward: -0.2315,                 loss: nan
Episode: 79721/101000 (78.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7335s / 13991.3842 s
agent0:                 episode reward: -0.0721,                 loss: 0.2144
agent1:                 episode reward: 0.0721,                 loss: nan
Episode: 79741/101000 (78.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0561s / 13996.4403 s
agent0:                 episode reward: 0.2400,                 loss: 0.2138
agent1:                 episode reward: -0.2400,                 loss: nan
Episode: 79761/101000 (78.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4579s / 14000.8982 s
agent0:                 episode reward: 0.1810,                 loss: 0.2136
agent1:                 episode reward: -0.1810,                 loss: 0.1475
Score delta: 1.556555029198614, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/79323_0.
Episode: 79781/101000 (78.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6711s / 14004.5694 s
agent0:                 episode reward: 0.0352,                 loss: nan
agent1:                 episode reward: -0.0352,                 loss: 0.1478
Episode: 79801/101000 (79.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8611s / 14009.4304 s
agent0:                 episode reward: -0.5522,                 loss: nan
agent1:                 episode reward: 0.5522,                 loss: 0.1454
Episode: 79821/101000 (79.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3572s / 14013.7877 s
agent0:                 episode reward: 0.0937,                 loss: nan
agent1:                 episode reward: -0.0937,                 loss: 0.1452
Episode: 79841/101000 (79.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3608s / 14019.1484 s
agent0:                 episode reward: 0.3530,                 loss: nan
agent1:                 episode reward: -0.3530,                 loss: 0.1454
Episode: 79861/101000 (79.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3653s / 14023.5138 s
agent0:                 episode reward: 0.0387,                 loss: nan
agent1:                 episode reward: -0.0387,                 loss: 0.1457
Episode: 79881/101000 (79.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6109s / 14028.1246 s
agent0:                 episode reward: -0.5841,                 loss: nan
agent1:                 episode reward: 0.5841,                 loss: 0.1458
Episode: 79901/101000 (79.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1534s / 14032.2781 s
agent0:                 episode reward: -0.2727,                 loss: nan
agent1:                 episode reward: 0.2727,                 loss: 0.1449
Episode: 79921/101000 (79.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7782s / 14037.0563 s
agent0:                 episode reward: -0.1844,                 loss: nan
agent1:                 episode reward: 0.1844,                 loss: 0.1444
Episode: 79941/101000 (79.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7504s / 14040.8067 s
agent0:                 episode reward: -0.1219,                 loss: nan
agent1:                 episode reward: 0.1219,                 loss: 0.1439
Episode: 79961/101000 (79.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 3.3823s / 14044.1889 s
agent0:                 episode reward: -0.1426,                 loss: nan
agent1:                 episode reward: 0.1426,                 loss: 0.1453
Episode: 79981/101000 (79.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3677s / 14048.5566 s
agent0:                 episode reward: -0.0926,                 loss: nan
agent1:                 episode reward: 0.0926,                 loss: 0.1482
Episode: 80001/101000 (79.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7275s / 14053.2841 s
agent0:                 episode reward: -0.3175,                 loss: nan
agent1:                 episode reward: 0.3175,                 loss: 0.1544
Episode: 80021/101000 (79.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9960s / 14057.2801 s
agent0:                 episode reward: -0.3840,                 loss: nan
agent1:                 episode reward: 0.3840,                 loss: 0.1519
Episode: 80041/101000 (79.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1027s / 14062.3828 s
agent0:                 episode reward: -0.0904,                 loss: nan
agent1:                 episode reward: 0.0904,                 loss: 0.1519
Episode: 80061/101000 (79.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0106s / 14066.3933 s
agent0:                 episode reward: -0.4298,                 loss: nan
agent1:                 episode reward: 0.4298,                 loss: 0.1514
Episode: 80081/101000 (79.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6238s / 14070.0172 s
agent0:                 episode reward: -0.2296,                 loss: nan
agent1:                 episode reward: 0.2296,                 loss: 0.1530
Episode: 80101/101000 (79.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9896s / 14074.0068 s
agent0:                 episode reward: -0.4492,                 loss: 0.1993
agent1:                 episode reward: 0.4492,                 loss: 0.1525
Score delta: 1.5448649998319537, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/79673_1.
Episode: 80121/101000 (79.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4137s / 14078.4205 s
agent0:                 episode reward: 0.2435,                 loss: 0.2018
agent1:                 episode reward: -0.2435,                 loss: nan
Episode: 80141/101000 (79.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1011s / 14083.5216 s
agent0:                 episode reward: 0.2507,                 loss: 0.2018
agent1:                 episode reward: -0.2507,                 loss: nan
Episode: 80161/101000 (79.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4927s / 14088.0143 s
agent0:                 episode reward: -0.1846,                 loss: 0.2040
agent1:                 episode reward: 0.1846,                 loss: nan
Episode: 80181/101000 (79.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0749s / 14093.0892 s
agent0:                 episode reward: -0.2637,                 loss: 0.2019
agent1:                 episode reward: 0.2637,                 loss: nan
Episode: 80201/101000 (79.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2178s / 14097.3070 s
agent0:                 episode reward: -0.0352,                 loss: 0.2015
agent1:                 episode reward: 0.0352,                 loss: nan
Episode: 80221/101000 (79.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3126s / 14102.6196 s
agent0:                 episode reward: -0.1552,                 loss: 0.2039
agent1:                 episode reward: 0.1552,                 loss: nan
Episode: 80241/101000 (79.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6448s / 14107.2644 s
agent0:                 episode reward: 0.0131,                 loss: 0.2098
agent1:                 episode reward: -0.0131,                 loss: nan
Episode: 80261/101000 (79.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9662s / 14112.2306 s
agent0:                 episode reward: -0.3461,                 loss: 0.2114
agent1:                 episode reward: 0.3461,                 loss: nan
Episode: 80281/101000 (79.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1802s / 14116.4108 s
agent0:                 episode reward: 0.1871,                 loss: 0.2096
agent1:                 episode reward: -0.1871,                 loss: 0.1469
Score delta: 1.5856811163461604, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/79852_0.
Episode: 80301/101000 (79.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7905s / 14121.2013 s
agent0:                 episode reward: -0.5307,                 loss: nan
agent1:                 episode reward: 0.5307,                 loss: 0.1472
Episode: 80321/101000 (79.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5518s / 14125.7531 s
agent0:                 episode reward: -0.5945,                 loss: nan
agent1:                 episode reward: 0.5945,                 loss: 0.1467
Episode: 80341/101000 (79.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2094s / 14129.9625 s
agent0:                 episode reward: -0.0311,                 loss: nan
agent1:                 episode reward: 0.0311,                 loss: 0.1462
Episode: 80361/101000 (79.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6601s / 14134.6226 s
agent0:                 episode reward: -0.4183,                 loss: nan
agent1:                 episode reward: 0.4183,                 loss: 0.1446
Episode: 80381/101000 (79.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9045s / 14138.5271 s
agent0:                 episode reward: 0.0481,                 loss: nan
agent1:                 episode reward: -0.0481,                 loss: 0.1455
Episode: 80401/101000 (79.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9238s / 14142.4509 s
agent0:                 episode reward: -0.2322,                 loss: nan
agent1:                 episode reward: 0.2322,                 loss: 0.1468
Episode: 80421/101000 (79.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9840s / 14147.4349 s
agent0:                 episode reward: -0.2563,                 loss: nan
agent1:                 episode reward: 0.2563,                 loss: 0.1455
Episode: 80441/101000 (79.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8273s / 14152.2623 s
agent0:                 episode reward: -0.5598,                 loss: 0.1941
agent1:                 episode reward: 0.5598,                 loss: 0.1429
Score delta: 1.6519259945682996, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/80006_1.
Episode: 80461/101000 (79.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7720s / 14156.0343 s
agent0:                 episode reward: 0.1224,                 loss: 0.1937
agent1:                 episode reward: -0.1224,                 loss: nan
Episode: 80481/101000 (79.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8314s / 14160.8656 s
agent0:                 episode reward: -0.1247,                 loss: 0.1944
agent1:                 episode reward: 0.1247,                 loss: nan
Episode: 80501/101000 (79.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2428s / 14166.1084 s
agent0:                 episode reward: -0.0954,                 loss: 0.1919
agent1:                 episode reward: 0.0954,                 loss: nan
Episode: 80521/101000 (79.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8178s / 14169.9262 s
agent0:                 episode reward: 0.2783,                 loss: 0.1950
agent1:                 episode reward: -0.2783,                 loss: nan
Episode: 80541/101000 (79.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1309s / 14174.0571 s
agent0:                 episode reward: -0.0462,                 loss: 0.1933
agent1:                 episode reward: 0.0462,                 loss: nan
Episode: 80561/101000 (79.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8288s / 14177.8860 s
agent0:                 episode reward: 0.0551,                 loss: 0.1941
agent1:                 episode reward: -0.0551,                 loss: nan
Episode: 80581/101000 (79.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4526s / 14183.3386 s
agent0:                 episode reward: 0.0012,                 loss: 0.1934
agent1:                 episode reward: -0.0012,                 loss: nan
Episode: 80601/101000 (79.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4461s / 14187.7847 s
agent0:                 episode reward: -0.0927,                 loss: 0.1947
agent1:                 episode reward: 0.0927,                 loss: nan
Episode: 80621/101000 (79.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1904s / 14192.9751 s
agent0:                 episode reward: 0.0748,                 loss: 0.1941
agent1:                 episode reward: -0.0748,                 loss: nan
Episode: 80641/101000 (79.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0997s / 14198.0748 s
agent0:                 episode reward: 0.1600,                 loss: 0.1947
agent1:                 episode reward: -0.1600,                 loss: nan
Episode: 80661/101000 (79.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6405s / 14202.7153 s
agent0:                 episode reward: 0.2369,                 loss: 0.1927
agent1:                 episode reward: -0.2369,                 loss: nan
Episode: 80681/101000 (79.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9151s / 14207.6304 s
agent0:                 episode reward: 0.3924,                 loss: 0.1931
agent1:                 episode reward: -0.3924,                 loss: nan
Episode: 80701/101000 (79.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0697s / 14212.7001 s
agent0:                 episode reward: -0.3386,                 loss: 0.1982
agent1:                 episode reward: 0.3386,                 loss: 0.1413
Score delta: 1.6353598677875278, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/80256_0.
Episode: 80721/101000 (79.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2829s / 14216.9830 s
agent0:                 episode reward: -0.1382,                 loss: nan
agent1:                 episode reward: 0.1382,                 loss: 0.1403
Episode: 80741/101000 (79.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1213s / 14222.1043 s
agent0:                 episode reward: -0.4228,                 loss: nan
agent1:                 episode reward: 0.4228,                 loss: 0.1499
Episode: 80761/101000 (79.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5871s / 14226.6914 s
agent0:                 episode reward: -0.3017,                 loss: nan
agent1:                 episode reward: 0.3017,                 loss: 0.1667
Episode: 80781/101000 (79.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2285s / 14231.9199 s
agent0:                 episode reward: 0.4390,                 loss: nan
agent1:                 episode reward: -0.4390,                 loss: 0.1608
Episode: 80801/101000 (80.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0843s / 14237.0042 s
agent0:                 episode reward: 0.1789,                 loss: nan
agent1:                 episode reward: -0.1789,                 loss: 0.1615
Episode: 80821/101000 (80.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1708s / 14242.1750 s
agent0:                 episode reward: 0.0327,                 loss: nan
agent1:                 episode reward: -0.0327,                 loss: 0.1585
Episode: 80841/101000 (80.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0610s / 14247.2360 s
agent0:                 episode reward: -0.0637,                 loss: nan
agent1:                 episode reward: 0.0637,                 loss: 0.1598
Episode: 80861/101000 (80.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5994s / 14251.8354 s
agent0:                 episode reward: 0.0765,                 loss: nan
agent1:                 episode reward: -0.0765,                 loss: 0.1597
Episode: 80881/101000 (80.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6770s / 14256.5125 s
agent0:                 episode reward: 0.1000,                 loss: nan
agent1:                 episode reward: -0.1000,                 loss: 0.1577
Episode: 80901/101000 (80.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4892s / 14261.0016 s
agent0:                 episode reward: 0.1435,                 loss: nan
agent1:                 episode reward: -0.1435,                 loss: 0.1583
Episode: 80921/101000 (80.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7363s / 14265.7380 s
agent0:                 episode reward: -0.2920,                 loss: nan
agent1:                 episode reward: 0.2920,                 loss: 0.1575
Episode: 80941/101000 (80.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0490s / 14270.7870 s
agent0:                 episode reward: -0.1631,                 loss: nan
agent1:                 episode reward: 0.1631,                 loss: 0.1581
Episode: 80961/101000 (80.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0988s / 14274.8858 s
agent0:                 episode reward: -0.5274,                 loss: 0.2335
agent1:                 episode reward: 0.5274,                 loss: 0.1572
Score delta: 1.574963121244836, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/80532_1.
Episode: 80981/101000 (80.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6628s / 14280.5485 s
agent0:                 episode reward: -0.1405,                 loss: 0.2199
agent1:                 episode reward: 0.1405,                 loss: nan
Episode: 81001/101000 (80.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7360s / 14286.2846 s
agent0:                 episode reward: -0.0820,                 loss: 0.2161
agent1:                 episode reward: 0.0820,                 loss: nan
Episode: 81021/101000 (80.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6357s / 14290.9202 s
agent0:                 episode reward: -0.0394,                 loss: 0.2142
agent1:                 episode reward: 0.0394,                 loss: nan
Episode: 81041/101000 (80.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7253s / 14295.6455 s
agent0:                 episode reward: 0.3755,                 loss: 0.2133
agent1:                 episode reward: -0.3755,                 loss: nan
Episode: 81061/101000 (80.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8032s / 14300.4488 s
agent0:                 episode reward: 0.1519,                 loss: 0.2135
agent1:                 episode reward: -0.1519,                 loss: nan
Episode: 81081/101000 (80.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0202s / 14305.4690 s
agent0:                 episode reward: -0.3231,                 loss: 0.2155
agent1:                 episode reward: 0.3231,                 loss: nan
Episode: 81101/101000 (80.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0784s / 14310.5474 s
agent0:                 episode reward: 0.8036,                 loss: 0.2156
agent1:                 episode reward: -0.8036,                 loss: nan
Episode: 81121/101000 (80.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3042s / 14314.8516 s
agent0:                 episode reward: 0.3455,                 loss: 0.2143
agent1:                 episode reward: -0.3455,                 loss: nan
Episode: 81141/101000 (80.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9079s / 14319.7595 s
agent0:                 episode reward: -0.1424,                 loss: 0.2133
agent1:                 episode reward: 0.1424,                 loss: nan
Episode: 81161/101000 (80.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1223s / 14324.8818 s
agent0:                 episode reward: 0.2047,                 loss: 0.2155
agent1:                 episode reward: -0.2047,                 loss: nan
Episode: 81181/101000 (80.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6353s / 14329.5171 s
agent0:                 episode reward: 0.1595,                 loss: 0.2140
agent1:                 episode reward: -0.1595,                 loss: 0.1597
Score delta: 1.5069978488638622, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/80743_0.
Episode: 81201/101000 (80.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5069s / 14334.0240 s
agent0:                 episode reward: -0.3555,                 loss: nan
agent1:                 episode reward: 0.3555,                 loss: 0.1588
Episode: 81221/101000 (80.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6634s / 14338.6874 s
agent0:                 episode reward: -0.3961,                 loss: nan
agent1:                 episode reward: 0.3961,                 loss: 0.1579
Episode: 81241/101000 (80.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0081s / 14343.6955 s
agent0:                 episode reward: -0.2417,                 loss: nan
agent1:                 episode reward: 0.2417,                 loss: 0.1573
Episode: 81261/101000 (80.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6655s / 14349.3611 s
agent0:                 episode reward: -0.3126,                 loss: nan
agent1:                 episode reward: 0.3126,                 loss: 0.1579
Episode: 81281/101000 (80.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6508s / 14355.0118 s
agent0:                 episode reward: -0.0748,                 loss: nan
agent1:                 episode reward: 0.0748,                 loss: 0.1582
Episode: 81301/101000 (80.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0551s / 14359.0669 s
agent0:                 episode reward: 0.1180,                 loss: nan
agent1:                 episode reward: -0.1180,                 loss: 0.1572
Episode: 81321/101000 (80.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1064s / 14363.1733 s
agent0:                 episode reward: 0.0566,                 loss: nan
agent1:                 episode reward: -0.0566,                 loss: 0.1546
Episode: 81341/101000 (80.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6150s / 14367.7883 s
agent0:                 episode reward: -0.0787,                 loss: nan
agent1:                 episode reward: 0.0787,                 loss: 0.1565
Episode: 81361/101000 (80.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5146s / 14373.3029 s
agent0:                 episode reward: -0.3251,                 loss: 0.2101
agent1:                 episode reward: 0.3251,                 loss: 0.1564
Score delta: 1.542047063542435, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/80925_1.
Episode: 81381/101000 (80.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0754s / 14377.3782 s
agent0:                 episode reward: 0.2455,                 loss: 0.2021
agent1:                 episode reward: -0.2455,                 loss: nan
Episode: 81401/101000 (80.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4734s / 14382.8517 s
agent0:                 episode reward: -0.4821,                 loss: 0.2019
agent1:                 episode reward: 0.4821,                 loss: nan
Episode: 81421/101000 (80.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0768s / 14386.9285 s
agent0:                 episode reward: 0.2382,                 loss: 0.2023
agent1:                 episode reward: -0.2382,                 loss: nan
Episode: 81441/101000 (80.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6337s / 14391.5622 s
agent0:                 episode reward: 0.1439,                 loss: 0.2003
agent1:                 episode reward: -0.1439,                 loss: nan
Episode: 81461/101000 (80.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3537s / 14396.9159 s
agent0:                 episode reward: 0.1167,                 loss: 0.1983
agent1:                 episode reward: -0.1167,                 loss: nan
Episode: 81481/101000 (80.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8593s / 14400.7752 s
agent0:                 episode reward: -0.1635,                 loss: 0.1986
agent1:                 episode reward: 0.1635,                 loss: nan
Episode: 81501/101000 (80.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5287s / 14406.3039 s
agent0:                 episode reward: -0.1018,                 loss: 0.2010
agent1:                 episode reward: 0.1018,                 loss: nan
Episode: 81521/101000 (80.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7437s / 14411.0476 s
agent0:                 episode reward: -0.3956,                 loss: 0.2061
agent1:                 episode reward: 0.3956,                 loss: nan
Episode: 81541/101000 (80.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7275s / 14415.7750 s
agent0:                 episode reward: 0.0577,                 loss: 0.2076
agent1:                 episode reward: -0.0577,                 loss: nan
Episode: 81561/101000 (80.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4865s / 14421.2615 s
agent0:                 episode reward: -0.2274,                 loss: 0.2068
agent1:                 episode reward: 0.2274,                 loss: nan
Episode: 81581/101000 (80.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1158s / 14426.3773 s
agent0:                 episode reward: -0.2947,                 loss: 0.2059
agent1:                 episode reward: 0.2947,                 loss: nan
Episode: 81601/101000 (80.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9159s / 14431.2932 s
agent0:                 episode reward: 0.1154,                 loss: 0.2074
agent1:                 episode reward: -0.1154,                 loss: nan
Episode: 81621/101000 (80.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9906s / 14436.2838 s
agent0:                 episode reward: 0.2604,                 loss: 0.2034
agent1:                 episode reward: -0.2604,                 loss: 0.1530
Score delta: 1.7686519161282388, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/81181_0.
Episode: 81641/101000 (80.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0795s / 14440.3633 s
agent0:                 episode reward: -0.3241,                 loss: nan
agent1:                 episode reward: 0.3241,                 loss: 0.1514
Episode: 81661/101000 (80.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7375s / 14444.1008 s
agent0:                 episode reward: -0.2747,                 loss: nan
agent1:                 episode reward: 0.2747,                 loss: 0.1516
Episode: 81681/101000 (80.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1058s / 14449.2066 s
agent0:                 episode reward: -0.3643,                 loss: nan
agent1:                 episode reward: 0.3643,                 loss: 0.1513
Episode: 81701/101000 (80.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6806s / 14453.8871 s
agent0:                 episode reward: -0.5462,                 loss: nan
agent1:                 episode reward: 0.5462,                 loss: 0.1511
Episode: 81721/101000 (80.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3945s / 14459.2816 s
agent0:                 episode reward: 0.1034,                 loss: nan
agent1:                 episode reward: -0.1034,                 loss: 0.1497
Episode: 81741/101000 (80.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3689s / 14464.6505 s
agent0:                 episode reward: -0.3486,                 loss: nan
agent1:                 episode reward: 0.3486,                 loss: 0.1518
Episode: 81761/101000 (80.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8540s / 14469.5046 s
agent0:                 episode reward: -0.1186,                 loss: nan
agent1:                 episode reward: 0.1186,                 loss: 0.1503
Episode: 81781/101000 (80.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7792s / 14475.2837 s
agent0:                 episode reward: -0.1617,                 loss: nan
agent1:                 episode reward: 0.1617,                 loss: 0.1507
Episode: 81801/101000 (80.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0711s / 14479.3548 s
agent0:                 episode reward: -0.0644,                 loss: nan
agent1:                 episode reward: 0.0644,                 loss: 0.1509
Episode: 81821/101000 (81.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5496s / 14483.9044 s
agent0:                 episode reward: -0.4596,                 loss: 0.2296
agent1:                 episode reward: 0.4596,                 loss: 0.1513
Score delta: 1.5193987310675667, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/81390_1.
Episode: 81841/101000 (81.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2275s / 14490.1319 s
agent0:                 episode reward: 0.1189,                 loss: 0.2229
agent1:                 episode reward: -0.1189,                 loss: nan
Episode: 81861/101000 (81.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3420s / 14495.4739 s
agent0:                 episode reward: -0.0640,                 loss: 0.2235
agent1:                 episode reward: 0.0640,                 loss: nan
Episode: 81881/101000 (81.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7254s / 14500.1992 s
agent0:                 episode reward: 0.0353,                 loss: 0.2210
agent1:                 episode reward: -0.0353,                 loss: nan
Episode: 81901/101000 (81.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3756s / 14505.5748 s
agent0:                 episode reward: 0.0324,                 loss: 0.2193
agent1:                 episode reward: -0.0324,                 loss: nan
Episode: 81921/101000 (81.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8867s / 14510.4615 s
agent0:                 episode reward: 0.0718,                 loss: 0.2204
agent1:                 episode reward: -0.0718,                 loss: nan
Episode: 81941/101000 (81.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3796s / 14514.8411 s
agent0:                 episode reward: 0.0643,                 loss: 0.2190
agent1:                 episode reward: -0.0643,                 loss: nan
Episode: 81961/101000 (81.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8543s / 14519.6955 s
agent0:                 episode reward: -0.3835,                 loss: 0.2203
agent1:                 episode reward: 0.3835,                 loss: nan
Episode: 81981/101000 (81.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5721s / 14524.2676 s
agent0:                 episode reward: 0.3105,                 loss: 0.2184
agent1:                 episode reward: -0.3105,                 loss: nan
Episode: 82001/101000 (81.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3653s / 14529.6329 s
agent0:                 episode reward: 0.0645,                 loss: 0.2161
agent1:                 episode reward: -0.0645,                 loss: nan
Episode: 82021/101000 (81.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3064s / 14534.9394 s
agent0:                 episode reward: 0.4528,                 loss: 0.2184
agent1:                 episode reward: -0.4528,                 loss: nan
Episode: 82041/101000 (81.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4198s / 14540.3591 s
agent0:                 episode reward: -0.0211,                 loss: 0.2183
agent1:                 episode reward: 0.0211,                 loss: nan
Episode: 82061/101000 (81.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2250s / 14544.5841 s
agent0:                 episode reward: 0.0801,                 loss: 0.2182
agent1:                 episode reward: -0.0801,                 loss: nan
Episode: 82081/101000 (81.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4462s / 14550.0303 s
agent0:                 episode reward: 0.1752,                 loss: 0.2218
agent1:                 episode reward: -0.1752,                 loss: nan
Episode: 82101/101000 (81.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5674s / 14554.5977 s
agent0:                 episode reward: -0.0563,                 loss: 0.2192
agent1:                 episode reward: 0.0563,                 loss: nan
Episode: 82121/101000 (81.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1631s / 14559.7608 s
agent0:                 episode reward: 0.1858,                 loss: 0.2191
agent1:                 episode reward: -0.1858,                 loss: 0.1596
Score delta: 1.7000313036889019, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/81684_0.
Episode: 82141/101000 (81.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4101s / 14564.1709 s
agent0:                 episode reward: 0.2807,                 loss: nan
agent1:                 episode reward: -0.2807,                 loss: 0.1576
Episode: 82161/101000 (81.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9651s / 14569.1360 s
agent0:                 episode reward: -0.3116,                 loss: nan
agent1:                 episode reward: 0.3116,                 loss: 0.1560
Episode: 82181/101000 (81.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6246s / 14573.7606 s
agent0:                 episode reward: -0.0487,                 loss: nan
agent1:                 episode reward: 0.0487,                 loss: 0.1518
Episode: 82201/101000 (81.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2958s / 14578.0564 s
agent0:                 episode reward: 0.1682,                 loss: nan
agent1:                 episode reward: -0.1682,                 loss: 0.1493
Episode: 82221/101000 (81.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2065s / 14582.2629 s
agent0:                 episode reward: 0.3826,                 loss: nan
agent1:                 episode reward: -0.3826,                 loss: 0.1503
Episode: 82241/101000 (81.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9149s / 14587.1778 s
agent0:                 episode reward: -0.0852,                 loss: nan
agent1:                 episode reward: 0.0852,                 loss: 0.1495
Episode: 82261/101000 (81.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5411s / 14590.7189 s
agent0:                 episode reward: -0.5484,                 loss: nan
agent1:                 episode reward: 0.5484,                 loss: 0.1498
Episode: 82281/101000 (81.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3263s / 14595.0452 s
agent0:                 episode reward: -0.1465,                 loss: nan
agent1:                 episode reward: 0.1465,                 loss: 0.1494
Episode: 82301/101000 (81.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4683s / 14599.5135 s
agent0:                 episode reward: -0.1649,                 loss: nan
agent1:                 episode reward: 0.1649,                 loss: 0.1500
Episode: 82321/101000 (81.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1675s / 14605.6810 s
agent0:                 episode reward: -0.1760,                 loss: nan
agent1:                 episode reward: 0.1760,                 loss: 0.1481
Episode: 82341/101000 (81.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5834s / 14610.2644 s
agent0:                 episode reward: -0.3476,                 loss: nan
agent1:                 episode reward: 0.3476,                 loss: 0.1478
Episode: 82361/101000 (81.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2113s / 14614.4756 s
agent0:                 episode reward: -0.0680,                 loss: nan
agent1:                 episode reward: 0.0680,                 loss: 0.1490
Episode: 82381/101000 (81.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5915s / 14619.0671 s
agent0:                 episode reward: -0.2485,                 loss: nan
agent1:                 episode reward: 0.2485,                 loss: 0.1477
Episode: 82401/101000 (81.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8408s / 14622.9079 s
agent0:                 episode reward: -0.4211,                 loss: 0.1986
agent1:                 episode reward: 0.4211,                 loss: 0.1426
Score delta: 1.5551342778418225, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/81957_1.
Episode: 82421/101000 (81.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9145s / 14626.8223 s
agent0:                 episode reward: 0.5982,                 loss: 0.1951
agent1:                 episode reward: -0.5982,                 loss: nan
Episode: 82441/101000 (81.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0709s / 14631.8932 s
agent0:                 episode reward: -0.0060,                 loss: 0.1918
agent1:                 episode reward: 0.0060,                 loss: nan
Episode: 82461/101000 (81.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5829s / 14637.4761 s
agent0:                 episode reward: 0.0586,                 loss: 0.1942
agent1:                 episode reward: -0.0586,                 loss: nan
Episode: 82481/101000 (81.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5370s / 14643.0131 s
agent0:                 episode reward: 0.2993,                 loss: 0.1913
agent1:                 episode reward: -0.2993,                 loss: nan
Episode: 82501/101000 (81.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9592s / 14647.9723 s
agent0:                 episode reward: -0.0302,                 loss: 0.1934
agent1:                 episode reward: 0.0302,                 loss: nan
Episode: 82521/101000 (81.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4707s / 14652.4430 s
agent0:                 episode reward: 0.2918,                 loss: 0.1892
agent1:                 episode reward: -0.2918,                 loss: nan
Episode: 82541/101000 (81.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9346s / 14657.3776 s
agent0:                 episode reward: -0.0358,                 loss: 0.1912
agent1:                 episode reward: 0.0358,                 loss: nan
Episode: 82561/101000 (81.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6118s / 14662.9894 s
agent0:                 episode reward: 0.0221,                 loss: 0.1909
agent1:                 episode reward: -0.0221,                 loss: nan
Episode: 82581/101000 (81.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2466s / 14667.2360 s
agent0:                 episode reward: 0.3101,                 loss: 0.1899
agent1:                 episode reward: -0.3101,                 loss: nan
Episode: 82601/101000 (81.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7329s / 14671.9689 s
agent0:                 episode reward: -0.1823,                 loss: 0.1918
agent1:                 episode reward: 0.1823,                 loss: nan
Episode: 82621/101000 (81.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9600s / 14676.9288 s
agent0:                 episode reward: 0.2137,                 loss: 0.1916
agent1:                 episode reward: -0.2137,                 loss: nan
Episode: 82641/101000 (81.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4492s / 14682.3781 s
agent0:                 episode reward: 0.1612,                 loss: 0.1904
agent1:                 episode reward: -0.1612,                 loss: nan
Episode: 82661/101000 (81.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2894s / 14687.6675 s
agent0:                 episode reward: 0.2094,                 loss: 0.2046
agent1:                 episode reward: -0.2094,                 loss: 0.1472
Score delta: 1.6976323608798647, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/82231_0.
Episode: 82681/101000 (81.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8547s / 14692.5222 s
agent0:                 episode reward: -0.5414,                 loss: nan
agent1:                 episode reward: 0.5414,                 loss: 0.1457
Episode: 82701/101000 (81.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5504s / 14697.0726 s
agent0:                 episode reward: 0.0022,                 loss: nan
agent1:                 episode reward: -0.0022,                 loss: 0.1451
Episode: 82721/101000 (81.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8046s / 14702.8772 s
agent0:                 episode reward: -0.7496,                 loss: nan
agent1:                 episode reward: 0.7496,                 loss: 0.1439
Episode: 82741/101000 (81.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4292s / 14706.3065 s
agent0:                 episode reward: -0.4192,                 loss: nan
agent1:                 episode reward: 0.4192,                 loss: 0.1442
Episode: 82761/101000 (81.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5179s / 14710.8243 s
agent0:                 episode reward: -0.3034,                 loss: nan
agent1:                 episode reward: 0.3034,                 loss: 0.1452
Episode: 82781/101000 (81.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0413s / 14715.8656 s
agent0:                 episode reward: -0.1563,                 loss: nan
agent1:                 episode reward: 0.1563,                 loss: 0.1504
Episode: 82801/101000 (81.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4473s / 14722.3129 s
agent0:                 episode reward: -0.3974,                 loss: nan
agent1:                 episode reward: 0.3974,                 loss: 0.1545
Episode: 82821/101000 (82.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4288s / 14727.7416 s
agent0:                 episode reward: -0.2721,                 loss: nan
agent1:                 episode reward: 0.2721,                 loss: 0.1527
Episode: 82841/101000 (82.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5929s / 14732.3345 s
agent0:                 episode reward: 0.2049,                 loss: nan
agent1:                 episode reward: -0.2049,                 loss: 0.1531
Episode: 82861/101000 (82.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5636s / 14737.8982 s
agent0:                 episode reward: -0.7729,                 loss: nan
agent1:                 episode reward: 0.7729,                 loss: 0.1525
Episode: 82881/101000 (82.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6245s / 14742.5226 s
agent0:                 episode reward: 0.0580,                 loss: 0.2156
agent1:                 episode reward: -0.0580,                 loss: 0.1464
Score delta: 1.6958764579004633, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/82436_1.
Episode: 82901/101000 (82.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1550s / 14747.6776 s
agent0:                 episode reward: -0.1203,                 loss: 0.2158
agent1:                 episode reward: 0.1203,                 loss: nan
Episode: 82921/101000 (82.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9018s / 14752.5794 s
agent0:                 episode reward: -0.1775,                 loss: 0.2131
agent1:                 episode reward: 0.1775,                 loss: nan
Episode: 82941/101000 (82.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8577s / 14757.4371 s
agent0:                 episode reward: 0.3521,                 loss: 0.2138
agent1:                 episode reward: -0.3521,                 loss: nan
Episode: 82961/101000 (82.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3277s / 14762.7648 s
agent0:                 episode reward: -0.1192,                 loss: 0.2134
agent1:                 episode reward: 0.1192,                 loss: nan
Episode: 82981/101000 (82.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1619s / 14766.9267 s
agent0:                 episode reward: 0.0775,                 loss: 0.2150
agent1:                 episode reward: -0.0775,                 loss: nan
Episode: 83001/101000 (82.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7462s / 14771.6729 s
agent0:                 episode reward: -0.0386,                 loss: 0.2149
agent1:                 episode reward: 0.0386,                 loss: nan
Episode: 83021/101000 (82.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0698s / 14776.7427 s
agent0:                 episode reward: 0.0867,                 loss: 0.2131
agent1:                 episode reward: -0.0867,                 loss: nan
Episode: 83041/101000 (82.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9648s / 14781.7075 s
agent0:                 episode reward: 0.2703,                 loss: 0.2131
agent1:                 episode reward: -0.2703,                 loss: nan
Episode: 83061/101000 (82.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6740s / 14787.3815 s
agent0:                 episode reward: -0.1626,                 loss: 0.2127
agent1:                 episode reward: 0.1626,                 loss: nan
Episode: 83081/101000 (82.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6597s / 14793.0412 s
agent0:                 episode reward: 0.0146,                 loss: 0.2141
agent1:                 episode reward: -0.0146,                 loss: 0.1504
Score delta: 1.5604586318487417, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/82645_0.
Episode: 83101/101000 (82.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5856s / 14797.6268 s
agent0:                 episode reward: -0.4608,                 loss: nan
agent1:                 episode reward: 0.4608,                 loss: 0.1487
Episode: 83121/101000 (82.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1690s / 14801.7958 s
agent0:                 episode reward: -0.2638,                 loss: nan
agent1:                 episode reward: 0.2638,                 loss: 0.1497
Episode: 83141/101000 (82.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2650s / 14806.0608 s
agent0:                 episode reward: -0.3676,                 loss: nan
agent1:                 episode reward: 0.3676,                 loss: 0.1499
Episode: 83161/101000 (82.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3110s / 14810.3718 s
agent0:                 episode reward: 0.2114,                 loss: nan
agent1:                 episode reward: -0.2114,                 loss: 0.1487
Episode: 83181/101000 (82.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1371s / 14815.5089 s
agent0:                 episode reward: -0.6741,                 loss: nan
agent1:                 episode reward: 0.6741,                 loss: 0.1488
Episode: 83201/101000 (82.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8222s / 14821.3311 s
agent0:                 episode reward: -0.3082,                 loss: nan
agent1:                 episode reward: 0.3082,                 loss: 0.1505
Episode: 83221/101000 (82.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2665s / 14825.5976 s
agent0:                 episode reward: -0.2182,                 loss: nan
agent1:                 episode reward: 0.2182,                 loss: 0.1479
Episode: 83241/101000 (82.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9099s / 14830.5076 s
agent0:                 episode reward: 0.1507,                 loss: nan
agent1:                 episode reward: -0.1507,                 loss: 0.1491
Episode: 83261/101000 (82.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0230s / 14834.5306 s
agent0:                 episode reward: -0.2032,                 loss: nan
agent1:                 episode reward: 0.2032,                 loss: 0.1471
Episode: 83281/101000 (82.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9005s / 14839.4311 s
agent0:                 episode reward: -0.2535,                 loss: 0.2244
agent1:                 episode reward: 0.2535,                 loss: 0.1503
Score delta: 1.7654939543496013, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/82846_1.
Episode: 83301/101000 (82.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8861s / 14844.3171 s
agent0:                 episode reward: 0.2464,                 loss: 0.2239
agent1:                 episode reward: -0.2464,                 loss: nan
Episode: 83321/101000 (82.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2611s / 14849.5783 s
agent0:                 episode reward: 0.0957,                 loss: 0.2190
agent1:                 episode reward: -0.0957,                 loss: nan
Episode: 83341/101000 (82.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1840s / 14854.7622 s
agent0:                 episode reward: -0.0185,                 loss: 0.2190
agent1:                 episode reward: 0.0185,                 loss: nan
Episode: 83361/101000 (82.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3287s / 14859.0910 s
agent0:                 episode reward: 0.3111,                 loss: 0.2194
agent1:                 episode reward: -0.3111,                 loss: nan
Episode: 83381/101000 (82.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1467s / 14864.2376 s
agent0:                 episode reward: -0.0068,                 loss: 0.2195
agent1:                 episode reward: 0.0068,                 loss: nan
Episode: 83401/101000 (82.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7421s / 14868.9797 s
agent0:                 episode reward: 0.1284,                 loss: 0.2155
agent1:                 episode reward: -0.1284,                 loss: nan
Episode: 83421/101000 (82.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0159s / 14874.9956 s
agent0:                 episode reward: -0.0663,                 loss: 0.2145
agent1:                 episode reward: 0.0663,                 loss: nan
Episode: 83441/101000 (82.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0560s / 14881.0516 s
agent0:                 episode reward: -0.1132,                 loss: 0.2124
agent1:                 episode reward: 0.1132,                 loss: nan
Episode: 83461/101000 (82.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3985s / 14885.4500 s
agent0:                 episode reward: 0.3088,                 loss: 0.2124
agent1:                 episode reward: -0.3088,                 loss: nan
Episode: 83481/101000 (82.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2006s / 14890.6506 s
agent0:                 episode reward: 0.1826,                 loss: 0.2129
agent1:                 episode reward: -0.1826,                 loss: 0.1563
Score delta: 1.546108669489748, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/83052_0.
Episode: 83501/101000 (82.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2384s / 14895.8890 s
agent0:                 episode reward: -0.7707,                 loss: nan
agent1:                 episode reward: 0.7707,                 loss: 0.1539
Episode: 83521/101000 (82.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4146s / 14900.3036 s
agent0:                 episode reward: -0.1279,                 loss: nan
agent1:                 episode reward: 0.1279,                 loss: 0.1539
Episode: 83541/101000 (82.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5296s / 14905.8333 s
agent0:                 episode reward: -0.5456,                 loss: nan
agent1:                 episode reward: 0.5456,                 loss: 0.1503
Episode: 83561/101000 (82.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2915s / 14910.1247 s
agent0:                 episode reward: 0.1678,                 loss: nan
agent1:                 episode reward: -0.1678,                 loss: 0.1503
Episode: 83581/101000 (82.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9359s / 14916.0607 s
agent0:                 episode reward: -0.2677,                 loss: nan
agent1:                 episode reward: 0.2677,                 loss: 0.1501
Episode: 83601/101000 (82.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9123s / 14919.9729 s
agent0:                 episode reward: -0.2264,                 loss: nan
agent1:                 episode reward: 0.2264,                 loss: 0.1484
Episode: 83621/101000 (82.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3949s / 14924.3679 s
agent0:                 episode reward: -0.4980,                 loss: nan
agent1:                 episode reward: 0.4980,                 loss: 0.1496
Episode: 83641/101000 (82.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7924s / 14929.1603 s
agent0:                 episode reward: 0.1925,                 loss: nan
agent1:                 episode reward: -0.1925,                 loss: 0.1497
Episode: 83661/101000 (82.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7042s / 14933.8645 s
agent0:                 episode reward: -0.1268,                 loss: nan
agent1:                 episode reward: 0.1268,                 loss: 0.1477
Episode: 83681/101000 (82.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5401s / 14939.4046 s
agent0:                 episode reward: -0.3589,                 loss: 0.2319
agent1:                 episode reward: 0.3589,                 loss: 0.1489
Score delta: 1.5341457002748264, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/83246_1.
Episode: 83701/101000 (82.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9934s / 14944.3980 s
agent0:                 episode reward: 0.2020,                 loss: 0.2207
agent1:                 episode reward: -0.2020,                 loss: nan
Episode: 83721/101000 (82.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5784s / 14948.9763 s
agent0:                 episode reward: 0.0350,                 loss: 0.2220
agent1:                 episode reward: -0.0350,                 loss: nan
Episode: 83741/101000 (82.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9893s / 14952.9656 s
agent0:                 episode reward: 0.0822,                 loss: 0.2194
agent1:                 episode reward: -0.0822,                 loss: nan
Episode: 83761/101000 (82.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0003s / 14958.9659 s
agent0:                 episode reward: -0.1293,                 loss: 0.2184
agent1:                 episode reward: 0.1293,                 loss: nan
Episode: 83781/101000 (82.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0728s / 14964.0387 s
agent0:                 episode reward: 0.1786,                 loss: 0.2197
agent1:                 episode reward: -0.1786,                 loss: nan
Episode: 83801/101000 (82.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7733s / 14968.8120 s
agent0:                 episode reward: 0.0553,                 loss: 0.2196
agent1:                 episode reward: -0.0553,                 loss: nan
Episode: 83821/101000 (82.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5945s / 14973.4065 s
agent0:                 episode reward: 0.3325,                 loss: 0.2187
agent1:                 episode reward: -0.3325,                 loss: nan
Episode: 83841/101000 (83.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9241s / 14978.3306 s
agent0:                 episode reward: -0.1315,                 loss: 0.2170
agent1:                 episode reward: 0.1315,                 loss: nan
Episode: 83861/101000 (83.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8245s / 14983.1550 s
agent0:                 episode reward: -0.1091,                 loss: 0.2164
agent1:                 episode reward: 0.1091,                 loss: nan
Episode: 83881/101000 (83.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5221s / 14987.6771 s
agent0:                 episode reward: -0.1576,                 loss: 0.2192
agent1:                 episode reward: 0.1576,                 loss: nan
Episode: 83901/101000 (83.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8874s / 14993.5645 s
agent0:                 episode reward: -0.0004,                 loss: 0.2184
agent1:                 episode reward: 0.0004,                 loss: nan
Episode: 83921/101000 (83.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5568s / 14998.1213 s
agent0:                 episode reward: 0.0702,                 loss: 0.2202
agent1:                 episode reward: -0.0702,                 loss: nan
Episode: 83941/101000 (83.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1862s / 15003.3076 s
agent0:                 episode reward: -0.0801,                 loss: 0.2221
agent1:                 episode reward: 0.0801,                 loss: nan
Episode: 83961/101000 (83.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8455s / 15008.1530 s
agent0:                 episode reward: 0.0056,                 loss: 0.2199
agent1:                 episode reward: -0.0056,                 loss: nan
Episode: 83981/101000 (83.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9777s / 15013.1308 s
agent0:                 episode reward: 0.1854,                 loss: 0.2229
agent1:                 episode reward: -0.1854,                 loss: nan
Episode: 84001/101000 (83.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2456s / 15018.3763 s
agent0:                 episode reward: 0.2166,                 loss: 0.2244
agent1:                 episode reward: -0.2166,                 loss: 0.1781
Score delta: 1.5745472853049018, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/83562_0.
Episode: 84021/101000 (83.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9671s / 15023.3434 s
agent0:                 episode reward: -0.1200,                 loss: nan
agent1:                 episode reward: 0.1200,                 loss: 0.1759
Episode: 84041/101000 (83.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3761s / 15027.7196 s
agent0:                 episode reward: -0.5471,                 loss: nan
agent1:                 episode reward: 0.5471,                 loss: 0.1756
Episode: 84061/101000 (83.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2090s / 15031.9286 s
agent0:                 episode reward: -0.3012,                 loss: nan
agent1:                 episode reward: 0.3012,                 loss: 0.1764
Episode: 84081/101000 (83.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6095s / 15036.5381 s
agent0:                 episode reward: 0.0241,                 loss: nan
agent1:                 episode reward: -0.0241,                 loss: 0.1736
Episode: 84101/101000 (83.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9138s / 15042.4519 s
agent0:                 episode reward: -0.2338,                 loss: nan
agent1:                 episode reward: 0.2338,                 loss: 0.1740
Episode: 84121/101000 (83.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3996s / 15046.8515 s
agent0:                 episode reward: -0.2260,                 loss: nan
agent1:                 episode reward: 0.2260,                 loss: 0.1731
Episode: 84141/101000 (83.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6351s / 15051.4866 s
agent0:                 episode reward: -0.0315,                 loss: nan
agent1:                 episode reward: 0.0315,                 loss: 0.1740
Episode: 84161/101000 (83.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7550s / 15057.2417 s
agent0:                 episode reward: 0.0645,                 loss: nan
agent1:                 episode reward: -0.0645,                 loss: 0.1733
Episode: 84181/101000 (83.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5656s / 15062.8073 s
agent0:                 episode reward: -0.2756,                 loss: nan
agent1:                 episode reward: 0.2756,                 loss: 0.1592
Episode: 84201/101000 (83.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8945s / 15067.7018 s
agent0:                 episode reward: -0.4799,                 loss: 0.2196
agent1:                 episode reward: 0.4799,                 loss: 0.1489
Score delta: 1.7718031608921447, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/83763_1.
Episode: 84221/101000 (83.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2203s / 15071.9221 s
agent0:                 episode reward: -0.0742,                 loss: 0.2150
agent1:                 episode reward: 0.0742,                 loss: nan
Episode: 84241/101000 (83.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4464s / 15076.3686 s
agent0:                 episode reward: -0.2812,                 loss: 0.2135
agent1:                 episode reward: 0.2812,                 loss: nan
Episode: 84261/101000 (83.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9328s / 15081.3014 s
agent0:                 episode reward: 0.0469,                 loss: 0.2128
agent1:                 episode reward: -0.0469,                 loss: nan
Episode: 84281/101000 (83.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2457s / 15085.5471 s
agent0:                 episode reward: 0.1608,                 loss: 0.2140
agent1:                 episode reward: -0.1608,                 loss: nan
Episode: 84301/101000 (83.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9964s / 15089.5435 s
agent0:                 episode reward: 0.1164,                 loss: 0.2118
agent1:                 episode reward: -0.1164,                 loss: nan
Episode: 84321/101000 (83.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4260s / 15093.9696 s
agent0:                 episode reward: 0.0841,                 loss: 0.2108
agent1:                 episode reward: -0.0841,                 loss: nan
Episode: 84341/101000 (83.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0857s / 15099.0552 s
agent0:                 episode reward: -0.1309,                 loss: 0.2117
agent1:                 episode reward: 0.1309,                 loss: nan
Episode: 84361/101000 (83.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9522s / 15105.0074 s
agent0:                 episode reward: 0.2819,                 loss: 0.2130
agent1:                 episode reward: -0.2819,                 loss: 0.1638
Score delta: 1.6693987861484598, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/83927_0.
Episode: 84381/101000 (83.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0901s / 15109.0975 s
agent0:                 episode reward: 0.3211,                 loss: nan
agent1:                 episode reward: -0.3211,                 loss: 0.1598
Episode: 84401/101000 (83.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7701s / 15113.8676 s
agent0:                 episode reward: -0.4738,                 loss: nan
agent1:                 episode reward: 0.4738,                 loss: 0.1588
Episode: 84421/101000 (83.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6714s / 15117.5389 s
agent0:                 episode reward: -0.4326,                 loss: nan
agent1:                 episode reward: 0.4326,                 loss: 0.1595
Episode: 84441/101000 (83.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1050s / 15121.6440 s
agent0:                 episode reward: 0.0294,                 loss: nan
agent1:                 episode reward: -0.0294,                 loss: 0.1567
Episode: 84461/101000 (83.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2376s / 15126.8815 s
agent0:                 episode reward: -0.3932,                 loss: nan
agent1:                 episode reward: 0.3932,                 loss: 0.1575
Episode: 84481/101000 (83.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5114s / 15132.3929 s
agent0:                 episode reward: -0.3308,                 loss: nan
agent1:                 episode reward: 0.3308,                 loss: 0.1592
Episode: 84501/101000 (83.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 3.5328s / 15135.9257 s
agent0:                 episode reward: -0.5335,                 loss: nan
agent1:                 episode reward: 0.5335,                 loss: 0.1576
Episode: 84521/101000 (83.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4634s / 15141.3891 s
agent0:                 episode reward: -0.3566,                 loss: nan
agent1:                 episode reward: 0.3566,                 loss: 0.1580
Episode: 84541/101000 (83.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1178s / 15145.5070 s
agent0:                 episode reward: -0.1924,                 loss: 0.2273
agent1:                 episode reward: 0.1924,                 loss: 0.1554
Score delta: 1.6905736747513072, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/84097_1.
Episode: 84561/101000 (83.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4443s / 15149.9512 s
agent0:                 episode reward: 0.0333,                 loss: 0.2218
agent1:                 episode reward: -0.0333,                 loss: nan
Episode: 84581/101000 (83.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1829s / 15155.1342 s
agent0:                 episode reward: -0.2161,                 loss: 0.2237
agent1:                 episode reward: 0.2161,                 loss: nan
Episode: 84601/101000 (83.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8895s / 15160.0237 s
agent0:                 episode reward: 0.1939,                 loss: 0.2234
agent1:                 episode reward: -0.1939,                 loss: nan
Episode: 84621/101000 (83.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6374s / 15164.6611 s
agent0:                 episode reward: 0.0344,                 loss: 0.2217
agent1:                 episode reward: -0.0344,                 loss: nan
Episode: 84641/101000 (83.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6172s / 15170.2783 s
agent0:                 episode reward: 0.0859,                 loss: 0.2208
agent1:                 episode reward: -0.0859,                 loss: nan
Episode: 84661/101000 (83.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9356s / 15174.2139 s
agent0:                 episode reward: -0.1934,                 loss: 0.2230
agent1:                 episode reward: 0.1934,                 loss: nan
Episode: 84681/101000 (83.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4477s / 15179.6616 s
agent0:                 episode reward: -0.0309,                 loss: 0.2206
agent1:                 episode reward: 0.0309,                 loss: nan
Episode: 84701/101000 (83.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3893s / 15186.0509 s
agent0:                 episode reward: 0.0009,                 loss: 0.2199
agent1:                 episode reward: -0.0009,                 loss: nan
Episode: 84721/101000 (83.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6205s / 15189.6714 s
agent0:                 episode reward: 0.1150,                 loss: 0.2210
agent1:                 episode reward: -0.1150,                 loss: nan
Episode: 84741/101000 (83.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0339s / 15194.7054 s
agent0:                 episode reward: 0.1568,                 loss: 0.2191
agent1:                 episode reward: -0.1568,                 loss: nan
Episode: 84761/101000 (83.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7751s / 15199.4804 s
agent0:                 episode reward: -0.3264,                 loss: 0.2196
agent1:                 episode reward: 0.3264,                 loss: nan
Episode: 84781/101000 (83.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1329s / 15204.6133 s
agent0:                 episode reward: 0.1819,                 loss: 0.2205
agent1:                 episode reward: -0.1819,                 loss: 0.1578
Score delta: 1.5201902786923391, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/84341_0.
Episode: 84801/101000 (83.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5259s / 15210.1393 s
agent0:                 episode reward: 0.0237,                 loss: nan
agent1:                 episode reward: -0.0237,                 loss: 0.1557
Episode: 84821/101000 (83.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9049s / 15215.0442 s
agent0:                 episode reward: -0.3058,                 loss: nan
agent1:                 episode reward: 0.3058,                 loss: 0.1539
Episode: 84841/101000 (84.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8380s / 15220.8821 s
agent0:                 episode reward: -0.3591,                 loss: nan
agent1:                 episode reward: 0.3591,                 loss: 0.1537
Episode: 84861/101000 (84.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5723s / 15225.4544 s
agent0:                 episode reward: -0.4892,                 loss: nan
agent1:                 episode reward: 0.4892,                 loss: 0.1536
Episode: 84881/101000 (84.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3926s / 15230.8471 s
agent0:                 episode reward: 0.0634,                 loss: nan
agent1:                 episode reward: -0.0634,                 loss: 0.1518
Episode: 84901/101000 (84.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7152s / 15235.5623 s
agent0:                 episode reward: -0.3239,                 loss: nan
agent1:                 episode reward: 0.3239,                 loss: 0.1533
Episode: 84921/101000 (84.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3980s / 15239.9603 s
agent0:                 episode reward: -0.3405,                 loss: nan
agent1:                 episode reward: 0.3405,                 loss: 0.1556
Episode: 84941/101000 (84.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5829s / 15245.5433 s
agent0:                 episode reward: 0.1318,                 loss: nan
agent1:                 episode reward: -0.1318,                 loss: 0.1562
Episode: 84961/101000 (84.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7469s / 15249.2901 s
agent0:                 episode reward: -0.4303,                 loss: nan
agent1:                 episode reward: 0.4303,                 loss: 0.1550
Episode: 84981/101000 (84.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3080s / 15253.5981 s
agent0:                 episode reward: -0.5431,                 loss: 0.2286
agent1:                 episode reward: 0.5431,                 loss: 0.1534
Score delta: 1.5459570661474153, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/84542_1.
Episode: 85001/101000 (84.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3429s / 15257.9410 s
agent0:                 episode reward: 0.1795,                 loss: 0.2127
agent1:                 episode reward: -0.1795,                 loss: nan
Episode: 85021/101000 (84.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3021s / 15263.2431 s
agent0:                 episode reward: -0.1272,                 loss: 0.2128
agent1:                 episode reward: 0.1272,                 loss: nan
Episode: 85041/101000 (84.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3519s / 15268.5950 s
agent0:                 episode reward: -0.2716,                 loss: 0.2113
agent1:                 episode reward: 0.2716,                 loss: nan
Episode: 85061/101000 (84.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8964s / 15273.4915 s
agent0:                 episode reward: 0.3640,                 loss: 0.2115
agent1:                 episode reward: -0.3640,                 loss: nan
Episode: 85081/101000 (84.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7497s / 15278.2412 s
agent0:                 episode reward: -0.0651,                 loss: 0.2112
agent1:                 episode reward: 0.0651,                 loss: nan
Episode: 85101/101000 (84.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3448s / 15283.5859 s
agent0:                 episode reward: -0.1620,                 loss: 0.2110
agent1:                 episode reward: 0.1620,                 loss: nan
Episode: 85121/101000 (84.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7897s / 15288.3756 s
agent0:                 episode reward: -0.1449,                 loss: 0.2107
agent1:                 episode reward: 0.1449,                 loss: nan
Episode: 85141/101000 (84.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7053s / 15293.0809 s
agent0:                 episode reward: -0.0738,                 loss: 0.2125
agent1:                 episode reward: 0.0738,                 loss: nan
Episode: 85161/101000 (84.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7068s / 15297.7878 s
agent0:                 episode reward: -0.4442,                 loss: 0.2144
agent1:                 episode reward: 0.4442,                 loss: 0.1536
Score delta: 1.5553091125990017, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/84716_0.
Episode: 85181/101000 (84.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6242s / 15302.4119 s
agent0:                 episode reward: -0.3182,                 loss: nan
agent1:                 episode reward: 0.3182,                 loss: 0.1533
Episode: 85201/101000 (84.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7033s / 15307.1153 s
agent0:                 episode reward: -0.0311,                 loss: nan
agent1:                 episode reward: 0.0311,                 loss: 0.1520
Episode: 85221/101000 (84.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6167s / 15311.7320 s
agent0:                 episode reward: -0.1287,                 loss: nan
agent1:                 episode reward: 0.1287,                 loss: 0.1534
Episode: 85241/101000 (84.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6293s / 15318.3613 s
agent0:                 episode reward: -0.3336,                 loss: nan
agent1:                 episode reward: 0.3336,                 loss: 0.1529
Episode: 85261/101000 (84.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0530s / 15324.4143 s
agent0:                 episode reward: -0.1618,                 loss: nan
agent1:                 episode reward: 0.1618,                 loss: 0.1533
Episode: 85281/101000 (84.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9475s / 15329.3618 s
agent0:                 episode reward: -0.3024,                 loss: nan
agent1:                 episode reward: 0.3024,                 loss: 0.1512
Episode: 85301/101000 (84.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6912s / 15334.0530 s
agent0:                 episode reward: -0.3270,                 loss: nan
agent1:                 episode reward: 0.3270,                 loss: 0.1521
Episode: 85321/101000 (84.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2163s / 15338.2693 s
agent0:                 episode reward: -0.1695,                 loss: nan
agent1:                 episode reward: 0.1695,                 loss: 0.1519
Episode: 85341/101000 (84.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9202s / 15343.1895 s
agent0:                 episode reward: -0.3545,                 loss: 0.2037
agent1:                 episode reward: 0.3545,                 loss: 0.1532
Score delta: 1.6577348855881735, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/84911_1.
Episode: 85361/101000 (84.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6813s / 15347.8708 s
agent0:                 episode reward: 0.3831,                 loss: 0.2030
agent1:                 episode reward: -0.3831,                 loss: nan
Episode: 85381/101000 (84.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6082s / 15352.4790 s
agent0:                 episode reward: -0.1476,                 loss: 0.2033
agent1:                 episode reward: 0.1476,                 loss: nan
Episode: 85401/101000 (84.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4229s / 15357.9019 s
agent0:                 episode reward: 0.2648,                 loss: 0.2038
agent1:                 episode reward: -0.2648,                 loss: nan
Episode: 85421/101000 (84.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1414s / 15363.0433 s
agent0:                 episode reward: -0.1989,                 loss: 0.2046
agent1:                 episode reward: 0.1989,                 loss: nan
Episode: 85441/101000 (84.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6641s / 15367.7074 s
agent0:                 episode reward: 0.3072,                 loss: 0.2027
agent1:                 episode reward: -0.3072,                 loss: nan
Episode: 85461/101000 (84.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6548s / 15373.3622 s
agent0:                 episode reward: -0.1223,                 loss: 0.2010
agent1:                 episode reward: 0.1223,                 loss: nan
Episode: 85481/101000 (84.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1253s / 15378.4875 s
agent0:                 episode reward: -0.3306,                 loss: 0.2017
agent1:                 episode reward: 0.3306,                 loss: nan
Episode: 85501/101000 (84.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3926s / 15382.8801 s
agent0:                 episode reward: 0.2100,                 loss: 0.2033
agent1:                 episode reward: -0.2100,                 loss: nan
Episode: 85521/101000 (84.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7408s / 15387.6209 s
agent0:                 episode reward: 0.0415,                 loss: 0.2024
agent1:                 episode reward: -0.0415,                 loss: nan
Episode: 85541/101000 (84.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7628s / 15392.3837 s
agent0:                 episode reward: 0.1115,                 loss: 0.2026
agent1:                 episode reward: -0.1115,                 loss: nan
Episode: 85561/101000 (84.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7197s / 15397.1034 s
agent0:                 episode reward: 0.3168,                 loss: 0.2027
agent1:                 episode reward: -0.3168,                 loss: nan
Episode: 85581/101000 (84.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9289s / 15401.0323 s
agent0:                 episode reward: 0.3022,                 loss: 0.2028
agent1:                 episode reward: -0.3022,                 loss: nan
Episode: 85601/101000 (84.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0541s / 15406.0864 s
agent0:                 episode reward: 0.3425,                 loss: 0.2018
agent1:                 episode reward: -0.3425,                 loss: nan
Episode: 85621/101000 (84.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0522s / 15411.1385 s
agent0:                 episode reward: -0.0101,                 loss: 0.2042
agent1:                 episode reward: 0.0101,                 loss: nan
Episode: 85641/101000 (84.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6598s / 15415.7984 s
agent0:                 episode reward: 0.1212,                 loss: 0.2015
agent1:                 episode reward: -0.1212,                 loss: nan
Episode: 85661/101000 (84.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5931s / 15421.3914 s
agent0:                 episode reward: -0.1643,                 loss: 0.2014
agent1:                 episode reward: 0.1643,                 loss: nan
Episode: 85681/101000 (84.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1814s / 15426.5729 s
agent0:                 episode reward: -0.5314,                 loss: 0.2014
agent1:                 episode reward: 0.5314,                 loss: nan
Episode: 85701/101000 (84.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6448s / 15431.2177 s
agent0:                 episode reward: 0.0197,                 loss: 0.2152
agent1:                 episode reward: -0.0197,                 loss: nan
Episode: 85721/101000 (84.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6388s / 15435.8565 s
agent0:                 episode reward: -0.0988,                 loss: 0.2183
agent1:                 episode reward: 0.0988,                 loss: nan
Episode: 85741/101000 (84.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6488s / 15440.5053 s
agent0:                 episode reward: -0.2545,                 loss: 0.2167
agent1:                 episode reward: 0.2545,                 loss: nan
Episode: 85761/101000 (84.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3452s / 15445.8505 s
agent0:                 episode reward: 0.2641,                 loss: 0.2153
agent1:                 episode reward: -0.2641,                 loss: nan
Episode: 85781/101000 (84.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4355s / 15450.2860 s
agent0:                 episode reward: 0.1486,                 loss: 0.2142
agent1:                 episode reward: -0.1486,                 loss: nan
Episode: 85801/101000 (84.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4528s / 15455.7388 s
agent0:                 episode reward: 0.1223,                 loss: 0.2161
agent1:                 episode reward: -0.1223,                 loss: nan
Episode: 85821/101000 (84.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5893s / 15460.3281 s
agent0:                 episode reward: 0.1797,                 loss: 0.2172
agent1:                 episode reward: -0.1797,                 loss: nan
Episode: 85841/101000 (84.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8723s / 15465.2004 s
agent0:                 episode reward: 0.3389,                 loss: 0.2180
agent1:                 episode reward: -0.3389,                 loss: nan
Episode: 85861/101000 (85.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1853s / 15470.3857 s
agent0:                 episode reward: 0.0119,                 loss: 0.2156
agent1:                 episode reward: -0.0119,                 loss: nan
Episode: 85881/101000 (85.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4268s / 15475.8125 s
agent0:                 episode reward: 0.3119,                 loss: 0.2161
agent1:                 episode reward: -0.3119,                 loss: nan
Episode: 85901/101000 (85.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3695s / 15480.1820 s
agent0:                 episode reward: 0.1391,                 loss: 0.2166
agent1:                 episode reward: -0.1391,                 loss: nan
Episode: 85921/101000 (85.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4686s / 15484.6507 s
agent0:                 episode reward: 0.3966,                 loss: 0.2160
agent1:                 episode reward: -0.3966,                 loss: nan
Episode: 85941/101000 (85.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7792s / 15489.4299 s
agent0:                 episode reward: -0.1094,                 loss: 0.2123
agent1:                 episode reward: 0.1094,                 loss: 0.1546
Score delta: 1.6112663600276225, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/85496_0.
Episode: 85961/101000 (85.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7959s / 15494.2258 s
agent0:                 episode reward: -0.1393,                 loss: nan
agent1:                 episode reward: 0.1393,                 loss: 0.1537
Episode: 85981/101000 (85.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5771s / 15499.8029 s
agent0:                 episode reward: -0.1554,                 loss: nan
agent1:                 episode reward: 0.1554,                 loss: 0.1549
Episode: 86001/101000 (85.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4817s / 15504.2846 s
agent0:                 episode reward: -0.2188,                 loss: nan
agent1:                 episode reward: 0.2188,                 loss: 0.1523
Episode: 86021/101000 (85.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7762s / 15510.0608 s
agent0:                 episode reward: -0.3253,                 loss: nan
agent1:                 episode reward: 0.3253,                 loss: 0.1558
Episode: 86041/101000 (85.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4469s / 15514.5077 s
agent0:                 episode reward: -0.6976,                 loss: nan
agent1:                 episode reward: 0.6976,                 loss: 0.1549
Episode: 86061/101000 (85.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6597s / 15519.1674 s
agent0:                 episode reward: -0.4548,                 loss: nan
agent1:                 episode reward: 0.4548,                 loss: 0.1536
Episode: 86081/101000 (85.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2376s / 15524.4050 s
agent0:                 episode reward: 0.1656,                 loss: nan
agent1:                 episode reward: -0.1656,                 loss: 0.1545
Episode: 86101/101000 (85.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1196s / 15529.5246 s
agent0:                 episode reward: 0.3044,                 loss: nan
agent1:                 episode reward: -0.3044,                 loss: 0.1540
Episode: 86121/101000 (85.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3510s / 15533.8756 s
agent0:                 episode reward: -0.2510,                 loss: nan
agent1:                 episode reward: 0.2510,                 loss: 0.1535
Episode: 86141/101000 (85.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9021s / 15538.7778 s
agent0:                 episode reward: -0.2858,                 loss: nan
agent1:                 episode reward: 0.2858,                 loss: 0.1529
Episode: 86161/101000 (85.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6819s / 15544.4596 s
agent0:                 episode reward: -0.0800,                 loss: 0.1997
agent1:                 episode reward: 0.0800,                 loss: 0.1555
Score delta: 1.6381863242440649, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/85720_1.
Episode: 86181/101000 (85.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0798s / 15549.5394 s
agent0:                 episode reward: -0.2244,                 loss: 0.1994
agent1:                 episode reward: 0.2244,                 loss: nan
Episode: 86201/101000 (85.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6443s / 15554.1837 s
agent0:                 episode reward: 0.2805,                 loss: 0.1993
agent1:                 episode reward: -0.2805,                 loss: nan
Episode: 86221/101000 (85.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0398s / 15560.2235 s
agent0:                 episode reward: 0.2998,                 loss: 0.1996
agent1:                 episode reward: -0.2998,                 loss: nan
Episode: 86241/101000 (85.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5622s / 15564.7857 s
agent0:                 episode reward: -0.0910,                 loss: 0.2013
agent1:                 episode reward: 0.0910,                 loss: nan
Episode: 86261/101000 (85.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0359s / 15569.8216 s
agent0:                 episode reward: -0.0996,                 loss: 0.2060
agent1:                 episode reward: 0.0996,                 loss: nan
Episode: 86281/101000 (85.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4090s / 15575.2306 s
agent0:                 episode reward: -0.1512,                 loss: 0.2058
agent1:                 episode reward: 0.1512,                 loss: nan
Episode: 86301/101000 (85.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2476s / 15580.4782 s
agent0:                 episode reward: 0.3678,                 loss: 0.2069
agent1:                 episode reward: -0.3678,                 loss: nan
Episode: 86321/101000 (85.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9891s / 15585.4673 s
agent0:                 episode reward: 0.2427,                 loss: 0.2055
agent1:                 episode reward: -0.2427,                 loss: nan
Episode: 86341/101000 (85.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7691s / 15591.2365 s
agent0:                 episode reward: 0.0882,                 loss: 0.2053
agent1:                 episode reward: -0.0882,                 loss: nan
Episode: 86361/101000 (85.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2542s / 15596.4906 s
agent0:                 episode reward: -0.1592,                 loss: 0.2054
agent1:                 episode reward: 0.1592,                 loss: nan
Episode: 86381/101000 (85.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 3.2131s / 15599.7037 s
agent0:                 episode reward: 0.1981,                 loss: 0.2058
agent1:                 episode reward: -0.1981,                 loss: 0.1517
Score delta: 1.6514820182447831, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/85949_0.
Episode: 86401/101000 (85.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7118s / 15604.4156 s
agent0:                 episode reward: -0.0440,                 loss: nan
agent1:                 episode reward: 0.0440,                 loss: 0.1506
Episode: 86421/101000 (85.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9727s / 15610.3883 s
agent0:                 episode reward: 0.0563,                 loss: nan
agent1:                 episode reward: -0.0563,                 loss: 0.1505
Episode: 86441/101000 (85.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6887s / 15615.0769 s
agent0:                 episode reward: -0.2377,                 loss: nan
agent1:                 episode reward: 0.2377,                 loss: 0.1508
Episode: 86461/101000 (85.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9348s / 15620.0117 s
agent0:                 episode reward: 0.4228,                 loss: nan
agent1:                 episode reward: -0.4228,                 loss: 0.1499
Episode: 86481/101000 (85.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4111s / 15624.4228 s
agent0:                 episode reward: -0.2803,                 loss: nan
agent1:                 episode reward: 0.2803,                 loss: 0.1493
Episode: 86501/101000 (85.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2423s / 15629.6652 s
agent0:                 episode reward: -0.4365,                 loss: nan
agent1:                 episode reward: 0.4365,                 loss: 0.1493
Episode: 86521/101000 (85.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9772s / 15634.6424 s
agent0:                 episode reward: -0.3270,                 loss: nan
agent1:                 episode reward: 0.3270,                 loss: 0.1479
Episode: 86541/101000 (85.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9605s / 15639.6028 s
agent0:                 episode reward: 0.1814,                 loss: nan
agent1:                 episode reward: -0.1814,                 loss: 0.1486
Episode: 86561/101000 (85.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4056s / 15644.0085 s
agent0:                 episode reward: -0.4072,                 loss: 0.2003
agent1:                 episode reward: 0.4072,                 loss: 0.1487
Score delta: 1.5246446498928587, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/86125_1.
Episode: 86581/101000 (85.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2227s / 15648.2312 s
agent0:                 episode reward: 0.0015,                 loss: 0.1984
agent1:                 episode reward: -0.0015,                 loss: nan
Episode: 86601/101000 (85.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6084s / 15652.8396 s
agent0:                 episode reward: 0.5180,                 loss: 0.2001
agent1:                 episode reward: -0.5180,                 loss: nan
Episode: 86621/101000 (85.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3338s / 15658.1734 s
agent0:                 episode reward: 0.1184,                 loss: 0.1994
agent1:                 episode reward: -0.1184,                 loss: nan
Episode: 86641/101000 (85.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8998s / 15663.0732 s
agent0:                 episode reward: -0.0532,                 loss: 0.1975
agent1:                 episode reward: 0.0532,                 loss: nan
Episode: 86661/101000 (85.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9721s / 15668.0453 s
agent0:                 episode reward: -0.1329,                 loss: 0.1993
agent1:                 episode reward: 0.1329,                 loss: nan
Episode: 86681/101000 (85.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3255s / 15673.3708 s
agent0:                 episode reward: -0.0146,                 loss: 0.1969
agent1:                 episode reward: 0.0146,                 loss: nan
Episode: 86701/101000 (85.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0073s / 15678.3781 s
agent0:                 episode reward: 0.0925,                 loss: 0.1983
agent1:                 episode reward: -0.0925,                 loss: nan
Episode: 86721/101000 (85.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0980s / 15682.4761 s
agent0:                 episode reward: 0.0727,                 loss: 0.1987
agent1:                 episode reward: -0.0727,                 loss: nan
Episode: 86741/101000 (85.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2491s / 15687.7252 s
agent0:                 episode reward: 0.0846,                 loss: 0.1996
agent1:                 episode reward: -0.0846,                 loss: nan
Episode: 86761/101000 (85.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0090s / 15692.7341 s
agent0:                 episode reward: -0.2494,                 loss: 0.2029
agent1:                 episode reward: 0.2494,                 loss: nan
Episode: 86781/101000 (85.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8872s / 15697.6213 s
agent0:                 episode reward: 0.4615,                 loss: 0.2032
agent1:                 episode reward: -0.4615,                 loss: 0.1553
Score delta: 1.5877305396121628, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/86351_0.
Episode: 86801/101000 (85.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3837s / 15702.0050 s
agent0:                 episode reward: -0.1487,                 loss: nan
agent1:                 episode reward: 0.1487,                 loss: 0.1541
Episode: 86821/101000 (85.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8967s / 15706.9017 s
agent0:                 episode reward: -0.0172,                 loss: nan
agent1:                 episode reward: 0.0172,                 loss: 0.1476
Episode: 86841/101000 (85.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3176s / 15711.2193 s
agent0:                 episode reward: -0.2177,                 loss: nan
agent1:                 episode reward: 0.2177,                 loss: 0.1482
Episode: 86861/101000 (86.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1196s / 15716.3390 s
agent0:                 episode reward: -0.3172,                 loss: nan
agent1:                 episode reward: 0.3172,                 loss: 0.1466
Episode: 86881/101000 (86.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5346s / 15720.8735 s
agent0:                 episode reward: -0.3347,                 loss: nan
agent1:                 episode reward: 0.3347,                 loss: 0.1471
Episode: 86901/101000 (86.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0479s / 15725.9214 s
agent0:                 episode reward: -0.1140,                 loss: nan
agent1:                 episode reward: 0.1140,                 loss: 0.1468
Episode: 86921/101000 (86.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8944s / 15730.8158 s
agent0:                 episode reward: -0.3630,                 loss: nan
agent1:                 episode reward: 0.3630,                 loss: 0.1469
Episode: 86941/101000 (86.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7761s / 15734.5919 s
agent0:                 episode reward: -0.2969,                 loss: nan
agent1:                 episode reward: 0.2969,                 loss: 0.1476
Episode: 86961/101000 (86.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1649s / 15739.7568 s
agent0:                 episode reward: -0.2155,                 loss: nan
agent1:                 episode reward: 0.2155,                 loss: 0.1468
Episode: 86981/101000 (86.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1939s / 15743.9507 s
agent0:                 episode reward: 0.0638,                 loss: nan
agent1:                 episode reward: -0.0638,                 loss: 0.1463
Episode: 87001/101000 (86.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8178s / 15748.7685 s
agent0:                 episode reward: -0.0944,                 loss: nan
agent1:                 episode reward: 0.0944,                 loss: 0.1471
Episode: 87021/101000 (86.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2715s / 15753.0399 s
agent0:                 episode reward: -0.0001,                 loss: nan
agent1:                 episode reward: 0.0001,                 loss: 0.1455
Episode: 87041/101000 (86.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0434s / 15758.0833 s
agent0:                 episode reward: 0.0453,                 loss: nan
agent1:                 episode reward: -0.0453,                 loss: 0.1468
Episode: 87061/101000 (86.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9930s / 15762.0763 s
agent0:                 episode reward: -0.1942,                 loss: 0.2267
agent1:                 episode reward: 0.1942,                 loss: 0.1463
Score delta: 1.5659289597339416, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/86628_1.
Episode: 87081/101000 (86.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5806s / 15766.6569 s
agent0:                 episode reward: 0.2383,                 loss: 0.2223
agent1:                 episode reward: -0.2383,                 loss: nan
Episode: 87101/101000 (86.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9809s / 15771.6378 s
agent0:                 episode reward: 0.0043,                 loss: 0.2215
agent1:                 episode reward: -0.0043,                 loss: nan
Episode: 87121/101000 (86.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8689s / 15776.5067 s
agent0:                 episode reward: -0.0691,                 loss: 0.2215
agent1:                 episode reward: 0.0691,                 loss: nan
Episode: 87141/101000 (86.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9982s / 15781.5049 s
agent0:                 episode reward: 0.3233,                 loss: 0.2215
agent1:                 episode reward: -0.3233,                 loss: nan
Episode: 87161/101000 (86.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7897s / 15786.2946 s
agent0:                 episode reward: 0.6569,                 loss: 0.2211
agent1:                 episode reward: -0.6569,                 loss: nan
Episode: 87181/101000 (86.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8898s / 15791.1844 s
agent0:                 episode reward: -0.4578,                 loss: 0.2218
agent1:                 episode reward: 0.4578,                 loss: nan
Episode: 87201/101000 (86.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3374s / 15796.5218 s
agent0:                 episode reward: 0.0220,                 loss: 0.2219
agent1:                 episode reward: -0.0220,                 loss: nan
Episode: 87221/101000 (86.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2686s / 15801.7904 s
agent0:                 episode reward: -0.0934,                 loss: 0.2223
agent1:                 episode reward: 0.0934,                 loss: nan
Episode: 87241/101000 (86.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5380s / 15806.3283 s
agent0:                 episode reward: -0.1865,                 loss: 0.2208
agent1:                 episode reward: 0.1865,                 loss: nan
Episode: 87261/101000 (86.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0017s / 15810.3300 s
agent0:                 episode reward: -0.3474,                 loss: 0.2207
agent1:                 episode reward: 0.3474,                 loss: nan
Episode: 87281/101000 (86.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5681s / 15814.8981 s
agent0:                 episode reward: 0.1167,                 loss: 0.2207
agent1:                 episode reward: -0.1167,                 loss: nan
Episode: 87301/101000 (86.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4042s / 15820.3023 s
agent0:                 episode reward: 0.0257,                 loss: 0.2212
agent1:                 episode reward: -0.0257,                 loss: nan
Episode: 87321/101000 (86.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0109s / 15825.3133 s
agent0:                 episode reward: 0.0511,                 loss: 0.2203
agent1:                 episode reward: -0.0511,                 loss: nan
Episode: 87341/101000 (86.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0818s / 15829.3951 s
agent0:                 episode reward: 0.2366,                 loss: 0.2206
agent1:                 episode reward: -0.2366,                 loss: nan
Episode: 87361/101000 (86.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1485s / 15834.5436 s
agent0:                 episode reward: 0.0641,                 loss: 0.2198
agent1:                 episode reward: -0.0641,                 loss: nan
Episode: 87381/101000 (86.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5312s / 15839.0748 s
agent0:                 episode reward: 0.1943,                 loss: 0.2126
agent1:                 episode reward: -0.1943,                 loss: nan
Episode: 87401/101000 (86.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1116s / 15844.1864 s
agent0:                 episode reward: 0.0792,                 loss: 0.2099
agent1:                 episode reward: -0.0792,                 loss: nan
Episode: 87421/101000 (86.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1962s / 15848.3826 s
agent0:                 episode reward: -0.0638,                 loss: 0.2102
agent1:                 episode reward: 0.0638,                 loss: nan
Episode: 87441/101000 (86.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2909s / 15853.6735 s
agent0:                 episode reward: -0.2062,                 loss: 0.2108
agent1:                 episode reward: 0.2062,                 loss: nan
Episode: 87461/101000 (86.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3659s / 15859.0394 s
agent0:                 episode reward: 0.3768,                 loss: 0.2105
agent1:                 episode reward: -0.3768,                 loss: 0.1634
Score delta: 1.5227794247478315, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/87029_0.
Episode: 87481/101000 (86.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0125s / 15864.0519 s
agent0:                 episode reward: 0.0050,                 loss: nan
agent1:                 episode reward: -0.0050,                 loss: 0.1585
Episode: 87501/101000 (86.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4048s / 15868.4567 s
agent0:                 episode reward: -0.2778,                 loss: nan
agent1:                 episode reward: 0.2778,                 loss: 0.1561
Episode: 87521/101000 (86.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8167s / 15874.2734 s
agent0:                 episode reward: -0.4457,                 loss: nan
agent1:                 episode reward: 0.4457,                 loss: 0.1550
Episode: 87541/101000 (86.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2153s / 15879.4887 s
agent0:                 episode reward: -0.5054,                 loss: nan
agent1:                 episode reward: 0.5054,                 loss: 0.1575
Episode: 87561/101000 (86.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5098s / 15884.9985 s
agent0:                 episode reward: -0.3610,                 loss: nan
agent1:                 episode reward: 0.3610,                 loss: 0.1560
Episode: 87581/101000 (86.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0996s / 15890.0981 s
agent0:                 episode reward: -0.4124,                 loss: nan
agent1:                 episode reward: 0.4124,                 loss: 0.1561
Episode: 87601/101000 (86.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7578s / 15894.8559 s
agent0:                 episode reward: -0.5449,                 loss: nan
agent1:                 episode reward: 0.5449,                 loss: 0.1554
Episode: 87621/101000 (86.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0068s / 15899.8627 s
agent0:                 episode reward: -0.4454,                 loss: nan
agent1:                 episode reward: 0.4454,                 loss: 0.1554
Episode: 87641/101000 (86.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4609s / 15904.3235 s
agent0:                 episode reward: -0.1747,                 loss: 0.2436
agent1:                 episode reward: 0.1747,                 loss: 0.1546
Score delta: 1.71428902766213, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/87214_1.
Episode: 87661/101000 (86.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9235s / 15909.2470 s
agent0:                 episode reward: -0.0415,                 loss: 0.2293
agent1:                 episode reward: 0.0415,                 loss: nan
Episode: 87681/101000 (86.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3369s / 15913.5840 s
agent0:                 episode reward: 0.1285,                 loss: 0.2254
agent1:                 episode reward: -0.1285,                 loss: nan
Episode: 87701/101000 (86.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1253s / 15919.7093 s
agent0:                 episode reward: -0.4601,                 loss: 0.2236
agent1:                 episode reward: 0.4601,                 loss: nan
Episode: 87721/101000 (86.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9096s / 15924.6189 s
agent0:                 episode reward: -0.2940,                 loss: 0.2249
agent1:                 episode reward: 0.2940,                 loss: nan
Episode: 87741/101000 (86.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9166s / 15929.5354 s
agent0:                 episode reward: 0.5935,                 loss: 0.2240
agent1:                 episode reward: -0.5935,                 loss: nan
Episode: 87761/101000 (86.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8231s / 15934.3586 s
agent0:                 episode reward: -0.0946,                 loss: 0.2224
agent1:                 episode reward: 0.0946,                 loss: nan
Episode: 87781/101000 (86.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2513s / 15939.6098 s
agent0:                 episode reward: -0.3958,                 loss: 0.2228
agent1:                 episode reward: 0.3958,                 loss: nan
Episode: 87801/101000 (86.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9720s / 15945.5819 s
agent0:                 episode reward: -0.1025,                 loss: 0.2237
agent1:                 episode reward: 0.1025,                 loss: nan
Episode: 87821/101000 (86.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2652s / 15949.8470 s
agent0:                 episode reward: -0.3224,                 loss: 0.2204
agent1:                 episode reward: 0.3224,                 loss: nan
Episode: 87841/101000 (86.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0310s / 15953.8781 s
agent0:                 episode reward: 0.0897,                 loss: 0.2232
agent1:                 episode reward: -0.0897,                 loss: nan
Episode: 87861/101000 (86.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1354s / 15960.0135 s
agent0:                 episode reward: -0.3670,                 loss: 0.2228
agent1:                 episode reward: 0.3670,                 loss: nan
Episode: 87881/101000 (87.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2379s / 15964.2514 s
agent0:                 episode reward: 0.1087,                 loss: 0.2200
agent1:                 episode reward: -0.1087,                 loss: nan
Episode: 87901/101000 (87.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5311s / 15968.7825 s
agent0:                 episode reward: -0.0091,                 loss: 0.2138
agent1:                 episode reward: 0.0091,                 loss: nan
Episode: 87921/101000 (87.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4397s / 15973.2222 s
agent0:                 episode reward: 0.2515,                 loss: 0.2165
agent1:                 episode reward: -0.2515,                 loss: nan
Episode: 87941/101000 (87.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3608s / 15978.5830 s
agent0:                 episode reward: 0.3099,                 loss: 0.2153
agent1:                 episode reward: -0.3099,                 loss: 0.1563
Score delta: 1.814497042336218, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/87501_0.
Episode: 87961/101000 (87.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1740s / 15983.7569 s
agent0:                 episode reward: -0.2273,                 loss: nan
agent1:                 episode reward: 0.2273,                 loss: 0.1565
Episode: 87981/101000 (87.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0795s / 15988.8365 s
agent0:                 episode reward: -0.7875,                 loss: nan
agent1:                 episode reward: 0.7875,                 loss: 0.1541
Episode: 88001/101000 (87.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1153s / 15992.9517 s
agent0:                 episode reward: -0.4362,                 loss: nan
agent1:                 episode reward: 0.4362,                 loss: 0.1545
Episode: 88021/101000 (87.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7732s / 15997.7249 s
agent0:                 episode reward: -0.3345,                 loss: nan
agent1:                 episode reward: 0.3345,                 loss: 0.1542
Episode: 88041/101000 (87.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6493s / 16002.3743 s
agent0:                 episode reward: -0.5701,                 loss: nan
agent1:                 episode reward: 0.5701,                 loss: 0.1555
Episode: 88061/101000 (87.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7761s / 16007.1504 s
agent0:                 episode reward: -0.2715,                 loss: nan
agent1:                 episode reward: 0.2715,                 loss: 0.1541
Episode: 88081/101000 (87.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1654s / 16012.3158 s
agent0:                 episode reward: -0.3286,                 loss: nan
agent1:                 episode reward: 0.3286,                 loss: 0.1532
Episode: 88101/101000 (87.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0749s / 16017.3907 s
agent0:                 episode reward: -0.2333,                 loss: nan
agent1:                 episode reward: 0.2333,                 loss: 0.1531
Episode: 88121/101000 (87.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6958s / 16022.0864 s
agent0:                 episode reward: -0.5748,                 loss: nan
agent1:                 episode reward: 0.5748,                 loss: 0.1529
Score delta: 1.8259214018628203, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/87695_1.
Episode: 88141/101000 (87.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6305s / 16026.7170 s
agent0:                 episode reward: 0.0365,                 loss: 0.1987
agent1:                 episode reward: -0.0365,                 loss: nan
Episode: 88161/101000 (87.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9109s / 16031.6278 s
agent0:                 episode reward: -0.0251,                 loss: 0.2014
agent1:                 episode reward: 0.0251,                 loss: nan
Episode: 88181/101000 (87.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8192s / 16037.4470 s
agent0:                 episode reward: 0.0688,                 loss: 0.2017
agent1:                 episode reward: -0.0688,                 loss: nan
Episode: 88201/101000 (87.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5089s / 16042.9560 s
agent0:                 episode reward: 0.0589,                 loss: 0.1991
agent1:                 episode reward: -0.0589,                 loss: nan
Episode: 88221/101000 (87.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4539s / 16047.4098 s
agent0:                 episode reward: 0.3498,                 loss: 0.2002
agent1:                 episode reward: -0.3498,                 loss: nan
Episode: 88241/101000 (87.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4466s / 16051.8564 s
agent0:                 episode reward: 0.1053,                 loss: 0.2001
agent1:                 episode reward: -0.1053,                 loss: nan
Episode: 88261/101000 (87.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5779s / 16056.4343 s
agent0:                 episode reward: -0.1059,                 loss: 0.1984
agent1:                 episode reward: 0.1059,                 loss: nan
Episode: 88281/101000 (87.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3807s / 16061.8150 s
agent0:                 episode reward: -0.0277,                 loss: 0.1997
agent1:                 episode reward: 0.0277,                 loss: nan
Episode: 88301/101000 (87.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2952s / 16067.1102 s
agent0:                 episode reward: -0.0480,                 loss: 0.1984
agent1:                 episode reward: 0.0480,                 loss: nan
Episode: 88321/101000 (87.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4815s / 16071.5917 s
agent0:                 episode reward: -0.2074,                 loss: 0.2003
agent1:                 episode reward: 0.2074,                 loss: nan
Episode: 88341/101000 (87.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9339s / 16076.5256 s
agent0:                 episode reward: 0.0634,                 loss: 0.1991
agent1:                 episode reward: -0.0634,                 loss: nan
Episode: 88361/101000 (87.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8887s / 16081.4143 s
agent0:                 episode reward: -0.0168,                 loss: 0.1988
agent1:                 episode reward: 0.0168,                 loss: nan
Episode: 88381/101000 (87.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7906s / 16086.2049 s
agent0:                 episode reward: -0.0357,                 loss: 0.1994
agent1:                 episode reward: 0.0357,                 loss: nan
Episode: 88401/101000 (87.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8845s / 16091.0894 s
agent0:                 episode reward: 0.2076,                 loss: 0.1999
agent1:                 episode reward: -0.2076,                 loss: nan
Episode: 88421/101000 (87.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0118s / 16096.1012 s
agent0:                 episode reward: 0.1870,                 loss: 0.2053
agent1:                 episode reward: -0.1870,                 loss: nan
Episode: 88441/101000 (87.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3808s / 16101.4820 s
agent0:                 episode reward: -0.3033,                 loss: 0.2071
agent1:                 episode reward: 0.3033,                 loss: nan
Episode: 88461/101000 (87.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3585s / 16106.8405 s
agent0:                 episode reward: -0.3509,                 loss: 0.2047
agent1:                 episode reward: 0.3509,                 loss: nan
Episode: 88481/101000 (87.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5394s / 16111.3799 s
agent0:                 episode reward: 0.3004,                 loss: 0.2052
agent1:                 episode reward: -0.3004,                 loss: nan
Episode: 88501/101000 (87.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2674s / 16116.6473 s
agent0:                 episode reward: 0.0664,                 loss: 0.2038
agent1:                 episode reward: -0.0664,                 loss: nan
Episode: 88521/101000 (87.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5304s / 16123.1777 s
agent0:                 episode reward: 0.1609,                 loss: 0.2054
agent1:                 episode reward: -0.1609,                 loss: nan
Episode: 88541/101000 (87.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6211s / 16127.7988 s
agent0:                 episode reward: 0.1261,                 loss: 0.2044
agent1:                 episode reward: -0.1261,                 loss: nan
Episode: 88561/101000 (87.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4431s / 16132.2419 s
agent0:                 episode reward: -0.2210,                 loss: 0.2051
agent1:                 episode reward: 0.2210,                 loss: nan
Episode: 88581/101000 (87.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2151s / 16137.4570 s
agent0:                 episode reward: -0.0275,                 loss: 0.2061
agent1:                 episode reward: 0.0275,                 loss: nan
Episode: 88601/101000 (87.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 3.4037s / 16140.8607 s
agent0:                 episode reward: 0.1443,                 loss: 0.2060
agent1:                 episode reward: -0.1443,                 loss: nan
Episode: 88621/101000 (87.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5083s / 16146.3690 s
agent0:                 episode reward: 0.1888,                 loss: 0.2071
agent1:                 episode reward: -0.1888,                 loss: 0.1522
Score delta: 1.69149023444252, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/88181_0.
Episode: 88641/101000 (87.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7827s / 16151.1517 s
agent0:                 episode reward: -0.3071,                 loss: nan
agent1:                 episode reward: 0.3071,                 loss: 0.1534
Episode: 88661/101000 (87.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8501s / 16156.0019 s
agent0:                 episode reward: -0.0892,                 loss: nan
agent1:                 episode reward: 0.0892,                 loss: 0.1580
Episode: 88681/101000 (87.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9484s / 16159.9503 s
agent0:                 episode reward: -0.1126,                 loss: nan
agent1:                 episode reward: 0.1126,                 loss: 0.1560
Episode: 88701/101000 (87.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7450s / 16164.6953 s
agent0:                 episode reward: -0.0771,                 loss: nan
agent1:                 episode reward: 0.0771,                 loss: 0.1565
Episode: 88721/101000 (87.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2414s / 16168.9367 s
agent0:                 episode reward: -0.5567,                 loss: nan
agent1:                 episode reward: 0.5567,                 loss: 0.1558
Episode: 88741/101000 (87.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1989s / 16174.1356 s
agent0:                 episode reward: -0.3711,                 loss: nan
agent1:                 episode reward: 0.3711,                 loss: 0.1580
Episode: 88761/101000 (87.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9768s / 16179.1125 s
agent0:                 episode reward: -0.2378,                 loss: nan
agent1:                 episode reward: 0.2378,                 loss: 0.1573
Episode: 88781/101000 (87.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6718s / 16184.7843 s
agent0:                 episode reward: 0.0638,                 loss: nan
agent1:                 episode reward: -0.0638,                 loss: 0.1551
Episode: 88801/101000 (87.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6582s / 16189.4425 s
agent0:                 episode reward: -0.3641,                 loss: nan
agent1:                 episode reward: 0.3641,                 loss: 0.1566
Episode: 88821/101000 (87.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3040s / 16193.7464 s
agent0:                 episode reward: -0.5367,                 loss: 0.2242
agent1:                 episode reward: 0.5367,                 loss: 0.1553
Score delta: 1.5899774593378104, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/88385_1.
Episode: 88841/101000 (87.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3213s / 16198.0677 s
agent0:                 episode reward: -0.3978,                 loss: 0.2171
agent1:                 episode reward: 0.3978,                 loss: nan
Episode: 88861/101000 (87.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1955s / 16203.2632 s
agent0:                 episode reward: -0.0389,                 loss: 0.2191
agent1:                 episode reward: 0.0389,                 loss: nan
Episode: 88881/101000 (88.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4348s / 16208.6980 s
agent0:                 episode reward: 0.3137,                 loss: 0.2148
agent1:                 episode reward: -0.3137,                 loss: nan
Episode: 88901/101000 (88.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2685s / 16212.9665 s
agent0:                 episode reward: -0.3225,                 loss: 0.2168
agent1:                 episode reward: 0.3225,                 loss: nan
Episode: 88921/101000 (88.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7763s / 16217.7428 s
agent0:                 episode reward: 0.4389,                 loss: 0.2164
agent1:                 episode reward: -0.4389,                 loss: nan
Episode: 88941/101000 (88.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9866s / 16222.7295 s
agent0:                 episode reward: -0.2251,                 loss: 0.2144
agent1:                 episode reward: 0.2251,                 loss: nan
Episode: 88961/101000 (88.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3526s / 16227.0821 s
agent0:                 episode reward: 0.2197,                 loss: 0.2108
agent1:                 episode reward: -0.2197,                 loss: nan
Episode: 88981/101000 (88.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1958s / 16232.2779 s
agent0:                 episode reward: -0.2417,                 loss: 0.2098
agent1:                 episode reward: 0.2417,                 loss: nan
Episode: 89001/101000 (88.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4604s / 16237.7383 s
agent0:                 episode reward: 0.3454,                 loss: 0.2121
agent1:                 episode reward: -0.3454,                 loss: nan
Episode: 89021/101000 (88.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8348s / 16242.5731 s
agent0:                 episode reward: 0.3939,                 loss: 0.2117
agent1:                 episode reward: -0.3939,                 loss: 0.1499
Score delta: 1.659167728831727, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/88590_0.
Episode: 89041/101000 (88.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8934s / 16246.4666 s
agent0:                 episode reward: 0.4384,                 loss: nan
agent1:                 episode reward: -0.4384,                 loss: 0.1457
Episode: 89061/101000 (88.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5132s / 16250.9798 s
agent0:                 episode reward: -0.2737,                 loss: nan
agent1:                 episode reward: 0.2737,                 loss: 0.1439
Episode: 89081/101000 (88.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3395s / 16255.3192 s
agent0:                 episode reward: -0.4201,                 loss: nan
agent1:                 episode reward: 0.4201,                 loss: 0.1442
Episode: 89101/101000 (88.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6771s / 16259.9963 s
agent0:                 episode reward: -0.2885,                 loss: nan
agent1:                 episode reward: 0.2885,                 loss: 0.1424
Episode: 89121/101000 (88.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6609s / 16264.6572 s
agent0:                 episode reward: 0.1887,                 loss: nan
agent1:                 episode reward: -0.1887,                 loss: 0.1433
Episode: 89141/101000 (88.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0782s / 16269.7354 s
agent0:                 episode reward: -0.0968,                 loss: nan
agent1:                 episode reward: 0.0968,                 loss: 0.1437
Episode: 89161/101000 (88.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1487s / 16274.8841 s
agent0:                 episode reward: 0.1477,                 loss: nan
agent1:                 episode reward: -0.1477,                 loss: 0.1431
Episode: 89181/101000 (88.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9748s / 16279.8589 s
agent0:                 episode reward: -0.0933,                 loss: nan
agent1:                 episode reward: 0.0933,                 loss: 0.1476
Episode: 89201/101000 (88.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6087s / 16285.4676 s
agent0:                 episode reward: -0.2258,                 loss: 0.2271
agent1:                 episode reward: 0.2258,                 loss: 0.1534
Score delta: 1.6333910379484902, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/88769_1.
Episode: 89221/101000 (88.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6714s / 16291.1390 s
agent0:                 episode reward: 0.1893,                 loss: 0.2264
agent1:                 episode reward: -0.1893,                 loss: nan
Episode: 89241/101000 (88.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4654s / 16297.6044 s
agent0:                 episode reward: -0.0272,                 loss: 0.2247
agent1:                 episode reward: 0.0272,                 loss: nan
Episode: 89261/101000 (88.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4893s / 16302.0938 s
agent0:                 episode reward: -0.1826,                 loss: 0.2238
agent1:                 episode reward: 0.1826,                 loss: nan
Episode: 89281/101000 (88.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7647s / 16306.8584 s
agent0:                 episode reward: -0.1298,                 loss: 0.2234
agent1:                 episode reward: 0.1298,                 loss: nan
Episode: 89301/101000 (88.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5926s / 16312.4511 s
agent0:                 episode reward: -0.0662,                 loss: 0.2248
agent1:                 episode reward: 0.0662,                 loss: nan
Episode: 89321/101000 (88.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9023s / 16318.3533 s
agent0:                 episode reward: 0.2616,                 loss: 0.2203
agent1:                 episode reward: -0.2616,                 loss: nan
Episode: 89341/101000 (88.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0229s / 16323.3762 s
agent0:                 episode reward: 0.2917,                 loss: 0.2231
agent1:                 episode reward: -0.2917,                 loss: nan
Episode: 89361/101000 (88.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1587s / 16328.5349 s
agent0:                 episode reward: -0.0778,                 loss: 0.2237
agent1:                 episode reward: 0.0778,                 loss: nan
Episode: 89381/101000 (88.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0804s / 16332.6153 s
agent0:                 episode reward: 0.0454,                 loss: 0.2237
agent1:                 episode reward: -0.0454,                 loss: nan
Episode: 89401/101000 (88.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3631s / 16337.9784 s
agent0:                 episode reward: -0.2668,                 loss: 0.2227
agent1:                 episode reward: 0.2668,                 loss: nan
Episode: 89421/101000 (88.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3216s / 16343.3000 s
agent0:                 episode reward: 0.1987,                 loss: 0.2237
agent1:                 episode reward: -0.1987,                 loss: nan
Episode: 89441/101000 (88.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2729s / 16349.5729 s
agent0:                 episode reward: -0.3946,                 loss: 0.2227
agent1:                 episode reward: 0.3946,                 loss: nan
Episode: 89461/101000 (88.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5334s / 16354.1063 s
agent0:                 episode reward: 0.2116,                 loss: 0.2238
agent1:                 episode reward: -0.2116,                 loss: nan
Episode: 89481/101000 (88.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8223s / 16358.9286 s
agent0:                 episode reward: -0.5950,                 loss: 0.2259
agent1:                 episode reward: 0.5950,                 loss: 0.1543
Score delta: 1.5660345285292099, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/89037_0.
Episode: 89501/101000 (88.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4118s / 16364.3404 s
agent0:                 episode reward: -0.2225,                 loss: nan
agent1:                 episode reward: 0.2225,                 loss: 0.1534
Episode: 89521/101000 (88.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0671s / 16369.4075 s
agent0:                 episode reward: -0.2415,                 loss: nan
agent1:                 episode reward: 0.2415,                 loss: 0.1545
Episode: 89541/101000 (88.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3855s / 16373.7930 s
agent0:                 episode reward: -0.3988,                 loss: nan
agent1:                 episode reward: 0.3988,                 loss: 0.1527
Episode: 89561/101000 (88.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6679s / 16378.4609 s
agent0:                 episode reward: -0.0773,                 loss: nan
agent1:                 episode reward: 0.0773,                 loss: 0.1542
Episode: 89581/101000 (88.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4896s / 16382.9505 s
agent0:                 episode reward: -0.3853,                 loss: nan
agent1:                 episode reward: 0.3853,                 loss: 0.1543
Episode: 89601/101000 (88.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2509s / 16387.2014 s
agent0:                 episode reward: -0.1614,                 loss: nan
agent1:                 episode reward: 0.1614,                 loss: 0.1525
Episode: 89621/101000 (88.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2082s / 16391.4096 s
agent0:                 episode reward: -0.2885,                 loss: nan
agent1:                 episode reward: 0.2885,                 loss: 0.1535
Episode: 89641/101000 (88.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2773s / 16396.6869 s
agent0:                 episode reward: -0.4618,                 loss: 0.2253
agent1:                 episode reward: 0.4618,                 loss: 0.1518
Score delta: 1.5225211786708286, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/89209_1.
Episode: 89661/101000 (88.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8891s / 16402.5760 s
agent0:                 episode reward: -0.3308,                 loss: 0.2292
agent1:                 episode reward: 0.3308,                 loss: nan
Episode: 89681/101000 (88.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1272s / 16407.7032 s
agent0:                 episode reward: -0.2708,                 loss: 0.2272
agent1:                 episode reward: 0.2708,                 loss: nan
Episode: 89701/101000 (88.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9607s / 16412.6638 s
agent0:                 episode reward: 0.3473,                 loss: 0.2297
agent1:                 episode reward: -0.3473,                 loss: nan
Episode: 89721/101000 (88.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8583s / 16417.5221 s
agent0:                 episode reward: 0.1579,                 loss: 0.2280
agent1:                 episode reward: -0.1579,                 loss: nan
Episode: 89741/101000 (88.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0513s / 16422.5734 s
agent0:                 episode reward: 0.0442,                 loss: 0.2290
agent1:                 episode reward: -0.0442,                 loss: nan
Episode: 89761/101000 (88.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8752s / 16427.4485 s
agent0:                 episode reward: 0.4101,                 loss: 0.2298
agent1:                 episode reward: -0.4101,                 loss: nan
Episode: 89781/101000 (88.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2270s / 16432.6755 s
agent0:                 episode reward: 0.0298,                 loss: 0.2267
agent1:                 episode reward: -0.0298,                 loss: nan
Episode: 89801/101000 (88.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1281s / 16437.8037 s
agent0:                 episode reward: 0.3950,                 loss: 0.2282
agent1:                 episode reward: -0.3950,                 loss: nan
Episode: 89821/101000 (88.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7836s / 16442.5873 s
agent0:                 episode reward: -0.2572,                 loss: 0.2279
agent1:                 episode reward: 0.2572,                 loss: nan
Episode: 89841/101000 (88.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9155s / 16447.5028 s
agent0:                 episode reward: 0.3987,                 loss: 0.2248
agent1:                 episode reward: -0.3987,                 loss: nan
Episode: 89861/101000 (88.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2610s / 16452.7638 s
agent0:                 episode reward: 0.2208,                 loss: 0.2267
agent1:                 episode reward: -0.2208,                 loss: nan
Episode: 89881/101000 (88.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9450s / 16456.7088 s
agent0:                 episode reward: 0.1172,                 loss: 0.2267
agent1:                 episode reward: -0.1172,                 loss: nan
Episode: 89901/101000 (89.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3358s / 16462.0446 s
agent0:                 episode reward: -0.3819,                 loss: 0.2274
agent1:                 episode reward: 0.3819,                 loss: nan
Episode: 89921/101000 (89.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2522s / 16467.2969 s
agent0:                 episode reward: -0.1962,                 loss: 0.2267
agent1:                 episode reward: 0.1962,                 loss: nan
Episode: 89941/101000 (89.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1448s / 16472.4417 s
agent0:                 episode reward: 0.1767,                 loss: 0.2257
agent1:                 episode reward: -0.1767,                 loss: nan
Episode: 89961/101000 (89.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6798s / 16477.1215 s
agent0:                 episode reward: -0.2965,                 loss: 0.2254
agent1:                 episode reward: 0.2965,                 loss: nan
Episode: 89981/101000 (89.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3973s / 16482.5188 s
agent0:                 episode reward: -0.0013,                 loss: 0.2096
agent1:                 episode reward: 0.0013,                 loss: nan
Episode: 90001/101000 (89.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8962s / 16487.4151 s
agent0:                 episode reward: -0.0782,                 loss: 0.2119
agent1:                 episode reward: 0.0782,                 loss: 0.1414
Score delta: 1.6053974349460272, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/89560_0.
Episode: 90021/101000 (89.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9415s / 16492.3566 s
agent0:                 episode reward: -0.0817,                 loss: nan
agent1:                 episode reward: 0.0817,                 loss: 0.1389
Episode: 90041/101000 (89.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1211s / 16496.4776 s
agent0:                 episode reward: -0.2804,                 loss: nan
agent1:                 episode reward: 0.2804,                 loss: 0.1393
Episode: 90061/101000 (89.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5411s / 16501.0187 s
agent0:                 episode reward: -0.1651,                 loss: nan
agent1:                 episode reward: 0.1651,                 loss: 0.1405
Episode: 90081/101000 (89.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9186s / 16504.9373 s
agent0:                 episode reward: -0.1475,                 loss: nan
agent1:                 episode reward: 0.1475,                 loss: 0.1386
Episode: 90101/101000 (89.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8179s / 16509.7552 s
agent0:                 episode reward: -0.4111,                 loss: nan
agent1:                 episode reward: 0.4111,                 loss: 0.1391
Episode: 90121/101000 (89.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9953s / 16514.7505 s
agent0:                 episode reward: 0.1166,                 loss: nan
agent1:                 episode reward: -0.1166,                 loss: 0.1393
Episode: 90141/101000 (89.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4392s / 16521.1898 s
agent0:                 episode reward: -0.0638,                 loss: nan
agent1:                 episode reward: 0.0638,                 loss: 0.1720
Episode: 90161/101000 (89.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0555s / 16526.2452 s
agent0:                 episode reward: -0.3745,                 loss: nan
agent1:                 episode reward: 0.3745,                 loss: 0.1671
Episode: 90181/101000 (89.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8467s / 16531.0919 s
agent0:                 episode reward: 0.3062,                 loss: nan
agent1:                 episode reward: -0.3062,                 loss: 0.1649
Episode: 90201/101000 (89.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1724s / 16536.2644 s
agent0:                 episode reward: -0.2470,                 loss: nan
agent1:                 episode reward: 0.2470,                 loss: 0.1652
Episode: 90221/101000 (89.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6513s / 16542.9157 s
agent0:                 episode reward: -0.3899,                 loss: nan
agent1:                 episode reward: 0.3899,                 loss: 0.1626
Episode: 90241/101000 (89.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6462s / 16547.5619 s
agent0:                 episode reward: -0.2630,                 loss: nan
agent1:                 episode reward: 0.2630,                 loss: 0.1623
Episode: 90261/101000 (89.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9613s / 16552.5232 s
agent0:                 episode reward: -0.0105,                 loss: nan
agent1:                 episode reward: 0.0105,                 loss: 0.1627
Episode: 90281/101000 (89.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5978s / 16558.1211 s
agent0:                 episode reward: -0.4137,                 loss: nan
agent1:                 episode reward: 0.4137,                 loss: 0.1621
Episode: 90301/101000 (89.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3023s / 16563.4234 s
agent0:                 episode reward: -0.1417,                 loss: nan
agent1:                 episode reward: 0.1417,                 loss: 0.1640
Episode: 90321/101000 (89.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8901s / 16568.3135 s
agent0:                 episode reward: 0.2054,                 loss: nan
agent1:                 episode reward: -0.2054,                 loss: 0.1616
Episode: 90341/101000 (89.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6907s / 16573.0042 s
agent0:                 episode reward: 0.0069,                 loss: nan
agent1:                 episode reward: -0.0069,                 loss: 0.1615
Episode: 90361/101000 (89.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6421s / 16578.6463 s
agent0:                 episode reward: -0.1648,                 loss: nan
agent1:                 episode reward: 0.1648,                 loss: 0.1643
Episode: 90381/101000 (89.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7240s / 16582.3704 s
agent0:                 episode reward: -0.2736,                 loss: 0.2082
agent1:                 episode reward: 0.2736,                 loss: 0.1623
Score delta: 1.6002983979689263, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/89950_1.
Episode: 90401/101000 (89.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7657s / 16587.1361 s
agent0:                 episode reward: 0.0593,                 loss: 0.2033
agent1:                 episode reward: -0.0593,                 loss: nan
Episode: 90421/101000 (89.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4059s / 16593.5420 s
agent0:                 episode reward: 0.6157,                 loss: 0.2055
agent1:                 episode reward: -0.6157,                 loss: nan
Episode: 90441/101000 (89.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8555s / 16599.3975 s
agent0:                 episode reward: -0.0830,                 loss: 0.2058
agent1:                 episode reward: 0.0830,                 loss: nan
Episode: 90461/101000 (89.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0840s / 16604.4814 s
agent0:                 episode reward: -0.0764,                 loss: 0.2042
agent1:                 episode reward: 0.0764,                 loss: nan
Episode: 90481/101000 (89.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3633s / 16608.8448 s
agent0:                 episode reward: 0.4630,                 loss: 0.2051
agent1:                 episode reward: -0.4630,                 loss: nan
Episode: 90501/101000 (89.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7458s / 16613.5905 s
agent0:                 episode reward: -0.0324,                 loss: 0.2062
agent1:                 episode reward: 0.0324,                 loss: nan
Episode: 90521/101000 (89.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6524s / 16618.2429 s
agent0:                 episode reward: 0.1303,                 loss: 0.2032
agent1:                 episode reward: -0.1303,                 loss: nan
Episode: 90541/101000 (89.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0290s / 16623.2719 s
agent0:                 episode reward: -0.1691,                 loss: 0.2051
agent1:                 episode reward: 0.1691,                 loss: nan
Episode: 90561/101000 (89.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2137s / 16628.4856 s
agent0:                 episode reward: 0.1773,                 loss: 0.2055
agent1:                 episode reward: -0.1773,                 loss: nan
Episode: 90581/101000 (89.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5914s / 16634.0770 s
agent0:                 episode reward: -0.3583,                 loss: 0.2062
agent1:                 episode reward: 0.3583,                 loss: nan
Episode: 90601/101000 (89.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3369s / 16639.4140 s
agent0:                 episode reward: -0.0544,                 loss: 0.2049
agent1:                 episode reward: 0.0544,                 loss: nan
Episode: 90621/101000 (89.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0972s / 16644.5111 s
agent0:                 episode reward: -0.3360,                 loss: 0.2071
agent1:                 episode reward: 0.3360,                 loss: nan
Episode: 90641/101000 (89.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9791s / 16649.4903 s
agent0:                 episode reward: 0.2679,                 loss: 0.2042
agent1:                 episode reward: -0.2679,                 loss: nan
Episode: 90661/101000 (89.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2146s / 16654.7048 s
agent0:                 episode reward: 0.3241,                 loss: 0.2038
agent1:                 episode reward: -0.3241,                 loss: nan
Episode: 90681/101000 (89.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3351s / 16660.0399 s
agent0:                 episode reward: 0.2214,                 loss: 0.2081
agent1:                 episode reward: -0.2214,                 loss: nan
Episode: 90701/101000 (89.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3349s / 16664.3748 s
agent0:                 episode reward: 0.0928,                 loss: 0.2108
agent1:                 episode reward: -0.0928,                 loss: nan
Episode: 90721/101000 (89.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9095s / 16669.2843 s
agent0:                 episode reward: 0.1185,                 loss: 0.2108
agent1:                 episode reward: -0.1185,                 loss: nan
Episode: 90741/101000 (89.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4318s / 16674.7161 s
agent0:                 episode reward: 0.2481,                 loss: 0.2118
agent1:                 episode reward: -0.2481,                 loss: nan
Episode: 90761/101000 (89.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2542s / 16679.9703 s
agent0:                 episode reward: 0.2335,                 loss: 0.2083
agent1:                 episode reward: -0.2335,                 loss: nan
Episode: 90781/101000 (89.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6872s / 16684.6575 s
agent0:                 episode reward: -0.4171,                 loss: 0.2115
agent1:                 episode reward: 0.4171,                 loss: nan
Episode: 90801/101000 (89.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3265s / 16688.9840 s
agent0:                 episode reward: -0.0820,                 loss: 0.2114
agent1:                 episode reward: 0.0820,                 loss: nan
Episode: 90821/101000 (89.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6385s / 16694.6224 s
agent0:                 episode reward: 0.2628,                 loss: 0.2069
agent1:                 episode reward: -0.2628,                 loss: 0.1863
Score delta: 1.77494671515268, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/90389_0.
Episode: 90841/101000 (89.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9649s / 16699.5874 s
agent0:                 episode reward: 0.0351,                 loss: nan
agent1:                 episode reward: -0.0351,                 loss: 0.1792
Episode: 90861/101000 (89.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6090s / 16704.1964 s
agent0:                 episode reward: 0.0922,                 loss: nan
agent1:                 episode reward: -0.0922,                 loss: 0.1781
Episode: 90881/101000 (89.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4736s / 16708.6700 s
agent0:                 episode reward: -0.0140,                 loss: nan
agent1:                 episode reward: 0.0140,                 loss: 0.1768
Episode: 90901/101000 (90.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1466s / 16713.8166 s
agent0:                 episode reward: 0.2765,                 loss: nan
agent1:                 episode reward: -0.2765,                 loss: 0.1710
Episode: 90921/101000 (90.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0574s / 16719.8739 s
agent0:                 episode reward: -0.2149,                 loss: nan
agent1:                 episode reward: 0.2149,                 loss: 0.1482
Episode: 90941/101000 (90.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1672s / 16724.0411 s
agent0:                 episode reward: -0.2315,                 loss: nan
agent1:                 episode reward: 0.2315,                 loss: 0.1477
Episode: 90961/101000 (90.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3806s / 16728.4217 s
agent0:                 episode reward: -0.0860,                 loss: nan
agent1:                 episode reward: 0.0860,                 loss: 0.1488
Episode: 90981/101000 (90.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2004s / 16733.6221 s
agent0:                 episode reward: -0.1297,                 loss: nan
agent1:                 episode reward: 0.1297,                 loss: 0.1467
Episode: 91001/101000 (90.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6202s / 16739.2423 s
agent0:                 episode reward: -0.3932,                 loss: nan
agent1:                 episode reward: 0.3932,                 loss: 0.1479
Episode: 91021/101000 (90.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5921s / 16744.8344 s
agent0:                 episode reward: 0.1091,                 loss: 0.2100
agent1:                 episode reward: -0.1091,                 loss: 0.1472
Score delta: 1.510193429502668, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/90581_1.
Episode: 91041/101000 (90.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6196s / 16749.4540 s
agent0:                 episode reward: 0.2236,                 loss: 0.2071
agent1:                 episode reward: -0.2236,                 loss: nan
Episode: 91061/101000 (90.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3701s / 16754.8241 s
agent0:                 episode reward: -0.1679,                 loss: 0.2060
agent1:                 episode reward: 0.1679,                 loss: nan
Episode: 91081/101000 (90.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8470s / 16759.6711 s
agent0:                 episode reward: -0.6116,                 loss: 0.2058
agent1:                 episode reward: 0.6116,                 loss: nan
Episode: 91101/101000 (90.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6880s / 16764.3591 s
agent0:                 episode reward: -0.2866,                 loss: 0.2057
agent1:                 episode reward: 0.2866,                 loss: nan
Episode: 91121/101000 (90.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4026s / 16769.7617 s
agent0:                 episode reward: 0.1009,                 loss: 0.2052
agent1:                 episode reward: -0.1009,                 loss: nan
Episode: 91141/101000 (90.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6868s / 16774.4484 s
agent0:                 episode reward: -0.0660,                 loss: 0.2044
agent1:                 episode reward: 0.0660,                 loss: nan
Episode: 91161/101000 (90.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0573s / 16779.5057 s
agent0:                 episode reward: 0.0850,                 loss: 0.2021
agent1:                 episode reward: -0.0850,                 loss: nan
Episode: 91181/101000 (90.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1680s / 16784.6737 s
agent0:                 episode reward: 0.2100,                 loss: 0.2042
agent1:                 episode reward: -0.2100,                 loss: nan
Episode: 91201/101000 (90.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6435s / 16789.3172 s
agent0:                 episode reward: -0.0417,                 loss: 0.2050
agent1:                 episode reward: 0.0417,                 loss: nan
Episode: 91221/101000 (90.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8158s / 16794.1330 s
agent0:                 episode reward: -0.0107,                 loss: 0.2083
agent1:                 episode reward: 0.0107,                 loss: nan
Episode: 91241/101000 (90.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1719s / 16799.3050 s
agent0:                 episode reward: 0.0971,                 loss: 0.2089
agent1:                 episode reward: -0.0971,                 loss: nan
Episode: 91261/101000 (90.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0908s / 16804.3958 s
agent0:                 episode reward: 0.4412,                 loss: 0.2098
agent1:                 episode reward: -0.4412,                 loss: 0.1581
Score delta: 1.8260366860045294, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/90833_0.
Episode: 91281/101000 (90.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7405s / 16811.1363 s
agent0:                 episode reward: 0.4560,                 loss: nan
agent1:                 episode reward: -0.4560,                 loss: 0.1577
Episode: 91301/101000 (90.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0643s / 16816.2006 s
agent0:                 episode reward: -0.1502,                 loss: nan
agent1:                 episode reward: 0.1502,                 loss: 0.1555
Episode: 91321/101000 (90.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9616s / 16821.1621 s
agent0:                 episode reward: -0.2452,                 loss: nan
agent1:                 episode reward: 0.2452,                 loss: 0.1565
Episode: 91341/101000 (90.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8657s / 16826.0278 s
agent0:                 episode reward: -0.0791,                 loss: nan
agent1:                 episode reward: 0.0791,                 loss: 0.1549
Episode: 91361/101000 (90.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4430s / 16831.4709 s
agent0:                 episode reward: -0.2275,                 loss: nan
agent1:                 episode reward: 0.2275,                 loss: 0.1562
Episode: 91381/101000 (90.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0206s / 16835.4915 s
agent0:                 episode reward: -0.0717,                 loss: nan
agent1:                 episode reward: 0.0717,                 loss: 0.1543
Episode: 91401/101000 (90.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7064s / 16840.1979 s
agent0:                 episode reward: -0.0594,                 loss: nan
agent1:                 episode reward: 0.0594,                 loss: 0.1539
Episode: 91421/101000 (90.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0409s / 16845.2387 s
agent0:                 episode reward: -0.0888,                 loss: nan
agent1:                 episode reward: 0.0888,                 loss: 0.1544
Episode: 91441/101000 (90.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9559s / 16849.1946 s
agent0:                 episode reward: -0.4829,                 loss: 0.2151
agent1:                 episode reward: 0.4829,                 loss: 0.1541
Score delta: 1.635573491893215, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/91013_1.
Episode: 91461/101000 (90.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3150s / 16854.5096 s
agent0:                 episode reward: -0.6313,                 loss: 0.1949
agent1:                 episode reward: 0.6313,                 loss: nan
Episode: 91481/101000 (90.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7482s / 16859.2578 s
agent0:                 episode reward: -0.1749,                 loss: 0.1922
agent1:                 episode reward: 0.1749,                 loss: nan
Episode: 91501/101000 (90.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2986s / 16863.5564 s
agent0:                 episode reward: -0.4264,                 loss: 0.1915
agent1:                 episode reward: 0.4264,                 loss: nan
Episode: 91521/101000 (90.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1668s / 16867.7232 s
agent0:                 episode reward: 0.1762,                 loss: 0.1932
agent1:                 episode reward: -0.1762,                 loss: nan
Episode: 91541/101000 (90.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2302s / 16872.9534 s
agent0:                 episode reward: 0.0282,                 loss: 0.1925
agent1:                 episode reward: -0.0282,                 loss: nan
Episode: 91561/101000 (90.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1182s / 16879.0716 s
agent0:                 episode reward: -0.0702,                 loss: 0.1908
agent1:                 episode reward: 0.0702,                 loss: nan
Episode: 91581/101000 (90.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6589s / 16883.7306 s
agent0:                 episode reward: 0.0192,                 loss: 0.1919
agent1:                 episode reward: -0.0192,                 loss: nan
Episode: 91601/101000 (90.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8570s / 16888.5875 s
agent0:                 episode reward: 0.0902,                 loss: 0.1911
agent1:                 episode reward: -0.0902,                 loss: nan
Episode: 91621/101000 (90.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3352s / 16892.9227 s
agent0:                 episode reward: 0.6531,                 loss: 0.1915
agent1:                 episode reward: -0.6531,                 loss: 0.1544
Score delta: 1.5214594115111038, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/91188_0.
Episode: 91641/101000 (90.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3825s / 16898.3052 s
agent0:                 episode reward: 0.0532,                 loss: nan
agent1:                 episode reward: -0.0532,                 loss: 0.1501
Episode: 91661/101000 (90.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7862s / 16903.0914 s
agent0:                 episode reward: -0.0047,                 loss: nan
agent1:                 episode reward: 0.0047,                 loss: 0.1573
Episode: 91681/101000 (90.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7919s / 16907.8832 s
agent0:                 episode reward: -0.4560,                 loss: nan
agent1:                 episode reward: 0.4560,                 loss: 0.1643
Episode: 91701/101000 (90.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8391s / 16913.7223 s
agent0:                 episode reward: 0.3054,                 loss: nan
agent1:                 episode reward: -0.3054,                 loss: 0.1613
Episode: 91721/101000 (90.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0957s / 16918.8180 s
agent0:                 episode reward: -0.0962,                 loss: nan
agent1:                 episode reward: 0.0962,                 loss: 0.1581
Episode: 91741/101000 (90.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4076s / 16923.2256 s
agent0:                 episode reward: -0.1776,                 loss: nan
agent1:                 episode reward: 0.1776,                 loss: 0.1581
Episode: 91761/101000 (90.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5929s / 16928.8185 s
agent0:                 episode reward: -0.0618,                 loss: nan
agent1:                 episode reward: 0.0618,                 loss: 0.1567
Episode: 91781/101000 (90.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4086s / 16933.2270 s
agent0:                 episode reward: -0.1057,                 loss: nan
agent1:                 episode reward: 0.1057,                 loss: 0.1569
Episode: 91801/101000 (90.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5882s / 16937.8152 s
agent0:                 episode reward: 0.1964,                 loss: nan
agent1:                 episode reward: -0.1964,                 loss: 0.1563
Episode: 91821/101000 (90.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7067s / 16942.5220 s
agent0:                 episode reward: -0.1248,                 loss: nan
agent1:                 episode reward: 0.1248,                 loss: 0.1566
Episode: 91841/101000 (90.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2379s / 16947.7599 s
agent0:                 episode reward: -0.0476,                 loss: 0.2142
agent1:                 episode reward: 0.0476,                 loss: 0.1570
Score delta: 1.7990604842030529, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/91414_1.
Episode: 91861/101000 (90.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 3.8890s / 16951.6489 s
agent0:                 episode reward: 0.1404,                 loss: 0.2229
agent1:                 episode reward: -0.1404,                 loss: nan
Episode: 91881/101000 (90.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1913s / 16955.8402 s
agent0:                 episode reward: 0.0552,                 loss: 0.2249
agent1:                 episode reward: -0.0552,                 loss: nan
Episode: 91901/101000 (90.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5310s / 16960.3712 s
agent0:                 episode reward: 0.0577,                 loss: 0.2271
agent1:                 episode reward: -0.0577,                 loss: nan
Episode: 91921/101000 (91.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9838s / 16965.3550 s
agent0:                 episode reward: -0.2020,                 loss: 0.2247
agent1:                 episode reward: 0.2020,                 loss: nan
Episode: 91941/101000 (91.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3878s / 16969.7427 s
agent0:                 episode reward: 0.0804,                 loss: 0.2240
agent1:                 episode reward: -0.0804,                 loss: nan
Episode: 91961/101000 (91.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3142s / 16975.0569 s
agent0:                 episode reward: 0.0471,                 loss: 0.2276
agent1:                 episode reward: -0.0471,                 loss: nan
Episode: 91981/101000 (91.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8847s / 16979.9416 s
agent0:                 episode reward: 0.0857,                 loss: 0.2259
agent1:                 episode reward: -0.0857,                 loss: nan
Episode: 92001/101000 (91.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9616s / 16984.9032 s
agent0:                 episode reward: -0.1288,                 loss: 0.2237
agent1:                 episode reward: 0.1288,                 loss: nan
Episode: 92021/101000 (91.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9704s / 16989.8736 s
agent0:                 episode reward: 0.0736,                 loss: 0.2262
agent1:                 episode reward: -0.0736,                 loss: nan
Episode: 92041/101000 (91.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2207s / 16994.0943 s
agent0:                 episode reward: -0.2284,                 loss: 0.2257
agent1:                 episode reward: 0.2284,                 loss: nan
Episode: 92061/101000 (91.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6704s / 16998.7646 s
agent0:                 episode reward: -0.0702,                 loss: 0.2241
agent1:                 episode reward: 0.0702,                 loss: nan
Episode: 92081/101000 (91.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3253s / 17005.0899 s
agent0:                 episode reward: -0.2284,                 loss: 0.2230
agent1:                 episode reward: 0.2284,                 loss: nan
Episode: 92101/101000 (91.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0735s / 17009.1634 s
agent0:                 episode reward: 0.5665,                 loss: 0.2241
agent1:                 episode reward: -0.5665,                 loss: nan
Episode: 92121/101000 (91.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2703s / 17014.4338 s
agent0:                 episode reward: -0.1088,                 loss: 0.2241
agent1:                 episode reward: 0.1088,                 loss: 0.1765
Score delta: 1.6334870825129157, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/91677_0.
Episode: 92141/101000 (91.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3204s / 17020.7542 s
agent0:                 episode reward: -0.3611,                 loss: nan
agent1:                 episode reward: 0.3611,                 loss: 0.1757
Episode: 92161/101000 (91.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0046s / 17025.7587 s
agent0:                 episode reward: 0.0560,                 loss: nan
agent1:                 episode reward: -0.0560,                 loss: 0.1742
Episode: 92181/101000 (91.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5215s / 17030.2802 s
agent0:                 episode reward: -0.2299,                 loss: nan
agent1:                 episode reward: 0.2299,                 loss: 0.1753
Episode: 92201/101000 (91.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8553s / 17036.1356 s
agent0:                 episode reward: -0.5218,                 loss: nan
agent1:                 episode reward: 0.5218,                 loss: 0.1734
Episode: 92221/101000 (91.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4411s / 17042.5767 s
agent0:                 episode reward: -0.1235,                 loss: nan
agent1:                 episode reward: 0.1235,                 loss: 0.1743
Episode: 92241/101000 (91.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9926s / 17047.5693 s
agent0:                 episode reward: -0.2316,                 loss: nan
agent1:                 episode reward: 0.2316,                 loss: 0.1742
Episode: 92261/101000 (91.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0845s / 17052.6538 s
agent0:                 episode reward: -0.1408,                 loss: nan
agent1:                 episode reward: 0.1408,                 loss: 0.1642
Episode: 92281/101000 (91.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 3.6822s / 17056.3360 s
agent0:                 episode reward: -0.4453,                 loss: nan
agent1:                 episode reward: 0.4453,                 loss: 0.1484
Episode: 92301/101000 (91.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7241s / 17061.0601 s
agent0:                 episode reward: -0.2618,                 loss: nan
agent1:                 episode reward: 0.2618,                 loss: 0.1485
Episode: 92321/101000 (91.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5357s / 17066.5958 s
agent0:                 episode reward: 0.3518,                 loss: nan
agent1:                 episode reward: -0.3518,                 loss: 0.1483
Episode: 92341/101000 (91.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1498s / 17071.7456 s
agent0:                 episode reward: -0.3714,                 loss: nan
agent1:                 episode reward: 0.3714,                 loss: 0.1481
Episode: 92361/101000 (91.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5449s / 17078.2905 s
agent0:                 episode reward: 0.4125,                 loss: 0.2082
agent1:                 episode reward: -0.4125,                 loss: 0.1488
Score delta: 1.5905918515252329, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/91917_1.
Episode: 92381/101000 (91.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6867s / 17083.9771 s
agent0:                 episode reward: 0.3228,                 loss: 0.2045
agent1:                 episode reward: -0.3228,                 loss: nan
Episode: 92401/101000 (91.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6397s / 17089.6168 s
agent0:                 episode reward: -0.4441,                 loss: 0.2050
agent1:                 episode reward: 0.4441,                 loss: nan
Episode: 92421/101000 (91.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2636s / 17094.8804 s
agent0:                 episode reward: 0.3225,                 loss: 0.2041
agent1:                 episode reward: -0.3225,                 loss: nan
Episode: 92441/101000 (91.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2771s / 17100.1575 s
agent0:                 episode reward: 0.3655,                 loss: 0.2057
agent1:                 episode reward: -0.3655,                 loss: nan
Episode: 92461/101000 (91.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3051s / 17105.4626 s
agent0:                 episode reward: -0.0995,                 loss: 0.2040
agent1:                 episode reward: 0.0995,                 loss: nan
Episode: 92481/101000 (91.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8576s / 17110.3202 s
agent0:                 episode reward: -0.2550,                 loss: 0.2049
agent1:                 episode reward: 0.2550,                 loss: nan
Episode: 92501/101000 (91.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9485s / 17115.2687 s
agent0:                 episode reward: -0.1080,                 loss: 0.2033
agent1:                 episode reward: 0.1080,                 loss: nan
Episode: 92521/101000 (91.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5094s / 17120.7782 s
agent0:                 episode reward: -0.1570,                 loss: 0.2048
agent1:                 episode reward: 0.1570,                 loss: nan
Episode: 92541/101000 (91.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0227s / 17126.8008 s
agent0:                 episode reward: 0.2824,                 loss: 0.2142
agent1:                 episode reward: -0.2824,                 loss: nan
Episode: 92561/101000 (91.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6462s / 17132.4471 s
agent0:                 episode reward: -0.1719,                 loss: 0.2148
agent1:                 episode reward: 0.1719,                 loss: nan
Episode: 92581/101000 (91.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8969s / 17138.3440 s
agent0:                 episode reward: -0.3950,                 loss: 0.2139
agent1:                 episode reward: 0.3950,                 loss: nan
Episode: 92601/101000 (91.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6399s / 17142.9839 s
agent0:                 episode reward: 0.3203,                 loss: 0.2148
agent1:                 episode reward: -0.3203,                 loss: 0.1571
Score delta: 1.8492705063884993, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/92166_0.
Episode: 92621/101000 (91.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6242s / 17147.6081 s
agent0:                 episode reward: 0.1234,                 loss: nan
agent1:                 episode reward: -0.1234,                 loss: 0.1575
Episode: 92641/101000 (91.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1928s / 17152.8009 s
agent0:                 episode reward: -0.2764,                 loss: nan
agent1:                 episode reward: 0.2764,                 loss: 0.1584
Episode: 92661/101000 (91.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5244s / 17157.3253 s
agent0:                 episode reward: -0.2379,                 loss: nan
agent1:                 episode reward: 0.2379,                 loss: 0.1573
Episode: 92681/101000 (91.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 3.9803s / 17161.3057 s
agent0:                 episode reward: -0.1799,                 loss: nan
agent1:                 episode reward: 0.1799,                 loss: 0.1572
Episode: 92701/101000 (91.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8233s / 17167.1290 s
agent0:                 episode reward: -0.1880,                 loss: nan
agent1:                 episode reward: 0.1880,                 loss: 0.1555
Episode: 92721/101000 (91.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0296s / 17173.1586 s
agent0:                 episode reward: -0.1705,                 loss: nan
agent1:                 episode reward: 0.1705,                 loss: 0.1570
Episode: 92741/101000 (91.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9527s / 17178.1113 s
agent0:                 episode reward: -0.2535,                 loss: nan
agent1:                 episode reward: 0.2535,                 loss: 0.1558
Episode: 92761/101000 (91.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1787s / 17182.2899 s
agent0:                 episode reward: -0.1786,                 loss: nan
agent1:                 episode reward: 0.1786,                 loss: 0.1570
Episode: 92781/101000 (91.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2641s / 17187.5541 s
agent0:                 episode reward: -0.0630,                 loss: nan
agent1:                 episode reward: 0.0630,                 loss: 0.1564
Episode: 92801/101000 (91.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9255s / 17192.4796 s
agent0:                 episode reward: -0.3152,                 loss: nan
agent1:                 episode reward: 0.3152,                 loss: 0.1557
Episode: 92821/101000 (91.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0644s / 17197.5440 s
agent0:                 episode reward: -0.2519,                 loss: nan
agent1:                 episode reward: 0.2519,                 loss: 0.1567
Episode: 92841/101000 (91.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6863s / 17203.2303 s
agent0:                 episode reward: 0.2558,                 loss: nan
agent1:                 episode reward: -0.2558,                 loss: 0.1560
Episode: 92861/101000 (91.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9376s / 17208.1679 s
agent0:                 episode reward: 0.3923,                 loss: nan
agent1:                 episode reward: -0.3923,                 loss: 0.1555
Episode: 92881/101000 (91.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8578s / 17213.0257 s
agent0:                 episode reward: -0.2690,                 loss: nan
agent1:                 episode reward: 0.2690,                 loss: 0.1548
Episode: 92901/101000 (91.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6952s / 17217.7209 s
agent0:                 episode reward: -0.1378,                 loss: nan
agent1:                 episode reward: 0.1378,                 loss: 0.1571
Episode: 92921/101000 (92.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1107s / 17222.8316 s
agent0:                 episode reward: -0.7858,                 loss: 0.2440
agent1:                 episode reward: 0.7858,                 loss: 0.1566
Score delta: 1.736440381402229, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/92481_1.
Episode: 92941/101000 (92.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3426s / 17228.1742 s
agent0:                 episode reward: -0.2188,                 loss: 0.2306
agent1:                 episode reward: 0.2188,                 loss: nan
Episode: 92961/101000 (92.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6577s / 17233.8319 s
agent0:                 episode reward: -0.1798,                 loss: 0.2307
agent1:                 episode reward: 0.1798,                 loss: nan
Episode: 92981/101000 (92.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5802s / 17239.4121 s
agent0:                 episode reward: -0.3789,                 loss: 0.2303
agent1:                 episode reward: 0.3789,                 loss: nan
Episode: 93001/101000 (92.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4902s / 17243.9023 s
agent0:                 episode reward: 0.0782,                 loss: 0.2260
agent1:                 episode reward: -0.0782,                 loss: nan
Episode: 93021/101000 (92.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5594s / 17250.4616 s
agent0:                 episode reward: 0.0591,                 loss: 0.2285
agent1:                 episode reward: -0.0591,                 loss: nan
Episode: 93041/101000 (92.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3709s / 17255.8326 s
agent0:                 episode reward: 0.6849,                 loss: 0.2257
agent1:                 episode reward: -0.6849,                 loss: nan
Episode: 93061/101000 (92.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7057s / 17260.5383 s
agent0:                 episode reward: 0.3217,                 loss: 0.2277
agent1:                 episode reward: -0.3217,                 loss: nan
Episode: 93081/101000 (92.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0534s / 17266.5917 s
agent0:                 episode reward: 0.2439,                 loss: 0.2298
agent1:                 episode reward: -0.2439,                 loss: nan
Episode: 93101/101000 (92.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2944s / 17271.8861 s
agent0:                 episode reward: -0.0164,                 loss: 0.2248
agent1:                 episode reward: 0.0164,                 loss: nan
Episode: 93121/101000 (92.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5111s / 17277.3972 s
agent0:                 episode reward: -0.5012,                 loss: 0.2245
agent1:                 episode reward: 0.5012,                 loss: nan
Episode: 93141/101000 (92.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8494s / 17282.2467 s
agent0:                 episode reward: -0.0880,                 loss: 0.2257
agent1:                 episode reward: 0.0880,                 loss: nan
Episode: 93161/101000 (92.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5504s / 17287.7971 s
agent0:                 episode reward: 0.2358,                 loss: 0.2279
agent1:                 episode reward: -0.2358,                 loss: nan
Episode: 93181/101000 (92.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2974s / 17293.0945 s
agent0:                 episode reward: -0.1237,                 loss: 0.2317
agent1:                 episode reward: 0.1237,                 loss: 0.1517
Score delta: 1.7986194590209632, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/92739_0.
Episode: 93201/101000 (92.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6774s / 17298.7719 s
agent0:                 episode reward: -0.0014,                 loss: nan
agent1:                 episode reward: 0.0014,                 loss: 0.1536
Episode: 93221/101000 (92.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6481s / 17304.4200 s
agent0:                 episode reward: -0.6065,                 loss: nan
agent1:                 episode reward: 0.6065,                 loss: 0.1512
Episode: 93241/101000 (92.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2341s / 17309.6541 s
agent0:                 episode reward: -0.1693,                 loss: nan
agent1:                 episode reward: 0.1693,                 loss: 0.1517
Episode: 93261/101000 (92.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8250s / 17315.4791 s
agent0:                 episode reward: -0.2068,                 loss: nan
agent1:                 episode reward: 0.2068,                 loss: 0.1513
Episode: 93281/101000 (92.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9908s / 17320.4699 s
agent0:                 episode reward: -0.3743,                 loss: nan
agent1:                 episode reward: 0.3743,                 loss: 0.1510
Episode: 93301/101000 (92.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0393s / 17325.5091 s
agent0:                 episode reward: 0.0353,                 loss: nan
agent1:                 episode reward: -0.0353,                 loss: 0.1514
Episode: 93321/101000 (92.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1141s / 17330.6232 s
agent0:                 episode reward: -0.3225,                 loss: nan
agent1:                 episode reward: 0.3225,                 loss: 0.1502
Episode: 93341/101000 (92.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3324s / 17335.9556 s
agent0:                 episode reward: -0.2518,                 loss: nan
agent1:                 episode reward: 0.2518,                 loss: 0.1508
Episode: 93361/101000 (92.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5045s / 17341.4602 s
agent0:                 episode reward: 0.2620,                 loss: nan
agent1:                 episode reward: -0.2620,                 loss: 0.1510
Episode: 93381/101000 (92.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6669s / 17346.1271 s
agent0:                 episode reward: -0.3498,                 loss: 0.2154
agent1:                 episode reward: 0.3498,                 loss: 0.1520
Score delta: 1.7363283264122458, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/92953_1.
Episode: 93401/101000 (92.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8383s / 17351.9654 s
agent0:                 episode reward: -0.0447,                 loss: 0.2319
agent1:                 episode reward: 0.0447,                 loss: nan
Episode: 93421/101000 (92.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5734s / 17357.5388 s
agent0:                 episode reward: 0.2132,                 loss: 0.2296
agent1:                 episode reward: -0.2132,                 loss: nan
Episode: 93441/101000 (92.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7348s / 17362.2736 s
agent0:                 episode reward: -0.2583,                 loss: 0.2293
agent1:                 episode reward: 0.2583,                 loss: nan
Episode: 93461/101000 (92.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1358s / 17367.4094 s
agent0:                 episode reward: -0.0799,                 loss: 0.2277
agent1:                 episode reward: 0.0799,                 loss: nan
Episode: 93481/101000 (92.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6853s / 17373.0947 s
agent0:                 episode reward: 0.1275,                 loss: 0.2293
agent1:                 episode reward: -0.1275,                 loss: nan
Episode: 93501/101000 (92.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0888s / 17378.1835 s
agent0:                 episode reward: 0.0525,                 loss: 0.2290
agent1:                 episode reward: -0.0525,                 loss: nan
Episode: 93521/101000 (92.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2242s / 17383.4077 s
agent0:                 episode reward: -0.1523,                 loss: 0.2283
agent1:                 episode reward: 0.1523,                 loss: nan
Episode: 93541/101000 (92.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2434s / 17388.6511 s
agent0:                 episode reward: -0.0161,                 loss: 0.2278
agent1:                 episode reward: 0.0161,                 loss: nan
Episode: 93561/101000 (92.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5826s / 17394.2336 s
agent0:                 episode reward: 0.0939,                 loss: 0.2290
agent1:                 episode reward: -0.0939,                 loss: nan
Episode: 93581/101000 (92.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4489s / 17398.6825 s
agent0:                 episode reward: 0.5691,                 loss: 0.2276
agent1:                 episode reward: -0.5691,                 loss: 0.1469
Score delta: 1.9374183106370242, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/93150_0.
Episode: 93601/101000 (92.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7672s / 17403.4497 s
agent0:                 episode reward: -0.1438,                 loss: nan
agent1:                 episode reward: 0.1438,                 loss: 0.1455
Episode: 93621/101000 (92.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4736s / 17408.9233 s
agent0:                 episode reward: 0.3681,                 loss: nan
agent1:                 episode reward: -0.3681,                 loss: 0.1468
Episode: 93641/101000 (92.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8119s / 17413.7352 s
agent0:                 episode reward: -0.0712,                 loss: nan
agent1:                 episode reward: 0.0712,                 loss: 0.1507
Episode: 93661/101000 (92.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8808s / 17418.6160 s
agent0:                 episode reward: 0.0126,                 loss: nan
agent1:                 episode reward: -0.0126,                 loss: 0.1510
Episode: 93681/101000 (92.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1766s / 17423.7926 s
agent0:                 episode reward: -0.6161,                 loss: nan
agent1:                 episode reward: 0.6161,                 loss: 0.1501
Episode: 93701/101000 (92.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6001s / 17429.3927 s
agent0:                 episode reward: -0.0018,                 loss: nan
agent1:                 episode reward: 0.0018,                 loss: 0.1489
Episode: 93721/101000 (92.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3375s / 17434.7302 s
agent0:                 episode reward: -0.0645,                 loss: nan
agent1:                 episode reward: 0.0645,                 loss: 0.1495
Episode: 93741/101000 (92.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5162s / 17440.2464 s
agent0:                 episode reward: -0.4506,                 loss: nan
agent1:                 episode reward: 0.4506,                 loss: 0.1490
Episode: 93761/101000 (92.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0129s / 17445.2593 s
agent0:                 episode reward: 0.1523,                 loss: nan
agent1:                 episode reward: -0.1523,                 loss: 0.1476
Episode: 93781/101000 (92.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7781s / 17451.0374 s
agent0:                 episode reward: -0.1614,                 loss: nan
agent1:                 episode reward: 0.1614,                 loss: 0.1496
Episode: 93801/101000 (92.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0713s / 17456.1086 s
agent0:                 episode reward: -0.0076,                 loss: nan
agent1:                 episode reward: 0.0076,                 loss: 0.1470
Episode: 93821/101000 (92.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7310s / 17460.8397 s
agent0:                 episode reward: -0.2809,                 loss: nan
agent1:                 episode reward: 0.2809,                 loss: 0.1486
Episode: 93841/101000 (92.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3400s / 17466.1797 s
agent0:                 episode reward: 0.2459,                 loss: nan
agent1:                 episode reward: -0.2459,                 loss: 0.1485
Episode: 93861/101000 (92.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6003s / 17471.7799 s
agent0:                 episode reward: -0.4437,                 loss: nan
agent1:                 episode reward: 0.4437,                 loss: 0.1478
Episode: 93881/101000 (92.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7051s / 17478.4851 s
agent0:                 episode reward: -0.0671,                 loss: 0.1546
agent1:                 episode reward: 0.0671,                 loss: 0.1388
Score delta: 1.585201004119822, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/93436_1.
Episode: 93901/101000 (92.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7379s / 17484.2230 s
agent0:                 episode reward: -0.4466,                 loss: 0.1443
agent1:                 episode reward: 0.4466,                 loss: nan
Episode: 93921/101000 (92.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2410s / 17489.4640 s
agent0:                 episode reward: -0.1229,                 loss: 0.1413
agent1:                 episode reward: 0.1229,                 loss: nan
Episode: 93941/101000 (93.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0892s / 17494.5532 s
agent0:                 episode reward: -0.5377,                 loss: 0.1399
agent1:                 episode reward: 0.5377,                 loss: nan
Episode: 93961/101000 (93.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8021s / 17500.3553 s
agent0:                 episode reward: 0.1802,                 loss: 0.1395
agent1:                 episode reward: -0.1802,                 loss: nan
Episode: 93981/101000 (93.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5099s / 17505.8652 s
agent0:                 episode reward: -0.0586,                 loss: 0.1378
agent1:                 episode reward: 0.0586,                 loss: nan
Episode: 94001/101000 (93.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3163s / 17511.1815 s
agent0:                 episode reward: -0.2198,                 loss: 0.1406
agent1:                 episode reward: 0.2198,                 loss: nan
Episode: 94021/101000 (93.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5488s / 17515.7302 s
agent0:                 episode reward: -0.5227,                 loss: 0.2021
agent1:                 episode reward: 0.5227,                 loss: nan
Episode: 94041/101000 (93.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1001s / 17521.8303 s
agent0:                 episode reward: -0.4366,                 loss: 0.2008
agent1:                 episode reward: 0.4366,                 loss: nan
Episode: 94061/101000 (93.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2963s / 17527.1267 s
agent0:                 episode reward: -0.6016,                 loss: 0.2003
agent1:                 episode reward: 0.6016,                 loss: nan
Episode: 94081/101000 (93.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4685s / 17532.5951 s
agent0:                 episode reward: -0.1215,                 loss: 0.1996
agent1:                 episode reward: 0.1215,                 loss: nan
Episode: 94101/101000 (93.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3492s / 17537.9443 s
agent0:                 episode reward: 0.1154,                 loss: 0.1989
agent1:                 episode reward: -0.1154,                 loss: nan
Episode: 94121/101000 (93.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6172s / 17542.5615 s
agent0:                 episode reward: -0.1652,                 loss: 0.2028
agent1:                 episode reward: 0.1652,                 loss: 0.1563
Score delta: 1.6864717377347695, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/93681_0.
Episode: 94141/101000 (93.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7952s / 17548.3567 s
agent0:                 episode reward: -0.0675,                 loss: nan
agent1:                 episode reward: 0.0675,                 loss: 0.1535
Episode: 94161/101000 (93.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6830s / 17554.0397 s
agent0:                 episode reward: -0.3386,                 loss: nan
agent1:                 episode reward: 0.3386,                 loss: 0.1531
Episode: 94181/101000 (93.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3631s / 17559.4028 s
agent0:                 episode reward: 0.0591,                 loss: nan
agent1:                 episode reward: -0.0591,                 loss: 0.1530
Episode: 94201/101000 (93.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7222s / 17566.1249 s
agent0:                 episode reward: -0.1790,                 loss: nan
agent1:                 episode reward: 0.1790,                 loss: 0.1532
Episode: 94221/101000 (93.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7791s / 17571.9040 s
agent0:                 episode reward: -0.2893,                 loss: nan
agent1:                 episode reward: 0.2893,                 loss: 0.1499
Episode: 94241/101000 (93.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0896s / 17576.9936 s
agent0:                 episode reward: -0.4238,                 loss: nan
agent1:                 episode reward: 0.4238,                 loss: 0.1505
Episode: 94261/101000 (93.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7981s / 17582.7917 s
agent0:                 episode reward: -0.3083,                 loss: nan
agent1:                 episode reward: 0.3083,                 loss: 0.1489
Episode: 94281/101000 (93.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8287s / 17588.6204 s
agent0:                 episode reward: -0.5991,                 loss: nan
agent1:                 episode reward: 0.5991,                 loss: 0.1488
Episode: 94301/101000 (93.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3935s / 17594.0139 s
agent0:                 episode reward: -0.3822,                 loss: nan
agent1:                 episode reward: 0.3822,                 loss: 0.1477
Episode: 94321/101000 (93.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8679s / 17599.8817 s
agent0:                 episode reward: 0.0303,                 loss: 0.2274
agent1:                 episode reward: -0.0303,                 loss: 0.1494
Score delta: 1.5985763318727075, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/93878_1.
Episode: 94341/101000 (93.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0529s / 17605.9346 s
agent0:                 episode reward: 0.2395,                 loss: 0.2239
agent1:                 episode reward: -0.2395,                 loss: nan
Episode: 94361/101000 (93.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1291s / 17611.0637 s
agent0:                 episode reward: -0.1833,                 loss: 0.2253
agent1:                 episode reward: 0.1833,                 loss: nan
Episode: 94381/101000 (93.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0521s / 17616.1158 s
agent0:                 episode reward: -0.1280,                 loss: 0.2253
agent1:                 episode reward: 0.1280,                 loss: nan
Episode: 94401/101000 (93.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2661s / 17620.3819 s
agent0:                 episode reward: -0.1141,                 loss: 0.2258
agent1:                 episode reward: 0.1141,                 loss: nan
Episode: 94421/101000 (93.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0797s / 17626.4616 s
agent0:                 episode reward: 0.4621,                 loss: 0.2279
agent1:                 episode reward: -0.4621,                 loss: nan
Episode: 94441/101000 (93.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2160s / 17632.6776 s
agent0:                 episode reward: 0.1668,                 loss: 0.2268
agent1:                 episode reward: -0.1668,                 loss: nan
Episode: 94461/101000 (93.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3988s / 17638.0764 s
agent0:                 episode reward: 0.5471,                 loss: 0.2251
agent1:                 episode reward: -0.5471,                 loss: nan
Episode: 94481/101000 (93.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9730s / 17643.0494 s
agent0:                 episode reward: -0.3492,                 loss: 0.2237
agent1:                 episode reward: 0.3492,                 loss: nan
Episode: 94501/101000 (93.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1874s / 17649.2368 s
agent0:                 episode reward: 0.1966,                 loss: 0.2252
agent1:                 episode reward: -0.1966,                 loss: nan
Episode: 94521/101000 (93.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9344s / 17654.1712 s
agent0:                 episode reward: -0.0788,                 loss: 0.2268
agent1:                 episode reward: 0.0788,                 loss: nan
Episode: 94541/101000 (93.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1115s / 17660.2827 s
agent0:                 episode reward: 0.0244,                 loss: 0.2236
agent1:                 episode reward: -0.0244,                 loss: nan
Episode: 94561/101000 (93.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9347s / 17665.2174 s
agent0:                 episode reward: -0.1044,                 loss: 0.2170
agent1:                 episode reward: 0.1044,                 loss: nan
Episode: 94581/101000 (93.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4596s / 17669.6770 s
agent0:                 episode reward: 0.3240,                 loss: 0.2171
agent1:                 episode reward: -0.3240,                 loss: nan
Episode: 94601/101000 (93.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0128s / 17674.6898 s
agent0:                 episode reward: -0.1375,                 loss: 0.2165
agent1:                 episode reward: 0.1375,                 loss: nan
Episode: 94621/101000 (93.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4107s / 17680.1005 s
agent0:                 episode reward: 0.5085,                 loss: 0.2145
agent1:                 episode reward: -0.5085,                 loss: 0.1423
Score delta: 1.7646229498177615, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/94191_0.
Episode: 94641/101000 (93.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2903s / 17685.3908 s
agent0:                 episode reward: -0.6176,                 loss: nan
agent1:                 episode reward: 0.6176,                 loss: 0.1399
Episode: 94661/101000 (93.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6821s / 17690.0729 s
agent0:                 episode reward: -0.2000,                 loss: nan
agent1:                 episode reward: 0.2000,                 loss: 0.1398
Episode: 94681/101000 (93.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8346s / 17694.9075 s
agent0:                 episode reward: 0.0477,                 loss: nan
agent1:                 episode reward: -0.0477,                 loss: 0.1392
Episode: 94701/101000 (93.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0972s / 17700.0048 s
agent0:                 episode reward: 0.2640,                 loss: nan
agent1:                 episode reward: -0.2640,                 loss: 0.1379
Episode: 94721/101000 (93.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0241s / 17705.0288 s
agent0:                 episode reward: -0.3295,                 loss: nan
agent1:                 episode reward: 0.3295,                 loss: 0.1380
Episode: 94741/101000 (93.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9172s / 17709.9461 s
agent0:                 episode reward: -0.0629,                 loss: nan
agent1:                 episode reward: 0.0629,                 loss: 0.1380
Episode: 94761/101000 (93.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5926s / 17714.5387 s
agent0:                 episode reward: -0.2338,                 loss: nan
agent1:                 episode reward: 0.2338,                 loss: 0.1392
Episode: 94781/101000 (93.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5771s / 17720.1158 s
agent0:                 episode reward: -0.1112,                 loss: nan
agent1:                 episode reward: 0.1112,                 loss: 0.1379
Episode: 94801/101000 (93.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8590s / 17725.9747 s
agent0:                 episode reward: -0.3393,                 loss: nan
agent1:                 episode reward: 0.3393,                 loss: 0.1371
Episode: 94821/101000 (93.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6531s / 17730.6278 s
agent0:                 episode reward: -0.0370,                 loss: nan
agent1:                 episode reward: 0.0370,                 loss: 0.1366
Episode: 94841/101000 (93.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2877s / 17735.9155 s
agent0:                 episode reward: 0.1020,                 loss: nan
agent1:                 episode reward: -0.1020,                 loss: 0.1377
Episode: 94861/101000 (93.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2728s / 17741.1883 s
agent0:                 episode reward: -0.1767,                 loss: nan
agent1:                 episode reward: 0.1767,                 loss: 0.1500
Episode: 94881/101000 (93.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6558s / 17745.8441 s
agent0:                 episode reward: 0.3893,                 loss: nan
agent1:                 episode reward: -0.3893,                 loss: 0.1525
Episode: 94901/101000 (93.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5046s / 17750.3487 s
agent0:                 episode reward: -0.0522,                 loss: nan
agent1:                 episode reward: 0.0522,                 loss: 0.1538
Episode: 94921/101000 (93.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2705s / 17756.6192 s
agent0:                 episode reward: -0.2485,                 loss: nan
agent1:                 episode reward: 0.2485,                 loss: 0.1541
Episode: 94941/101000 (94.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1932s / 17761.8123 s
agent0:                 episode reward: -0.2107,                 loss: nan
agent1:                 episode reward: 0.2107,                 loss: 0.1550
Episode: 94961/101000 (94.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2072s / 17767.0195 s
agent0:                 episode reward: -0.2045,                 loss: nan
agent1:                 episode reward: 0.2045,                 loss: 0.1542
Episode: 94981/101000 (94.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7994s / 17772.8189 s
agent0:                 episode reward: 0.1286,                 loss: nan
agent1:                 episode reward: -0.1286,                 loss: 0.1523
Episode: 95001/101000 (94.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0540s / 17777.8730 s
agent0:                 episode reward: -0.4398,                 loss: 0.2425
agent1:                 episode reward: 0.4398,                 loss: 0.1555
Score delta: 1.623308573049598, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/94566_1.
Episode: 95021/101000 (94.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1796s / 17783.0525 s
agent0:                 episode reward: 0.1461,                 loss: 0.2324
agent1:                 episode reward: -0.1461,                 loss: nan
Episode: 95041/101000 (94.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7210s / 17787.7735 s
agent0:                 episode reward: 0.1880,                 loss: 0.2327
agent1:                 episode reward: -0.1880,                 loss: nan
Episode: 95061/101000 (94.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1137s / 17792.8872 s
agent0:                 episode reward: 0.4660,                 loss: 0.2317
agent1:                 episode reward: -0.4660,                 loss: nan
Episode: 95081/101000 (94.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8055s / 17797.6927 s
agent0:                 episode reward: 0.1571,                 loss: 0.2334
agent1:                 episode reward: -0.1571,                 loss: nan
Episode: 95101/101000 (94.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8940s / 17803.5866 s
agent0:                 episode reward: -0.3740,                 loss: 0.2306
agent1:                 episode reward: 0.3740,                 loss: nan
Episode: 95121/101000 (94.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7963s / 17808.3830 s
agent0:                 episode reward: 0.1907,                 loss: 0.2304
agent1:                 episode reward: -0.1907,                 loss: nan
Episode: 95141/101000 (94.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7439s / 17813.1269 s
agent0:                 episode reward: 0.3709,                 loss: 0.2308
agent1:                 episode reward: -0.3709,                 loss: nan
Episode: 95161/101000 (94.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2957s / 17818.4226 s
agent0:                 episode reward: 0.2904,                 loss: 0.2305
agent1:                 episode reward: -0.2904,                 loss: nan
Episode: 95181/101000 (94.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8076s / 17823.2302 s
agent0:                 episode reward: 0.2478,                 loss: 0.2305
agent1:                 episode reward: -0.2478,                 loss: 0.1597
Score delta: 1.5443666788184998, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/94751_0.
Episode: 95201/101000 (94.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0143s / 17828.2444 s
agent0:                 episode reward: -0.4995,                 loss: nan
agent1:                 episode reward: 0.4995,                 loss: 0.1539
Episode: 95221/101000 (94.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5084s / 17832.7528 s
agent0:                 episode reward: 0.0291,                 loss: nan
agent1:                 episode reward: -0.0291,                 loss: 0.1534
Episode: 95241/101000 (94.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7636s / 17838.5165 s
agent0:                 episode reward: -0.4957,                 loss: nan
agent1:                 episode reward: 0.4957,                 loss: 0.1545
Episode: 95261/101000 (94.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0986s / 17842.6151 s
agent0:                 episode reward: -0.1662,                 loss: nan
agent1:                 episode reward: 0.1662,                 loss: 0.1523
Episode: 95281/101000 (94.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7246s / 17847.3396 s
agent0:                 episode reward: 0.0089,                 loss: nan
agent1:                 episode reward: -0.0089,                 loss: 0.1543
Episode: 95301/101000 (94.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7444s / 17853.0841 s
agent0:                 episode reward: 0.0253,                 loss: nan
agent1:                 episode reward: -0.0253,                 loss: 0.1545
Episode: 95321/101000 (94.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9958s / 17858.0799 s
agent0:                 episode reward: -0.0988,                 loss: nan
agent1:                 episode reward: 0.0988,                 loss: 0.1530
Episode: 95341/101000 (94.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0233s / 17864.1031 s
agent0:                 episode reward: -0.2183,                 loss: nan
agent1:                 episode reward: 0.2183,                 loss: 0.1532
Episode: 95361/101000 (94.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0994s / 17869.2025 s
agent0:                 episode reward: -0.2691,                 loss: nan
agent1:                 episode reward: 0.2691,                 loss: 0.1545
Episode: 95381/101000 (94.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7097s / 17873.9122 s
agent0:                 episode reward: -0.4228,                 loss: 0.2405
agent1:                 episode reward: 0.4228,                 loss: 0.1581
Score delta: 1.5572613928040506, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/94941_1.
Episode: 95401/101000 (94.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8818s / 17878.7940 s
agent0:                 episode reward: 0.3583,                 loss: 0.2325
agent1:                 episode reward: -0.3583,                 loss: nan
Episode: 95421/101000 (94.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8961s / 17883.6901 s
agent0:                 episode reward: 0.2418,                 loss: 0.2327
agent1:                 episode reward: -0.2418,                 loss: nan
Episode: 95441/101000 (94.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6493s / 17889.3393 s
agent0:                 episode reward: -0.1046,                 loss: 0.2245
agent1:                 episode reward: 0.1046,                 loss: nan
Episode: 95461/101000 (94.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6792s / 17895.0185 s
agent0:                 episode reward: 0.1983,                 loss: 0.2232
agent1:                 episode reward: -0.1983,                 loss: nan
Episode: 95481/101000 (94.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9851s / 17900.0036 s
agent0:                 episode reward: 0.0380,                 loss: 0.2206
agent1:                 episode reward: -0.0380,                 loss: nan
Episode: 95501/101000 (94.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3244s / 17904.3280 s
agent0:                 episode reward: -0.0863,                 loss: 0.2184
agent1:                 episode reward: 0.0863,                 loss: nan
Episode: 95521/101000 (94.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0238s / 17909.3518 s
agent0:                 episode reward: -0.0562,                 loss: 0.2199
agent1:                 episode reward: 0.0562,                 loss: nan
Episode: 95541/101000 (94.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6162s / 17914.9680 s
agent0:                 episode reward: -0.0263,                 loss: 0.2177
agent1:                 episode reward: 0.0263,                 loss: nan
Episode: 95561/101000 (94.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8663s / 17919.8342 s
agent0:                 episode reward: -0.0190,                 loss: 0.2189
agent1:                 episode reward: 0.0190,                 loss: nan
Episode: 95581/101000 (94.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7382s / 17925.5725 s
agent0:                 episode reward: -0.1346,                 loss: 0.2195
agent1:                 episode reward: 0.1346,                 loss: nan
Episode: 95601/101000 (94.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7454s / 17931.3178 s
agent0:                 episode reward: 0.0632,                 loss: 0.2192
agent1:                 episode reward: -0.0632,                 loss: nan
Episode: 95621/101000 (94.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 3.7606s / 17935.0784 s
agent0:                 episode reward: -0.0110,                 loss: 0.2182
agent1:                 episode reward: 0.0110,                 loss: nan
Episode: 95641/101000 (94.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7254s / 17940.8038 s
agent0:                 episode reward: 0.1025,                 loss: 0.2186
agent1:                 episode reward: -0.1025,                 loss: 0.1551
Score delta: 1.510369788799026, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/95204_0.
Episode: 95661/101000 (94.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5995s / 17945.4033 s
agent0:                 episode reward: -0.2445,                 loss: nan
agent1:                 episode reward: 0.2445,                 loss: 0.1522
Episode: 95681/101000 (94.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6243s / 17950.0276 s
agent0:                 episode reward: -0.3260,                 loss: nan
agent1:                 episode reward: 0.3260,                 loss: 0.1537
Episode: 95701/101000 (94.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4889s / 17954.5165 s
agent0:                 episode reward: -0.3178,                 loss: nan
agent1:                 episode reward: 0.3178,                 loss: 0.1522
Episode: 95721/101000 (94.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2814s / 17959.7979 s
agent0:                 episode reward: -0.2482,                 loss: nan
agent1:                 episode reward: 0.2482,                 loss: 0.1523
Episode: 95741/101000 (94.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5157s / 17964.3136 s
agent0:                 episode reward: -0.2980,                 loss: nan
agent1:                 episode reward: 0.2980,                 loss: 0.1541
Episode: 95761/101000 (94.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6233s / 17969.9369 s
agent0:                 episode reward: -0.0455,                 loss: nan
agent1:                 episode reward: 0.0455,                 loss: 0.1517
Episode: 95781/101000 (94.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0336s / 17974.9705 s
agent0:                 episode reward: -0.5486,                 loss: nan
agent1:                 episode reward: 0.5486,                 loss: 0.1529
Episode: 95801/101000 (94.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4682s / 17980.4386 s
agent0:                 episode reward: -0.1232,                 loss: nan
agent1:                 episode reward: 0.1232,                 loss: 0.1535
Episode: 95821/101000 (94.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7031s / 17985.1418 s
agent0:                 episode reward: -0.2018,                 loss: nan
agent1:                 episode reward: 0.2018,                 loss: 0.1530
Episode: 95841/101000 (94.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8586s / 17991.0003 s
agent0:                 episode reward: -0.1768,                 loss: nan
agent1:                 episode reward: 0.1768,                 loss: 0.1541
Episode: 95861/101000 (94.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4343s / 17996.4346 s
agent0:                 episode reward: -0.1172,                 loss: nan
agent1:                 episode reward: 0.1172,                 loss: 0.1536
Episode: 95881/101000 (94.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9983s / 18002.4329 s
agent0:                 episode reward: -0.0294,                 loss: nan
agent1:                 episode reward: 0.0294,                 loss: 0.1532
Episode: 95901/101000 (94.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4324s / 18007.8653 s
agent0:                 episode reward: -0.2905,                 loss: nan
agent1:                 episode reward: 0.2905,                 loss: 0.1527
Episode: 95921/101000 (94.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5080s / 18013.3733 s
agent0:                 episode reward: -0.3473,                 loss: nan
agent1:                 episode reward: 0.3473,                 loss: 0.1523
Episode: 95941/101000 (94.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3279s / 18018.7011 s
agent0:                 episode reward: -0.4644,                 loss: 0.2615
agent1:                 episode reward: 0.4644,                 loss: 0.1523
Score delta: 1.7433371991496824, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/95509_1.
Episode: 95961/101000 (95.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6571s / 18024.3582 s
agent0:                 episode reward: -0.3620,                 loss: 0.2418
agent1:                 episode reward: 0.3620,                 loss: nan
Episode: 95981/101000 (95.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1638s / 18029.5220 s
agent0:                 episode reward: 0.5121,                 loss: 0.2406
agent1:                 episode reward: -0.5121,                 loss: nan
Episode: 96001/101000 (95.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2992s / 18035.8212 s
agent0:                 episode reward: -0.1532,                 loss: 0.2395
agent1:                 episode reward: 0.1532,                 loss: nan
Episode: 96021/101000 (95.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6236s / 18040.4448 s
agent0:                 episode reward: -0.3067,                 loss: 0.2387
agent1:                 episode reward: 0.3067,                 loss: nan
Episode: 96041/101000 (95.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1424s / 18046.5872 s
agent0:                 episode reward: 0.2503,                 loss: 0.2423
agent1:                 episode reward: -0.2503,                 loss: nan
Episode: 96061/101000 (95.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4181s / 18052.0053 s
agent0:                 episode reward: -0.3612,                 loss: 0.2382
agent1:                 episode reward: 0.3612,                 loss: nan
Episode: 96081/101000 (95.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8107s / 18057.8160 s
agent0:                 episode reward: -0.1322,                 loss: 0.2238
agent1:                 episode reward: 0.1322,                 loss: nan
Episode: 96101/101000 (95.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8056s / 18063.6216 s
agent0:                 episode reward: -0.4163,                 loss: 0.2167
agent1:                 episode reward: 0.4163,                 loss: nan
Episode: 96121/101000 (95.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3514s / 18068.9730 s
agent0:                 episode reward: -0.0871,                 loss: 0.2161
agent1:                 episode reward: 0.0871,                 loss: nan
Episode: 96141/101000 (95.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3305s / 18074.3035 s
agent0:                 episode reward: -0.0638,                 loss: 0.2156
agent1:                 episode reward: 0.0638,                 loss: nan
Episode: 96161/101000 (95.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7907s / 18080.0942 s
agent0:                 episode reward: -0.2398,                 loss: 0.2137
agent1:                 episode reward: 0.2398,                 loss: nan
Episode: 96181/101000 (95.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7856s / 18084.8798 s
agent0:                 episode reward: -0.4819,                 loss: 0.2173
agent1:                 episode reward: 0.4819,                 loss: nan
Episode: 96201/101000 (95.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8725s / 18090.7524 s
agent0:                 episode reward: -0.4643,                 loss: 0.2165
agent1:                 episode reward: 0.4643,                 loss: nan
Episode: 96221/101000 (95.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1383s / 18096.8906 s
agent0:                 episode reward: -0.0684,                 loss: 0.2157
agent1:                 episode reward: 0.0684,                 loss: nan
Episode: 96241/101000 (95.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1817s / 18102.0724 s
agent0:                 episode reward: 0.2251,                 loss: 0.2176
agent1:                 episode reward: -0.2251,                 loss: 0.1491
Score delta: 1.5595928711429614, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/95805_0.
Episode: 96261/101000 (95.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5686s / 18106.6410 s
agent0:                 episode reward: 0.0905,                 loss: nan
agent1:                 episode reward: -0.0905,                 loss: 0.1516
Episode: 96281/101000 (95.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2823s / 18110.9233 s
agent0:                 episode reward: 0.0521,                 loss: nan
agent1:                 episode reward: -0.0521,                 loss: 0.1539
Episode: 96301/101000 (95.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0409s / 18115.9642 s
agent0:                 episode reward: -0.4207,                 loss: nan
agent1:                 episode reward: 0.4207,                 loss: 0.1523
Episode: 96321/101000 (95.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7195s / 18121.6837 s
agent0:                 episode reward: 0.1487,                 loss: nan
agent1:                 episode reward: -0.1487,                 loss: 0.1521
Episode: 96341/101000 (95.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6778s / 18127.3615 s
agent0:                 episode reward: -0.5170,                 loss: nan
agent1:                 episode reward: 0.5170,                 loss: 0.1529
Episode: 96361/101000 (95.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3303s / 18133.6918 s
agent0:                 episode reward: -0.3528,                 loss: nan
agent1:                 episode reward: 0.3528,                 loss: 0.1538
Episode: 96381/101000 (95.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3772s / 18139.0689 s
agent0:                 episode reward: -0.0940,                 loss: nan
agent1:                 episode reward: 0.0940,                 loss: 0.1528
Episode: 96401/101000 (95.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2403s / 18144.3092 s
agent0:                 episode reward: -0.1486,                 loss: nan
agent1:                 episode reward: 0.1486,                 loss: 0.1541
Episode: 96421/101000 (95.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5155s / 18149.8247 s
agent0:                 episode reward: -0.3862,                 loss: nan
agent1:                 episode reward: 0.3862,                 loss: 0.1534
Episode: 96441/101000 (95.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4373s / 18155.2620 s
agent0:                 episode reward: -0.2442,                 loss: nan
agent1:                 episode reward: 0.2442,                 loss: 0.1520
Episode: 96461/101000 (95.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3581s / 18160.6201 s
agent0:                 episode reward: -0.0457,                 loss: nan
agent1:                 episode reward: 0.0457,                 loss: 0.1522
Episode: 96481/101000 (95.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6612s / 18166.2813 s
agent0:                 episode reward: -0.1431,                 loss: nan
agent1:                 episode reward: 0.1431,                 loss: 0.1519
Episode: 96501/101000 (95.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4156s / 18171.6969 s
agent0:                 episode reward: -0.1282,                 loss: nan
agent1:                 episode reward: 0.1282,                 loss: 0.1527
Episode: 96521/101000 (95.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0493s / 18176.7461 s
agent0:                 episode reward: 0.1019,                 loss: nan
agent1:                 episode reward: -0.1019,                 loss: 0.1534
Episode: 96541/101000 (95.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8646s / 18182.6108 s
agent0:                 episode reward: -0.0625,                 loss: nan
agent1:                 episode reward: 0.0625,                 loss: 0.1521
Episode: 96561/101000 (95.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6162s / 18188.2270 s
agent0:                 episode reward: -0.6386,                 loss: 0.2443
agent1:                 episode reward: 0.6386,                 loss: 0.1535
Score delta: 1.6161024225726464, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/96121_1.
Episode: 96581/101000 (95.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3296s / 18193.5565 s
agent0:                 episode reward: -0.1714,                 loss: 0.2345
agent1:                 episode reward: 0.1714,                 loss: nan
Episode: 96601/101000 (95.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4440s / 18199.0005 s
agent0:                 episode reward: 0.0653,                 loss: 0.2323
agent1:                 episode reward: -0.0653,                 loss: nan
Episode: 96621/101000 (95.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0888s / 18205.0893 s
agent0:                 episode reward: 0.1379,                 loss: 0.2317
agent1:                 episode reward: -0.1379,                 loss: nan
Episode: 96641/101000 (95.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8085s / 18210.8978 s
agent0:                 episode reward: -0.0384,                 loss: 0.2336
agent1:                 episode reward: 0.0384,                 loss: nan
Episode: 96661/101000 (95.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1179s / 18216.0158 s
agent0:                 episode reward: 0.1279,                 loss: 0.2323
agent1:                 episode reward: -0.1279,                 loss: nan
Episode: 96681/101000 (95.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6500s / 18220.6658 s
agent0:                 episode reward: 0.2028,                 loss: 0.2295
agent1:                 episode reward: -0.2028,                 loss: nan
Episode: 96701/101000 (95.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7470s / 18227.4128 s
agent0:                 episode reward: 0.3954,                 loss: 0.2311
agent1:                 episode reward: -0.3954,                 loss: nan
Episode: 96721/101000 (95.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3618s / 18232.7746 s
agent0:                 episode reward: 0.4413,                 loss: 0.2307
agent1:                 episode reward: -0.4413,                 loss: nan
Episode: 96741/101000 (95.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3637s / 18238.1383 s
agent0:                 episode reward: 0.4139,                 loss: 0.2225
agent1:                 episode reward: -0.4139,                 loss: 0.1456
Score delta: 2.017053856253134, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/96305_0.
Episode: 96761/101000 (95.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0265s / 18243.1648 s
agent0:                 episode reward: 0.0008,                 loss: nan
agent1:                 episode reward: -0.0008,                 loss: 0.1419
Episode: 96781/101000 (95.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7310s / 18247.8958 s
agent0:                 episode reward: -0.1876,                 loss: nan
agent1:                 episode reward: 0.1876,                 loss: 0.1541
Episode: 96801/101000 (95.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3687s / 18253.2645 s
agent0:                 episode reward: -0.1858,                 loss: nan
agent1:                 episode reward: 0.1858,                 loss: 0.1652
Episode: 96821/101000 (95.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0083s / 18258.2727 s
agent0:                 episode reward: -0.5005,                 loss: nan
agent1:                 episode reward: 0.5005,                 loss: 0.1629
Episode: 96841/101000 (95.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9982s / 18264.2709 s
agent0:                 episode reward: -0.1931,                 loss: nan
agent1:                 episode reward: 0.1931,                 loss: 0.1600
Episode: 96861/101000 (95.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3790s / 18269.6500 s
agent0:                 episode reward: -0.0064,                 loss: nan
agent1:                 episode reward: 0.0064,                 loss: 0.1587
Episode: 96881/101000 (95.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1215s / 18274.7715 s
agent0:                 episode reward: 0.0566,                 loss: nan
agent1:                 episode reward: -0.0566,                 loss: 0.1589
Episode: 96901/101000 (95.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0632s / 18279.8347 s
agent0:                 episode reward: -0.2334,                 loss: nan
agent1:                 episode reward: 0.2334,                 loss: 0.1587
Episode: 96921/101000 (95.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8091s / 18284.6438 s
agent0:                 episode reward: -0.4302,                 loss: nan
agent1:                 episode reward: 0.4302,                 loss: 0.1582
Episode: 96941/101000 (95.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4066s / 18290.0504 s
agent0:                 episode reward: 0.0029,                 loss: nan
agent1:                 episode reward: -0.0029,                 loss: 0.1595
Episode: 96961/101000 (96.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7722s / 18295.8226 s
agent0:                 episode reward: -0.0278,                 loss: nan
agent1:                 episode reward: 0.0278,                 loss: 0.1587
Episode: 96981/101000 (96.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7553s / 18300.5779 s
agent0:                 episode reward: -0.2092,                 loss: nan
agent1:                 episode reward: 0.2092,                 loss: 0.1588
Episode: 97001/101000 (96.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4628s / 18306.0407 s
agent0:                 episode reward: -0.3365,                 loss: nan
agent1:                 episode reward: 0.3365,                 loss: 0.1585
Episode: 97021/101000 (96.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5349s / 18311.5756 s
agent0:                 episode reward: -0.5158,                 loss: 0.2298
agent1:                 episode reward: 0.5158,                 loss: 0.1576
Score delta: 1.5292538689578081, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/96588_1.
Episode: 97041/101000 (96.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1831s / 18316.7588 s
agent0:                 episode reward: 0.1105,                 loss: 0.2211
agent1:                 episode reward: -0.1105,                 loss: nan
Episode: 97061/101000 (96.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9510s / 18322.7098 s
agent0:                 episode reward: 0.0647,                 loss: 0.2182
agent1:                 episode reward: -0.0647,                 loss: nan
Episode: 97081/101000 (96.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0965s / 18328.8062 s
agent0:                 episode reward: -0.1918,                 loss: 0.2189
agent1:                 episode reward: 0.1918,                 loss: nan
Episode: 97101/101000 (96.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1425s / 18333.9487 s
agent0:                 episode reward: 0.2416,                 loss: 0.2162
agent1:                 episode reward: -0.2416,                 loss: nan
Episode: 97121/101000 (96.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4061s / 18339.3548 s
agent0:                 episode reward: 0.1809,                 loss: 0.2174
agent1:                 episode reward: -0.1809,                 loss: nan
Episode: 97141/101000 (96.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4528s / 18345.8077 s
agent0:                 episode reward: 0.3282,                 loss: 0.2187
agent1:                 episode reward: -0.3282,                 loss: nan
Episode: 97161/101000 (96.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5446s / 18350.3523 s
agent0:                 episode reward: 0.0519,                 loss: 0.2162
agent1:                 episode reward: -0.0519,                 loss: nan
Episode: 97181/101000 (96.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8787s / 18355.2311 s
agent0:                 episode reward: 0.0232,                 loss: 0.2163
agent1:                 episode reward: -0.0232,                 loss: nan
Episode: 97201/101000 (96.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8172s / 18361.0483 s
agent0:                 episode reward: -0.1512,                 loss: 0.2149
agent1:                 episode reward: 0.1512,                 loss: nan
Episode: 97221/101000 (96.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0664s / 18366.1146 s
agent0:                 episode reward: 0.3483,                 loss: 0.2152
agent1:                 episode reward: -0.3483,                 loss: nan
Episode: 97241/101000 (96.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4227s / 18371.5374 s
agent0:                 episode reward: 0.6296,                 loss: 0.2164
agent1:                 episode reward: -0.6296,                 loss: 0.1490
Score delta: 1.6898834573707144, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/96814_0.
Episode: 97261/101000 (96.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1075s / 18376.6449 s
agent0:                 episode reward: -0.0301,                 loss: nan
agent1:                 episode reward: 0.0301,                 loss: 0.1534
Episode: 97281/101000 (96.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1004s / 18381.7453 s
agent0:                 episode reward: 0.0367,                 loss: nan
agent1:                 episode reward: -0.0367,                 loss: 0.1506
Episode: 97301/101000 (96.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3757s / 18388.1210 s
agent0:                 episode reward: 0.0734,                 loss: nan
agent1:                 episode reward: -0.0734,                 loss: 0.1515
Episode: 97321/101000 (96.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1926s / 18393.3136 s
agent0:                 episode reward: -0.1269,                 loss: nan
agent1:                 episode reward: 0.1269,                 loss: 0.1521
Episode: 97341/101000 (96.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6116s / 18398.9252 s
agent0:                 episode reward: 0.1851,                 loss: nan
agent1:                 episode reward: -0.1851,                 loss: 0.1525
Episode: 97361/101000 (96.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9442s / 18403.8695 s
agent0:                 episode reward: -0.1878,                 loss: nan
agent1:                 episode reward: 0.1878,                 loss: 0.1573
Episode: 97381/101000 (96.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1533s / 18408.0228 s
agent0:                 episode reward: 0.0536,                 loss: nan
agent1:                 episode reward: -0.0536,                 loss: 0.1573
Episode: 97401/101000 (96.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8465s / 18413.8693 s
agent0:                 episode reward: -0.0079,                 loss: nan
agent1:                 episode reward: 0.0079,                 loss: 0.1554
Episode: 97421/101000 (96.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2416s / 18420.1110 s
agent0:                 episode reward: -0.4351,                 loss: nan
agent1:                 episode reward: 0.4351,                 loss: 0.1572
Episode: 97441/101000 (96.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4750s / 18425.5860 s
agent0:                 episode reward: -0.2701,                 loss: 0.2253
agent1:                 episode reward: 0.2701,                 loss: 0.1564
Score delta: 1.5163103333303423, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/97003_1.
Episode: 97461/101000 (96.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9193s / 18430.5053 s
agent0:                 episode reward: 0.1210,                 loss: 0.2261
agent1:                 episode reward: -0.1210,                 loss: nan
Episode: 97481/101000 (96.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9137s / 18435.4191 s
agent0:                 episode reward: -0.3006,                 loss: 0.2284
agent1:                 episode reward: 0.3006,                 loss: nan
Episode: 97501/101000 (96.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8500s / 18440.2691 s
agent0:                 episode reward: -0.0339,                 loss: 0.2266
agent1:                 episode reward: 0.0339,                 loss: nan
Episode: 97521/101000 (96.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0103s / 18446.2794 s
agent0:                 episode reward: 0.1900,                 loss: 0.2272
agent1:                 episode reward: -0.1900,                 loss: nan
Episode: 97541/101000 (96.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8725s / 18452.1519 s
agent0:                 episode reward: 0.3520,                 loss: 0.2102
agent1:                 episode reward: -0.3520,                 loss: nan
Episode: 97561/101000 (96.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1842s / 18457.3361 s
agent0:                 episode reward: 0.0834,                 loss: 0.2096
agent1:                 episode reward: -0.0834,                 loss: nan
Episode: 97581/101000 (96.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1428s / 18462.4789 s
agent0:                 episode reward: 0.2409,                 loss: 0.2086
agent1:                 episode reward: -0.2409,                 loss: nan
Episode: 97601/101000 (96.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2931s / 18467.7720 s
agent0:                 episode reward: 0.1344,                 loss: 0.2088
agent1:                 episode reward: -0.1344,                 loss: nan
Episode: 97621/101000 (96.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8739s / 18472.6460 s
agent0:                 episode reward: -0.3528,                 loss: 0.2059
agent1:                 episode reward: 0.3528,                 loss: nan
Episode: 97641/101000 (96.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4400s / 18477.0859 s
agent0:                 episode reward: -0.0482,                 loss: 0.2075
agent1:                 episode reward: 0.0482,                 loss: nan
Episode: 97661/101000 (96.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4364s / 18482.5224 s
agent0:                 episode reward: 0.2434,                 loss: 0.2069
agent1:                 episode reward: -0.2434,                 loss: nan
Episode: 97681/101000 (96.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4422s / 18486.9646 s
agent0:                 episode reward: 0.1493,                 loss: 0.2069
agent1:                 episode reward: -0.1493,                 loss: 0.1490
Score delta: 1.6187434564011167, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/97249_0.
Episode: 97701/101000 (96.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3350s / 18492.2995 s
agent0:                 episode reward: -0.7034,                 loss: nan
agent1:                 episode reward: 0.7034,                 loss: 0.1486
Episode: 97721/101000 (96.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0777s / 18496.3772 s
agent0:                 episode reward: -0.1944,                 loss: nan
agent1:                 episode reward: 0.1944,                 loss: 0.1479
Episode: 97741/101000 (96.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2638s / 18501.6411 s
agent0:                 episode reward: -0.1997,                 loss: nan
agent1:                 episode reward: 0.1997,                 loss: 0.1456
Episode: 97761/101000 (96.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2924s / 18506.9335 s
agent0:                 episode reward: -0.5255,                 loss: nan
agent1:                 episode reward: 0.5255,                 loss: 0.1480
Episode: 97781/101000 (96.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6133s / 18512.5468 s
agent0:                 episode reward: -0.1437,                 loss: nan
agent1:                 episode reward: 0.1437,                 loss: 0.1474
Episode: 97801/101000 (96.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3678s / 18517.9146 s
agent0:                 episode reward: -0.1808,                 loss: nan
agent1:                 episode reward: 0.1808,                 loss: 0.1471
Episode: 97821/101000 (96.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7612s / 18523.6758 s
agent0:                 episode reward: -0.1431,                 loss: nan
agent1:                 episode reward: 0.1431,                 loss: 0.1458
Episode: 97841/101000 (96.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7644s / 18529.4401 s
agent0:                 episode reward: -0.1191,                 loss: nan
agent1:                 episode reward: 0.1191,                 loss: 0.1469
Episode: 97861/101000 (96.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 6.6341s / 18536.0743 s
agent0:                 episode reward: -0.0666,                 loss: nan
agent1:                 episode reward: 0.0666,                 loss: 0.1459
Episode: 97881/101000 (96.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6573s / 18541.7316 s
agent0:                 episode reward: -0.0890,                 loss: 0.2209
agent1:                 episode reward: 0.0890,                 loss: 0.1468
Score delta: 1.6653922301339221, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/97442_1.
Episode: 97901/101000 (96.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7973s / 18547.5289 s
agent0:                 episode reward: -0.0592,                 loss: 0.2171
agent1:                 episode reward: 0.0592,                 loss: nan
Episode: 97921/101000 (96.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7008s / 18553.2297 s
agent0:                 episode reward: -0.1404,                 loss: 0.2155
agent1:                 episode reward: 0.1404,                 loss: nan
Episode: 97941/101000 (96.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0210s / 18558.2507 s
agent0:                 episode reward: 0.2548,                 loss: 0.2150
agent1:                 episode reward: -0.2548,                 loss: nan
Episode: 97961/101000 (96.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4685s / 18562.7193 s
agent0:                 episode reward: 0.1259,                 loss: 0.2160
agent1:                 episode reward: -0.1259,                 loss: nan
Episode: 97981/101000 (97.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9244s / 18567.6436 s
agent0:                 episode reward: 0.5610,                 loss: 0.2133
agent1:                 episode reward: -0.5610,                 loss: nan
Episode: 98001/101000 (97.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5469s / 18573.1905 s
agent0:                 episode reward: -0.1912,                 loss: 0.2160
agent1:                 episode reward: 0.1912,                 loss: nan
Episode: 98021/101000 (97.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3261s / 18578.5165 s
agent0:                 episode reward: -0.2830,                 loss: 0.2157
agent1:                 episode reward: 0.2830,                 loss: nan
Episode: 98041/101000 (97.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3815s / 18583.8980 s
agent0:                 episode reward: -0.1476,                 loss: 0.2141
agent1:                 episode reward: 0.1476,                 loss: nan
Episode: 98061/101000 (97.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1074s / 18589.0055 s
agent0:                 episode reward: -0.0446,                 loss: 0.2133
agent1:                 episode reward: 0.0446,                 loss: nan
Episode: 98081/101000 (97.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0870s / 18594.0925 s
agent0:                 episode reward: 0.1979,                 loss: 0.2105
agent1:                 episode reward: -0.1979,                 loss: nan
Episode: 98101/101000 (97.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1923s / 18598.2848 s
agent0:                 episode reward: 0.3607,                 loss: 0.2106
agent1:                 episode reward: -0.3607,                 loss: 0.1525
Score delta: 1.641841294421333, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/97671_0.
Episode: 98121/101000 (97.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2557s / 18603.5405 s
agent0:                 episode reward: -0.1265,                 loss: nan
agent1:                 episode reward: 0.1265,                 loss: 0.1525
Episode: 98141/101000 (97.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 4.4649s / 18608.0054 s
agent0:                 episode reward: -0.1837,                 loss: nan
agent1:                 episode reward: 0.1837,                 loss: 0.1530
Episode: 98161/101000 (97.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9972s / 18613.0027 s
agent0:                 episode reward: 0.3015,                 loss: nan
agent1:                 episode reward: -0.3015,                 loss: 0.1534
Episode: 98181/101000 (97.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2600s / 18618.2627 s
agent0:                 episode reward: -0.0847,                 loss: nan
agent1:                 episode reward: 0.0847,                 loss: 0.1527
Episode: 98201/101000 (97.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3611s / 18623.6237 s
agent0:                 episode reward: -0.4697,                 loss: nan
agent1:                 episode reward: 0.4697,                 loss: 0.1530
Episode: 98221/101000 (97.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4729s / 18629.0966 s
agent0:                 episode reward: -0.3852,                 loss: nan
agent1:                 episode reward: 0.3852,                 loss: 0.1523
Episode: 98241/101000 (97.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4781s / 18635.5747 s
agent0:                 episode reward: -0.6616,                 loss: nan
agent1:                 episode reward: 0.6616,                 loss: 0.1513
Episode: 98261/101000 (97.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8004s / 18641.3751 s
agent0:                 episode reward: -0.4349,                 loss: nan
agent1:                 episode reward: 0.4349,                 loss: 0.1528
Episode: 98281/101000 (97.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1738s / 18646.5489 s
agent0:                 episode reward: -0.2583,                 loss: nan
agent1:                 episode reward: 0.2583,                 loss: 0.1523
Episode: 98301/101000 (97.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6158s / 18652.1647 s
agent0:                 episode reward: -0.2339,                 loss: nan
agent1:                 episode reward: 0.2339,                 loss: 0.1531
Episode: 98321/101000 (97.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9603s / 18657.1250 s
agent0:                 episode reward: -0.4593,                 loss: 0.2172
agent1:                 episode reward: 0.4593,                 loss: 0.1555
Score delta: 1.754466142105755, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/97877_1.
Episode: 98341/101000 (97.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1495s / 18662.2745 s
agent0:                 episode reward: 0.0612,                 loss: 0.2157
agent1:                 episode reward: -0.0612,                 loss: nan
Episode: 98361/101000 (97.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8289s / 18668.1033 s
agent0:                 episode reward: 0.0402,                 loss: 0.2168
agent1:                 episode reward: -0.0402,                 loss: nan
Episode: 98381/101000 (97.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7322s / 18673.8355 s
agent0:                 episode reward: 0.4464,                 loss: 0.2157
agent1:                 episode reward: -0.4464,                 loss: nan
Episode: 98401/101000 (97.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4579s / 18679.2934 s
agent0:                 episode reward: 0.3289,                 loss: 0.2152
agent1:                 episode reward: -0.3289,                 loss: nan
Episode: 98421/101000 (97.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7470s / 18685.0405 s
agent0:                 episode reward: 0.2752,                 loss: 0.2143
agent1:                 episode reward: -0.2752,                 loss: nan
Episode: 98441/101000 (97.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4049s / 18690.4454 s
agent0:                 episode reward: -0.0315,                 loss: 0.2151
agent1:                 episode reward: 0.0315,                 loss: nan
Episode: 98461/101000 (97.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2065s / 18695.6519 s
agent0:                 episode reward: 0.1590,                 loss: 0.2157
agent1:                 episode reward: -0.1590,                 loss: nan
Episode: 98481/101000 (97.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1233s / 18701.7751 s
agent0:                 episode reward: -0.0274,                 loss: 0.2165
agent1:                 episode reward: 0.0274,                 loss: nan
Episode: 98501/101000 (97.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7600s / 18707.5352 s
agent0:                 episode reward: 0.2068,                 loss: 0.2151
agent1:                 episode reward: -0.2068,                 loss: nan
Episode: 98521/101000 (97.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1800s / 18712.7152 s
agent0:                 episode reward: -0.2134,                 loss: 0.2143
agent1:                 episode reward: 0.2134,                 loss: nan
Episode: 98541/101000 (97.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9667s / 18718.6819 s
agent0:                 episode reward: 0.1033,                 loss: 0.2132
agent1:                 episode reward: -0.1033,                 loss: nan
Episode: 98561/101000 (97.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3170s / 18723.9989 s
agent0:                 episode reward: 0.2809,                 loss: 0.2147
agent1:                 episode reward: -0.2809,                 loss: nan
Episode: 98581/101000 (97.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8330s / 18728.8319 s
agent0:                 episode reward: 0.2942,                 loss: 0.2163
agent1:                 episode reward: -0.2942,                 loss: 0.1476
Score delta: 1.5375193296991334, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/98149_0.
Episode: 98601/101000 (97.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1664s / 18733.9983 s
agent0:                 episode reward: 0.2672,                 loss: nan
agent1:                 episode reward: -0.2672,                 loss: 0.1455
Episode: 98621/101000 (97.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2955s / 18739.2938 s
agent0:                 episode reward: 0.2938,                 loss: nan
agent1:                 episode reward: -0.2938,                 loss: 0.1434
Episode: 98641/101000 (97.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5426s / 18744.8364 s
agent0:                 episode reward: -0.4193,                 loss: nan
agent1:                 episode reward: 0.4193,                 loss: 0.1431
Episode: 98661/101000 (97.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5935s / 18750.4299 s
agent0:                 episode reward: 0.3526,                 loss: nan
agent1:                 episode reward: -0.3526,                 loss: 0.1430
Episode: 98681/101000 (97.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9782s / 18755.4082 s
agent0:                 episode reward: -0.4372,                 loss: nan
agent1:                 episode reward: 0.4372,                 loss: 0.1418
Episode: 98701/101000 (97.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3560s / 18760.7641 s
agent0:                 episode reward: 0.2800,                 loss: nan
agent1:                 episode reward: -0.2800,                 loss: 0.1427
Episode: 98721/101000 (97.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1193s / 18764.8835 s
agent0:                 episode reward: 0.1608,                 loss: nan
agent1:                 episode reward: -0.1608,                 loss: 0.1416
Episode: 98741/101000 (97.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0283s / 18769.9118 s
agent0:                 episode reward: -0.3607,                 loss: nan
agent1:                 episode reward: 0.3607,                 loss: 0.1407
Episode: 98761/101000 (97.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7436s / 18775.6554 s
agent0:                 episode reward: 0.1471,                 loss: nan
agent1:                 episode reward: -0.1471,                 loss: 0.1644
Episode: 98781/101000 (97.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4460s / 18781.1014 s
agent0:                 episode reward: -0.4649,                 loss: nan
agent1:                 episode reward: 0.4649,                 loss: 0.1661
Episode: 98801/101000 (97.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8432s / 18785.9446 s
agent0:                 episode reward: 0.0528,                 loss: nan
agent1:                 episode reward: -0.0528,                 loss: 0.1641
Episode: 98821/101000 (97.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6830s / 18791.6276 s
agent0:                 episode reward: -0.3636,                 loss: nan
agent1:                 episode reward: 0.3636,                 loss: 0.1638
Episode: 98841/101000 (97.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0690s / 18796.6966 s
agent0:                 episode reward: -0.3737,                 loss: nan
agent1:                 episode reward: 0.3737,                 loss: 0.1641
Episode: 98861/101000 (97.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8468s / 18802.5434 s
agent0:                 episode reward: 0.3296,                 loss: 0.2208
agent1:                 episode reward: -0.3296,                 loss: 0.1614
Score delta: 1.5039727214242617, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/98416_1.
Episode: 98881/101000 (97.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3336s / 18807.8770 s
agent0:                 episode reward: -0.0970,                 loss: 0.2189
agent1:                 episode reward: 0.0970,                 loss: nan
Episode: 98901/101000 (97.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8932s / 18812.7702 s
agent0:                 episode reward: -0.2646,                 loss: 0.2175
agent1:                 episode reward: 0.2646,                 loss: nan
Episode: 98921/101000 (97.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9605s / 18818.7308 s
agent0:                 episode reward: -0.2262,                 loss: 0.2171
agent1:                 episode reward: 0.2262,                 loss: nan
Episode: 98941/101000 (97.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7105s / 18824.4413 s
agent0:                 episode reward: -0.4292,                 loss: 0.2169
agent1:                 episode reward: 0.4292,                 loss: nan
Episode: 98961/101000 (97.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6616s / 18830.1029 s
agent0:                 episode reward: 0.0654,                 loss: 0.2153
agent1:                 episode reward: -0.0654,                 loss: nan
Episode: 98981/101000 (98.0010%),                 avg. length: 2.0,                last time consumption/overall running time: 4.1705s / 18834.2734 s
agent0:                 episode reward: 0.0046,                 loss: 0.2165
agent1:                 episode reward: -0.0046,                 loss: nan
Episode: 99001/101000 (98.0208%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0214s / 18840.2948 s
agent0:                 episode reward: 0.2322,                 loss: 0.2168
agent1:                 episode reward: -0.2322,                 loss: nan
Episode: 99021/101000 (98.0406%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6084s / 18845.9032 s
agent0:                 episode reward: 0.0792,                 loss: 0.2144
agent1:                 episode reward: -0.0792,                 loss: nan
Episode: 99041/101000 (98.0604%),                 avg. length: 2.0,                last time consumption/overall running time: 6.2824s / 18852.1856 s
agent0:                 episode reward: 0.1631,                 loss: 0.2171
agent1:                 episode reward: -0.1631,                 loss: nan
Episode: 99061/101000 (98.0802%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2498s / 18857.4354 s
agent0:                 episode reward: 0.2999,                 loss: 0.2154
agent1:                 episode reward: -0.2999,                 loss: 0.1460
Score delta: 1.674510118293636, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/98628_0.
Episode: 99081/101000 (98.1000%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8032s / 18863.2386 s
agent0:                 episode reward: 0.0719,                 loss: nan
agent1:                 episode reward: -0.0719,                 loss: 0.1483
Episode: 99101/101000 (98.1198%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9783s / 18869.2169 s
agent0:                 episode reward: -0.4348,                 loss: nan
agent1:                 episode reward: 0.4348,                 loss: 0.1501
Episode: 99121/101000 (98.1396%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3157s / 18873.5325 s
agent0:                 episode reward: -0.2242,                 loss: nan
agent1:                 episode reward: 0.2242,                 loss: 0.1470
Episode: 99141/101000 (98.1594%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5744s / 18879.1070 s
agent0:                 episode reward: -0.0639,                 loss: nan
agent1:                 episode reward: 0.0639,                 loss: 0.1488
Episode: 99161/101000 (98.1792%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1901s / 18884.2970 s
agent0:                 episode reward: -0.4354,                 loss: nan
agent1:                 episode reward: 0.4354,                 loss: 0.1490
Episode: 99181/101000 (98.1990%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5000s / 18889.7970 s
agent0:                 episode reward: -0.1745,                 loss: nan
agent1:                 episode reward: 0.1745,                 loss: 0.1478
Episode: 99201/101000 (98.2188%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6039s / 18895.4008 s
agent0:                 episode reward: -0.2164,                 loss: nan
agent1:                 episode reward: 0.2164,                 loss: 0.1485
Episode: 99221/101000 (98.2386%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3031s / 18900.7040 s
agent0:                 episode reward: -0.2813,                 loss: nan
agent1:                 episode reward: 0.2813,                 loss: 0.1469
Episode: 99241/101000 (98.2584%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7889s / 18906.4929 s
agent0:                 episode reward: -0.3281,                 loss: nan
agent1:                 episode reward: 0.3281,                 loss: 0.1479
Episode: 99261/101000 (98.2782%),                 avg. length: 2.0,                last time consumption/overall running time: 4.5813s / 18911.0742 s
agent0:                 episode reward: -0.5882,                 loss: nan
agent1:                 episode reward: 0.5882,                 loss: 0.1478
Episode: 99281/101000 (98.2980%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3235s / 18916.3977 s
agent0:                 episode reward: -0.1037,                 loss: nan
agent1:                 episode reward: 0.1037,                 loss: 0.1491
Episode: 99301/101000 (98.3178%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6321s / 18921.0299 s
agent0:                 episode reward: -0.0655,                 loss: nan
agent1:                 episode reward: 0.0655,                 loss: 0.1522
Episode: 99321/101000 (98.3376%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1984s / 18926.2282 s
agent0:                 episode reward: -0.0741,                 loss: nan
agent1:                 episode reward: 0.0741,                 loss: 0.1551
Episode: 99341/101000 (98.3574%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9386s / 18932.1668 s
agent0:                 episode reward: -0.2903,                 loss: nan
agent1:                 episode reward: 0.2903,                 loss: 0.1557
Episode: 99361/101000 (98.3772%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4028s / 18937.5697 s
agent0:                 episode reward: 0.0391,                 loss: 0.2174
agent1:                 episode reward: -0.0391,                 loss: 0.1553
Score delta: 1.5790990626958104, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/98918_1.
Episode: 99381/101000 (98.3970%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7466s / 18943.3163 s
agent0:                 episode reward: 0.2393,                 loss: 0.2173
agent1:                 episode reward: -0.2393,                 loss: nan
Episode: 99401/101000 (98.4168%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7591s / 18948.0754 s
agent0:                 episode reward: -0.0539,                 loss: 0.2184
agent1:                 episode reward: 0.0539,                 loss: nan
Episode: 99421/101000 (98.4366%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0863s / 18953.1617 s
agent0:                 episode reward: 0.2150,                 loss: 0.2180
agent1:                 episode reward: -0.2150,                 loss: nan
Episode: 99441/101000 (98.4564%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8277s / 18957.9894 s
agent0:                 episode reward: 0.2303,                 loss: 0.2179
agent1:                 episode reward: -0.2303,                 loss: nan
Episode: 99461/101000 (98.4762%),                 avg. length: 2.0,                last time consumption/overall running time: 5.9839s / 18963.9733 s
agent0:                 episode reward: -0.2285,                 loss: 0.2156
agent1:                 episode reward: 0.2285,                 loss: nan
Episode: 99481/101000 (98.4960%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0483s / 18970.0216 s
agent0:                 episode reward: -0.0755,                 loss: 0.2143
agent1:                 episode reward: 0.0755,                 loss: nan
Episode: 99501/101000 (98.5158%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9900s / 18975.0116 s
agent0:                 episode reward: 0.1702,                 loss: 0.2130
agent1:                 episode reward: -0.1702,                 loss: nan
Episode: 99521/101000 (98.5356%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2283s / 18980.2400 s
agent0:                 episode reward: 0.5406,                 loss: 0.2129
agent1:                 episode reward: -0.5406,                 loss: nan
Episode: 99541/101000 (98.5554%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6578s / 18984.8978 s
agent0:                 episode reward: 0.2718,                 loss: 0.2132
agent1:                 episode reward: -0.2718,                 loss: nan
Episode: 99561/101000 (98.5752%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4043s / 18990.3021 s
agent0:                 episode reward: 0.3175,                 loss: 0.2130
agent1:                 episode reward: -0.3175,                 loss: nan
Episode: 99581/101000 (98.5950%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3255s / 18995.6276 s
agent0:                 episode reward: -0.2629,                 loss: 0.2122
agent1:                 episode reward: 0.2629,                 loss: nan
Episode: 99601/101000 (98.6149%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0034s / 19001.6310 s
agent0:                 episode reward: 0.2023,                 loss: 0.2139
agent1:                 episode reward: -0.2023,                 loss: nan
Episode: 99621/101000 (98.6347%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0437s / 19006.6747 s
agent0:                 episode reward: 0.2078,                 loss: 0.2136
agent1:                 episode reward: -0.2078,                 loss: nan
Episode: 99641/101000 (98.6545%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8907s / 19011.5654 s
agent0:                 episode reward: -0.0023,                 loss: 0.2121
agent1:                 episode reward: 0.0023,                 loss: nan
Episode: 99661/101000 (98.6743%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7807s / 19016.3461 s
agent0:                 episode reward: -0.0708,                 loss: 0.2140
agent1:                 episode reward: 0.0708,                 loss: nan
Episode: 99681/101000 (98.6941%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0904s / 19021.4365 s
agent0:                 episode reward: 0.0426,                 loss: 0.2136
agent1:                 episode reward: -0.0426,                 loss: nan
Episode: 99701/101000 (98.7139%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8523s / 19027.2889 s
agent0:                 episode reward: -0.2021,                 loss: 0.2132
agent1:                 episode reward: 0.2021,                 loss: nan
Episode: 99721/101000 (98.7337%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0245s / 19032.3134 s
agent0:                 episode reward: -0.0271,                 loss: 0.2110
agent1:                 episode reward: 0.0271,                 loss: nan
Episode: 99741/101000 (98.7535%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7848s / 19038.0982 s
agent0:                 episode reward: 0.2543,                 loss: 0.2112
agent1:                 episode reward: -0.2543,                 loss: nan
Episode: 99761/101000 (98.7733%),                 avg. length: 2.0,                last time consumption/overall running time: 6.3671s / 19044.4653 s
agent0:                 episode reward: 0.2928,                 loss: 0.2114
agent1:                 episode reward: -0.2928,                 loss: 0.1459
Score delta: 1.7302129556892125, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/99327_0.
Episode: 99781/101000 (98.7931%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9163s / 19049.3816 s
agent0:                 episode reward: -0.4671,                 loss: nan
agent1:                 episode reward: 0.4671,                 loss: 0.1488
Episode: 99801/101000 (98.8129%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9470s / 19054.3286 s
agent0:                 episode reward: -0.4904,                 loss: nan
agent1:                 episode reward: 0.4904,                 loss: 0.1478
Episode: 99821/101000 (98.8327%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8272s / 19060.1558 s
agent0:                 episode reward: -0.4140,                 loss: nan
agent1:                 episode reward: 0.4140,                 loss: 0.1455
Episode: 99841/101000 (98.8525%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6420s / 19065.7978 s
agent0:                 episode reward: 0.0842,                 loss: nan
agent1:                 episode reward: -0.0842,                 loss: 0.1464
Episode: 99861/101000 (98.8723%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6127s / 19071.4105 s
agent0:                 episode reward: -0.0220,                 loss: nan
agent1:                 episode reward: 0.0220,                 loss: 0.1454
Episode: 99881/101000 (98.8921%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7761s / 19078.1867 s
agent0:                 episode reward: -0.2569,                 loss: nan
agent1:                 episode reward: 0.2569,                 loss: 0.1480
Episode: 99901/101000 (98.9119%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5935s / 19083.7801 s
agent0:                 episode reward: -0.4011,                 loss: nan
agent1:                 episode reward: 0.4011,                 loss: 0.1468
Episode: 99921/101000 (98.9317%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2522s / 19089.0323 s
agent0:                 episode reward: -0.1922,                 loss: nan
agent1:                 episode reward: 0.1922,                 loss: 0.1457
Episode: 99941/101000 (98.9515%),                 avg. length: 2.0,                last time consumption/overall running time: 4.6906s / 19093.7230 s
agent0:                 episode reward: -0.4100,                 loss: nan
agent1:                 episode reward: 0.4100,                 loss: 0.1449
Episode: 99961/101000 (98.9713%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9219s / 19098.6448 s
agent0:                 episode reward: -0.4186,                 loss: 0.2486
agent1:                 episode reward: 0.4186,                 loss: 0.1449
Score delta: 1.6357972198332735, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/99531_1.
Episode: 99981/101000 (98.9911%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2122s / 19102.8570 s
agent0:                 episode reward: 0.1239,                 loss: 0.2415
agent1:                 episode reward: -0.1239,                 loss: nan
Episode: 100001/101000 (99.0109%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1265s / 19107.9835 s
agent0:                 episode reward: -0.0569,                 loss: 0.2419
agent1:                 episode reward: 0.0569,                 loss: nan
Episode: 100021/101000 (99.0307%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6668s / 19113.6503 s
agent0:                 episode reward: 0.3638,                 loss: 0.2376
agent1:                 episode reward: -0.3638,                 loss: nan
Episode: 100041/101000 (99.0505%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8900s / 19118.5403 s
agent0:                 episode reward: -0.1771,                 loss: 0.2193
agent1:                 episode reward: 0.1771,                 loss: nan
Episode: 100061/101000 (99.0703%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5272s / 19124.0675 s
agent0:                 episode reward: -0.1274,                 loss: 0.2181
agent1:                 episode reward: 0.1274,                 loss: nan
Episode: 100081/101000 (99.0901%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5829s / 19129.6504 s
agent0:                 episode reward: 0.0297,                 loss: 0.2185
agent1:                 episode reward: -0.0297,                 loss: nan
Episode: 100101/101000 (99.1099%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6408s / 19135.2911 s
agent0:                 episode reward: 0.1802,                 loss: 0.2156
agent1:                 episode reward: -0.1802,                 loss: nan
Episode: 100121/101000 (99.1297%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7145s / 19141.0056 s
agent0:                 episode reward: 0.0377,                 loss: 0.2162
agent1:                 episode reward: -0.0377,                 loss: nan
Episode: 100141/101000 (99.1495%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7596s / 19146.7652 s
agent0:                 episode reward: -0.2348,                 loss: 0.2158
agent1:                 episode reward: 0.2348,                 loss: nan
Episode: 100161/101000 (99.1693%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0769s / 19152.8421 s
agent0:                 episode reward: 0.1855,                 loss: 0.2167
agent1:                 episode reward: -0.1855,                 loss: nan
Episode: 100181/101000 (99.1891%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2691s / 19158.1112 s
agent0:                 episode reward: -0.0693,                 loss: 0.2158
agent1:                 episode reward: 0.0693,                 loss: nan
Episode: 100201/101000 (99.2089%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6800s / 19163.7912 s
agent0:                 episode reward: 0.2893,                 loss: 0.2150
agent1:                 episode reward: -0.2893,                 loss: nan
Episode: 100221/101000 (99.2287%),                 avg. length: 2.0,                last time consumption/overall running time: 4.7394s / 19168.5307 s
agent0:                 episode reward: -0.1827,                 loss: 0.2155
agent1:                 episode reward: 0.1827,                 loss: nan
Episode: 100241/101000 (99.2485%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7113s / 19175.2420 s
agent0:                 episode reward: 0.2637,                 loss: 0.2157
agent1:                 episode reward: -0.2637,                 loss: nan
Episode: 100261/101000 (99.2683%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8819s / 19180.1239 s
agent0:                 episode reward: -0.4244,                 loss: 0.2169
agent1:                 episode reward: 0.4244,                 loss: nan
Episode: 100281/101000 (99.2881%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6049s / 19185.7288 s
agent0:                 episode reward: 0.4030,                 loss: 0.2160
agent1:                 episode reward: -0.4030,                 loss: nan
Score delta: 1.959698446222377, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/99855_0.
Episode: 100301/101000 (99.3079%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1501s / 19191.8789 s
agent0:                 episode reward: -0.1263,                 loss: nan
agent1:                 episode reward: 0.1263,                 loss: 0.1557
Episode: 100321/101000 (99.3277%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4596s / 19197.3385 s
agent0:                 episode reward: -0.1549,                 loss: nan
agent1:                 episode reward: 0.1549,                 loss: 0.1572
Episode: 100341/101000 (99.3475%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8641s / 19203.2026 s
agent0:                 episode reward: -0.3406,                 loss: nan
agent1:                 episode reward: 0.3406,                 loss: 0.1563
Episode: 100361/101000 (99.3673%),                 avg. length: 2.0,                last time consumption/overall running time: 4.9438s / 19208.1464 s
agent0:                 episode reward: -0.3932,                 loss: nan
agent1:                 episode reward: 0.3932,                 loss: 0.1563
Episode: 100381/101000 (99.3871%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6202s / 19213.7666 s
agent0:                 episode reward: -0.1026,                 loss: nan
agent1:                 episode reward: 0.1026,                 loss: 0.1614
Episode: 100401/101000 (99.4069%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8528s / 19219.6194 s
agent0:                 episode reward: 0.4113,                 loss: nan
agent1:                 episode reward: -0.4113,                 loss: 0.1609
Episode: 100421/101000 (99.4267%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4453s / 19225.0647 s
agent0:                 episode reward: -0.6484,                 loss: nan
agent1:                 episode reward: 0.6484,                 loss: 0.1605
Episode: 100441/101000 (99.4465%),                 avg. length: 2.0,                last time consumption/overall running time: 5.4787s / 19230.5434 s
agent0:                 episode reward: 0.1214,                 loss: nan
agent1:                 episode reward: -0.1214,                 loss: 0.1590
Episode: 100461/101000 (99.4663%),                 avg. length: 2.0,                last time consumption/overall running time: 4.8200s / 19235.3634 s
agent0:                 episode reward: -0.2675,                 loss: nan
agent1:                 episode reward: 0.2675,                 loss: 0.1612
Episode: 100481/101000 (99.4861%),                 avg. length: 2.0,                last time consumption/overall running time: 5.8117s / 19241.1751 s
agent0:                 episode reward: -0.2881,                 loss: nan
agent1:                 episode reward: 0.2881,                 loss: 0.1591
Episode: 100501/101000 (99.5059%),                 avg. length: 2.0,                last time consumption/overall running time: 4.3575s / 19245.5327 s
agent0:                 episode reward: -0.3356,                 loss: nan
agent1:                 episode reward: 0.3356,                 loss: 0.1613
Episode: 100521/101000 (99.5257%),                 avg. length: 2.0,                last time consumption/overall running time: 4.0001s / 19249.5328 s
agent0:                 episode reward: 0.0414,                 loss: nan
agent1:                 episode reward: -0.0414,                 loss: 0.1616
Episode: 100541/101000 (99.5455%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0866s / 19254.6194 s
agent0:                 episode reward: -0.3570,                 loss: nan
agent1:                 episode reward: 0.3570,                 loss: 0.1608
Episode: 100561/101000 (99.5653%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7853s / 19260.4047 s
agent0:                 episode reward: 0.0955,                 loss: nan
agent1:                 episode reward: -0.0955,                 loss: 0.1614
Episode: 100581/101000 (99.5851%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3671s / 19265.7718 s
agent0:                 episode reward: -0.3261,                 loss: nan
agent1:                 episode reward: 0.3261,                 loss: 0.1586
Episode: 100601/101000 (99.6050%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0006s / 19270.7724 s/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr = np.asanyarray(arr)

agent0:                 episode reward: -0.1830,                 loss: nan
agent1:                 episode reward: 0.1830,                 loss: 0.1592
Episode: 100621/101000 (99.6248%),                 avg. length: 2.0,                last time consumption/overall running time: 6.4772s / 19277.2496 s
agent0:                 episode reward: -0.2019,                 loss: nan
agent1:                 episode reward: 0.2019,                 loss: 0.1625
Episode: 100641/101000 (99.6446%),                 avg. length: 2.0,                last time consumption/overall running time: 6.7473s / 19283.9969 s
agent0:                 episode reward: -0.4080,                 loss: 0.2273
agent1:                 episode reward: 0.4080,                 loss: 0.1586
Score delta: 1.71877183110758, save the model to .//data/model/20220117155229/mdp_arbitrary_mdp_fictitious_selfplay2/100208_1.
Episode: 100661/101000 (99.6644%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6912s / 19289.6881 s
agent0:                 episode reward: -0.4660,                 loss: 0.2171
agent1:                 episode reward: 0.4660,                 loss: nan
Episode: 100681/101000 (99.6842%),                 avg. length: 2.0,                last time consumption/overall running time: 4.2719s / 19293.9601 s
agent0:                 episode reward: -0.2574,                 loss: 0.2174
agent1:                 episode reward: 0.2574,                 loss: nan
Episode: 100701/101000 (99.7040%),                 avg. length: 2.0,                last time consumption/overall running time: 5.7725s / 19299.7326 s
agent0:                 episode reward: -0.1587,                 loss: 0.2155
agent1:                 episode reward: 0.1587,                 loss: nan
Episode: 100721/101000 (99.7238%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6968s / 19305.4294 s
agent0:                 episode reward: 0.0307,                 loss: 0.2040
agent1:                 episode reward: -0.0307,                 loss: nan
Episode: 100741/101000 (99.7436%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0884s / 19310.5178 s
agent0:                 episode reward: 0.2083,                 loss: 0.2012
agent1:                 episode reward: -0.2083,                 loss: nan
Episode: 100761/101000 (99.7634%),                 avg. length: 2.0,                last time consumption/overall running time: 6.5620s / 19317.0798 s
agent0:                 episode reward: 0.0769,                 loss: 0.2026
agent1:                 episode reward: -0.0769,                 loss: nan
Episode: 100781/101000 (99.7832%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0280s / 19323.1078 s
agent0:                 episode reward: 0.0630,                 loss: 0.2035
agent1:                 episode reward: -0.0630,                 loss: nan
Episode: 100801/101000 (99.8030%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5874s / 19328.6953 s
agent0:                 episode reward: -0.0864,                 loss: 0.2021
agent1:                 episode reward: 0.0864,                 loss: nan
Episode: 100821/101000 (99.8228%),                 avg. length: 2.0,                last time consumption/overall running time: 5.5609s / 19334.2562 s
agent0:                 episode reward: -0.2409,                 loss: 0.1992
agent1:                 episode reward: 0.2409,                 loss: nan
Episode: 100841/101000 (99.8426%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3997s / 19339.6558 s
agent0:                 episode reward: -0.2853,                 loss: 0.2025
agent1:                 episode reward: 0.2853,                 loss: nan
Episode: 100861/101000 (99.8624%),                 avg. length: 2.0,                last time consumption/overall running time: 5.3368s / 19344.9926 s
agent0:                 episode reward: -0.5474,                 loss: 0.2014
agent1:                 episode reward: 0.5474,                 loss: nan
Episode: 100881/101000 (99.8822%),                 avg. length: 2.0,                last time consumption/overall running time: 5.6780s / 19350.6706 s
agent0:                 episode reward: 0.1285,                 loss: 0.2014
agent1:                 episode reward: -0.1285,                 loss: nan
Episode: 100901/101000 (99.9020%),                 avg. length: 2.0,                last time consumption/overall running time: 5.0385s / 19355.7091 s
agent0:                 episode reward: 0.1437,                 loss: 0.2004
agent1:                 episode reward: -0.1437,                 loss: nan
Episode: 100921/101000 (99.9218%),                 avg. length: 2.0,                last time consumption/overall running time: 5.1576s / 19360.8667 s
agent0:                 episode reward: 0.0101,                 loss: 0.2009
agent1:                 episode reward: -0.0101,                 loss: nan
Episode: 100941/101000 (99.9416%),                 avg. length: 2.0,                last time consumption/overall running time: 5.2951s / 19366.1618 s
agent0:                 episode reward: -0.0083,                 loss: 0.2025
agent1:                 episode reward: 0.0083,                 loss: nan
Episode: 100961/101000 (99.9614%),                 avg. length: 2.0,                last time consumption/overall running time: 6.1114s / 19372.2732 s
agent0:                 episode reward: 0.0165,                 loss: 0.2012
agent1:                 episode reward: -0.0165,                 loss: nan
Episode: 100981/101000 (99.9812%),                 avg. length: 2.0,                last time consumption/overall running time: 6.0156s / 19378.2888 s
agent0:                 episode reward: -0.0425,                 loss: 0.2009
agent1:                 episode reward: 0.0425,                 loss: nan
